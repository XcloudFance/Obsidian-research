# 视频分析库文档

## 概述

这个库提供了使用大语言模型（LLMs）分析视频内容的功能。它可以提取实体、检测人物、分析他们在视频中的出现情况。该库设计灵活，可以通过通用接口与不同的大语言模型实现配合使用。

## 主要功能

- 从视频中提取实体
- 人物检测和跟踪
- 时间段分析
- 视频帧提取和边界框可视化
- 灵活的大语言模型集成

## 安装要求

库依赖以下组件：
- OpenCV (cv2)
- PIL (Python Imaging Library)
- 任何兼容的语言模型实现

## 快速开始

```python
from video_analysis_lib import VideoAnalyzer, LanguageModelInterface

# 初始化你的语言模型实现
model = YourLanguageModel()

# 创建分析器实例
analyzer = VideoAnalyzer(model, "path/to/video.mp4")

# 提取实体
entities = analyzer.extract_entities()

# 分析人物
if entities.get("PERSON"):
    detections = analyzer.analyze_persons(entities["PERSON"])
```

## 核心组件

### VideoAnalyzer

视频分析操作的主类。

#### 构造函数

```python
VideoAnalyzer(model: LanguageModelInterface, video_path: str, output_dir: str = "boundingbox_output")
```

参数：
- `model`：LanguageModelInterface 的实现
- `video_path`：视频文件路径
- `output_dir`：输出文件保存目录（默认："boundingbox_output"）

#### 方法

##### extract_entities()
从视频内容中提取各种实体。

返回值：
- `Dict[str, List[str]]`：按类型分类的实体字典：
  - DATE：日期或年份
  - ORG：组织机构
  - PERSON：人物或角色
  - FIELD：研究领域或行业
  - LOC：位置
  - OTHER：其他重要术语

##### analyze_persons(person_names: List[str])
分析指定人物在视频中的出现情况。

参数：
- `person_names`：要分析的人物名称列表

返回值：
- `Dict[str, PersonDetection]`：包含每个人物检测结果的字典

### LanguageModelInterface

必须实现的抽象基类，用于提供语言模型功能。

#### 必需方法

##### upload_file(path: str, mime_type: str) -> Any
将文件上传到语言模型服务。

参数：
- `path`：文件路径
- `mime_type`：文件的 MIME 类型

返回值：
- 特定于语言模型实现的文件对象

##### check_file_active(filename: str) -> Optional[Any]
检查文件在语言模型服务中是否处于活动状态。

参数：
- `filename`：要检查的文件名

返回值：
- 如果找到并且处于活动状态则返回文件对象，否则返回 None

##### call_model(prompt: str, file_data: Optional[Dict[str, Any]] = None) -> str
调用语言模型。

参数：
- `prompt`：要发送的提示文本
- `file_data`：可选的文件数据

返回值：
- 模型响应的字符串

## 辅助函数

### extract_frame(video_path: str, timestamp: int, output_dir: str) -> str
在指定时间戳从视频中提取帧。

参数：
- `video_path`：视频文件路径
- `timestamp`：时间（秒）
- `output_dir`：提取帧的保存目录

返回值：
- 保存的帧图像路径

### frame_to_image_data(frame_path: str) -> Dict[str, Any]
将帧图像转换为与大语言模型 API 兼容的格式。

参数：
- `frame_path`：帧图像路径

返回值：
- 包含图像数据和 MIME 类型的字典

### draw_bounding_box(frame_path: str, bounding_box: List[float], output_path: str) -> None
在帧图像上绘制边界框。

参数：
- `frame_path`：帧图像路径
- `bounding_box`：坐标 [ymin, xmin, ymax, xmax]
- `output_path`：注释图像的保存路径

### log_error(error: Exception, context: str = "") -> None
记录带有上下文信息的错误详情。

参数：
- `error`：异常对象
- `context`：可选的上下文字符串

## 数据类型

### TimeSegment（时间段）
```python
class TimeSegment(TypedDict):
    start_time: float  # 开始时间
    end_time: float    # 结束时间
```

### PersonSegments（人物片段）
```python
class PersonSegments(TypedDict):
    segments: List[TimeSegment]  # 时间段列表
```

### BoundingBoxCoords（边界框坐标）
```python
class BoundingBoxCoords(TypedDict):
    coordinates: List[float]  # [ymin, xmin, ymax, xmax]
```

### PersonDetection（人物检测）
```python
class PersonDetection(TypedDict):
    time_points: List[int]              # 时间点列表
    bounding_boxes: List[List[float]]   # 边界框列表
    frame_paths: List[str]              # 帧路径列表
    segments: List[TimeSegment]         # 时间段列表
```

### VideoAnalysisResult（视频分析结果）
```python
class VideoAnalysisResult(TypedDict):
    video_path: str                     # 视频路径
    timestamp: str                      # 时间戳
    entities: Dict[str, List[str]]      # 实体字典
    detections: Dict[str, PersonDetection]  # 检测结果字典
```

## 示例实现

仓库包含了使用 Gemini 模型的示例实现：

```python
class GeminiModel(LanguageModelInterface):
    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.min_delay = 65  # API调用最小延迟（秒）
        self.last_call_time = 0
        self.history = []    # 保存对话历史
```

完整的 Gemini 模型实现示例请参见 `paste-2.txt`。

## 注意事项

- 库实现了 API 调用的速率限制和重试逻辑
- 结果以 JSON 格式保存以便进一步处理
- 提取的帧和边界框可视化保存在输出目录中
- 库处理各种错误情况并提供详细的日志记录
- 所有的实体提取都会确保不包含影片工作人员信息
- 实体名称只保留一种语言版本（优先选择原始语言）