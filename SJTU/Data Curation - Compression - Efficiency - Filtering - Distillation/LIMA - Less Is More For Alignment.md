
1000 pairs on LLama
The model tends to generalize well to unseen tasks that did not appear in the training data.

Source: community forums: Stack Exchange, wikiHow..

Sampling for Diversity and Quality

---
![[Pasted image 20241011094519.png]]

---
Hypothesis:
**A modelâ€™s knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users**


![[Pasted image 20241011095022.png]]
