{"path":"CVPR/16966_DataS3_Dataset_Subselect.pdf","text":"CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. DataS3: Dataset Subset Selection for Specialization Anonymous CVPR submission Paper ID ***** Abstract In many real-world machine learning (ML) applications (e.g.001 detecting broken bones in x-ray images, detecting species002 in camera traps), in practice models need to perform well003 on specific deployments (e.g. a specific hospital, a specific004 national park) rather than the domain broadly. However,005 deployments often have imbalanced, unique data distribu-006 tions. Discrepancy between the training distribution and the007 deployment distribution can lead to suboptimal performance,008 highlighting the need to select deployment-specialized sub-009 sets from the available training data. We formalize dataset010 subset selection for specialization (DS3): given a training011 set drawn from a general distribution and a (potentially unla-012 beled) query set drawn from the desired deployment-specific013 distribution, the goal is to select a subset of the training data014 that optimizes deployment performance.015 We introduce DA T AS3; the first dataset and benchmark016 designed specifically for the DS3 problem. DA T AS3 en-017 compasses diverse real-world application domains, each018 with a set of distinct deployments to specialize in. We con-019 duct a comprehensive study evaluating algorithms from var-020 ious families—including coresets, data filtering, and data021 curation—on DA T AS 3, and find that general-distribution022 methods consistently fail on deployment-specific tasks. Addi-023 tionally, we demonstrate the existence of manually curated024 (deployment-specific) expert subsets that outperform train-025 ing on all available data by up to 51.3%. Our benchmark026 highlights the critical role of tailored dataset curation in en-027 hancing performance and training efficiency on deployment-028 specific distributions, which we posit will only become more029 important as global, public datasets become available across030 domains and ML models are deployed in the real world.031 1. Background and Motivation032 Machine learning models are typically trained on large033 datasets with the assumption that the training distribution034 closely matches the distribution of the deployment where the035 model will be applied. However, in real-world applications,036 deployment data distributions often diverge from general037 and/or global training set distributions [SRC24, TDS +20].038 Selecting relevant data subsets aligned with specific deploy- 039 ments is crucial for maximizing in-field performance. The 040 problem of data subset selection for specialization (DS3) is 041 thus critical: given all available training data for a domain 042 and a (small, usually unlabeled) query set that represents the 043 desired deployment, the goal is to identify a subset of the 044 training data, such that training the ML model on this subset 045 maximises performance on the deployment distribution. 046 Real world example. Consider a wildlife ecologist who 047 aims to build a classifier to detect the presence of invasive ro- 048 dents in camera trap images collected at the Channel Islands. 049 Existing data on invasive rodents in this context is limited, 050 as they have been mostly eradicated by previous successful 051 conservation action, thus training a classifier from scratch is 052 likely to be unsuccessful. Oftentimes when faced with such 053 challenges, a common approach has been to use a general 054 pre-trained model (such as ViT or CLIP) and then finetune 055 on all relevant camera trap data. But what does \"relevant 056 data\" mean? Would using similar species data from other 057 camera trap locations (perhaps on the mainland) improve 058 performance, or introduce noise? What about including data 059 from non-similar species at that location? While adding 060 data to a training set can sometimes improve performance, 061 it can also decrease individual subgroup performance in a 062 biased way [CZPR23] and introduce spurious correlations 063 that can enable models to learn potentially dangerous “short- 064 cuts,” resulting in biased predictions, shown across various 065 deployments [GJM +20, BZOR +18, WLL+21, BWE+22a]. 066 The Gap: General datasets vs. domain-specific needs. To 067 address the gap between general models and specific deploy- 068 ment needs, we highlight the need for research emphasis on 069 DS3: the development of methods that select optimal train- 070 ing data for deployment-specific model specialization. Cur- 071 rently, subset selection methods are evaluated on standard 072 CIFAR10/100 [KH+09] and ImageNet [DDS +09] datasets, 073 where test and validation sets have similar distribution to 074 their training sets. Current benchmarks for data filtering 075 [GIF+24] focus on generalization across many tasks, in con- 076 trast to specialization for a particular deployment. While 077 these works are valuable, they do not capture, and thus en- 078 able progress on, the DS3 challenge. 079 1 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. Figure 1. Foundation model training aims for broad generalization, by using all data available, usually from massive internet-scale datasets. In practice, we find these models are often suboptimal for specific deployments, which may exhibit different distributions over categories or data charateristics from the general training data pool. Dataset subset selection for specialization seeks to identify model training subsets closely aligned with the target deployment, achieving superior performance under the given distribution and attribute shifts. Our contributions. We propose DA T AS 3; a comprehensive080 benchmark to directly evaluate and compare deployment-081 specialization subset selection methods. Our key contribu-082 tions are the following:083 (i) DA T AS3: A DS3 benchmark of four datasets for eval-084 uating algorithms on the DS3 problem. The datasets085 represent real-world scientific and engineering appli-086 cations from different fields, including remote sensing087 for classifying religious buildings, camera traps to088 classify species, microscopy images to classify cell089 organelles, and vehicle footage to learn driving con-090 trollers. Each dataset includes multiple realistic de-091 ployments, e.g., the camera traps dataset aims to cate-092 gorize species, and the deployments to specialize to093 are geographic locations where scientists want to ana-094 lyze species, e.g., Central Africa or Southeast Asia.095 (ii) Manually curated expert subsets of the training data096 for each deployment showing that selecting a well-097 curated subset can consistently outperform models098 trained on the entire dataset.099 (iii) An extensive experimental study comparing current100 SOTA subset selection methods across the provided101 data pools and their respective deployments. After102 training a suite of baselines, our results clearly show103 that current subset selection methods fail on DS3,104 highlighting the need for our DA T AS 3 benchmark.105 2. Problem Statement106 DS3 problem formulation. Let X be a ground set of data107 points, T ⊂ X be a given training set drawn from a training108 (pool) distribution PT over X, and let Q ⊂ X be a query set109 drawn from the desired deployment-specific distribution110 PQ over X. Given a model θ, the objective of dataset111 subset selection for specialization (DS3), is to design an112 algorithm SubsetSelection-ALG, which takes T (the113 training set) and Q (the deployment representative query set)114 as input, and outputs a subset S∗ ⊂ T that minimizes the 115 expected loss of θ trained on S∗ over the desired deployment- 116 specific distribution PQ. More formally: 117 S∗ = arg min S⊂T Eq∼PQ [L(θ(S), q)] , (1) 118 where θ(S) denotes the model trained on the subset 119 S ⊂ T , and L(θ(S), q) is the loss function evaluated on a 120 single point q sampled from PQ and the trained model θ(S). 121 The term Eq∼PQ denotes the expected value over the distribu- 122 tion PQ. Hence, the algorithm SubsetSelection-ALG 123 outputs S∗, the subset of T that minimizes the expected 124 loss of the entire desired deployment distribution PQ. No- 125 tably, SubsetSelection-ALG can only access the de- 126 sired deployment-specific distribution via the query set Q. 127 Is the query set annotated/labeled? This formalization can 128 be divided into two cases: in the first, the query set Q is anno- 129 tated with a set of labels, formally, Q is a set of m > 0 pairs 130 Q = {(q1, y1), · · · , (qm, ym)}, where for every i ∈ [m], qi 131 is the ith feature vector describing the ith input, and yi is it 132 corresponding label/annotation. In this case the algorithm 133 SubsetSelection-ALG has access to the set of labels 134 {y1, · · · , yn}. In the second scenario, no labels are pro- 135 vided for Q, meaning that the SubsetSelection-ALG 136 does not have access to the set {y1, · · · , yn} and conse- 137 quently Q = {q1, · · · , qm}. Annotating Q for any specific 138 deployment requires time, money, and expertise. Thus, DS3 139 progress without labels has a high potential for impact. 140 Is SubsetSelection-ALG model agnostic? Similarly, 141 this formalization can be approached in two different ways: 142 one where the computation of S∗ depends on a given specific 143 model θ, i.e., SubsetSelection-ALG is model depen- 144 dent, and has access to the model θ we wish to train on. The 145 more general, model-agnostic formulation aims to find S∗ 146 that performs well across all possible models, meaning that 147 SubsetSelection-ALG has no access to θ. 148 2 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. 3. Related Work149 Traditional data subset selection approaches can be split150 into two main categories: 1) Data filtering or cleaning,151 which focuses on refining the dataset to enhance its qual-152 ity [ZRG+22, RSR +20], and 2) Coresets for dataset sub-153 set selection, aimed at reducing training time by a comput-154 ing a subset that effectively represents the larger training155 dataset [KSRI21, TZM +23].156 Data filtering for better learning. Data pruning is157 widely used in NLP to clean noisy datasets [Ano23], of-158 ten employing filtering and heuristics [BUSZ22]. Com-159 mon methods include excluding texts with blocklisted160 words [RSR +20], removing duplicates [ZRG +22], filtering161 out non-English texts [RSR +20, RBC +22], and discarding162 short sentences [RSR +20, RBC +22]. Perplexity-based fil-163 tering removes high-perplexity samples considered unnat-164 ural and detrimental to performance [MRB+23, WLC+20,165 LSW +23]. Although simple filtering can enhance language166 models [PMH+23, RSR +20], their effectiveness varies, and167 some studies report no benefits [BBH +22, BSA +23], possi-168 bly due to their simplicity. [ZLX +24] showed that manually169 selecting a small subset satisfying quality and diversity im-170 proves alignment performance. For vision tasks, a smaller171 number of methods have been suggested for data filter-172 ing [SGS+23] to obtain better trainable subsets [SRM +22]173 through the use of model signals [MBR +22].174 Coresets for efficient learning. Subset selection (hitherto175 referred to as coresets) is common for vision tasks. The goal176 is to compute a small subset from the training dataset, that177 approximates training on the full dataset, thus boosting the178 training process [BFL16, MEM+22]. Coresets proved to be179 useful in many applications such as regression [DDH+08,180 CDS20, TJF22, MMM +22, MJF19], clustering [HM04,181 Che09, HV20, JTMF20, CAGLS+22], low-rank approxi-182 mation [CMM17, BDM +20, MJTF21], support vector ma-183 chines (SVMs) [Cla10, TBFR21, MEM +22], and for com-184 pressing neural networks [BLG +22, LBL+19, TMM22].185 For boosting the training of neural networks, [CYM +19]186 used proxy functions to select subsets of training data ap-187 proximating the training process. Later [MBL20, MCL20]188 developed algorithms to estimate the full gradient of the deep189 neural network on the training data. These techniques were190 further refined by [KSR+21, KSRI21, PGD21, WPM +20].191 Other methods require a neural network forward pass to192 get embeddings [SS18, SGS +22, KZCI21]. Notably, these193 methods rely on the properties of the models in training194 to select data. Later, [TZM +23] provided a method that195 does not require access to the trained model, but demands196 assumptions on the model and its complexity.197 All these methods assume the training data well repre-198 sents the test (deployment) data, as the case in known diverse,199 high-quality vision benchmarks (CIFAR10 and ImageNet).200 Thus, the aim was to approximate the training data via a sub-201 set (coresets) or enhance training (filtering) assuming that 202 that the training and testing sets share the same distribution. 203 Active learning. There is a rich area of online active learn- 204 ing literature, which continually filters data while train- 205 ing [EDHG +20, WLY22, YLBG20, TND +22], requiring 206 to query an annotator for more labeled data and oftentimes, 207 rely on properties of the models in-training to select data. 208 Here, we are interested in exploring data subselection prior 209 to training and without knowledge of model weights. 210 Benchmarks. The works most related to ours are [GIF+24], 211 [MBY+23] and [FXC +24]. DataComp [GIF+24] intro- 212 duces a benchmark where the main challenge is to select 213 the optimal data subset for pretraining generalization. It 214 evaluates various data curation strategies using standardized 215 CLIP training code, followed by zero-shot assessments on 38 216 downstream datasets. [MBY +23] has multiple benchmarks 217 across domain-specific data sources, but is again aimed for 218 generalization rather than specialization. [FXC +24] focuses 219 on image-only models, which are smaller and easier to train 220 to high accuracy. 221 Our benchmark. In contrast to these benchmarks, DA T AS 3 222 is specifically designed to evaluate subset selection methods 223 for deployment-specific specialization, rather than general- 224 ization, where the training and testing (deployment) data 225 exhibit distributional shifts. 226 4. The DA T AS 3 Benchmark 227 Benchmark design. Traditional dataset subset selection 228 methods often aim to build a maximally generalizable model 229 with the least amount of training data. In contrast, The goal 230 of our benchmark is to identify the optimal subset for a spe- 231 cific deployment. Our benchmark includes four application- 232 domain datasets, and defines multiple distinct deployments 233 for each. Given a small query set Q from a deployment, 234 the objective is to select a subset of the training data that 235 achieves optimal performance on the given deployment of 236 Q. Subset selection methods are evaluated on four held-out 237 test sets, each corresponding to a specific deployment. 238 Datasets. Our benchmark includes four datasets, each cap- 239 turing a unique and diverse application of ML: Auto Arborist 240 for street-level tree classification [BWE+22b], iWildCam 241 for camera trap species identification [BACB21], GeoDE 242 for diverse object classification [RLZ +23], and NuScenes 243 for driving footage steering regression (autonomous- 244 driving) [CBL +20]. Each of these datasets inherently repre- 245 sents many of the real-world challenges that make dataset 246 subset selection a deployment-specific problem, including 247 covariate shifts, subpopulation shifts, and long-tailed distri- 248 butions. For each dataset, we provide an expert-knowledge- 249 guided subset that demonstrates the usefulness of dataset 250 subselection, with improvement over using all the training 251 data. These subsets were created using expert-driven knowl- 252 edge with access to information that benchmark users are 253 3 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. Figure 2. The iWildCam dataset is long-tailed, with each plots of the class counts showing significant label distribution shift per deployment. Additionally, there are major axes of variation be- tween deployments in images, ranging from background, species of interest, night/day captures, camera type, and more. not provided (e.g. metadata, GPS location, region, etc). In254 what follows, we discuss each dataset and its corresponding255 deployments. Additional details about each dataset can be256 found in Appendix A.257 4.1. iWildCam258 Background: Animal populations have declined by 68%259 on average since 1970 [Sta20]. To monitor this biodiver-260 sity loss, ecologists deploy camera traps—motion-activated261 cameras placed in the wild [WGK17]—and process the data262 with machine learning models [NMB+19, BMY19]. How-263 ever, variations in illumination, camera angle, background,264 vegetation, color, and animal frequencies across different265 locations cause these models to generalize poorly to new266 deployments. To specialize models for specific locations,267 selecting appropriate data subsets for deployment-specific268 (in this case location) specialization becomes essential.269 Problem Setting: To study this problem, we use the iWild-270 Cam 2020 dataset. The task is multi-class species classifica-271 tion. Concretely, the input x is a photo taken by a camera trap,272 the label y is one of 182 different animal species, and the273 deployment d is an integer that identifies the camera trap that274 took the photo. Performance is measured by classification275 overall accuracy for species identification.276 Data: The dataset comprises 203, 029 images from 323277 different camera traps spread across multiple countries in278 different parts of the world. The original camera trap data279 comes from the Wildlife Conservation Society (link). These280 images tend to be taken in short bursts following the motion-281 activation of a camera trap, so the images can be addition-282 ally grouped into sequences of images from the same burst,283 Figure 3. The GeoDE dataset has both covariate and distribution shift present. Deployments 1 and 2 have strong covariate shift, with the data being collected from specific countries, whereas de- ployments 3 and 4 have strong label shift, focusing on specific categories of objects. The training pool contains data from all coun- tries and object types, meaning that subset selection is uniquely helpful for specialization. though our baseline models do not exploit this information, 284 and our evaluation metric treats each image individually. 285 However, a grouped sequence is in the same split of the 286 data (train, test, query) in order to avoid model memoriza- 287 tion. Each image is associated with the following metadata: 288 camera trap ID, sequence ID, and datetime. 289 Deployments: Our deployments were defined to be split 290 across camera trap locations to simulate the common sce- 291 nario of researchers setting up new cameras within a re- 292 gion, with poor model generalization on the new cameras 293 [WGK17]. Our train/test split was done randomly across the 294 200 locations, with the four downstream test tasks created 295 by clustered by the latitude and longitude of camera GPS 296 location in 4 deployments: (1) Central America, (2) Eastern 297 Africa, (3) Southern Africa, and (4) Southeast Asia. Similar 298 to most other camera trap datasets, iWildCam has significant 299 long-tailed label distributions, with variation in species and 300 backgrounds between locations, as can be seen in Figure 2. 301 Expert Knowledge Subset: Expert subsets were generated 302 by selecting the camera locations within the training pool 303 within the location \"cluster\" as the deployment (eg. the 304 relevant geographic area) and then sampling the training data 305 to closely match the species distributions in the deployment. 306 307 4.2. GeoDE 308 Motivation. Object classification datasets are often con- 309 structed by scraping images from the web but contain geo- 310 graphical biases [SHB +17]. Instead of scraping images from 311 the web, GeoDE [RLZ +23] crowdsources a dataset that is 312 roughly balanced across 40 different objects and six world 313 regions, showing that common objects (stoves, bicycles, etc), 314 4 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. Figure 4. The Auto Arborist dataset is long-tailed, with each deployment having significant label distribution shift. Additionally, there are major axes of variation between deployments in images, coming from factors described in Sec. 4.3 vary in appearance across the world. Accordingly, special-315 izing models to different regions becomes useful when the316 objects have strong covariate shift.317 Problem setting. GeoDE is a diverse dataset comprising318 40 different objects collected from 6 world regions. The319 associated task is multiclass classification, where the goal is320 to predict the object depicted in each image.321 Data. GeoDE contains 61,490 images, each labelled with322 both a region (Africa, Americas, East Asia, Europe, South-323 east Asia, West Asia), and an object (examples: bag, back-324 yard, toothpaste, etc.). The dataset was crowdsourced, with325 participants submitting photographs for each object, which326 were then assessed for quality.327 Deployments: We propose 4 different deployments: (1)328 objects in Indonesia, (2) objects in Nigeria, (3) indoor ob-329 jects, and (4) outdoor objects, as shown in Figure 3. Nigeria330 and Indonesia were selected as the two countries with the331 poorest performance, and the indoor/outdoor deployment332 tasks were selected for enabling model specialization. The333 training dataset includes images from all countries, and the334 test data contains only images from Nigeria and Indonesia.335 Expert Knowledge Subset: Expert subsets were generated336 by selecting data from the relevant countries/categories in337 the training data.338 4.3. Auto Arborist339 Motivation: Ecological imagery for environmental moni-340 toring and Earth observation provides policymakers with341 critical, data-driven insights to support climate adapta-342 tion [BLF+16]. Automated tree classification, for instance,343 offers substantial benefits for humanitarian aid, disaster re-344 lief, forestry, agriculture, and urban planning, supporting345 applications in city planning, resource management, and346 environmental monitoring.347 Problem Setting: Automated tree classification in street- 348 level imagery is inherently difficult and is associated with 349 fundamental challenges such as: 350 • Noisy labels. Images are commonly mislabeled: genus 351 classification is difficult and requires specialized expertise, 352 GPS localization from the ground can be in error, there 353 are often multiple trees within a single image with only 354 a single label, and temporal inconsistencies can occur as 355 trees are not imaged and labeled at the same time. 356 • Non-IID data. Geospatial data also breaks the typical 357 deep learning assumption that data will be independent 358 and identically distributed (IID) spatially close examples 359 often contain correlations. For example, trees are often 360 planted in groups (e.g. a row of cherry trees along the 361 same street). 362 • Fine-grained and long-tailed class distribution. Tree 363 classification is fine-grained, with only subtle differences 364 between many genera, and the distribution of trees is long- 365 tailed. These characteristics tend to skew classification 366 models towards predicting predominant classes. 367 • Geospatial distribution shift. Finally, this dataset con- 368 tains significant covariate and subpopulation distribution 369 shift due to variations in weather, differences in urban 370 planning specific to each city, and temporal changes at 371 different locations. 372 Data: The Auto Arborist dataset is a multi-view, fine- 373 grained visual tree categorization dataset containing images 374 of over 1 million public zone trees from 300 genus-level 375 categories across 23 major cities in the US and Canada (We 376 note that the dataset represents only a portion of the total 377 tree population). Specifically, each tree record in the dataset 378 is associated with a street-level and aerial image. For our 379 benchmark, we focus on the street-level images. 380 Deployments: Deployments in Auto Arborist correspond 381 to the development models for use by individual cities. The 382 deployment cities of (1) Surrey with 66 distinct tree genus 383 classes, (2) Calgary with 30 classes, (3) Los Angeles with 384 175 classes, and (4) Washington DC with 67 classes were 385 chosen due to their diverse climates, species distributions, 386 and urban structures, as seen in Figure 4. Historical develop- 387 ment patterns also significantly change from city to city with 388 many trees being planted intentionally through city planning 389 not by random chance, causing city-specific signals for tree 390 genera. Moreover, the number of classes and distribution 391 of classes (long-tailedness) also varies significantly. City- 392 specific models allow us to capture the unique features of 393 each city’s tree population, optimizing the model to perform 394 best in the environment it will be deployed. As cities increas- 395 ingly rely on data-driven methods for urban planning and 396 environmental monitoring, having tailored models ensures 397 higher accuracy and utility, especially for cities with limited 398 resources for ground surveys. 399 5 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. Expert Knowledge Subset Expert-driven subsets were gen-400 erated by selecting data from the nearest cities in the training401 pool, and then sampling the training data to closely match402 the tree genus/class distributions in the deployment set.403 4.4. NuScenes404 Motivation: End-to-end autonomous driving systems405 streamline vehicle control by directly mapping sensory in-406 puts, such as images, to control outputs like steering an-407 gles [WMX +24]. This approach eliminates traditional,408 multi-step processing pipelines, enabling real-time adap-409 tation to complex environments. By integrating perception410 and control, these systems enhance efficiency, responsive-411 ness, and adaptability, crucial for safe autonomous naviga-412 tion. The idea is that adapting these systems to specialize413 in particular streets or environments is made easier as a sin-414 gle model encompasses the full system. Thus, training this415 model to specialize in a specific environment brings advan-416 tages, capturing detailed local road layouts, typical traffic417 patterns, area-specific obstacles, and more.418 Problem Setting: We explore vision-based control for self-419 driving across diverse environments (e.g., different city ar-420 eas) and driving scenarios (e.g., pedestrians crossing, con-421 struction zones), formulated as a regression task. The422 model’s goal is to predict a single scalar value represent-423 ing the car’s steering angle. Performance is evaluated in an424 open-loop manner using metrics like mean squared error.425 Data: This dataset includes 88, 461 images from the426 NuScenes dataset, subsampled from the image sweeps at427 a rate of 2. The images were captured from a video stream428 recorded while driving a car. Each image is paired with429 a steering angle control from the CAN bus, synchronized430 with the sensor timestamps of both the camera and CAN bus431 data. To label each image with the correct steering angle, we432 apply 1D interpolation to create a continuous function of the433 steering angle and query it based on the camera’s timestamp.434 The steering angle, measured in radians, ranges from -7.7 to435 6.3, with 0 indicating straight driving, positive values indicat-436 ing left turns, and negative values indicating right turns. To437 ensure alignment between images and steering control data,438 samples with vehicle velocities below 1 m/s are removed.439 Deployments: Deployments are organized by the geo-440 graphic locations where the data was collected, including (1)441 Boston Seaport, (2) Singapore Holland Village, (3) Singa-442 pore One-North, and (4) Singapore Queenstown. While all443 tasks are based on expert demonstrations of driving and gen-444 eral driving behaviors, each location presents varying envi-445 ronmental features—such as vegetation, road types, roadside446 infrastructure, and weather—as well as differences in driv-447 ing style and road regulations. Train/test splits are randomly448 sampled within each deployment.449 Expert knowledge subset: Expert subsets were generated450 by selecting data from the relevant areas in the training data.451 Figure 5. The NuScenes dataset. The driving area influences envi- ronmental features, which, in turn, impact the control outputs. In Boston’s Seaport, trucks are common on the roads, unlike in Queen- stown or Holland Village in Singapore, where trucks are less preva- lent. Similarly, the number of vehicles requiring the car’s attention varies by area. Speed control outputs may be higher in Queenstown and One-North, Singapore, due to the nature of the streets, com- pared to Boston’s Seaport or Holland Village, where speeds tend to be lower. Steering angles are also affected by location-specific road layouts; for instance, Queenstown features more frequent sharp right curves, which are less common in other areas. 4.5. Benchmark Pipeline 452 Within our benchmark, each dataset has a two-step process 453 for evaluation: (i) Given a small query set representing the 454 deployment data, curate a subset of data from the training 455 for a specific deployment. (ii) Finetune/train a fixed model 456 on the chosen subset from the training pool and evaluate 457 on the deployment (test) set. For each dataset, we fix the 458 training procedure for all subsets of data, fixing model ar- 459 chitecture, optimizers, and loss functions. We run a small 460 hyperparameter sweep for each training subset across batch 461 sizes {32, 64, 128} and learning rates {0.01, 0.001, 0.0001} 462 for each deployment. For all datasets, we use ResNet50 463 [HZRS15] for full-finetuning and a ViT [DBK +20] for lin- 464 ear probes of the training subsets, chosen for efficiency due 465 to the number of baselines. Full details are in Appendix B. 466 4.6. Metrics 467 Participants are evaluated across 12 deployments from 4 468 datasets, as outlined in Section 4. For the classification task 469 6 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. datasets of GeoDE, Auto Arborist, and iWildCam, we report470 accuracy for each deployment, and for the regression task471 dataste NuScenes, we report mean squared error. For each472 deployment, we evaluate participants of the benchmark on473 overall accuracy and size of the training subset; the less data474 used the better while balancing optimal performance. We475 also report precision and recall in Appendix C.476 5. Baselines477 We compare performance of coreset/data filtering algorithms478 for dataset subselection across our benchmark, across dif-479 ferent scenarios: (a) access to an unlabeled query set, and480 (b) access to a labeled query set. We also curate a third481 category, (c), which leverages domain expertise to generate482 expert-selected subsets, in order to demonstrate the existence483 of better-than-all subsets for these deployments.484 Non-subset comparisons:485 No filtering: Performance of a model trained on the entire486 training pool, without any filtering.487 Query Sets: As a comparison, we also include performance488 of a model trained directly on the labeled query set for each489 deployment. Note that this would require access to query490 labels, which are not always available. When labels are491 available, performance of models trained on the small query492 sets are often poor, hence the value of learning from larger-493 scale general-pool data. As a logistical point, none of the494 baselines we show in our results train on query set data.495 Expert-Driven Subsets: We contribute curated, \"expert-496 driven\" subsets using domain knowledge and/or metadata.497 We find these knowledge-guided subsets often outperform498 using all samples in the training pool (no filtering). The499 creation of these subsets is described in Sec. 4.500 Unlabeled-query baselines:501 Image-alignment (Image-Align): We take the cosine similar-502 ity between the training and query embedding space, using503 examples that exceed a threshold for at least x samples,504 where x is a hyperparameter chosen from {1,10,100}.505 Nearest neighbors features (Near-Nbors): To better align506 our method with the downstream deployment, we explore507 using examples whose embedding space overlaps with the508 query set of data. To do so, we cluster image embeddings509 extracted by an OpenAI ViT model for each image into510 1000 clusters using Faiss [JDJ19]. Then, we find the nearest511 neighbor clusters for every query set example and keep the512 training cluster closest to each query set cluster. This method513 was inspired by the similar DataComp baseline [GIF +24].514 Labeled-query baselines:515 CLIP score filtering (CLIP-score): We also experiment with516 CLIP score filtering, using examples that exceed a threshold517 for cosine similarity between CLIP image and text similarity.518 Text for each image was created with manual captioning (e.g.519 for iWildCam, \"This is a camera trap image of a lion taken520 at time 10-2-2016 at 04:26:13 in Nigeria\"). We select the 521 subset that exceeds a threshold of CLIP-score similarity, with 522 the threshold calculated for subsets that make up 25%, 50%, 523 75%, and 90% of the dataset. 524 Matching relative frequency (Match-Dist): We explore hav- 525 ing access to the relative frequency of each label in the 526 downstream deployment. For example, a domain expert at a 527 national park might know the relative frequency of species 528 (deployment-specific domain knowledge) that we can utilize 529 for dataset subset selection. We create subsets by sampling 530 25%, 50%, 75%, and 90% of the training pool to match the 531 label distribution of the deployment. 532 Matching labels (Match-Label): Similarly, a domain expert 533 may know the classes present in the downstream deployment. 534 For example, a domain expert at a national park might know 535 the species present (deployment-specific domain knowledge) 536 that we can utilize for dataset subset selection. For these 537 subsets, we simply remove the classes present in the training 538 pool that are not present in the testing pool. 539 6. Results and discussion 540 Well chosen subsets outperform training on all data. 541 The expert-driven subsets in Table 1 show that deployment- 542 specific well-chosen subsets of the data can significantly out- 543 perform models trained on all the data, with improvements 544 in deployment accuracy up to 3.6% for GeoDE, 11.9% for 545 iWildCam, 51.3% for Auto Arborist, and a 0.03 reduction in 546 MSE for NuScenes. Even when the expert subsets underper- 547 form all training data, as in NuScenes Deployment 2, there 548 exist subsets from other baselines that outperform using all 549 the data. Due to the extreme long-tailed nature and signif- 550 icant label distribution shift between the training pool and 551 deployments of iWildCam and Auto Arborist, well-chosen 552 subsets improve performance significantly. This indicates 553 that using \"irrelevent\" data from the training pool is actively 554 harmful to performance for specialized deployments, com- 555 pared to a closer in-distribution subset. As an example, 556 the iWildCam’s training pool contains many Thompson’s 557 Gazelle, but only Deployment 2 has Thompson’s Gazelle 558 present. Accordingly, Deployments 1, 3, and 4 had more 559 improvement between all data and expert subsets than De- 560 ployment 2 since the former had a greater label distribution 561 shift from the training pool. 562 There is a need for unsupervised methods for dataset 563 subselection. While the expert-driven subsets in Table 1 564 demonstrate that a well-chosen subset does exist for all de- 565 ployments, finding this subset without expert knowledge is 566 still an open problem. While some of our baselines require 567 access to query labels, this requirement can in many cases 568 be unrealistic in the deployable ML setting (labels can be 569 expensive or difficult to collect). The two unsupervised base- 570 lines, the nearest neighbors and image alignment methods, 571 do not perform optimally on the deployments, often under- 572 7 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. Dataset Deploy # Non subset Expert subset Unlabeled query set Labeled query set Query-set All-data Image-Align Near-Nbors CLIP-score Match-Label Match-Dist GeoDE (Acc) Deploy 1 0.872 0.885 0.921 0.879 0.88 0.887 0.882 0.886 Deploy 2 0.450 0.890 0.910 0.897 0.892 0.899 0.900 0.882 Deploy 3 0.950 0.821 0.85 0.845 0.760 0.838 0.83 0.879 Deploy 4 0.827 0.829 0.845 0.791 0.783 0.828 0.841 0.830 iWildCam (Acc) Deploy 1 0.703 0.655 0.650 0.555 0.502 0.503 0.740 0.743 Deploy 2 0.780 0.341 0.346 0.438 0.469 0.463 0.350 0.490 Deploy 3 0.438 0.716 0.745 0.537 0.450 0.420 0.723 0.750 Deploy 4 0.463 0.660 0.670 0.599 0.600 0.290 0.687 0.741 Auto Arborist(Acc) Deploy 1 0.159 0.348 0.861 0.382 0.392 0.380 0.665 0.740 Deploy 2 0.197 0.483 0.859 0.114 0.141 0.137 0.650 0.560 Deploy 3 0.124 0.157 0.382 0.159 0.099 0.167 0.159 0.234 Deploy 4 0.119 0.135 0.392 0.102 0.108 0.106 0.102 0.230 NuScenes (MSE) Deploy 1 0.063 0.050 0.029 0.040 0.040 0.073 - - Deploy 2 0.070 0.021 0.049 0.147 0.042 0.032 - - Deploy 3 0.089 0.068 0.038 0.049 0.125 0.071 - - Deploy 4 0.123 0.048 0.039 0.086 0.389 0.050 - - Table 1. Best-performing subsets across hyperparameters for baseline methods across all datasets and deployments (abbreviated as Deploy) for ResNet50 full-finetuning. Overall accuracy is reported for the classification tasks of GeoDE, iWildCam, and Auto Arborist (greater is better) and MSE is reported for the regression task of NuScenes (smaller is better). Match-Dist and Match-Label are not applicable for NuScenes, as it is a regression task and does not have clear classes/labels for these methods. Baselines are distinguished from one another by their access to information, with each baseline having access to expert knowledge, or a labeled/unlabeled query set. We do not report the random baseline in this table, but demonstrate results in Appendix C as it mainly refers to subset size. Well-chosen subsets outperform using all the training data in each deployment, indicated in bold. performing using all the training data. Our benchmark opens573 up the line of research for potential unsupervised methods574 for this data subselection process.575 Training on more data has diminishing returns. For all576 deployments, we see that we can achieve near-optimal per-577 formance with subsets of the data. Appendix C shows that578 even 25% of the data can perform near-optimally in some579 cases, with little performance loss with 50% of the data.580 Additionally, while not a realistic deployment scenario, the581 \"lower bound\" of training on the small query set (results in582 Table 1) performs close to optimally in several deployments583 (this is expected, since the query sets are in distribution with584 the deployments). However, this again indicates that hav-585 ing a small relevant subset of data is most useful. Overall,586 these results demonstrate that greater efficiency for training587 specialised ML models is possible, potentially reducing com-588 putational and data storage burdens in deployable settings.589 We hypothesize this is because many deployments have sig-590 nificant distribution shift from the training pool, so as the591 data added gets farther from the deployment distribution, it592 becomes less relevant for optimal performance.593 7. Conclusions594 We present DA T AS 3, a benchmark to explore model special-595 ization via dataset subselection for scientific and engineering596 domains, and provide: (1) a test suite for the problem across597 4 ML application domains, each represented by a dataset598 containing a general training data pool and 4 distinct deploy-599 ment scenarios (2) expert- and knowledge-guided subsets 600 for each deployment which outperform training on all data, 601 sometimes by a significant margin, demonstrating the value 602 of specialized training data curation (3) an extensive exper- 603 imental study highlighting that current methods for subset 604 selection, designed for generalization instead of specializa- 605 tion, do not perform well on DA T AS 3. 606 We find that there does not currently exist a winning 607 method that performs well across multiple domains/datasets, 608 posing an open challenge to the research community. While 609 well-performing subsets exist via expert-driven knowledge, 610 models without access to labeled query sets systematically 611 underperform. We also find that certain datasets are more 612 challenging than others– perhaps different subselection meth- 613 ods are necessary for different domains or types of shifts. 614 Limitations and future work Due to computational con- 615 straints, hyperparameter searches were restricted to learning 616 rate and batch size. Additionally, while smaller-scale models 617 were used (ResNet50) for full finetuning with a larger model 618 only used for linear probes, future work could explore larger 619 model finetuning and its effects. 620 Additionally, model specialization for deployments isn’t 621 limited to the domains we include in our benchmark. We 622 plan to expand this benchmark to capture more scientific do- 623 mains with similar needs, including the tasks of histopathol- 624 ogy disease prediction, medical eICU record mortality pre- 625 diction, satellite imagery for crop type classification, and 626 astrophysics galaxy classification. 627 8 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. References628 [Ano23] Anonymous. When less is more: Investi-629 gating data pruning for pretraining LLMs at630 scale. In NeurIPS Workshop on Attributing631 Model Behavior at Scale, 2023. 3632 [BACB21] Sara Beery, Arushi Agarwal, Elijah Cole, and633 Vighnesh Birodkar. The iwildcam 2021 com-634 petition dataset, 2021. 3635 [BBH +22] Sidney Black, Stella Biderman, Eric Hal-636 lahan, Quentin Anthony, Leo Gao, Lau-637 rence Golding, Horace He, Connor Leahy,638 Kyle McDonell, Jason Phang, Michael Pieler,639 Usvsn Sai Prashanth, Shivanshu Purohit,640 Laria Reynolds, Jonathan Tow, Ben Wang,641 and Samuel Weinbach. GPT-NeoX-20B: An642 open-source autoregressive language model.643 In Proceedings of BigScience Episode #5 –644 Workshop on Challenges & Perspectives in645 Creating Large Language Models, pages 95–646 136, virtual+Dublin, May 2022. Association647 for Computational Linguistics. 3648 [BDM +20] Vladimir Braverman, Petros Drineas,649 Cameron Musco, Christopher Musco, Jalaj650 Upadhyay, David P. Woodruff, and Samson651 Zhou. Near optimal linear algebra in the652 online and sliding window models. In 61st653 IEEE Annual Symposium on Foundations of654 Computer Science, FOCS, pages 517–528,655 2020. 3656 [BFL16] Vladimir Braverman, Dan Feldman, and657 Harry Lang. New frameworks for offline658 and streaming coreset constructions. arXiv659 preprint arXiv:1612.00889, 2016. 3660 [BLF +16] Leslie A. Brandt, Abigail Derby Lewis,661 Robert T. Fahey, Lydia Scott, Lindsay E. Dar-662 ling, and Christopher W. Swanston. A frame-663 work for adapting urban forests to climate664 change. Environmental Science & Policy,665 66:393–402, 2016. 5666 [BLG +22] Cenk Baykal, Lucas Liebenwein, Igor667 Gilitschenski, Dan Feldman, and Daniela668 Rus. Sensitivity-informed provable pruning669 of neural networks. SIAM J. Math. Data Sci.,670 4(1):26–45, 2022. 3671 [BMY19] Sara Beery, Dan Morris, and Siyu Yang. Ef-672 ficient pipeline for camera trap image review,673 2019. 4674 [BSA +23] Stella Biderman, Hailey Schoelkopf, Quentin675 Anthony, Herbie Bradley, Kyle O’Brien, Eric676 Hallahan, Mohammad Aflah Khan, Shivan-677 shu Purohit, USVSN Sai Prashanth, Edward678 Raff, Aviya Skowron, Lintang Sutawika, and679 Oskar van der Wal. Pythia: A suite for ana- 680 lyzing large language models across training 681 and scaling, 2023. 3 682 [BUSZ22] Fred Bane, Celia Soler Uguet, Wiktor 683 Stribi˙zew, and Anna Zaretskaya. A com- 684 parison of data filtering methods for neu- 685 ral machine translation. In Proceedings of 686 the 15th Biennial Conference of the Associa- 687 tion for Machine Translation in the Americas 688 (Volume 2: Users and Providers Track and 689 Government Track), pages 313–325, Orlando, 690 USA, September 2022. Association for Ma- 691 chine Translation in the Americas. 3 692 [BWE +22a] Sara Beery, Guanhang Wu, Trevor Ed- 693 wards, Filip Pavetic, Bohdan S. Majewski, 694 Shreyasee Mukherjee, Stanley Chan, John 695 Morgan, Vivek Rathod, and Jonathan Huang. 696 The auto arborist dataset: A large-scale 697 benchmark for multiview urban forest moni- 698 toring under domain shift. 2022 IEEE/CVF 699 Conference on Computer Vision and Pattern 700 Recognition (CVPR), pages 21262–21275, 701 2022. 1 702 [BWE +22b] Sara Meghan Beery, Guanhang Wu, Trevor 703 Edwards, Filip Paveti´c, Bo Majewski, 704 Shreyasee Mukherjee, Stan Chan, John Mor- 705 gan, Vivek Mansing Rathod, and Jonathan 706 Chung-kuan Huang. The auto-arborist 707 dataset: A large-scale benchmark for gener- 708 alizable, multimodal urban forest monitoring. 709 2022. 3 710 [BZOR +18] Marcus A. Badgeley, John R. Zech, Luke 711 Oakden-Rayner, Benjamin Scott Glicksberg, 712 Manway Liu, William Gale, Michael V. Mc- 713 Connell, Bethany Percha, Thomas M. Snyder, 714 and Joel T. Dudley. Deep learning predicts 715 hip fracture using confounding patient and 716 healthcare variables. NPJ Digital Medicine, 717 2, 2018. 1 718 [CAGLS +22] Vincent Cohen-Addad, Kasper Green Larsen, 719 David Saulpic, Chris Schwiegelshohn, and 720 Omar Ali Sheikh-Omar. Improved coresets 721 for euclidean k-means. Advances in Neural 722 Information Processing Systems, 35:2679– 723 2694, 2022. 3 724 [CBL +20] Holger Caesar, Varun Bankiti, Alex H Lang, 725 Sourabh Vora, Venice Erin Liong, Qiang Xu, 726 Anush Krishnan, Yu Pan, Giancarlo Baldan, 727 and Oscar Beijbom. nuscenes: A multimodal 728 dataset for autonomous driving. In Proceed- 729 ings of the IEEE/CVF conference on com- 730 puter vision and pattern recognition, pages 731 11621–11631, 2020. 3 732 9 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. [CDS20] Rachit Chhaya, Anirban Dasgupta, and733 Supratim Shit. On coresets for regularized734 regression. In Proceedings of the 37th Inter-735 national Conference on Machine Learning,736 ICML, 2020. 3737 [Che09] Ke Chen. On coresets for k-median and738 k-means clustering in metric and euclidean739 spaces and their applications. SIAM J. Com-740 put., 39(3):923–947, 2009. 3741 [Cla10] Kenneth L. Clarkson. Coresets, sparse greedy742 approximation, and the frank-wolfe algo-743 rithm. ACM Trans. Algorithms, 6(4):63:1–744 63:30, 2010. 3745 [CMM17] Michael B. Cohen, Cameron Musco, and746 Christopher Musco. Input sparsity time low-747 rank approximation via ridge leverage score748 sampling. In Proceedings of the Twenty-749 Eighth Annual ACM-SIAM Symposium on750 Discrete Algorithms, SODA, pages 1758–751 1777, 2017. 3752 [CYM +19] Cody Coleman, Christopher Yeh, Stephen753 Mussmann, Baharan Mirzasoleiman, Peter754 Bailis, Percy Liang, Jure Leskovec, and755 Matei Zaharia. Selection via proxy: Effi-756 cient data selection for deep learning. In757 International Conference on Learning Repre-758 sentations, 2019. 3759 [CZPR23] Rhys Compton, Lily H. Zhang,760 Aahlad Manas Puli, and Rajesh Ran-761 ganath. When more is less: Incorporating762 additional datasets can hurt performance by763 introducing spurious correlations. ArXiv,764 abs/2308.04431, 2023. 1765 [DBK +20] Alexey Dosovitskiy, Lucas Beyer, Alexan-766 der Kolesnikov, Dirk Weissenborn, Xiao-767 hua Zhai, Thomas Unterthiner, Mostafa De-768 hghani, Matthias Minderer, Georg Heigold,769 Sylvain Gelly, Jakob Uszkoreit, and Neil770 Houlsby. An image is worth 16x16 words:771 Transformers for image recognition at scale.772 ArXiv, abs/2010.11929, 2020. 6773 [DDH +08] Anirban Dasgupta, Petros Drineas, Boulos774 Harb, Ravi Kumar, and Michael W. Mahoney.775 Sampling algorithms and coresets for lp re-776 gression. In Proceedings of the Nineteenth777 Annual ACM-SIAM Symposium on Discrete778 Algorithms, SODA, pages 932–941, 2008. 3779 [DDS +09] Jia Deng, Wei Dong, Richard Socher, Li-Jia780 Li, Kai Li, and Li Fei-Fei. Imagenet: A large-781 scale hierarchical image database. In 2009782 IEEE Conference on Computer Vision and783 Pattern Recognition, pages 248–255, 2009. 1784 [EDHG +20] Liat Ein-Dor, Alon Halfon, Ariel Gera, Eyal785 Shnarch, Lena Dankin, Leshem Choshen, 786 Marina Danilevsky, Ranit Aharonov, Yoav 787 Katz, and Noam Slonim. Active Learning 788 for BERT: An Empirical Study. In Proceed- 789 ings of the 2020 Conference on Empirical 790 Methods in Natural Language Processing 791 (EMNLP), pages 7949–7962, Online, Novem- 792 ber 2020. Association for Computational Lin- 793 guistics. 3 794 [FXC +24] Benjamin Feuer, Jiawei Xu, Niv Cohen, 795 Patrick Yubeaton, Govind Mittal, and Chin- 796 may Hegde. Select: A large-scale benchmark 797 of data curation strategies for image classi- 798 fication. arXiv preprint arXiv:2410.05057, 799 2024. 3 800 [GIF +24] Samir Yitzhak Gadre, Gabriel Ilharco, Alex 801 Fang, Jonathan Hayase, Georgios Smyrnis, 802 Thao Nguyen, Ryan Marten, Mitchell Worts- 803 man, Dhruba Ghosh, Jieyu Zhang, et al. Dat- 804 acomp: In search of the next generation of 805 multimodal datasets. Advances in Neural In- 806 formation Processing Systems, 36, 2024. 1, 807 3, 7 808 [GJM +20] Robert Geirhos, Jörn-Henrik Jacobsen, Clau- 809 dio Michaelis, Richard S. Zemel, Wieland 810 Brendel, Matthias Bethge, and Felix Wich- 811 mann. Shortcut learning in deep neural net- 812 works. Nature Machine Intelligence, 2:665 – 813 673, 2020. 1 814 [HM04] Sariel Har-Peled and Soham Mazumdar. On 815 coresets for k-means and k-median cluster- 816 ing. In Proceedings of the 36th Annual ACM 817 Symposium on Theory of Computing, pages 818 291–300, 2004. 3 819 [HV20] Lingxiao Huang and Nisheeth K. Vish- 820 noi. Coresets for clustering in euclidean 821 spaces: importance sampling is nearly op- 822 timal. In Proccedings of the 52nd Annual 823 ACM SIGACT Symposium on Theory of Com- 824 puting, STOC, pages 1416–1429, 2020. 3 825 [HZRS15] Kaiming He, X. Zhang, Shaoqing Ren, and 826 Jian Sun. Deep residual learning for im- 827 age recognition. 2016 IEEE Conference on 828 Computer Vision and Pattern Recognition 829 (CVPR), pages 770–778, 2015. 6 830 [JDJ19] Jeff Johnson, Matthijs Douze, and Hervé Jé- 831 gou. Billion-scale similarity search with 832 GPUs. IEEE Transactions on Big Data, 833 7(3):535–547, 2019. 7 834 [JTMF20] Ibrahim Jubran, Murad Tukan, Alaa Maalouf, 835 and Dan Feldman. Sets clustering. In Inter- 836 national Conference on Machine Learning, 837 pages 4994–5005. PMLR, 2020. 3 838 10 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. [KH +09] Alex Krizhevsky, Geoffrey Hinton, et al.839 Learning multiple layers of features from tiny840 images, 2009. 1841 [KSR +21] KrishnaTeja Killamsetty, Durga Sivasubra-842 manian, Ganesh Ramakrishnan, Abir De, and843 Rishabh K. Iyer. GRAD-MATCH: gradi-844 ent matching based data subset selection for845 efficient deep model training. In Proceed-846 ings of the 38th International Conference on847 Machine Learning, ICML, pages 5464–5474,848 2021. 3849 [KSRI21] KrishnaTeja Killamsetty, Durga Sivasub-850 ramanian, Ganesh Ramakrishnan, and851 Rishabh K. Iyer. GLISTER: generalization852 based data subset selection for efficient and853 robust learning. In Thirty-Fifth AAAI Confer-854 ence on Artificial Intelligence, AAAI, 2021.855 3856 [KZCI21] Krishnateja Killamsetty, Xujiang Zhao, Feng857 Chen, and Rishabh Iyer. Retrieve: Core-858 set selection for efficient and robust semi-859 supervised learning. In Advances in Neural860 Information Processing Systems (NeurIPS),861 2021. 3862 [LBL +19] Lucas Liebenwein, Cenk Baykal, Harry863 Lang, Dan Feldman, and Daniela Rus. Prov-864 able filter pruning for efficient neural net-865 works. In International Conference on Learn-866 ing Representations, 2019. 3867 [LSW +23] Hugo Laurençon, Lucile Saulnier, Thomas868 Wang, Christopher Akiki, Albert Villanova869 del Moral, Teven Le Scao, Leandro Von870 Werra, Chenghao Mou, Eduardo González871 Ponferrada, Huu Nguyen, Jörg Frohberg,872 Mario Šaško, Quentin Lhoest, Angelina873 McMillan-Major, Gerard Dupont, Stella Bi-874 derman, Anna Rogers, Loubna Ben allal,875 Francesco De Toni, Giada Pistilli, Olivier876 Nguyen, Somaieh Nikpoor, Maraim Ma-877 soud, Pierre Colombo, Javier de la Rosa,878 Paulo Villegas, Tristan Thrush, Shayne Long-879 pre, Sebastian Nagel, Leon Weber, Manuel880 Muñoz, Jian Zhu, Daniel Van Strien, Zaid881 Alyafeai, Khalid Almubarak, Minh Chien882 Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle883 Lo, Manan Dey, Pedro Ortiz Suarez, Aaron884 Gokaslan, Shamik Bose, David Adelani,885 Long Phan, Hieu Tran, Ian Yu, Suhas Pai,886 Jenny Chim, Violette Lepercq, Suzana Ilic,887 Margaret Mitchell, Sasha Alexandra Luc-888 cioni, and Yacine Jernite. The bigscience889 roots corpus: A 1.6tb composite multilingual890 dataset, 2023. 3891 [MBL20] Baharan Mirzasoleiman, Jeff A. Bilmes, and 892 Jure Leskovec. Coresets for data-efficient 893 training of machine learning models. In Pro- 894 ceedings of the 37th International Confer- 895 ence on Machine Learning, ICML, pages 896 6950–6960, 2020. 3 897 [MBR +22] Sören Mindermann, Jan Brauner, 898 Muhammed Razzak, Mrinank Sharma, 899 Andreas Kirsch, Winnie Xu, Benedikt 900 Höltgen, Aidan N. Gomez, Adrien Morisot, 901 Sebastian Farquhar, and Yarin Gal. Priori- 902 tized training on points that are learnable, 903 worth learning, and not yet learnt, 2022. 3 904 [MBY +23] Mark Mazumder, Colby Banbury, Xiaozhe 905 Yao, Bojan Karlaš, William Gaviria Rojas, 906 Sudnya Diamos, Greg Diamos, Lynn He, 907 Alicia Parrish, Hannah Rose Kirk, Jessica 908 Quaye, Charvi Rastogi, Douwe Kiela, David 909 Jurado, David Kanter, Rafael Mosquera, Juan 910 Ciro, Lora Aroyo, Bilge Acun, Lingjiao 911 Chen, Mehul Smriti Raje, Max Bartolo, Sabri 912 Eyuboglu, Amirata Ghorbani, Emmett Good- 913 man, Oana Inel, Tariq Kane, Christine R. 914 Kirkpatrick, Tzu-Sheng Kuo, Jonas Mueller, 915 Tristan Thrush, Joaquin Vanschoren, Mar- 916 garet Warren, Adina Williams, Serena Yeung, 917 Newsha Ardalani, Praveen Paritosh, Lilith 918 Bat-Leah, Ce Zhang, James Zou, Carole- 919 Jean Wu, Cody Coleman, Andrew Ng, Peter 920 Mattson, and Vijay Janapa Reddi. Dataperf: 921 Benchmarks for data-centric ai development, 922 2023. 3 923 [MCL20] Baharan Mirzasoleiman, Kaidi Cao, and Jure 924 Leskovec. Coresets for robust training of 925 deep neural networks against noisy labels. In 926 Advances in Neural Information Processing 927 Systems 33: Annual Conference on Neural 928 Information Processing Systems, NeurIPS, 929 2020. 3 930 [MEM +22] Alaa Maalouf, Gilad Eini, Ben Mussay, Dan 931 Feldman, and Margarita Osadchy. A unified 932 approach to coreset learning. IEEE Trans- 933 actions on Neural Networks and Learning 934 Systems, 2022. 3 935 [MJF19] Alaa Maalouf, Ibrahim Jubran, and Dan Feld- 936 man. Fast and accurate least-mean-squares 937 solvers. In Proceedings of the 33rd Inter- 938 national Conference on Neural Information 939 Processing Systems, pages 8307–8318, 2019. 940 3 941 [MJTF21] Alaa Maalouf, Ibrahim Jubran, Murad Tukan, 942 and Dan Feldman. Coresets for the aver- 943 11 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. age case error for finite query sets. Sensors,944 21(19):6689, 2021. 3945 [MMM +22] Raphael A. Meyer, Cameron Musco, Christo-946 pher Musco, David P. Woodruff, and Samson947 Zhou. Fast regression for structured inputs.948 In The Tenth International Conference on949 Learning Representations, ICLR, 2022. 3950 [MRB +23] Niklas Muennighoff, Alexander M. Rush,951 Boaz Barak, Teven Le Scao, Aleksandra952 Piktus, Nouamane Tazi, Sampo Pyysalo,953 Thomas Wolf, and Colin Raffel. Scaling data-954 constrained language models, 2023. 3955 [NMB +19] Mohammad Sadegh Norouzzadeh, Dan Mor-956 ris, Sara Beery, Neel Joshi, Nebojsa Jojic,957 and Jeff Clune. A deep active learning sys-958 tem for species identification and counting in959 camera trap images, 2019. 4960 [PGD21] Mansheej Paul, Surya Ganguli, and961 Gintare Karolina Dziugaite. Deep learning962 on a data diet: Finding important examples963 early in training. In Association for the964 Advancement of Artificial Intelligence965 (AAAI), 2021. 3966 [PMH +23] Guilherme Penedo, Quentin Malartic, Daniel967 Hesslow, Ruxandra Cojocaru, Alessandro968 Cappelli, Hamza Alobeidli, Baptiste Pan-969 nier, Ebtesam Almazrouei, and Julien Lau-970 nay. The refinedweb dataset for falcon llm:971 Outperforming curated corpora with web972 data, and web data only, 2023. 3973 [RBC +22] Jack W. Rae, Sebastian Borgeaud, Trevor974 Cai, Katie Millican, Jordan Hoffmann, Fran-975 cis Song, John Aslanides, Sarah Henderson,976 Roman Ring, Susannah Young, Eliza Ruther-977 ford, Tom Hennigan, Jacob Menick, Albin978 Cassirer, Richard Powell, George van den979 Driessche, Lisa Anne Hendricks, Maribeth980 Rauh, Po-Sen Huang, Amelia Glaese, Jo-981 hannes Welbl, Sumanth Dathathri, Saffron982 Huang, Jonathan Uesato, John Mellor, Irina983 Higgins, Antonia Creswell, Nat McAleese,984 Amy Wu, Erich Elsen, Siddhant Jayakumar,985 Elena Buchatskaya, David Budden, Esme986 Sutherland, Karen Simonyan, Michela Pa-987 ganini, Laurent Sifre, Lena Martens, Xi-988 ang Lorraine Li, Adhiguna Kuncoro, Aida989 Nematzadeh, Elena Gribovskaya, Domenic990 Donato, Angeliki Lazaridou, Arthur Mensch,991 Jean-Baptiste Lespiau, Maria Tsimpoukelli,992 Nikolai Grigorev, Doug Fritz, Thibault Sot-993 tiaux, Mantas Pajarskas, Toby Pohlen, Zhi-994 tao Gong, Daniel Toyama, Cyprien de Mas-995 son d’Autume, Yujia Li, Tayfun Terzi,996 Vladimir Mikulik, Igor Babuschkin, Aidan 997 Clark, Diego de Las Casas, Aurelia Guy, 998 Chris Jones, James Bradbury, Matthew John- 999 son, Blake Hechtman, Laura Weidinger, Ia- 1000 son Gabriel, William Isaac, Ed Lockhart, Si- 1001 mon Osindero, Laura Rimell, Chris Dyer, 1002 Oriol Vinyals, Kareem Ayoub, Jeff Stanway, 1003 Lorrayne Bennett, Demis Hassabis, Koray 1004 Kavukcuoglu, and Geoffrey Irving. Scaling 1005 language models: Methods, analysis and in- 1006 sights from training gopher, 2022. 3 1007 [RLZ +23] Vikram V. Ramaswamy, Sing Yu Lin, Dora 1008 Zhao, Aaron B. Adcock, Laurens van der 1009 Maaten, Deepti Ghadiyaram, and Olga Rus- 1010 sakovsky. Geode: a geographically diverse 1011 evaluation dataset for object recognition, 1012 2023. 3, 4 1013 [RSR +20] Colin Raffel, Noam Shazeer, Adam Roberts, 1014 Katherine Lee, Sharan Narang, Michael 1015 Matena, Yanqi Zhou, Wei Li, and Peter J. 1016 Liu. Exploring the limits of transfer learning 1017 with a unified text-to-text transformer, 2020. 1018 3 1019 [SGS +22] Ben Sorscher, Robert Geirhos, Shashank 1020 Shekhar, Surya Ganguli, and Ari S. Morcos. 1021 Beyond neural scaling laws: beating power 1022 law scaling via data pruning. arXiv, 2022. 3 1023 [SGS +23] Ben Sorscher, Robert Geirhos, Shashank 1024 Shekhar, Surya Ganguli, and Ari S. Morcos. 1025 Beyond neural scaling laws: beating power 1026 law scaling via data pruning, 2023. 3 1027 [SHB +17] Shreya Shankar, Yoni Halpern, Eric Breck, 1028 James Atwood, Jimbo Wilson, and D. Scul- 1029 ley. No classification without representation: 1030 Assessing geodiversity issues in open data 1031 sets for the developing world. arXiv: Ma- 1032 chine Learning, 2017. 4 1033 [SRC24] Judy Hanwen Shen, Inioluwa Deborah Raji, 1034 and Irene Y. Chen. The data addition 1035 dilemma, 2024. 1 1036 [SRM +22] Shoaib Ahmed Siddiqui, Nitarshan Rajku- 1037 mar, Tegan Maharaj, David Krueger, and 1038 Sara Hooker. Metadata archaeology: Un- 1039 earthing data subsets by leveraging training 1040 dynamics, 2022. 3 1041 [SS18] Ozan Sener and Silvio Savarese. Active learn- 1042 ing for convolutional neural networks: A 1043 core-set approach. In International Confer- 1044 ence on Learning Representations (ICLR), 1045 2018. 3 1046 [Sta20] Francis Staub. Living planet report 2020: 1047 Bending the curve of biodiversity loss, Sep 1048 2020. 4 1049 12 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. [TBFR21] Murad Tukan, Cenk Baykal, Dan Feldman,1050 and Daniela Rus. On coresets for support vec-1051 tor machines. Theor. Comput. Sci., 890:171–1052 191, 2021. 31053 [TDS +20] Rohan Taori, Achal Dave, Vaishaal Shankar,1054 Nicholas Carlini, Benjamin Recht, and Lud-1055 wig Schmidt. Measuring robustness to natu-1056 ral distribution shifts in image classification.1057 ArXiv, abs/2007.00644, 2020. 11058 [TJF22] Elad Tolochinsky, Ibrahim Jubran, and Dan1059 Feldman. Generic coreset for scalable learn-1060 ing of monotonic kernels: Logistic regres-1061 sion, sigmoid and more. In International1062 Conference on Machine Learning, ICML,1063 2022. 31064 [TMM22] Murad Tukan, Loay Mualem, and Alaa1065 Maalouf. Pruning neural networks via core-1066 sets and convex geometry: Towards no as-1067 sumptions. In Proceedings of the 36th Inter-1068 national Conference on Neural Information1069 Processing Systems, 2022. 31070 [TND +22] Alex Tamkin, Dat Nguyen, Salil Deshpande,1071 Jesse Mu, and Noah Goodman. Active learn-1072 ing helps pretrained models learn the in-1073 tended task, 2022. 31074 [TZM +23] Murad Tukan, Samson Zhou, Alaa Maalouf,1075 Daniela Rus, Vladimir Braverman, and Dan1076 Feldman. Provable data subset selection for1077 efficient neural networks training. In Inter-1078 national Conference on Machine Learning,1079 pages 34533–34555. PMLR, 2023. 31080 [WGK17] Oliver Wearn and Paul Glover-Kapfer.1081 Camera-trapping for conservation: a guide to1082 best-practices, 10 2017. 41083 [WLC +20] Guillaume Wenzek, Marie-Anne Lachaux,1084 Alexis Conneau, Vishrav Chaudhary, Fran-1085 cisco Guzmán, Armand Joulin, and Edouard1086 Grave. CCNet: Extracting high quality mono-1087 lingual datasets from web crawl data. In1088 Proceedings of the Twelfth Language Re-1089 sources and Evaluation Conference, pages1090 4003–4012, Marseille, France, May 2020.1091 European Language Resources Association.1092 31093 [WLL +21] Jindong Wang, Cuiling Lan, Chang Liu, Yi-1094 dong Ouyang, and Tao Qin. Generalizing to1095 unseen domains: A survey on domain gener-1096 alization. IEEE Transactions on Knowledge1097 and Data Engineering, 35:8052–8072, 2021.1098 11099 [WLY22] Xudong Wang, Long Lian, and Stella X Yu.1100 Unsupervised selective labeling for more ef-1101 fective semi-supervised learning. In Euro-1102 pean Conference on Computer Vision, pages 1103 427–445. Springer, 2022. 3 1104 [WMX +24] Tsun-Hsuan Wang, Alaa Maalouf, Wei Xiao, 1105 Yutong Ban, Alexander Amini, Guy Ros- 1106 man, Sertac Karaman, and Daniela Rus. 1107 Drive anywhere: Generalizable end-to-end 1108 autonomous driving with multi-modal foun- 1109 dation models. In 2024 IEEE International 1110 Conference on Robotics and Automation 1111 (ICRA), pages 6687–6694. IEEE, 2024. 6 1112 [WPM +20] Xinyi Wang, Hieu Pham, Paul Michel, Anto- 1113 nios Anastasopoulos, Jaime Carbonell, and 1114 Graham Neubig. Optimizing data usage via 1115 differentiable rewards. In International Con- 1116 ference on Machine Learning (ICML), 2020. 1117 3 1118 [YLBG20] Michelle Yuan, Hsuan-Tien Lin, and Jor- 1119 dan Boyd-Graber. Cold-start active learning 1120 through self-supervised language modeling. 1121 In Proceedings of the 2020 Conference on 1122 Empirical Methods in Natural Language Pro- 1123 cessing (EMNLP), pages 7935–7948, Online, 1124 November 2020. Association for Computa- 1125 tional Linguistics. 3 1126 [ZLX +24] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini- 1127 vasan Iyer, Jiao Sun, Yuning Mao, Xuezhe 1128 Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: 1129 Less is more for alignment. Advances in 1130 Neural Information Processing Systems, 36, 1131 2024. 3 1132 [ZRG +22] Susan Zhang, Stephen Roller, Naman Goyal, 1133 Mikel Artetxe, Moya Chen, Shuohui Chen, 1134 Christopher Dewan, Mona Diab, Xian Li, 1135 Xi Victoria Lin, Todor Mihaylov, Myle Ott, 1136 Sam Shleifer, Kurt Shuster, Daniel Simig, 1137 Punit Singh Koura, Anjali Sridhar, Tianlu 1138 Wang, and Luke Zettlemoyer. Opt: Open pre- 1139 trained transformer language models, 2022. 1140 3 1141 13 CVPR #***** CVPR #***** CVPR 2025 Submission #*****. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE. A. Additional Dataset Details1142 B. Additional Training Details1143 C. Additional Results1144 C.1. Efficiency1145 C.2. Linear Probing1146 14","libVersion":"0.3.2","langs":""}