{"path":"GenAIUnleaning/General_techniques/Image Image constrained.pdf","text":"Controllable Unlearning for Image-to-Image Generative Models via Îµ-Constrained Optimization Xiaohua Feng College of Computer Science Zhejiang University fengxiaohua@zju.edu.cn Chaochao Chen College of Computer Science Zhejiang University zjuccc@zju.edu.cn Yuyuan Li College of Communication Engineering Hangzhou Dianzi University y2li@hdu.edu.cn Li Zhang College of Computer Science Zhejiang University zhanglizl80@gmail.com Abstract While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient Îµ to control the trade-off. We reformulate the I2I generative model unlearning problem into a Îµ-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearning boundaries. These boundaries define the valid range for the control coefficient. Within this range, every yielded solution is theoretically guaranteed with Pareto optimality. We also analyze the convergence rate of our framework under various control functions. Extensive experiments on two benchmark datasets across three mainstream I2I models demonstrate the effectiveness of our controllable unlearning framework. 1 Introduction Generative models have recently made significant progress in fields such as image recognition [29, 16] and natural language processing [45, 61], capturing significant academic interest due to their boundless generative potential. Typically trained on vast datasets from the Internet, generative models inevitably assimilate latent biases and expose private information [55]. Existing studies [34, 60, 8] have revealed that generative models have a strong tendency to recall specific instances encountered during training, raising concerns that the models might output biases and leak private information when put into practical situations. Machine unlearning [43] presents a viable solution to address this issue. It aims to eliminate the knowledge learned from specific training data (forget set) while preserving the knowledge learned from the remaining data (retain set). Implementing unlearning for generative models serves dual objectives, i.e., fulfilling privacy re- quirements and enhancing model reliability. On the one hand, legislation such as the General Data Protection Regulation [63] grants individuals the right to be forgotten. Consequently, service providers must unlearn specific private information from the model in response to an individualâ€™s Preprint. Under review.arXiv:2408.01689v2 [cs.LG] 14 Sep 2024 Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree Forget Set Retain Set Feasible Region Pareto Optimal Points Degree of CompletenessModel UtilityNon-Pareto Optimal Points ğŸ ğœºğ’ğ’Šğ’ ğœºğŸ ğœºğŸ ğœºğ’ğ’‚ğ’™ Pareto Front ğœºğŸ‘ Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Relax-ğœºğŸ 25% Relax-ğœºğŸ 50% Relax-ğœºğŸ‘ 75% Figure 1: An overview of controllable unlearning. On the left, the first and second rows represent the forget set and the retain set, respectively. We first present the effect of unlearning in I2I generative models, followed by a collection of controllable solutions, where Îµ is the control coefficient. On the right, we demonstrate that for each Îµ, our solution is guaranteed with the Pareto optimality. request. On the other hand, the data available on the Internet is rife with biases and inaccuracies, which compromises model performance when used for training. By proactively unlearning the biased and inaccurate data, the service providers can improve the liability of their models. In this paper, we focus on the unlearning problem in Image-to-Image (I2I) generative models [69], where unlearning is defined by the modelâ€™s incapacity to reconstruct the full image from a partially cropped one [36], as shown in Figure 1. Previous study [36] frames machine unlearning in generative models as a single-objective optimization problem, with the loss defined as a combination of perfor- mance on both the forget and retain sets. However, this approach faces three main challenges: i) First and foremost, this approach offers a solitary resolution, ignoring the real-world need for flexible trade-offs between model utility and unlearning completeness aligned with varying user expectations. Regrettably, this challenge remains overlooked in the majority of current research on unlearning. ii) This approach relies wholly on fine-tuning with manual terminating conditions, lacking a theoretical guarantee for convergence. iii) This approach integrates two optimization objectives into a single loss function, which compromises unlearning efficiency due to the competition or conflict between different objectives. To address these challenges, we propose a controllable unlearning approach that provides a set of Pareto optimal solutions to cater to varied user expectations. Users can select a solution based on the degree of unlearning completeness through a simple control coefficient Îµ. Specifically, we reframe machine unlearning of I2I generative models into a bi-objective optimization problem [31], i.e., unlearning the forget set (1st objective, unlearning completeness) while preserving the retain set (2nd objective, model utility). Due to legislation requirements, the first objective prioritizes the second objective, meaning that minimizing the negative impact on the retain set only arises once the unlearning objective is sufficiently optimized. Therefore, we reformulate the bi-objective optimization problem into a Îµ-constrained optimization problem, where the unlearning objective is treated as a constraint (primary to satisfy) and Îµ is the control coefficient. Utilizing gradient-based methods to solve this Îµ-constrained optimization, we can obtain two Pareto optimal solutions for the boundaries of unlearning with theoretical guarantee, which can be used to determine the valid range of values for Îµ. Subsequently, we select the value of Îµ within its valid range and relax the constraints on the unlearning objective by increasing Îµ. As a result, we obtain a set of solutions that dynamically fulfill userâ€™s varied expectations regarding the trade-off between unlearning completeness and model utility. Finally, to enhance the efficiency of unlearning, we analyze the convergence rates of our unlearning framework under various settings of the control function which is utilized to govern the direction of parameter updates. The main contributions of this paper are summarized as follows: â€¢ We focus on I2I generative models, and propose a controllable unlearning approach that balances unlearning completeness and model utility, providing a set of solutions to fulfill varied user expectations. To the best of our knowledge, we are the first to study controllable unlearning. â€¢ We reformulate the machine unlearning of generative models as a Îµ-constrained optimization problem with unlearning the forget set as the constraint, guaranteeing optimal theoretical solutions for the boundaries of unlearning. By progressively relaxing the unlearning constraint, we obtain the Pareto set and plot the corresponding Pareto front. 2 â€¢ We utilize gradient-based methods to solve the Îµ-constrained optimization problem. To enhance the efficiency of unlearning, we analyze our frameworkâ€™s performance across different settings of the control function and validate with multiple combinations. â€¢ We conduct extensive experiments to evaluate our proposed method over diverse I2I generative models. The results from two large datasets demonstrate that the Pareto optimal solutions yielded by our method significantly outperform baseline methods. Additionally, the solution set achieves controllable unlearning to fulfill varied expectations regarding the trade-off between unlearning completeness and model utility. 2 Related Work 2.1 I2I Generative Models Many computer vision tasks can be formulated as I2I generation processes, e.g., style transfer [74], im- age extension [9], restoration [59], and image synthesis [71]. There are mainly three architectures for I2I generative models, i.e., Auto-Encoders (AEs) [1], Generative Adversarial Networks (GANs) [24], and diffusion models [29]. AEs mainly aim to reduce the mean squared error between generated and ground truth images but often produce lower-quality outputs [17, 19]. GANs, through adversarial training, significantly improve generation quality, despite their unstable training [2, 26, 7]. Diffusion models, which use a diffusion-then-denoising approach, aim for stable training and high-quality generation by minimizing the distributional distance between generated images and ground truth images [29, 57, 54]. However, diffusion models require a greater amount of data and computational resources [52, 50]. In this paper, we aim to design a universal unlearning method that can be applied across different I2I models. 2.2 Machine Unlearning Machine unlearning aims to eliminate the influence of specific training data (unlearning target) from a trained model. A naive approach is to retrain the model from scratch using a modified dataset that excludes the unlearning target. However, this approach can be computationally prohibitive in practice. Based on the degree of unlearning completeness, machine unlearning can be categorized into exact unlearning and approximate unlearning [67]. Exact unlearning aims to ensure that the unlearning target is fully unlearned, i.e., as complete as retraining from scratch [5, 68, 38]. This approach, which typically relies on retraining, is limited to unlearning specific instances and cannot be readily extended to generative models with strong feature generalizations. Approximate unlearning aims to obtain an approximate model, whose performance closely aligns with a retrained model [22, 56]. This approach estimates the influence of unlearning targets, and updates the model accordingly, usually through gradient-based updates, avoiding full retraining [3, 40]. However, accurate influence estimation is still challenging [25], reducing the applicability of this approach to generative models. In generative models, the exploration of unlearning is accomplished by minimizing a composite loss, which is a combination of training loss on the retain and the forget sets [36]. This approach is highly dependent on manual parameter tuning and cannot guarantee unlearning completeness. As for comparison, the solutions yielded by our proposed controllable unlearning framework are theoretically guaranteed with Pareto optimality. 3 Preliminary 3.1 Unlearning Principles As outlined in [11, 39], an unlearning task typically has three main principles: i) unlearning com- pleteness, which involves eliminating the influence of specific data from an already trained model; ii) unlearning efficiency, which focuses on enhancing the speed of the unlearning process; and iii) model utility, which aims to ensure that the performance of the unlearned model remains comparable to that of a model retrained from scratch. 3 3.2 Pareto Optimality Consider a multi-objective optimization problem formulated as: minÎ¸ f (Î¸) = (f1(Î¸), f2(Î¸), Â· Â· Â· , fm(Î¸))âŠ¤, where fi(Î¸) denotes the loss for the i-th objective. Pareto dominance. Let Î¸a, Î¸b be two points in feasible set â„¦, Î¸a is said to dominate Î¸b (Î¸a â‰º Î¸b) if and only if fi (Î¸a) â‰¤ fi (Î¸b) , âˆ€i âˆˆ {1, . . . , m} and fj (Î¸a) < fj (Î¸b) , âˆƒj âˆˆ {1, . . . , m}. Pareto optimality [41]. A point Î¸âˆ— is Pareto optimal if there is no Ë†Î¸ âˆˆ â„¦ for which Ë†Î¸ â‰º Î¸âˆ—. The collection of all such Pareto optimal points forms the Pareto set, and the surface of this set in the loss space is called the Pareto front. 3.3 I2I Generative Model Unlearning Model architecture. Encoder-decoder structures are widely used in I2I models, with: i) an encoder EÎ³ reducing images to the latent space, and ii) a decoder DÏ• reconstructing images from the latent space. For model IÎ¸ with input image x, the output is: IÎ¸(x) = DÏ•(EÎ³(T (x))), (1) where T (x) denotes the cropping operation (such as center cropping or random cropping), and Î¸ = {Î³, Ï•} denotes the full parameter set. Unlearning objective. Define the unlearning task for an I2I generative model IÎ¸0 involving data partitions Df (forget set) and Dr (retain set). Consider an IÎ¸0 , i.e., the original model, with training data D = Df âˆª Dr. Assume that IÎ¸0 is proficiently trained to generate satisfactory results on both Df and Dr. The objective of unlearning is to obtain an unlearned model IÎ¸ that cannot generate satisfactory results on Df (1st objective, unlearning completeness) while maintaining comparable performance on DR (2nd objective, model utility). Formally, max Î¸ (Div(P(Xf )âˆ¥P(IÎ¸(T (Xf ))))), and min Î¸ (Div(P(Xr)âˆ¥P(IÎ¸(T (Xr))) )), (2) where Xf and Xr are the variables for ground truth images in Df and Dr, P(IÎ¸(X)) is the model output distribution for input variable X, and Div(Â·||Â·) represents distributional distance, measured by Kullback-Leibler (KL) divergence in this paper. Following prior work [32, 66, 64], as the model is proficiently trained, we hypothesize that IÎ¸0 can approximately replicate the distributions over both forget and retain sets [32, 66, 64], i.e., P(Xf ) â‰ˆ P(IÎ¸0 (T (Xf ))), and P(Xr) â‰ˆ P(IÎ¸0(T (Xr))). Let PX := P(IÎ¸0 (T (X))) and P Ë†X := P(IÎ¸(T (X))). Then, Eq. (2) can be simplified to: max Î¸ Div(PXf ||P Ë†Xf ), and min Î¸ Div(PXr ||P Ë†Xr), (3) where PXf and P Ë†Xf represent the output distributions of the forget set before and after unlearning respectively. Similarly, PXr and P Ë†Xr represent those for the retain set. 4 Methodology In this section, we first introduce a controllable unlearning framework for I2I generative models, which formulates unlearning as a constrained optimization with the unlearning objective as a constraint. We utilize a gradient-based method to obtain the boundaries of unlearning. Then we relax the constraint within the boundaries to derive a set of Pareto optimal solutions to fulfill varied user expectations. 4.1 Îµ-Constrained Optimization Formulation The unlearning task for I2I models is reformulated as a bi-objective optimization (Eq. (3)), with the first objective to maximize Div(PXf ||P Ë†Xf ). Nonetheless, the value of Div(Â·||Â·) can theoretically be maximized to infinity, yielding an infinite number of possible P Ë†Xf [36], consequently resulting in extremely diminished model utility. To balance unlearning completeness and model utility, we bound Div(PXf ||P Ë†Xf ) by Lemma 1. 4 Lemma 1 (Divergence Upper Bound [13]). Assuming the forget set with distribution PXf character- ized by a zero-mean and covariance matrix Î£, and a signal P Ë†Xf with the same statistical properties, the maximal KL divergence is realized when P Ë†Xf = N (0, Î£). Div(PXf ||P Ë†Xf ) â‰¤ Div(PXf ||N (0, Î£)). (4) As image normalization typically involves mean subtraction [18], we can assume PXf and P Ë†Xf follow zero-mean distributions for conciseness without sacrificing generality. Lemma 1 reveals that the upper bound of Div(PXf ||P Ë†Xf ) is achieved when P Ë†Xf âˆ¼ N (0, Î£). This suggests that maximizing Div(PXf ||P Ë†Xf ) equates to minimizing Div(P Ë†Xf ||N (0, Î£)). Consequently, we rewrite Eq. (3) as: min Î¸ Div(N (0, Î£)||P Ë†Xf ), and min Î¸ Div(PXr ||P Ë†Xr ). (5) As both terms in Eq. (5) depend on Î¸, we define f1(Î¸) := Div(N (0, Î£)||P Ë†Xf ) and f2(Î¸) := Div(PXr ||P Ë†Xr ) for conciseness. However, unlike classification models where their outputs are precisely univariate discrete distributions [35, 72], high-dimensional KL divergence calculations in I2I generative models are intractable. Thus, following [36], we adopt the L2 loss as a surrogate. Due to privacy legal requirements, unlearning objectives typically takes precedence. Thus, we set f1(Î¸) as the primary constraint and treat Eq. (5) as a Îµ-constrained optimization problem: min Î¸âˆˆRd f2(Î¸) s.t. f1(Î¸) â‰¤ Îµ, (6) where Îµ is a parameter to control the completeness of unlearning. We minimize f2(Î¸) inside the feasible set â„¦ = {Î¸ : f1(Î¸) â‰¤ Îµ}, which implies that our priority lies in unlearning the forget set rather than mitigating performance degradation on the retain set. 4.2 Solving the Îµ-Constraint Optimization To solve the Îµ-constrained optimization problem in Eq. (6), approaches such as Sequential Quadratic Programming (SQP) [44, 4], penalty function method [70], and interior point method [48] are com- monly employed. Given the extensive parameter set of the I2I generative model, we select a special variant of the SQP algorithm for its lower complexity and comparable convergence guarantee [44, 21]. Specifically, we employ a gradient-based method to solve Eq. (6), updating the parameter by Î¸t+1 â†âˆ’ Î¸t âˆ’ Âµtgt. Here, Âµt denotes the step size, and gt represents the direction of the parameter update, which is determined by solving a convex quadratic programming problem w.r.t. g (for a detailed derivation, refer to the Appendix C.1): gt = min gâˆˆRd { âˆ¥âˆ‡f2(Î¸t) âˆ’ gâˆ¥ 2 s.t. âˆ‡f1(Î¸t)âŠ¤g â‰¥ f1(Î¸t) âˆ’ Îµ} . (7) Due to the inability to obtain the effective range of Îµ in the early stages of unlearning, direct computation of f1(Î¸t) âˆ’ Îµ is not feasible. Consequently, we adjust the constraint of Eq. 7 by employing a control function Ïˆ(Î¸t) (i.e., âˆ‡f1(Î¸t) âŠ¤g â‰¥ Ïˆ(Î¸t)), which must satisfy sign(Ïˆ(Î¸t)) = sign(f1(Î¸t) âˆ’ Îµ), where sign(x) = x/|x| for x Ì¸= 0 and sign(0) = 0. This ensures that the direction of updates remains as consistent as possible before and after the substitution. Further, we provide a summary of our proposed unlearning algorithm in Algorithm 1 (see Appendix B). Assumption 1. Assume f1(Î¸) and f2(Î¸) are continuously differentiable, and the trajectory {Î¸t : t âˆˆ [0, +âˆ)} follows the continuous-time dynamics Ë™Î¸t = âˆ’gt, where gt is defined in Eq. (7) and maxtâˆˆ[0,+âˆ) Î·t < +âˆ. The convergence analysis of Algorithm 1 regarding Eq. (6) utilizes the continuous-time framework given by Ë™Î¸t = âˆ’gt, as mentioned in Assumption 1. Please refer to Theorem 2 in Appendix C.2 for further details of convergence. 4.3 A Controllable Unlearning Framework Our controllable unlearning framework consists of two phases. In Phase I, we reformulate Eq. (6) into a special form to obtain the solution for the boundaries of unlearning. In Phase II, we adjust the value Îµ within its valid range to relax the unlearning constraint and obtain the Pareto optimal solutions for controllable unlearning. This relaxation of unlearning completeness allows for a controllable trade-off between completeness and model utility, thereby catering to varied user expectations. 5 (b) Controllable Unlearning ğ’‡ğŸ âˆ— ğ’‡ğŸ âˆ— ğœ½ğ’•ğœ½ğ’•+ğŸ ğ’‡(ğœ½) ğŸ ğ’‡ğŸ(ğœ½) ğ’‡ğŸ(ğœ½) Parameter ğœ½ update direction ğœºğ’ğ’Šğ’ ğœ½ğŸ âˆ— 1 Lowest Unlearning Completeness ğ’‡ğŸ âˆ— ğ’‡ğŸ âˆ— ğœ½ğ’• ğœ½ğ’•+ğŸ ğœ½ ğ’‡(ğœ½) ğŸ ğ’‡ğŸ(ğœ½) ğ’‡ğŸ(ğœ½) ğœºğ’ğ’‚ğ’™ Parameter ğœ½ update direction ğœ½ğŸ âˆ— 2 Phase I Highest Unlearning Completeness ğœ½ ğœ½ ğ’‡ğŸ âˆ— ğœ½ğ’•ğœ½ğ’•+ğŸ ğ’‡ğŸ âˆ— ğœ½ğ’•+ğŸğœ½ğ’•ğŸ ğœº ğœºğ’ğ’Šğ’ Relax Strict ğœºğ’ğ’‚ğ’™ Case2: ğ’‡ğŸ ğœ½ğ’• â‰¤ ğœº Case1: ğ’‡ğŸ ğœ½ğ’• > ğœºğ’‡(ğœ½) Feasible Region âˆ’ğœµğ’‡ğŸ(ğœ½ğ’•) âˆ’ğœµğ’‡ğŸ(ğœ½ğ’•) âˆ’ğ’ˆğ’•Random Sample Phase II ğœº âˆ¶ Control Coefficient of Unlearning Completeness ğ’‡ğŸ(ğœ½) ğ’‡ğŸ(ğœ½) ğ‘¥ğ‘“ ğ‘¥ğ‘Ÿ Forget set Retain setğ“(ğŸ, ğœ®) ğ‘¥ğ‘› ğ¼ğœƒ(ğ’¯(ğ‘¥ğ‘“)) ğ¼ğœƒ0(ğ’¯(ğ‘¥ğ‘›)) ğ¼ğœƒ(ğ’¯(ğ‘¥ğ‘Ÿ))ğ¼ğœƒ0(ğ’¯(ğ‘¥ğ‘Ÿ)) Min ğ’‡ğŸ(ğœ½) Target model Original model Constraint ğ’‡ğŸ(ğœ½) ğœº Objective (a) ğœº-Constrained Optimization Encoder Decoder Encoder Decoder Figure 2: Pipeline of the controllable unlearning framework. (a) shows the unlearning task of the I2I generative model which is framed as a Îµ-constrained optimization problem. (b) shows that the implementation of controllable unlearning unfolds in two phases: i) initially identifying two boundary points of unlearning, necessitating a strict reduction in f1(Î¸) (or f2(Î¸)) for optimality; and ii) then locating the given Îµâ€™s Pareto optimal point, with strict reduction in f1(Î¸) when f1(Î¸t) > Îµ and permitting an increase when f1(Î¸t) â‰¤ Îµ. Phase I: Boundaries of unlearning. The boundaries of unlearning refer to the two Pareto optimal solutions with the highest and lowest degrees of unlearning completeness. To obtain the Pareto optimal solutions with the highest degrees of unlearning completeness, we reformulate Eq. (6)into the following special form: min Î¸âˆˆRd f2(Î¸) s.t. f1(Î¸) â‰¤ Îµ, where Îµ = f âˆ— 1 , and f âˆ— 1 := inf Î¸âˆˆRd f1(Î¸). (8) The solution of this optimization problem can be obtained by Algorithm 1. According to Assump- tion 1, we need to ensure that Ïˆ(Î¸) â‰¥ 0 in Eq. (8) to guarantee the same sign with f1(Î¸) âˆ’ Îµ. In this paper, we we simply define Ïˆ(Î¸) = Î±âˆ¥âˆ‡f1(Î¸)âˆ¥ Î´ with Î± > 0 and Î´ â‰¥ 1. Proposition 1 (Boundary of Pareto Set). Under Assumption 1, let f âˆ— 1 > âˆ’âˆ and f âˆ— 2 > âˆ’âˆ be the infimum of f1(Î¸), f2(Î¸), respectively. Further, let Ïˆ(Î¸) be continuous and âˆ‡f1(Î¸) be continuously differentiable. If Î¸t â†’ Î¸âˆ— and gt â†’ 0 as t â†’ +âˆ, with âˆ‡2f1(Î¸) of constant rank near Î¸âˆ— and f1(Î¸), f2(Î¸) being convex near Î¸t, then Î¸âˆ— is a Pareto optimal solution and f1(Î¸âˆ—) = f âˆ— 1 . Proof. The proof can be found in Appendix C.3. Proposition 1 ensures that the solution Î¸âˆ— 1 obtained by Algorithm 1 for solving Eq. (8) is on the boundary of the Pareto set, specifically refer to the highest degree of unlearning completeness. Meanwhile, f1(Î¸âˆ— 1) achieve the infimum of f1(Î¸). Obtaining the Pareto optimal solution with the lowest unlearning completeness is similar to the process mentioned above, with the difference of exchanging the positions of f1(Î¸) and f2(Î¸) in Eq. (8). This new problem is formulated as minÎ¸âˆˆRd f1(Î¸), s.t. f2(Î¸) â‰¤ Îµ, where Îµ = f âˆ— 2 , and f âˆ— 2 := inf Î¸âˆˆRd f2(Î¸). The solution Î¸âˆ— 2 obtained by solving this problem is another boundary the Pareto set, i.e., the Pareto optimal solution with the lowest unlearning completeness, with f2(Î¸âˆ— 2) achieving the infimum of f2(Î¸). Phase II: Controllable unlearning. To adjust the trade-off between unlearning completeness and model utility, we relax the unlearning constraint by defining f1(Î¸âˆ— 1) < Îµ < f1(Î¸âˆ— 2) in Eq. (6), where Î¸âˆ— 1 and Î¸âˆ— 2 have already been obtained in Phase I. Then we rewrite Eq. (8) for controllable unlearning: min Î¸âˆˆRd f2(Î¸) s.t. f1(Î¸) â‰¤ Îµ, where Îµ > f âˆ— 1 , and f âˆ— 1 := inf Î¸âˆˆRd f1(Î¸), (9) 6 where Îµ âˆˆ R is used to adjust the completeness of unlearning. In Phase II, according to the sign condition in Assumption 1, we simply set Ïˆ(Î¸) = Î²(f1(Î¸) âˆ’ Îµ) Î´ with Î² > 0, Î´ = 2n + 1 and n âˆˆ N. Proposition 2 (Interior of Paret Set). Under Assumption 1, let f âˆ— 2 = inf Î¸âˆˆRd f2(Î¸) > âˆ’âˆ and suptâˆˆ[0,+âˆ) Î·t = Î·max < +âˆ. If Î¸t is a stationary point with gt = 0 and Î·t < +âˆ, and both f1(Î¸) and f2(Î¸) are convex at Î¸t, then Î¸t is a Pareto optimal solution w.r.t. Îµ. Proof. The proof can be found in Appendix C.4. From Proposition 2, Eq. (9) provides a Pareto optimal solution w.r.t. Îµ. By progressively increasing Îµ from f âˆ— 1 , which is estimated by f1(Î¸âˆ— 1) in Phase I, we can trace a path of Pareto optimal solutions for different completeness of unlearning. As a result, this path offers controllable unlearning for varied user expectations. 4.4 Enhancing the Efficiency of Unlearning To enhance the efficiency of unlearning, we investigate the influence of the control function Ïˆ(Î¸) on convergence rates across different phases, as outlined in the proposition below: Proposition 3. Under Assumption 1, with f âˆ— 2 = inf Î¸âˆˆRd f2(Î¸) > âˆ’âˆ, then: 1. For Phase I, if Ïˆ(Î¸) = Î±âˆ¥âˆ‡f1(Î¸)âˆ¥ Î´ with Î± > 0 and Î´ â‰¥ 1, the convergence rates of f1(Î¸) and f2(Î¸) are O ( 1/t 1 Î´ ) and O (1/t 1 2 âˆ’ 1 2Î´ ), respectively. 2. For Phase II, if Ïˆ(Î¸) = Î²(f1(Î¸)âˆ’Îµ) Î´ with Î² > 0, Î´ = 2n+1, n âˆˆ N, and suptâˆˆ[0,+âˆ) Î·t = Î·max < +âˆ, the convergence rate of [f1(Î¸) âˆ’ Îµ]+ is O (1/t 1 Î´ ). Proof. The proof can be found in Appendix C.5. Proposition 3 demonstrates that the convergence rate depends on the exponent Î´ in Ïˆ(Î¸), where higher values of Î´ result in a faster convergence rate of f1(Î¸). However, excessively large Î´ can also lead to a slower convergence rate of f2(Î¸) and instabilities in training. To balance convergence rate and training stability, we explore various Îµ in Ïˆ(Î¸) in both phases with extensive empirical studies. The results can be found in Section 5.4. 5 Experiments 5.1 Experimental Settings We evaluate our proposed method on three mainstream I2I generative models, i.e., Masked Autoen- coder (MAE) [27], Vector Quantized Generative Adversarial Networks (VQ-GAN) [37], and diffusion probabilistic models [51]. Datasets: Following [36], we conduct experiments on the following two large-scale datasets: i) ImageNet-1K [15], from which we randomly select 200 classes, designating 100 of these as the forget set and the remaining 100 as the retain set. Each class contains 150 images, with 100 allocated for training and the remaining for validation; and ii) Places-365 [73], from which we randomly select 100 classes, designating 50 of these as the forget set and the remaining 50 as the retain set. Each class contains 5500 images, with 5000 allocated for training and the remaining 500 for validation. Baselines: We first report the performance of the original model (i.e., before unlearning) as a reference. Following [36], we set the following baselines: i) Max Loss [65, 20], which maximizes the training loss on the forget set; ii) Retain Label [33], which minimizes training loss by setting the true values of the retain samples as those of the forget set; iii) Noisy Label [25, 20], which minimizes the training loss by introducing Gaussian noise to the ground truth images of the forget set; and iv) Composite Loss [36], the State-Of-The-Art (SOTA) method, which builds upon Noisy Label by calculating the loss on the retain set and obtaining their weighted sum, thereby minimizing this weighted training loss. Evaluation metrics. We adopt three different types of metrics to comprehensively compare our method with other baselines: i) Inception Score (IS) of the generated images [53]; ii) the FrechÃ©t Inception Distance (FID) between the generated images and the ground truth images [28]; and iii) the cosine similarity between the CLIP embeddings of the 7 Table 1: Results of center cropping 50% of the images. â€˜Fâ€™ and â€˜Râ€™ stand for the forget set and retain set, respectively. Here, \"Ours\" refers to the boundary points of unlearning obtained in Phase I, that is, the solution with the highest degree of unlearning completeness. The best results are highlighted in bold, and secondary results are highlighted with underline. MAE VQ-GAN Diffusion Models IS FID CLIP IS FID CLIP IS FID CLIP F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ Original 21.59 21.83 16.28 14.87 0.88 0.88 23.74 24.06 21.80 18.17 0.78 0.85 16.90 19.65 82.12 81.51 0.89 0.91 Max Loss 15.42 16.55 129.54 87.13 0.72 0.72 19.20 21.23 23.52 43.88 0.77 0.75 17.27 18.10 95.93 108.70 0.83 0.79 Retain Label 20.74 14.14 90.62 103.72 0.71 0.73 14.44 19.24 106.01 46.25 0.47 0.75 17.02 19.08 86.10 89.18 0.87 0.83 Noisy Label 15.38 17.97 135.47 63.89 0.71 0.77 15.95 20.63 93.55 47.03 0.49 0.74 17.15 18.36 125.99 121.55 0.72 0.76 Composite Loss 13.96 15.71 149.78 74.14 0.70 0.72 14.34 21.60 103.17 37.92 0.48 0.77 14.33 17.80 149.22 98.82 0.64 0.80 Ours 12.33 17.47 154.60 68.453 0.69 0.75 13.23 22.55 139.21 26.39 0.46 0.82 11.84 18.47 165.05 95.42 0.55 0.81 Forget Set Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours Figure 3: Generated images of cropping 50% at the center of the image on VQ-GAN. From left to right, the images generated by baselines are presented. Our method results in the highest degree of unlearning completeness while maintaining a minimal reduction in model utility. generated images and the ground truth images [47]. IS evaluates the quality of the generated images independently, while the FID further measures the similarity between the generated and ground truth images. On the other hand, the distance of CLIP embeddings assesses whether the generated images still capture similar semantics. 5.2 Unlearning Performance We test our method on image extension, inpainting, and reconstruction tasks. We report the results for center uncropping (i.e., inpainting) in Tabel 1, and the others in Appendix H.1. Baseline comparison: As shown in Table 1, compared to the original model, our method retains almost the same performance on the retain set or only exhibits minor degradation. Meanwhile, there is a significant reduction in the three metrics on the forget set. In contrast, these baselines generally cannot perform well simultaneously on both the forget set and the retain set. For instance, in MAE, Composite Loss has the least performance degradation on the retain set, but its performance on the forget set is also the worst. We also observe similar findings for Max Loss in VQ-GAN. Furthermore, we provide some examples of generated images in Figure 3, and more images in Appendix F. T-SNE analysis: Following [36], we conduct a T-SNE analysis [62] to further analyze our methodâ€™s effectiveness. Using our unlearned model, we generate 50 images for both the retain set and the forget set. We then calculate the CLIP embedding vectors for these images and their corresponding ground truth images. As illustrated in Figure 4, after unlearning, the embeddings of retain set are close to that of the ground truth images, while most of the generated images on the forget set diverge significantly from the ground truth one. Unlearning robustness: We validate the performance of our controllable unlearning framework in different image generation tasks by changing the cropping patterns. The results indicate that our framework is robust to various image generation tasks and generally outperforms baselines, with detailed results provided in Appendix H.1. Moreover, we examine the unlearning effects of our controllable unlearning framework under different crop ratios. The results in Appendix H.3 demonstrate that our framework is robust to different crop ratios. Furthermore, we find that the visual effects of unlearning control are more prominent with larger crop ratios. Summary: These results validate the effectiveness of our proposed method, which is universally applicable to mainstream I2I generative models as well as a variety of image generation tasks, consistently achieving favorable outcomes across all these tasks. 8 Before Unlearning: Forget Set Before Unlearning: Retain Set Unlearning: Forget SetÃ— Unlearning: Retain SetÃ— (c) Diffusion Model(b) VQ-GAN(a) MAE Figure 4: T-SNE analysis between images generated by our method and ground truth images. Table 2: Results of center cropping 50% of the images under different unlearning completeness. â€œHighestâ€ and â€œLowestâ€ respectively represent the two boundary points of unlearning identified in Phase I. Îµ is a coefficient used to control the unlearning completeness in Phase II. MAE VQ-GAN Diffusion Models IS FID CLIP IS FID CLIP IS FID CLIP F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ Original 21.59 21.83 16.28 14.87 0.88 0.88 23.74 24.06 21.80 18.17 0.78 0.85 16.90 19.65 82.12 81.51 0.89 0.91 Highest 12.33 17.47 154.60 68.453 0.69 0.75 13.23 22.55 139.21 26.39 0.46 0.82 11.84 18.47 165.05 95.42 0.55 0.81 Îµ-25% 17.93 20.55 85.36 59.09 0.74 0.77 14.14 22.65 130.71 24.57 0.46 0.82 15.12 19.27 137.95 84.21 0.60 0.81 Îµ-50% 19.47 21.42 57.81 50.99 0.77 0.79 14.60 22.25 123.32 22.65 0.47 0.83 15.92 18.70 118.76 71.43 0.66 0.83 Îµ-75% 20.68 22.87 42.51 31.80 0.80 0.82 15.20 22.53 116.59 20.63 0.47 0.84 16.33 19.53 104.21 63.62 0.73 0.83 Lowest 21.23 22.92 31.28 25.83 0.82 0.84 15.77 22.75 109.28 20.26 0.48 0.84 16.36 20.78 90.03 52.96 0.77 0.84 5.3 Controllable Unlearning We also evaluate the controllability of our method which provides a set of solutions for varied user expectations. First, we obtain two boundary points of unlearning, thereby establishing the valid range of values for Îµ. We linearly increase the value of Îµ within this range, adding 25% of the range interval each time, to obtain optimum solutions corresponding to different Îµ values. We provide some generated images corresponding to these solutions in Figure 1. Due to the space limit, please refer to Appendix G for more examples. For results of more fine-grained control (i.e., smaller increments of the linear increase of Îµ), please refer to Appendix H.2. We verify the unlearned models at different Îµ values, and report results in Table 2. As Îµ increases, we observe a trade-off: the unlearning completeness decreases, while the generated imagesâ€™ performance on the forget set progressively improves, and, simultaneously, the performance on the retain set also improves. This observation clearly demonstrates the controllability of our proposed method, which can cater to varied user expectations. Please refer to Appendix I for additional results of the generated images and T-SNE analysis, which corroborates the above numerical results. 5.4 Unlearning Efficiency To enhance the efficiency of our controllable unlearning framework, we modify the selections of control function Ïˆ(Î¸) during various phases. Specifically, we empirically examine the convergence under these conditions to assess the frameworkâ€™s unlearning performance of efficiency. In Phase I, with the control function satisfying Ïˆ(Î¸) = Î±âˆ¥âˆ‡f1(Î¸)âˆ¥ Î´, we manipulate the value of the exponent Î´ to change the control function. Additionally, we verify the changes in the convergence rates of f1(Î¸) and f2(Î¸) under four different Î´ values across three models, with results shown in Appendix J. It is evident that f1(Î¸) and f2(Î¸) achieve an optimal balance in convergence rates when Î´ = 2, and the overall rate of convergence is fastest. In Phase II, where the control function satisfies Ïˆ(Î¸) = Î²(f1(Î¸) âˆ’ Îµ) Î´, we test the changes in the convergence rates of f1(Î¸) and f2(Î¸) for two different Î´ values on three models. To stabilize the optimization process, we scale the form of the control function (i.e., Ïˆ(Î¸) = Î²(f1(Î¸) âˆ’ Îµ)Î´âˆ¥âˆ‡f1(Î¸)âˆ¥ 2), selecting two different Î´ values, with results presented in Appendix J. It can be observed that at Î´ = 1 the overall rate of convergence was optimized. 9 6 Conclusion In this paper, we propose a controllable unlearning framework for I2I generative models to overcome the limitation of the existing methodâ€™s incapability to fulfill varied user expectations. Our approach allows for a controllable trade-off between unlearning completeness and model utility by introducing a control coefficient Îµ to control the degrees of unlearning completeness. We reformulate unlearning as a Îµ-constrained optimization problem and solve it with a gradient-based method to find two boundary points that guide the valid range for Îµ. Within this range, every chosen value of Îµ will lead to a Pareto optimal solution, addressing the existing methodâ€™s issue of lacking theoretical guarantee. Extensive experiments on two large datasets (i.e., ImageNet-1K and Places-365) across three mainstream I2I models (i.e., MAE, VQ-GAN, diffusion model) demonstrate significant advantages of our method over the SOTA methods with higher unlearning efficiency, and a controllable balance between the unlearning completeness and model utility. References [1] Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribution. The Journal of Machine Learning Research, 15(1):3563â€“3593, 2014. [2] Martin Arjovsky, Soumith Chintala, and LÃ©on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214â€“223. PMLR, 2017. [3] Samyadeep Basu, Philip Pope, and Soheil Feizi. Influence functions in deep learning are fragile. In International Conference on Learning Representations (ICLR), 2021. [4] Joseph-FrÃ©dÃ©ric Bonnans, Jean Charles Gilbert, Claude LemarÃ©chal, and Claudia A SagastizÃ¡bal. Numerical optimization: theoretical and practical aspects. Springer Science & Business Media, 2006. [5] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141â€“159. IEEE, 2021. [6] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. [7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations (ICLR), 2019. [8] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 5253â€“5270, 2023. [9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315â€“11325, 2022. [10] Vira Chankong and Yacov Y Haimes. On the characterization of noninferior solutions of the vector optimization problem. Automatica, 18(6):697â€“707, 1982. [11] Chong Chen, Fei Sun, Min Zhang, and Bolin Ding. Recommendation unlearning. In Proceedings of the ACM Web Conference 2022, pages 2768â€“2777, 2022. [12] Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen. Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance. Advances in Neural Information Processing Systems, 36, 2024. [13] T.M. Cover and J.A. Thomas. Elements of Information Theory. Number 12. Wiley, 2012. [14] Stephen Dempe, Nguyen Dinh, and Joydeep Dutta. Optimality conditions for a simple convex bilevel programming problem. Variational Analysis and Generalized Differentiation in Optimization and Control: In Honor of Boris S. Mordukhovich, pages 149â€“161, 2010. [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (CVPR), pages 248â€“255. Ieee, 2009. [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780â€“8794, 2021. [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. 10 [18] Mohamed Elasri, Omar Elharrouss, Somaya Al-Maadeed, and Hamid Tairi. Image generation: A review. Neural Processing Letters, 54(5):4609â€“4646, 2022. [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 12873â€“12883, 2021. [20] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2426â€“2436, 2023. [21] Philip E Gill and Elizabeth Wong. Sequential quadratic programming methods. In Mixed integer nonlinear programming, pages 147â€“224. Springer, 2011. [22] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304â€“9312, 2020. [23] Chengyue Gong, Xingchao Liu, and Qiang Liu. Automatic and harmless regularization with constrained and lexicographic optimization: A dynamic barrier approach. Advances in Neural Information Processing Systems, 34:29630â€“29642, 2021. [24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [25] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11516â€“11524, 2021. [26] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017. [27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 16000â€“16009, 2022. [28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840â€“6851, 2020. [30] Robert Janin. Directional derivative of the marginal function in nonlinear programming. Springer, 1984. [31] Il Yong Kim and Oliver L De Weck. Adaptive weighted-sum method for bi-objective optimization: Pareto front generation. Structural and multidisciplinary optimization, 29:149â€“158, 2005. [32] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations and TrendsÂ® in Machine Learning, 12(4):307â€“392, 2019. [33] Zhifeng Kong and Kamalika Chaudhuri. Data redaction from conditional generative models. arXiv preprint arXiv:2305.11351, 2023. [34] Aditya Kuppa, Lamine Aouad, and Nhien-An Le-Khac. Towards improving privacy of synthetic datasets. In Annual Privacy Forum, pages 106â€“119. Springer, 2021. [35] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. Advances in Neural Information Processing Systems, 36, 2024. [36] Guihong Li, Hsiang Hsu, Radu Marculescu, et al. Machine unlearning for image-to-image generative models. In International Conference on Learning Representations (ICLR), 2024. [37] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2142â€“2152, 2023. [38] Yuyuan Li, Chaochao Chen, Yizhao Zhang, Weiming Liu, Lingjuan Lyu, Xiaolin Zheng, Dan Meng, and Jun Wang. Ultrare: Enhancing receraser for recommendation unlearning via error decomposition. Advances in Neural Information Processing Systems, 36, 2024. [39] Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Junlin Liu, and Jun Wang. Making recommender systems forget: Learning and unlearning for erasable recommendation. Knowledge-Based Systems, 283:111124, 2024. [40] Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Yizhao Zhang, Biao Gong, Jun Wang, and Linxun Chen. Selective and collaborative influence function for efficient recommendation unlearning. Expert Systems with Applications, 234:121025, 2023. 11 [41] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. Advances in neural information processing systems, 32, 2019. [42] Kaisa Miettinen. Nonlinear multiobjective optimization, volume 12. Springer Science & Business Media, 1999. [43] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022. [44] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer. [45] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. [46] Panos M Pardalos, Antanas Å½ilinskas, Julius Å½ilinskas, et al. Non-convex multi-objective optimization. Springer, 2017. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748â€“8763. PMLR, 2021. [48] James Renegar. A mathematical view of interior-point methods in convex optimization. SIAM, 2001. [49] Stephen M Robinson. Perturbed kuhn-tucker points and rates of convergence for a class of nonlinear- programming algorithms. Mathematical programming, 7:1â€“16, 1974. [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684â€“10695, 2022. [51] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, pages 1â€“10, 2022. [52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to- image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479â€“36494, 2022. [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [54] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations (ICLR), 2022. [55] Katja Schwarz, Yiyi Liao, and Andreas Geiger. On the frequency bias of generative models. Advances in Neural Information Processing Systems, 34:18126â€“18136, 2021. [56] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:18075â€“18086, 2021. [57] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438â€“12448, 2020. [58] Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems. SIAM review, 19(4):634â€“662, 1977. [59] Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron Maschinot, David Belanger, Ce Liu, and William T Freeman. Boundless: Generative adversarial networks for image extension. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10521â€“10530, 2019. [60] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:38274â€“38290, 2022. [61] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [62] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [63] Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 10(3152676):10â€“5555, 2017. [64] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22532â€“ 22541, 2023. 12 [65] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. In 30th Annual Network and Distributed System Security Symposium NDSS, 2023. [66] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. IEEE transactions on pattern analysis and machine intelligence, 45(3):3121â€“3138, 2022. [67] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. Machine unlearning: A survey. Association for Computing Machinery, 56:36, 2023. [68] Haonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua Li, and Xiaodong Lin. Arcane: An efficient architecture for exact machine unlearning. In IJCAI, volume 6, page 19, 2022. [69] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1â€“39, 2023. [70] Ã–zgÃ¼r Yeniay. Penalty function methods for constrained optimization with genetic algorithms. Mathematical and computational Applications, 10(1):45â€“56, 2005. [71] Lu Yu, Joost van de Weijer, et al. Deepi2i: Enabling deep hierarchical image-to-image translation by transferring from gans. Advances in Neural Information Processing Systems, 33:11803â€“11815, 2020. [72] Xulong Zhang, Jianzong Wang, Ning Cheng, Yifu Sun, Chuanyao Zhang, and Jing Xiao. Machine unlearning methodology based on stochastic teacher network. In International Conference on Advanced Data Mining and Applications, pages 250â€“261. Springer, 2023. [73] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452â€“1464, 2017. [74] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223â€“2232, 2017. References [1] Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating distribution. The Journal of Machine Learning Research, 15(1):3563â€“3593, 2014. [2] Martin Arjovsky, Soumith Chintala, and LÃ©on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214â€“223. PMLR, 2017. [3] Samyadeep Basu, Philip Pope, and Soheil Feizi. Influence functions in deep learning are fragile. In International Conference on Learning Representations (ICLR), 2021. [4] Joseph-FrÃ©dÃ©ric Bonnans, Jean Charles Gilbert, Claude LemarÃ©chal, and Claudia A SagastizÃ¡bal. Numerical optimization: theoretical and practical aspects. Springer Science & Business Media, 2006. [5] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141â€“159. IEEE, 2021. [6] Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004. [7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations (ICLR), 2019. [8] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 5253â€“5270, 2023. [9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315â€“11325, 2022. [10] Vira Chankong and Yacov Y Haimes. On the characterization of noninferior solutions of the vector optimization problem. Automatica, 18(6):697â€“707, 1982. [11] Chong Chen, Fei Sun, Min Zhang, and Bolin Ding. Recommendation unlearning. In Proceedings of the ACM Web Conference 2022, pages 2768â€“2777, 2022. [12] Lisha Chen, Heshan Fernando, Yiming Ying, and Tianyi Chen. Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance. Advances in Neural Information Processing Systems, 36, 2024. [13] T.M. Cover and J.A. Thomas. Elements of Information Theory. Number 12. Wiley, 2012. 13 [14] Stephen Dempe, Nguyen Dinh, and Joydeep Dutta. Optimality conditions for a simple convex bilevel programming problem. Variational Analysis and Generalized Differentiation in Optimization and Control: In Honor of Boris S. Mordukhovich, pages 149â€“161, 2010. [15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (CVPR), pages 248â€“255. Ieee, 2009. [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780â€“8794, 2021. [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021. [18] Mohamed Elasri, Omar Elharrouss, Somaya Al-Maadeed, and Hamid Tairi. Image generation: A review. Neural Processing Letters, 54(5):4609â€“4646, 2022. [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 12873â€“12883, 2021. [20] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2426â€“2436, 2023. [21] Philip E Gill and Elizabeth Wong. Sequential quadratic programming methods. In Mixed integer nonlinear programming, pages 147â€“224. Springer, 2011. [22] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304â€“9312, 2020. [23] Chengyue Gong, Xingchao Liu, and Qiang Liu. Automatic and harmless regularization with constrained and lexicographic optimization: A dynamic barrier approach. Advances in Neural Information Processing Systems, 34:29630â€“29642, 2021. [24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [25] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 11516â€“11524, 2021. [26] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017. [27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pages 16000â€“16009, 2022. [28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840â€“6851, 2020. [30] Robert Janin. Directional derivative of the marginal function in nonlinear programming. Springer, 1984. [31] Il Yong Kim and Oliver L De Weck. Adaptive weighted-sum method for bi-objective optimization: Pareto front generation. Structural and multidisciplinary optimization, 29:149â€“158, 2005. [32] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations and TrendsÂ® in Machine Learning, 12(4):307â€“392, 2019. [33] Zhifeng Kong and Kamalika Chaudhuri. Data redaction from conditional generative models. arXiv preprint arXiv:2305.11351, 2023. [34] Aditya Kuppa, Lamine Aouad, and Nhien-An Le-Khac. Towards improving privacy of synthetic datasets. In Annual Privacy Forum, pages 106â€“119. Springer, 2021. [35] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. Advances in Neural Information Processing Systems, 36, 2024. [36] Guihong Li, Hsiang Hsu, Radu Marculescu, et al. Machine unlearning for image-to-image generative models. In International Conference on Learning Representations (ICLR), 2024. 14 [37] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2142â€“2152, 2023. [38] Yuyuan Li, Chaochao Chen, Yizhao Zhang, Weiming Liu, Lingjuan Lyu, Xiaolin Zheng, Dan Meng, and Jun Wang. Ultrare: Enhancing receraser for recommendation unlearning via error decomposition. Advances in Neural Information Processing Systems, 36, 2024. [39] Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Junlin Liu, and Jun Wang. Making recommender systems forget: Learning and unlearning for erasable recommendation. Knowledge-Based Systems, 283:111124, 2024. [40] Yuyuan Li, Chaochao Chen, Xiaolin Zheng, Yizhao Zhang, Biao Gong, Jun Wang, and Linxun Chen. Selective and collaborative influence function for efficient recommendation unlearning. Expert Systems with Applications, 234:121025, 2023. [41] Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning. Advances in neural information processing systems, 32, 2019. [42] Kaisa Miettinen. Nonlinear multiobjective optimization, volume 12. Springer Science & Business Media, 1999. [43] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022. [44] Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer. [45] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. [46] Panos M Pardalos, Antanas Å½ilinskas, Julius Å½ilinskas, et al. Non-convex multi-objective optimization. Springer, 2017. [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748â€“8763. PMLR, 2021. [48] James Renegar. A mathematical view of interior-point methods in convex optimization. SIAM, 2001. [49] Stephen M Robinson. Perturbed kuhn-tucker points and rates of convergence for a class of nonlinear- programming algorithms. Mathematical programming, 7:1â€“16, 1974. [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684â€“10695, 2022. [51] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 conference proceedings, pages 1â€“10, 2022. [52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to- image diffusion models with deep language understanding. Advances in neural information processing systems, 35:36479â€“36494, 2022. [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [54] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations (ICLR), 2022. [55] Katja Schwarz, Yiyi Liao, and Andreas Geiger. On the frequency bias of generative models. Advances in Neural Information Processing Systems, 34:18126â€“18136, 2021. [56] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:18075â€“18086, 2021. [57] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438â€“12448, 2020. [58] Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares problems. SIAM review, 19(4):634â€“662, 1977. [59] Piotr Teterwak, Aaron Sarna, Dilip Krishnan, Aaron Maschinot, David Belanger, Ce Liu, and William T Freeman. Boundless: Generative adversarial networks for image extension. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10521â€“10530, 2019. 15 [60] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Advances in Neural Information Processing Systems, 35:38274â€“38290, 2022. [61] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [62] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. [63] Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 10(3152676):10â€“5555, 2017. [64] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22532â€“ 22541, 2023. [65] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. In 30th Annual Network and Distributed System Security Symposium NDSS, 2023. [66] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. IEEE transactions on pattern analysis and machine intelligence, 45(3):3121â€“3138, 2022. [67] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. Machine unlearning: A survey. Association for Computing Machinery, 56:36, 2023. [68] Haonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua Li, and Xiaodong Lin. Arcane: An efficient architecture for exact machine unlearning. In IJCAI, volume 6, page 19, 2022. [69] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):1â€“39, 2023. [70] Ã–zgÃ¼r Yeniay. Penalty function methods for constrained optimization with genetic algorithms. Mathematical and computational Applications, 10(1):45â€“56, 2005. [71] Lu Yu, Joost van de Weijer, et al. Deepi2i: Enabling deep hierarchical image-to-image translation by transferring from gans. Advances in Neural Information Processing Systems, 33:11803â€“11815, 2020. [72] Xulong Zhang, Jianzong Wang, Ning Cheng, Yifu Sun, Chuanyao Zhang, and Jing Xiao. Machine unlearning methodology based on stochastic teacher network. In International Conference on Advanced Data Mining and Applications, pages 250â€“261. Springer, 2023. [73] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452â€“1464, 2017. [74] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223â€“2232, 2017. A Broader Impacts and Limitations The abundance of training data not only enhances the performance of generative models but also introduces issues with privacy, unfairness, and bias. Our proposed controllable unlearning framework offers a viable solution to these issues. Our proposed framework is not limited to unlearning in I2I generation models but can be easily extended to other types of generative models, including text-to-image and text-to-text models. However, the unlearning framework presented herein has certain limitations. Note that Propositions 1 and 2 in Section 4 assume the convexity of the objective function and the feasible set. This assumption is essential to guarantee that the yielded solutions are Pareto optimal. In cases where the objective function and the feasible set are non-convex, the solutions obtained from solving Eq. (6) can only be guaranteed to be weakly Pareto optimal [42] or Pareto stability [12]. B Algorithm Procedure of Controllable Unlearning Framework To address the Îµ-constrained optimization problem Eq. (6), we employ a gradient-based method. Specifically, the detailed algorithmic procedure of our controllable unlearning framework is as follows. 16 Algorithm 1 Gradient-based Optimization Method Require: Original model IÎ¸0, forget set Df , retain set Dr, control function Ïˆ(Î¸), step size Âµ, covariance matrix Î£, numerical stability variable Ï– = 1e âˆ’ 7. 1: Initial: Initialize t = 0, IÎ¸t = IÎ¸0; 2: for t = 0 to T âˆ’ 1 do 3: Sample {xf }, {xr} and {xn} from Df , Dr and N (0, Îµ) respectively, ensuring that |{xf }| = |{xr}| = |{xn}|; 4: Compute loss: 5: f1(Î¸t) = âˆ¥IÎ¸t(T (Df )) âˆ’ IÎ¸0 (T (xn))âˆ¥2 6: f2(Î¸t) = âˆ¥IÎ¸t(T (Dr)) âˆ’ IÎ¸0 (T (Dr))âˆ¥2 7: Compute gradient: âˆ‡f1(Î¸t), âˆ‡f2(Î¸t); 8: Compute the solution to the dual problem of Eq. (7): Î·t = max ( Ïˆ(Î¸t)âˆ’âˆ‡f2(Î¸t) âŠ¤âˆ‡f1(Î¸t) âˆ¥âˆ‡f1(Î¸t)âˆ¥2+Ï– , 0); 9: Compute parameter update direction: gt = âˆ‡f2(Î¸t) + Î·tâˆ‡f1(Î¸t); 10: Update the parameter of the target model IÎ¸t+1 : Î¸t+1 â†âˆ’ Î¸t âˆ’ Âµtgt; 11: end for 12: Return Unlearned model IÎ¸T ; C Theoretical Validation C.1 Proof of Equivalence Given the original problem min Î¸âˆˆRd f2(Î¸) s.t. f1(Î¸) â‰¤ Îµ, (10) which is a constrained nonlinear programming problem. To solve it, we formulate its Lagrangian equation: L(Î¸, Î») = f2(Î¸) + Î»(f1(Î¸) âˆ’ Îµ). (11) Further, we derive the KKT conditions for Eq.11: âˆ‡Î¸L(Î¸âˆ—, Î»âˆ—) = âˆ‡f2(Î¸âˆ—) + Î»âˆ—âˆ‡[f1(Î¸âˆ—) âˆ’ Îµ] = 0 f1(Î¸âˆ—) âˆ’ Îµ â‰¤ 0 Î»âˆ— â‰¥ 0 Î»âˆ—(f1(Î¸âˆ—) âˆ’ Îµ) = 0. (12) The standard Newtonâ€™s Method searches for the solution LÎ¸(Î¸, Î») = 0 by iterating the following equation: [ Î¸t+1 Î»t+1 ] = [ Î¸t Î»t ] âˆ’ [ âˆ‡2 Î¸L âˆ‡[f1(Î¸t) âˆ’ Îµ] âˆ‡[f1(Î¸t) âˆ’ Îµ] T 0 ]âˆ’1 ï¸¸ ï¸·ï¸· ï¸¸ âˆ‡2Lâˆ’1 [ âˆ‡Î¸L(Î¸t, Î»t) f1(Î¸t) âˆ’ Îµ ] ï¸¸ ï¸·ï¸· ï¸¸ âˆ‡L , (13) where âˆ‡2 Î¸ denotes the Hessian matrix. However, the Newton step gt = (âˆ‡2 Î¸L) âˆ’1âˆ‡Î¸L cannot be calculated directly and we also have other optimal condition in Eq. 12 introduced by the inequality constraints. Instead, the basic sequential quadratic programming algorithm defines an appropriate search direction gt at an iterate (Î¸t, Î»t), as a solution to the quadratic programming subproblem. Denoting by gt = (gÎ¸ t , gÎ» t ) the change in the variables at the current point (Î¸t, Î»t), where (gÎ¸ t , gÎ» t ) solve the Newtonâ€“KKT system [44]: âˆ‡Î¸L(Î¸t, Î»t)gÎ¸ t + âˆ‡[f1(Î¸t) âˆ’ Îµ]gÎ» t = âˆ’âˆ‡Î¸L(Î¸t, Î»t) f1(Î¸t) âˆ’ Îµ + âˆ‡[f1(Î¸t) âˆ’ Îµ]gÎ¸ t â‰¤ 0 Î»t + gÎ» t â‰¥ 0 (Î»t + gÎ» t )(f1(Î¸âˆ—) âˆ’ Îµ + âˆ‡[f1(Î¸t) âˆ’ Îµ]gÎ¸ t ) = 0. (14) Denoting by Î»t+1 = Î»t + gÎ» t , we have âˆ‡Î¸L(Î¸t, Î»t)gÎ¸ t + âˆ‡[f1(Î¸t) âˆ’ Îµ]Î»t+1 = âˆ’âˆ‡f2(Î¸t) f1(Î¸t) âˆ’ Îµ + âˆ‡[f1(Î¸t) âˆ’ Îµ]gÎ¸ t â‰¤ 0 Î»t+1 â‰¥ 0 Î»t+1( f1(Î¸âˆ—) âˆ’ Îµ + âˆ‡[f1(Î¸t) âˆ’ Îµ]gÎ¸ t ) = 0. (15) 17 It is easy to check that Eq. 15 is the optimality system of the following quadratic problem (QP) min g f2 (Î¸t) + âˆ‡f2 (Î¸t)âŠ¤ g + 1 2 gâŠ¤âˆ‡2 Î¸L (Î¸t, Î»t) g f1(Î¸t) âˆ’ Îµ + âˆ‡[f1(Î¸t) âˆ’ Îµ]g â‰¤ 0. (16) Setting gÎ¸ t = g, the KKT conditions for Eq. 16 are consistent with the constraints specified in Eq. 15. Further, according to Theorem 1, the optimal solution for Eq. 16, when approaching the optimal solution of the original Problem (i.e., Eq. 10), satisfies the KKT conditions of Eq. 10. Considering that the models discussed in this paper are all deep neural networks, based on previous studies [23], the Hessian matrix can be approximated as an identity matrix. Additionally, for consistency with the main text (i.e., Î¸t+1 â†âˆ’ Î¸t âˆ’ Âµtgt), setting g = âˆ’gt yields the following form: min gt âˆ‡f2 (Î¸t)âŠ¤ âˆ‡f2 (Î¸t) âˆ’ 2âˆ‡f2 (Î¸t)âŠ¤ gt + gâŠ¤ t gt âˆ‡f1(Î¸t)gt â‰¥ f1(Î¸t) âˆ’ Îµ. (17) Theorem 1. Theorem of Robinson [49]. Suppose that Î¸âˆ— is a local solution of Eq. 10 at which the KKT conditions are satisfied for some Î»âˆ—. Suppose, too, that the linear independence constraint qualification (LICQ), the strict complementarity condition, and the second-order sufficient conditions hold at (Î¸âˆ—, Î»âˆ—). Then if (Î¸t, Î»t) is sufficiently close to (Î¸âˆ—, Î»âˆ—), there is a local solution of the subproblem Eq. 16 whose active set At is the same as the active set A (Î¸âˆ—) of the nonlinear program Eq. 10 at Î¸âˆ—. C.2 Basic Components Before exploring the proofs of Propositions 1 and 2, it is essential to define some fundamental concepts and lemmas. This references some works [6, 46, 23] mentioned earlier; for the sake of readability, we will reiterate them here. Penalty Function. An alternative method to evaluate the optimality of Algorithm 1 involves the L1 penalty function given by: PÎ¾(Î¸) = f2(Î¸) + Î¾[f1(Î¸) âˆ’ Îµ]+, (18) where Î¾ > 0 is a scaling coefficient. The minima of Eq. (18) align with the solutions to Eq. (6) for sufficiently large values of Î¾ [44]. First-order KKT Condition and KKT Function. We revisit the first-order KKT condition [44] for the constrained optimization described in Eq. (9). Assume Î¸âˆ— is a local optimum with continuously differentiable f1(Î¸) and f2(Î¸), and âˆ¥âˆ‡f1(Î¸âˆ—)âˆ¥ Ì¸= 0. There exists a Lagrange multiplier Ï‰âˆ— âˆˆ [0, +âˆ) such that: âˆ‡f2(Î¸âˆ—) + Ï‰âˆ—âˆ‡f1(Î¸âˆ—) = 0, f1(Î¸âˆ—) â‰¤ Îµ, Ï‰âˆ—(f1(Î¸âˆ—) âˆ’ Îµ) = 0. (19) This setup highlights the importance of âˆ¥âˆ‡f1(Î¸âˆ—)âˆ¥ Ì¸= 0 as a constraint qualification condition. Utilizing Algorithm 1 for Eq. (9), and for Î· â‰¥ 0, the KKT function [23] to verify the first-order KKT condition is defined as: KÏ„ (Î¸t, Î·t) = âˆ¥âˆ‡f2(Î¸t) + Î·tâˆ‡f1(Î¸t)âˆ¥ 2 + Ï„ [Ïˆ(Î¸t)]+ + Î·t[âˆ’Ïˆ(Î¸t)]+, (20) where Ï„ > 0, and [x]+ = max(x, 0). It is clear that KÏ„ (Î¸t, Î·t) â‰¥ 0 for all Î¸t âˆˆ Rd and Î·t â‰¥ 0, achieving KÏ„ (Î¸t, Î·t) = 0 iff (Î¸t, Î·t) satisfies the first-order KKT condition. Second-order KKT Condition and KKT Function In the context of Algorithm 1 applied to Eq. (8), we expect that âˆ¥âˆ‡f1(Î¸t)âˆ¥ approaches zero, leading to Î·t potentially diverging to infinity. This scenario indicates a violation of the first-order KKT condition, potentially interpreted as Î·âˆ— = +âˆ. While the first-order condition (Eq. (19)) is inadequate, the second-order KKT conditions involving the Hessian âˆ‡ 2f1(Î¸) are applicable [14]. Consider the relaxed form of Eq. (8) as: min Î¸âˆˆRd f2(Î¸) s.t. âˆ‡f1(Î¸) = 0. (21) If Î¸âˆ— is a local minimum of Eq. (8), it coincides with a local minimum of Eq. (21). Assuming f2(Î¸) and âˆ‡f1(Î¸) are continuously differentiable, with the Hessian âˆ‡ 2f1(Î¸) maintaining constant rank near Î¸âˆ— [30], the first-order KKT condition for Eq. (21) can be formulated. There exists a vector Ï‰âˆ— âˆˆ Rd such that: âˆ‡f2 (Î¸âˆ—) + âˆ‡ 2f1 (Î¸âˆ—) Ï‰âˆ— = 0. (22) This condition implies that âˆ‡f2(Î¸âˆ—) is orthogonal to the null space of âˆ‡ 2f1(Î¸âˆ—), defining the tangent space of the stationary manifold {Î¸ : âˆ‡f1(Î¸) = 0} for f1(Î¸). 18 For verifying local optimality under the constraints of Eq. (8) where Ïˆ(Î¸) â‰¥ 0, the KKT function is proposed as: KÏ„ (Î¸t, Î·t) = âˆ¥âˆ‡f2 (Î¸t) + Î·tâˆ‡f1 (Î¸t)âˆ¥ 2 + Ï„ Ïˆ (Î¸t) , (23) where Ïˆ(Î¸t) = 0 asserts that Î¸t is stationary for f1(Î¸), and âˆ¥âˆ‡f2(Î¸t) + Î·tâˆ‡f1(Î¸t)âˆ¥ = 0 signifies local optimal- ity with respect to f2(Î¸), aligning with the KKT condition for the relaxed problem minÎ¸{f2(Î¸) s.t. f1(Î¸) â‰¤ Îµt}, with Îµt = f1(Î¸t). In the analysis of Algorithm 1, a fundamental theorem concerning the behavior of the penalty function PÎ¾(Î¸) and the KKT function KÏ„ (Î¸, Î·), given in Eqs. (20) and (23), is essential for understanding the algorithmâ€™s convergence and feasibility characteristics. This lemma is stated as follows: Theorem 2. Theorem 3.2 of Gong et al. (2021) [23]. Assume Assumption 1 holds, for any Î¾ â‰¥ 0, we have d dt PÎ¾(Î¸t) â‰¤ âˆ’KÎ¾âˆ’Î·t (Î¸t, Î·t), âˆ€t âˆˆ [0, +âˆ). (24) This equation indicates that PÎ¾(Î¸t) is non-increasing w.r.t. time t provided that KÎ¾âˆ’Î·t (Î¸t, Î·t) â‰¥ 0. This condition is satisfied if Î¾ is sufficiently large such that Î¾ âˆ’ Î·t â‰¥ 0, or when the constraint is met, i.e., f1(Î¸t) â‰¤ Îµ, ensuring [Ïˆ(Î¸t)]+ = 0. This lemma facilitates further deductions about the behavior of the algorithm under different settings of the parameter Î¾. For instance, setting Î¾ â†’ +âˆ allows us to demonstrate that the constraint [f1(Î¸t) âˆ’ Îµ]+ is non-increasing w.r.t. time t. This implies that f1(Î¸t) is decreasing w.r.t. time t outside the feasible region, and once Î¸t enters the feasible region, it remains therein. Conversely, setting Î¾ = 0 reveals that f2(Î¸t) monotonically decreases w.r.t. time t within the feasible set, progressing towards a KKT point. These observations are critical for understanding both the feasibility and optimality properties of Algorithm 1 under different operational scenarios. Proposition 4. Under Assumption 1, the following two propositions hold: 1. For any time t âˆˆ [0, +âˆ), minsâˆˆ[0,t][Ïˆ(Î¸s)]+ = O( 1 t ) holds. 2. If Ïˆ(Î¸) â‰¥ 0 holds, then minsâˆˆ[0,t] Ïˆ (Î¸s) â‰¤ 1 t (f1 (Î¸0) âˆ’ f âˆ— 1 ) for any time t âˆˆ [0, +âˆ). Proof of Proposition 4-1. At each time point t âˆˆ [0, +âˆ), dividing both sides of Eq. (24) by Î¾ > 0 and taking Î¾ â†’ +âˆ gives d dt [f1 (Î¸t) âˆ’ Îµ]+ â‰¤ âˆ’ [Ïˆ (Î¸t)]+ â‰¤ 0. Integrating this on time interval [0, t] gives min sâˆˆ[0,t] [Ïˆ (Î¸s)]+ â‰¤ 1 t âˆ« t 0 [Ïˆ (Î¸s)]+ ds â‰¤ 1 t ( [f1 (Î¸0) âˆ’ Îµ]+ âˆ’ [f1 (Î¸t) âˆ’ Îµ]+) â‰¤ 1 t [f1 (Î¸0) âˆ’ Îµ]+ . (25) min sâˆˆ[0,t] [Ïˆ (Î¸s)]+ â‰¤ 1 t âˆ« t 0 [Ïˆ (Î¸s)]+ ds â‰¤ 1 t ( [f1 (Î¸0) âˆ’ Îµ]+ âˆ’ [f1 (Î¸t) âˆ’ Îµ]+) â‰¤ 1 t [f1 (Î¸0) âˆ’ Îµ]+ . This implies that minsâˆˆ[0,t][Ïˆ(Î¸s)]+ = O( 1 t ). Proof of Proposition 4-2. Let f âˆ— 1 = inf Î¸âˆˆRd f1(Î¸) and f âˆ— 2 = inf Î¸âˆˆRd f2(Î¸). Since Ïˆ(Î¸) â‰¥ 0, by substituting Eq. (23) into Eq. (24), we have for any Î¾ â‰¥ 0, d dt ( f2 (Î¸t) + Î¾ [f1 (Î¸t) âˆ’ Îµ]+) â‰¤ âˆ’âˆ¥âˆ‡f2 (Î¸t) + Î·tâˆ‡f1 (Î¸t)âˆ¥ 2 âˆ’ (Î¾ âˆ’ Î·t) Ïˆ (Î¸t) , âˆ€t âˆˆ [0, +âˆ). Integrating both sides from 0 to t yields: âˆ« t 0 ( âˆ¥âˆ‡f2 (Î¸s) + Î·sâˆ‡f1 (Î¸s)âˆ¥ 2 + (Î¾ âˆ’ Î·s) Ïˆ (Î¸s)) ds â‰¤ (f2 (Î¸0) âˆ’ f2 (Î¸t)) + Î¾ [f1(Î¸0) âˆ’ Îµ]+ âˆ’ Î¾ [f1(Î¸t) âˆ’ Îµ]+ â‰¤ (f2 (Î¸0) âˆ’ f âˆ— 2 ) + Î¾ (f1 (Î¸0) âˆ’ f âˆ— 1 ) . (26) Taking Î¾ â†’ +âˆ in Eq. (26) gives âˆ« t 0 Ïˆ (Î¸s) ds â‰¤ f1 (Î¸0) âˆ’ f âˆ— 1 , (27) which implies minsâˆˆ[0,t] Ïˆ (Î¸s) â‰¤ 1 t âˆ« t 0 Ïˆ (Î¸s) ds â‰¤ 1 t (f1 (Î¸0) âˆ’ f âˆ— 1 ). 19 C.3 Proof of Proposition 1 Proof of Proposition 1. As Î¸t converges to Î¸âˆ— for t â†’ +âˆ and given the continuity of Ïˆ(Î¸) and âˆ‡f1(Î¸), it follows that limtâ†’+âˆ Ïˆ (Î¸t) = Ïˆ (Î¸âˆ—), and limtâ†’+âˆ âˆ¥âˆ‡f1 (Î¸t)âˆ¥ = âˆ¥âˆ‡f1 (Î¸âˆ—)âˆ¥. Given Ïˆ(Î¸) â‰¥ 0 and Îµ = f âˆ— 1 , Eq. (26) establishes that âˆ« +âˆ 0 Ïˆ (Î¸t) dt â‰¤ f1 (Î¸0) âˆ’ f âˆ— 1 < +âˆ. Consequently, limtâ†’+âˆ Ïˆ (Î¸t) = Ïˆ (Î¸âˆ—) = 0. Given Î¸âˆ— as a limit point of {Î¸t}, there exists an increasing sequence {tn : n = 1, 2, Â· Â· Â· } such that tn â†’ +âˆ and Î¸tn â†’ Î¸âˆ— as n â†’ +âˆ. The continuity of Ïˆ(Î¸) and âˆ‡f1(Î¸) ensures limnâ†’+âˆ Ïˆ (Î¸tn ) = Ïˆ (Î¸âˆ—) = 0, and limnâ†’+âˆ âˆ¥âˆ‡f1 (Î¸tn )âˆ¥ = âˆ¥âˆ‡f1 (Î¸âˆ—)âˆ¥. Since Ïˆ (Î¸âˆ—) = 0 and the sign condition of Ïˆ(Î¸), it implies sign (f1 (Î¸âˆ—) âˆ’ f âˆ— 1 ) = sign (Ïˆ (Î¸âˆ—)) = 0. There- fore f1 (Î¸âˆ—) = f âˆ— 1 and Î¸âˆ— is a minimum point of f1(Î¸). This gives limnâ†’+âˆ âˆ¥âˆ‡f1 (Î¸tn )âˆ¥ = âˆ¥âˆ‡f1 (Î¸âˆ—)âˆ¥ = 0. Given limtâ†’+âˆ gt = 0, we deduce that limtâ†’+âˆ âˆ¥âˆ‡f2 (Î¸t) + Î·tâˆ‡f1 (Î¸t)âˆ¥ = limtâ†’+âˆ âˆ¥gtâˆ¥ = 0. Addi- tionally, employing Ïˆ(Î¸) â‰¥ 0, Eq (23) implies limtâ†’+âˆ KÏ„ (Î¸t, Î·t) = 0 for some Ï„ > 0. Combining limtâ†’+âˆ âˆ¥âˆ‡f2 (Î¸t) + Î·tâˆ‡f1 (Î¸t)âˆ¥ = 0 and âˆ‡f1 (Î¸âˆ—) = limnâ†’+âˆ âˆ‡f1 (Î¸tn ) = 0, we can derive âˆ¥âˆ‡f2 (Î¸t) + Î·tâˆ‡f1 (Î¸t)âˆ¥ = âˆ¥âˆ‡f2 (Î¸t) + Î·t (âˆ‡f1 (Î¸t) âˆ’ âˆ‡f1 (Î¸âˆ—))âˆ¥ = âˆ¥ âˆ¥âˆ‡f2 (Î¸t) + Î·tâˆ‡2f1 ( Î¸â€² t) (Î¸t âˆ’ Î¸âˆ—)âˆ¥ âˆ¥ = âˆ¥ âˆ¥âˆ‡f2 (Î¸t) + âˆ‡ 2f1 ( Î¸â€² t) Ï‰â€² tâˆ¥ âˆ¥ . where Î¸â€² t is a convex combination of Î¸t and Î¸âˆ—, and we defined Ï‰â€² t = Î·t (Î¸t âˆ’ Î¸âˆ—). Define Ï‰t = ( âˆ‡2f1 (Î¸â€² t) )+ âˆ‡f2 (Î¸t), where ( âˆ‡ 2f1 (Î¸â€² t))+ denotes the Moore-Penrose pseudo-inverse of matrix âˆ‡2f1 (Î¸â€² t), which satisfies that Ï‰t = arg min Ï‰âˆˆRd { âˆ¥Ï‰âˆ¥ s.t. Ï‰ âˆˆ arg min w âˆ¥ âˆ¥âˆ‡f2 (Î¸t) + âˆ‡2f1 ( Î¸â€² t) Ï‰âˆ¥ âˆ¥ } . It follows that âˆ¥ âˆ¥âˆ‡f2 (Î¸t) + âˆ‡2f1 ( Î¸â€² t) Ï‰tâˆ¥ âˆ¥ â‰¤ âˆ¥ âˆ¥âˆ‡f2 (Î¸t) + âˆ‡ 2f1 (Î¸t) Ï‰â€² tâˆ¥ âˆ¥ = âˆ¥âˆ‡f2 (Î¸t) + Î·tâˆ‡f1 (Î¸t)âˆ¥ . Given âˆ¥âˆ‡f2 (Î¸tn ) + Î·tn âˆ‡f1 (Î¸tn )âˆ¥ â†’ 0 as n â†’ +âˆ, we have âˆ¥ âˆ¥âˆ‡f2 (Î¸tn ) + âˆ‡ 2f1 (Î¸â€² tn ) Ï‰tn âˆ¥ âˆ¥ â†’ 0. Assuming Î¸tn â†’ Î¸âˆ— and Î¸â€² tn â†’ Î¸âˆ— as n â†’ +âˆ, and by the constant rank condition and relevant corol- lary of Stewart (1977) (rephrased in Lemma 2), we deduce ( âˆ‡ 2f1 (Î¸â€² tn ) )+ â†’ ( âˆ‡ 2f1 (Î¸âˆ—))+ and hence Ï‰tn â†’ Ï‰âˆ— as n â†’ +âˆ, where Ï‰âˆ— := ( âˆ‡ 2f1 (Î¸âˆ—))+ âˆ‡f2 (Î¸âˆ—). Thus, âˆ¥ âˆ¥âˆ‡f2 (Î¸t) + âˆ‡ 2f1 (Î¸â€² t) Ï‰tâˆ¥ âˆ¥ â†’âˆ¥ âˆ¥âˆ‡f2 (Î¸âˆ—) + âˆ‡2f1 (Î¸âˆ—) Ï‰âˆ—âˆ¥ âˆ¥, leading to âˆ¥ âˆ¥âˆ‡f2 (Î¸âˆ—) + âˆ‡2f1 (Î¸âˆ—) Ï‰âˆ—âˆ¥ âˆ¥ = 0, which implies that Î¸âˆ— satisfies the second-order KKT conditions for Eq. (22). Given the convexity of f1(Î¸) and f2(Î¸) with respect to Î¸, then f2(Î¸âˆ—) is the minimum in the feasible set â„¦ = {Î¸ : f1(Î¸) â‰¤ Îµ}, without any Ë†Î¸ âˆˆ â„¦ such that f2(Ë†Î¸) < f2(Î¸âˆ—). Consequently, Î¸âˆ— is a solution to Eq. (8). According to Chankong and Haimes [10], this solution is unique without further checking, as affirmed by theorem of Miettinen (rephrased in Lemma 3), Î¸âˆ— is Pareto optimal. Therefore, combining the conclusions, Î¸âˆ— is established as both the minimum of f1(Î¸) and Pareto optimal, confirming its status as Pareto optimal for complete unlearning. Lemma 2. Corollary 3.5 of Stewart (1977) [58]. Let {At} be a sequence of matrices converging to Aâˆ— as t â†’ +âˆ. The condition limtâ†’+âˆ A + t = A + âˆ— is equivalent to the condition that rank(At) = rank(Aâˆ—) for all t sufficiently large. Lemma 3. Theorem 3.2.4 of Miettinen (1999) [42]. A point Î¸âˆ— âˆˆ â„¦ is Pareto optimal if it is a unique solution of Îµ-constraint problem (Eq. (6)) for any given upper bound vector Îµ = (Îµ1, . . . , Îµâ„“âˆ’1, Îµâ„“+1, . . . , Îµt) T . C.4 Proof of Proposition 2 Proof of Proposition 2. Since Î¸t is stationary, Ë™Î¸t = âˆ’gt = 0, implying d dt PÎ¾(Î¸t) = 0 for all Î¾ â‰¥ 0. From Eq. (24), we have d dt PÎ¾(Î¸t) â‰¤ âˆ’KÎ¾âˆ’Î·t (Î¸t, Î·t). Consequently, KÎ¾âˆ’Î·t (Î¸t, Î·t) â‰¤ 0 for all Î¾ â‰¥ Î·t. Setting Î¾ = Î·t + Ï„ , where Ï„ â‰¥ 0, it follows that KÏ„ (Î¸t, Î·t) = 0. This implies that Î¸âˆ— satisfies the first-order KKT conditions for Eq. (19), i.e., there exists a Lagrange multiplier Î·âˆ— âˆˆ [0, +âˆ) such that âˆ‡f2 (Î¸âˆ—) + Î·âˆ—âˆ‡f1 (Î¸âˆ—) = 0, f1 (Î¸âˆ—) â‰¤ Îµ, Î·âˆ— (f1 (Î¸âˆ—) âˆ’ Îµ) = 0. 20 As affirmed by theorem of Miettinen (rephrased in Lemma 4), Î¸t is a Pareto optimal solution. Lemma 4. Theorem 3.1.8 of Miettinen (1999) [42]. (Karush-Kuhn-Tucker sufficient condition for Pareto optimality) Let the objective and the constraint functions of problem Eq. (9) be convex and continuously differentiable at a decision vector Î¸âˆ— âˆˆ â„¦. A sufficient condition for Î¸âˆ— to be Pareto optimal is that there exist multipliers Âµ âˆ— > 0 and Î·âˆ— > 0 such that (1) Âµ âˆ—âˆ‡f2 (Î¸âˆ—) + Î·âˆ—âˆ‡f1 (Î¸âˆ—) = 0 (2) Î·âˆ— (f1 (Î¸âˆ—) âˆ’ Îµ) = 0. C.5 Proof of Proposition 3 Proof of Proposition 3-1. Given that Ïˆ(Î¸) â‰¥ 0, we recall conclusions from Proposition 4-2: min sâˆˆ[0,t] Ïˆ (Î¸s) â‰¤ 1 t (f1 (Î¸0) âˆ’ f âˆ— 1 ) . Taking Î¾ = 0 in Eq. (26) gives âˆ« t 0 âˆ¥âˆ‡f2 (Î¸s) + Î·sâˆ‡f1 (Î¸s)âˆ¥2ds â‰¤ âˆ« t 0 Î·sÏˆ (Î¸s) ds + (f2 (Î¸0) âˆ’ f âˆ— 2 ) . To derive an upper bound for âˆ« t 0 âˆ¥âˆ‡f2 (Î¸s) + Î·sâˆ‡f1 (Î¸s)âˆ¥ 2ds, the principal challenge lies in bounding âˆ« t 0 Î·sÏˆ (Î¸s) ds. Given the assumption 0 â‰¤ Ïˆ (Î¸t) â‰¤ Î±âˆ¥âˆ‡f1 (Î¸t)âˆ¥Î´, where Î´ â‰¥ 1, and applying Lemma 5, we obtain: Î·tÏˆ (Î¸t) â‰¤ (Î±âˆ¥âˆ‡f1 (Î¸t)âˆ¥Î´âˆ’1 + âˆ¥âˆ‡f2 (Î¸t)âˆ¥) Î± 1 Î´ Ïˆ (Î¸t) 1âˆ’ 1 Î´ â‰¤ Î¥Ïˆ (Î¸t)1âˆ’ 1 Î´ , where Î¥ = supÎ¸âˆˆRd ( Î±âˆ¥âˆ‡f1(Î¸)âˆ¥Î´âˆ’1 + âˆ¥âˆ‡f2(Î¸)âˆ¥) Î± 1 Î´ . This leads to âˆ« t 0 Î·sÏˆ (Î¸s) ds â‰¤ Î¥ âˆ« t 0 Ïˆ (Î¸s)1âˆ’ 1 Î´ ds â‰¤ Î¥ (âˆ« t 0 Ïˆ (Î¸s) ds)1âˆ’ 1 Î´ (âˆ« t 0 1ds) 1 Î´ â‰¤ Î¥ (âˆ« t 0 Ïˆ (Î¸s) ds)1âˆ’ 1 Î´ t 1 Î´ â‰¤ Î¥ (f1 (Î¸0) âˆ’ f âˆ— 1 ) 1âˆ’ 1 Î´ t 1 Î´ . Thus, it follows that min sâˆˆ[0,t] âˆ¥âˆ‡f2 (Î¸s) + Î·tâˆ‡f1 (Î¸s)âˆ¥2 â‰¤ 1 t âˆ« t 0 âˆ¥âˆ‡f2 (Î¸s) + Î·tâˆ‡f1 (Î¸s)âˆ¥2ds â‰¤ 1 t âˆ« t 0 Î·sÏˆ (Î¸s) dt + 1 t (f2 (Î¸0) âˆ’ f âˆ— 2 ) â‰¤ 1 t 1âˆ’ 1 Î´ Î¥ (f1 (Î¸0) âˆ’ f âˆ— 1 )1âˆ’ 1 Î´ + 1 t (f2 (Î¸0) âˆ’ f âˆ— 2 ) . Since âˆ¥gsâˆ¥ 2 = âˆ¥âˆ‡f2 (Î¸s) + Î·tâˆ‡f1 (Î¸s)âˆ¥2, for any time t âˆˆ [0, +âˆ), we derive the following inequalities min sâˆˆ[0,t] âˆ¥gsâˆ¥ 2 â‰¤ Î¥ ( f1(Î¸0) âˆ’ f âˆ— 1 t )1âˆ’ 1 Î´ + f2(Î¸0) âˆ’ f âˆ— 2 t . Combine the conclutions above, if Ïˆ(Î¸) = Î±âˆ¥âˆ‡f1(Î¸)âˆ¥Î´, we can further assert minsâˆˆ[0,t] âˆ¥âˆ‡f1 (Î¸s)âˆ¥ = O (1/t 1 Î´ ) and minsâˆˆ[0,t] âˆ¥gsâˆ¥ = O (1/t 1 2 âˆ’ 1 2Î´ ). Hence, the exponent Î´ controls the convergence rates of âˆ¥âˆ‡f1 (Î¸t)âˆ¥ (measuring the minimization of f1(Î¸)), and that of âˆ¥gtâˆ¥ (measuring the minimization of f2(Î¸)). If Ïˆ(Î¸) = c ln(1+Î³âˆ¥âˆ‡f1(Î¸)âˆ¥), where cÎ³ â‰¤ Î± and Î´ = 1. Since 0 â‰¤ Ïˆ(Î¸) â‰¤ Î±âˆ¥âˆ‡f1(Î¸)âˆ¥Î´, the assumptions of Proposition 4 are satisfied. Consequently, we obtain minsâˆˆ[0,t] âˆ¥âˆ‡f1 (Î¸s)âˆ¥ = O (e 1 ct ) and minsâˆˆ[0,t] âˆ¥gsâˆ¥2 = O (1/t 1 2 âˆ’ 1 2Î´ ). 21 Proof of Proposition 3-2. If Ïˆ(Î¸) = Î²(f1(Î¸)âˆ’Îµ)Î´, where Î´ = 2n+1, n âˆˆ N. According to Proposition 4-1,we deduce that minsâˆˆ[0,t] Î²[(f1(Î¸s) âˆ’ Îµ)Î´]+ â‰¤ 1 t [f1(Î¸0) âˆ’ Îµ]+. Consequently, we obtain minsâˆˆ[0,t][f1(Î¸s) âˆ’ Îµ]+ = O (1/t 1 Î´ ) . Lemma 5. Let Î·t = minÎ·â‰¥0 { âˆ¥âˆ‡f2 (Î¸t) + Î·âˆ‡f1 (Î¸t)âˆ¥2 âˆ’ Î·Ïˆ (Î¸t)} = max ( Ïˆ(Î¸t)âˆ’âˆ‡f2(Î¸t) âŠ¤âˆ‡f1(Î¸t) âˆ¥âˆ‡f1(Î¸t)âˆ¥2 , 0 ) and assume 0 â‰¤ Ïˆ(Î¸t) â‰¤ Î±âˆ¥âˆ‡f1(Î¸t)âˆ¥ Î´ for Î± â‰¥ 0 and Î´ â‰¥ 1. Then Î·tÏˆ(Î¸t) â‰¤ ( Î±âˆ¥âˆ‡f1(Î¸t)âˆ¥Î´âˆ’1 + âˆ¥âˆ‡f2(Î¸t)âˆ¥) Î± 1 Î´ Ïˆ(Î¸t)1âˆ’ 1 Î´ . (28) Proof of Lemma 5. Given Ïˆ(Î¸t) â‰¤ Î±âˆ¥âˆ‡f1(Î¸t)âˆ¥Î´, we have Ïˆ(Î¸t) âˆ¥âˆ‡f1(Î¸t)âˆ¥ â‰¤ Î±âˆ¥âˆ‡f1(Î¸t)âˆ¥Î´âˆ’1, and Ïˆ(Î¸t) âˆ¥âˆ‡f1(Î¸t)âˆ¥ â‰¤ Î± 1 Î´ Ïˆ(Î¸t) 1âˆ’ 1 Î´ . With Ïˆ(Î¸t) â‰¥ 0, the upper bound for Î·t simplifies to Î·t = max ( Ïˆ(Î¸t) âˆ’ âˆ‡f2(Î¸t)âŠ¤âˆ‡f1(Î¸t), 0 ) âˆ¥âˆ‡f1(Î¸t)âˆ¥ 2 â‰¤ Ïˆ(Î¸t) + âˆ¥âˆ‡f2(Î¸t)âˆ¥âˆ¥âˆ‡f1(Î¸t)âˆ¥ âˆ¥âˆ‡f1(Î¸t)âˆ¥2 . Therefore, Î·tÏˆ(Î¸t) â‰¤ Ïˆ(Î¸t)2 âˆ¥âˆ‡f1(Î¸t)âˆ¥2 + âˆ¥âˆ‡f2(Î¸t)âˆ¥ Ïˆ(Î¸t) âˆ¥âˆ‡f1(Î¸t)âˆ¥ â‰¤ ( Î±âˆ¥âˆ‡f1(Î¸t)âˆ¥Î´âˆ’1 + âˆ¥âˆ‡f2(Î¸t)âˆ¥) Ïˆ(Î¸t) âˆ¥âˆ‡f1(Î¸t)âˆ¥ â‰¤ ( Î±âˆ¥âˆ‡f1(Î¸t)âˆ¥Î´âˆ’1 + âˆ¥âˆ‡f2(Î¸t)âˆ¥) Î± 1 Î´ Ïˆ(Î¸t)1âˆ’ 1 Î´ . D More Details of Experiments D.1 Evaluation Metrics IS. Following [36], for ImageNet-1K, we directly use the Inception-v3 model checkpoint to calculate the IS score. For Places-365, we use the Resnet-50 model checkpoint to calculate IS scores [73]. FID. Regardless of whether it is ImageNet-1K or Places-365, we directly use the Inception-v3 model check- point to calculate the FID score. CLIP. Following [36], whether it is for ImageNet-1K or Places-365, we use the ViT-H-14 model checkpoint to calculate the clip embedding vectors of the generated images and the ground truth images [47]. Afterward, we calculate the cosine similarity between the two vectors as the clip score. D.2 Hyper-parameter of Experiments MAE. We set the learning rate to 10 âˆ’4 with no weight decay. Both baselines and our method employ AdamW as the foundational optimizer with Î² = (0.90, 0.95), with the distinction being that our method necessitates some improvements on the basic optimizer. We set the input image resolution to 224Ã—224 and batch size to 32. Simultaneously, we set the coefficient of Ïˆ(Î¸) in Phase I to Î± = 5, and the coefficient of Ïˆ in Phase II to Î² = 5, followed by training for 5 epochs. Overall, it takes an hour on an NVIDIA A40 (48G) server. VQ-GAN. We set the learning rate to 10âˆ’4 with no weight decay. Both baselines and our method employ AdamW as the foundational optimizer with Î² = (0.90, 0.95). Our method necessitates some improvements on the basic optimizer. We set the input image resolution to 256Ã—256 and batch size to 8. Simultaneously, we set the coefficient of Ïˆ(Î¸) in Phase I to Î± = 10, and the coefficient of Ïˆ(Î¸) in Phase II to Î² = 10, followed by training for 5 epochs. Overall, it takes two hours on an NVIDIA A40 (48G) server. Diffusion model. We set the learning rate to 10 âˆ’5 with no weight decay. Both baselines and our method employ Adam as the foundational optimizer. Our method necessitates some improvements on the basic optimizer. We set the input image resolution to 256Ã—256 and batch size to 8. Simultaneously, we set the coefficient of Ïˆ(Î¸) in Phase I to Î± = 1, and the coefficient of Ïˆ(Î¸) in Phase II to Î² = 1, followed by training for 4 epochs. Overall, it takes twelve hours on an NVIDIA A40 (48G) server. 22 E Robustness to Retain Samples Availability In machine unlearning, sometimes the real retain samples are not available due to data retention policies. To tackle this challenge, following [36], we assess our method using images from other classes as substitutes for real retain samples. For instance, on ImageNet-1K, since we have already selected 200 classes, we randomly chose some images from the remaining 800 classes to act as a \"proxy retain set\" during the unlearning process. We incrementally reduce the proportion of real retain samples in the retain set and increased the proportion of proxy retain samples, with the experimental results presented in Table 3. As demonstrated, our method is largely unaffected by the reduced availability of retain samples, indicating robust performance. Table 3: Results of center cropping 50% of the images under different retain set usage proportions. â†‘ indicates higher is better, and â†“ indicates lower is better. â€˜Fâ€™ and â€˜Râ€™ stand for the forget set and retain set, respectively. Here, all results are based on the solution with the highest degree of unlearning completeness in Phase I. MAE VQ-GAN Diffusion Models IS FID CLIP IS FID CLIP IS FID CLIP F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ F â†“ R â†‘ F â†‘ R â†“ F â†“ R â†‘ Original 21.59 21.83 16.28 14.87 0.88 0.88 23.74 24.06 21.80 18.17 0.78 0.85 16.90 19.65 82.12 81.51 0.89 0.91 100% 12.33 17.47 154.60 68.453 0.69 0.75 13.23 22.55 139.21 26.39 0.46 0.82 11.84 18.47 165.05 95.42 0.55 0.81 80% 12.32 17.46 150.05 73.14 0.70 0.73 13.27 22.30 138.49 24.83 0.46 0.81 11.91 18.10 167.32 98.82 0.55 0.80 60% 12.22 17.42 150.55 74.22 0.70 0.73 13.24 22.54 140.35 24.92 0.61 0.81 12.06 18.53 165.24 98.43 0.60 0.80 40% 112.29 17.43 150.27 73.63 0.70 0.74 12.77 22.39 141.67 25.84 0.61 0.81 12.05 18.64 168.83 96.42 0.60 0.79 20% 12.50 17.68 147.45 70.75 0.70 0.74 12.77 22.39 144.38 28.08 0.60 0.81 13.49 18.67 168.26 95.47 0.57 0.79 0 12.21 17.68 147.31 68.09 0.70 0.74 12.39 22.35 147.17 29.79 0.62 0.80 13.24 18.76 168.43 96.63 0.60 0.79 F More Generated Images: Baselines vs Ours We conduct various generative tasks on three mainstream I2I generative models (i.e., MAE, VQ-GAN, and the diffusion model), including image expansion, inpainting, and reconstruction, to assess both baselines and our proposed method. Specifically, we conduct evaluations of image inpainting and expansion tasks on VQ-GAN, image reconstruction tasks on MAE, and image inpainting tasks on the diffusion model. The results indicate that our method can adapt to mainstream I2I generative models and various image generation tasks. VQ-GAN. We conduct experiments on image inpainting and expansion task unlearning on VQ-GAN, where examples of the image inpainting tasks are illustrated in Figure 5, and examples of image expansion can be referred to in Appendix H. Our unlearning method is effective for both image inpainting and image expansion tasks, and it significantly surpasses baselines. MAE. We conduct experiments on unlearning image reconstruction tasks on the MAE. As shown in figure 6, our unlearning method is also effective in the task of image reconstruction, with the effects of unlearning showing a significant advantage over baselines. Diffusion model. We validate our unlearning framework on the diffusion model task for image inpainting. As shown in figure 7, the results indicate that our method is equally applicable to diffusion models, and the effectiveness of unlearning surpasses that of baselines. G More Generated Images: Different Degrees of Completeness We validate the control effect of our controllable unlearning framework across multiple generative tasks in three mainstream I2I generative models. The results demonstrate that our controllable unlearning framework can effectively control unlearning across various image generation tasks of mainstream I2I generative models. VQ-GAN. We center-cropp the image by 50% and utilize the VQ-GAN for image inpainting. Subsequently, we applied our unlearning framework to enforce unlearning. The results in Figure 8 demonstrate the effectiveness of our method, with the control effect being very pronounced. MAE. We verify the control effect of our controllable unlearning framework within the reconstruction task using the MAE. The results in Figure 9 indicate that our method can effectively control the completeness of unlearning in image reconstruction tasks as well. Diffusion model. We validate the control effect of our controllable unlearning framework within the inpainting task of a diffusion model. As shown in Figure 10, the findings illustrate that our method can 23 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 5: VQ-GAN: generated images of cropping 50% at the center of the image. The upper part (a) represents the forget set, while the lower part (b) represents the retain set. \"Ours\" denotes the boundary condition of unlearning obtained in Phase I, which represents the point of the highest degree of unlearning completeness. It is evident that our method significantly outperforms baselines in terms of the unlearning effect on the forget set, most closely approximating Gaussian noise, and exhibits the least performance degradation on the retain set. 24 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 6: MAE: reconstruction of random masked images. We set the proportion of the random mask to 50%. The upper part (a) represents the forget set, while the lower part (b) represents the retain set. \"Ours\" denotes the boundary condition of unlearning obtained in Phase I, which represents the point of the highest degree of unlearning completeness. 25 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 7: Diffusion model: generated images of cropping 50% at the center of the image. The upper part (a) represents the forget set, while the lower part (b) represents the retain set. \"Ours\" denotes the boundary condition of unlearning obtained in Phase I, which represents the point of the highest degree of unlearning completeness. 26 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Relax-ğœºğŸ 25% Relax-ğœºğŸ 50% Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree Relax-ğœºğŸ‘ 75% (a) Forget Set Ground Truth Input Original Model Relax-ğœºğŸ 25% Relax-ğœºğŸ 50% Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree Relax-ğœºğŸ‘ 75% (b) Retain Set Boundary-ğœºğ’ğ’Šğ’ Highest Degree (b) Retain Set Figure 8: VQ-GAN: generated images of cropping 50% at the center of the image under different degrees of unlearning completeness requirements. The upper half (a) represents the forget set, and the lower half (b) represents the retain set. Our method first determines the two boundary conditions of unlearning, and then linearly increases the value of Îµ within its range (here, we increase by 25% each time) to adjust the balance between unlearning completeness and model utility. 27 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Relax-ğœºğŸ 25% Relax-ğœºğŸ 50% Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree Relax-ğœºğŸ‘ 75% (a) Forget Set Ground Truth Input Original Model Relax-ğœºğŸ 25% Relax-ğœºğŸ 50% Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree Relax-ğœºğŸ‘ 75% (b) Retain Set Boundary-ğœºğ’ğ’Šğ’ Highest Degree (b) Retain Set Figure 9: MAE: construction of random masked images under different degrees of unlearning completeness requirements. We set the proportion of the random mask to 50%. The upper half (a) represents the forget set, and the lower half (b) represents the retain set. Our method first determines the two boundary conditions of unlearning, and then linearly increases the value of Îµ within its range (here, we increase by 25% each time) to adjust the balance between unlearning completeness and model utility. 28 effectively adjust the balance between the completeness of unlearning and the utility of the model in the context of a diffusion model. (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Relax-ğœºğŸ 25% Relax-ğœºğŸ 50% Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree Relax-ğœºğŸ‘ 75% (a) Forget Set Ground Truth Input Original Model Relax-ğœºğŸ 25% Relax-ğœºğŸ 50% Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree Relax-ğœºğŸ‘ 75% (b) Retain Set Boundary-ğœºğ’ğ’Šğ’ Highest Degree (b) Retain Set Figure 10: Diffusion model: generated images of cropping 50% at the center of the image under different degrees of unlearning completeness requirements. The upper half (a) represents the forget set, and the lower half (b) represents the retain set. Our method is also effective when applied to the diffusion model. H Ablation Study To verify the robustness of our method on mainstream I2I generative models and various image generation tasks, we conducted the following ablation studies: i) we vary the cropping patterns to demonstrate robustness across multiple image generation tasks; ii) we decrease the linear increment size of Îµ to validate that our method allows for more fine-grained control; and iii) we alter the cropping ratios to confirm the robustness of our method to changes in crop ratio. 29 H.1 More Generative Tasks In the main paper, we primarily showcase the performance of our controllable unlearning framework on the task of image inpainting. We validate the effectiveness of our controllable unlearning framework for image extension tasks on VQ-GAN by varying the patterns of cropping. The results indicate that our controllable unlearning framework is robust to different cropping patterns. H.1.1 Outpainting Task We retain 25% of the image center and utilize VQ-GAN for image outpainting. As shown in Figure 11, our method produces outpainting on the forget set that is most similar to Gaussian noise, and the outpainting performance on the retain set shows the least decline compared to the original model. H.1.2 Upward Extension Task We crop the upper half of the image, retain the lower half, and employ VQ-GAN for image extension. The results in Figure 12 indicate that our method produces extension on the unlearning set that closely resembles Gaussian noise, and on the retain set, the extension performance decreases the least compared to the original model. H.1.3 Leftward Extension Task We crop the right half of the image, retain the left half, and use VQ-GAN for image extension. As shown in Figure 13, our method produces leftward extension on the forget set that closest resembles Gaussian noise and, on the retain set, the leftward extension performance exhibits the minimal decrease compared to the original model. H.2 More Fine-grained control of unlearning completeness After obtaining two boundary points of unlearning, our controllable unlearning framework linearly increases within its valid range to balance the completeness of unlearning and the utility of the model. However, in the main paper, the increase of Îµ is by 25% each time. For example, if the range of Îµ is [1,9], then the sequence of Îµ values would be {3,5,7}. It is evident that the increments of Îµ are quite substantial, which results in a coarser granularity of control. Here, we reduce the linear increment of Îµ to extend the effectiveness of our controllable unlearning framework across various image generation tasks in VQ-GAN. The results show that our framework can achieve fine-grained control. H.2.1 Outpainting Task We retain the central 25% of the image and utilize VQ-GAN for image outpainting. The results in Figure 14 show that the performance of our controllable unlearning framework on the forget set gradually improves with the increase of Îµ, and the extent of decline in outpainting performance on the retain set, compared to the original model, is also reducing. H.2.2 Upward Extension Task We retain the lower half of the image center and crop the upper half, employing VQ-GAN for image extension. As shown in Figure 15, results indicate that, with an increase in the value of Îµ, the upward extension effectiveness on the forget set of our controllable unlearning framework gradually improves. Concurrently, the degree of decrease in upward extension effectiveness on the retain set, in comparison to the original model, also diminishes. H.2.3 Leftward Extension Task We retain the right half of the image and utilize VQ-GAN to extend the image from the left. The results in Figure 16 demonstrate that the leftward extension performance on the forget set of our controllable unlearning framework progressively improves with the increase of Îµ, and the reduction in leftward extension performance on the retain set is also diminishing compared to the original model. H.3 Varying Cropping Patterns and Ratios In the preceding sections, we have demonstrated the performance of our controllable unlearning framework under various cropping patterns, yet the cropping ratio remained constant. By altering the cropping ratio on VQ-GAN, we validate the effectiveness of our controllable unlearning framework at different cropping ratios. The results indicate that our controllable unlearning framework is robust to different cropping ratios. 30 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 11: Outpainting by VQ-GAN. We retain 25% of the image center. The upper half (a) designated as the unlearning set and the lower half (b) as the retain set. For each subset, we compared the performance of both the baselines and our method on the outpainting task, where \"Ours\" represents the boundary condition of unlearning in Phase I, indicating the point of highest degree of unlearning completeness. The results show that our method significantly outperforms the baselines on the outpainting task. 31 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 12: Upward extension by VQ-GAN. We retain 50% of the lower half of the image. The upper half (a) is the forget set, and the lower half (b) is the retain set. For each set, we compare the performance of the baselines and our method on the upward extension task, where \"Ours\" represents the unlearning boundary condition in Phase I, which is the point of the highest degree of unlearning completeness. The results suggest that our method also significantly outperforms the baselines on the upward extension task. 32 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 13: Leftward extension by VQ-GAN. We retain 50% of the right half of the image. The upper half (a) is the forget set, and the lower half (b) is the retain set. For each set, we compare the performance of the baselines and our method on the upward extension task, where \"Ours\" represents the unlearning boundary condition in Phase I, which is the point of highest degree of unlearning completeness. The results suggest that our method also significantly outperforms the baselines on the upward extension task. 33 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (a) Forget Set (b) Retain Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (b) Retain Set Figure 14: Outpainting by VQ-GAN under different degrees of unlearning completeness. We retain 25% of the image center. The upper half (a) is the forget set, while the lower half (b) is the retain set. For each part, we compare the unlearning effects of our method at different values of Îµ. \"Highest\" and \"Lowest\" represent the conditions of the highest and lowest degree of unlearning completeness, respectively. We increase Îµ 16% each time. Simultaneously, compared to larger cropping ratios, the extent of variation in the images generated under our controllable unlearning framework will be smaller for smaller cropping ratios. H.3.1 Inpainting Task We retain one-sixteenth of the image center and use VQ-GAN for image inpainting. The results in Figure 17 show that our controllable unlearning framework significantly outperforms the baselines in terms of unlearning effect on the forget set, most closely approximating Gaussian noise, and exhibits a lesser decline in unlearning effect on the retain set than the baselines. Simultaneously, we can finely control the balance between unlearning completeness and model utility. H.3.2 Downward Extension Task We crop the bottom 25% of the image and utilize VQ-GAN for image extension from the bottom. As shown in Figure 19, the results demonstrate that our controllable unlearning framework significantly surpasses the baselines in terms of the unlearning effect on the forget set, closely approximating Gaussian noise, and shows a lesser reduction in unlearning effect on the retain set compared to the baselines. At the same time, we can finely adjust the balance between unlearning completeness and model utility. 34 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (a) Forget Set (b) Retain Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (b) Retain Set Figure 15: Upward extension by VQ-GAN under different degrees of unlearning completeness. We retain 50% of the lower half of the image. The upper half (a) is the forget set, while the lower half (b) is the retain set. For each part, we compare the unlearning effects of our method at different values of Îµ. \"Highest\" and \"Lowest\" represent the conditions of the highest and lowest degree of unlearning completeness, respectively. We increase Îµ 16% each time. H.3.3 Rightward Extension Task We crop the right 25% of the image and utilize VQ-GAN for image extension from the bottom. The results in Figure 21 demonstrate that our controllable unlearning framework significantly surpasses the baselines in terms of the unlearning effect on the forget set, closely approximating Gaussian noise, and shows a lesser reduction in unlearning effect on the retain set compared to the baselines. At the same time, we can finely adjust the balance between unlearning completeness and model utility. I T-SNE Analysis for Controllable Unlearning In Table 2 of the main paper, we present the evaluation metrics corresponding to different degrees of unlearning completeness solutions (i.e., IS, FID and CLIP) obtained by our controllable unlearning framework in mainstream I2I generative models. Here, we analyze the images generated at different degrees of unlearning completeness for each corresponding model. We use T-SNE analysis to compare the clip embedding distances between the images generated on the forget set and retain set and the ground truth images. As shown in Figure 23, for any model, under the highest degree of unlearning completeness, the distance between the clip embeddings of the images generated on the forget set by the unlearned model and the ground truth images is larger, while the 35 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (a) Forget Set (b) Retain Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (b) Retain Set Figure 16: Leftward extension by VQ-GAN under different degrees of unlearning completeness. We retain 50% of the right half of the image. The upper half (a) is the forget set, while the lower half (b) is the retain set. For each part, we compare the unlearning effects of our method at different values of Îµ. \"Highest\" and \"Lowest\" represent the conditions of the highest and lowest degree of unlearning completeness, respectively. We increase Îµ 16% each time. distance on the retain set is smaller. Simultaneously, as Îµ increases, the distance between the clip embeddings of the images generated on the forget set by the unlearning model and the ground truth images gradually decreases (still significantly higher than the situation of the retain set), and the distance on the retain set also gradually decreases. Lastly, among these three mainstream I2I generation model structures, the effect of VQ-GAN is the most significant. J Efficiency Experiments for Controllable Unlearning Framework In the main paper, we analyze the convergence efficiency corresponding to different control functions Ïˆ(Î¸) at each phase from a theoretical perspective, and based upon this analysis, we aim to enhance the unlearning efficiency of our controllable unlearning framework. Here, we validate our analysis on three mainstream I2I generative models. During the two different phases of controllable unlearning, we design the form of the control function Ïˆ(Î¸) separately. Specifically, in Phase I, we set Ïˆ(Î¸) = Î±âˆ¥âˆ‡f1(Î¸)âˆ¥Î´, where we test the convergence rates of f1(Î¸) and f2(Î¸), as well as the overall convergence rate, for Î´ = 1, Î´ = 2, Î´ = 3, and Î´ = 4. As shown in Figure 24, It is apparent that at Phase I for c = 2, that is Ïˆ(Î¸) = Î±âˆ¥âˆ‡f1(Î¸)âˆ¥2, the overall convergence rate is optimal. 36 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 17: Generated images of cropping 25% at the center of the image. We crop the center 1/16 of the image. The upper half (a) is the forget set, and the lower half (b) is the retain set. For each set, we compare the performance of the baselines and our method on the inpainting task, where \"Ours\" represents the extreme case of the unlearning boundary in Phase I, that is, the point of highest degree of unlearning completeness. 37 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (a) Forget Set (b) Retain Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (b) Retain Set Figure 18: Generated images of cropping 50% at the center of the image under different degrees of unlearning completeness requirements. We crop the central 1/16 of the image. The upper half (a) represents the forget set, and the lower half (b) represents the retain set. For each section, we compare the effectiveness of our methodâ€™s unlearning under different values of Îµ. Here, \"Highest\" and \"Lowest\" indicate the conditions of the highest and lowest degree of unlearning completeness, respectively. 38 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 19: Downward extension by VQ-GAN. We crop the bottom 25% of the image. The upper half (a) is designated as the forget set, and the lower half (b) as the retain set. For each section, we compared the performance of the baselines and our method on the downward extension task, where \"Ours\" denotes the unlearning boundary condition in Phase I, that is, the point of highest degree of unlearning completeness. 39 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (a) Forget Set (b) Retain Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (b) Retain Set Figure 20: Downward extension by VQ-GAN under different degrees of unlearning completeness. We crop the bottom 25% of the image. The upper half (a) represents the forget set, and the lower half (b) represents the retain set. For each section, we compare the effectiveness of our methodâ€™s unlearning under different values of Îµ. Here, \"Highest\" and \"Lowest\" indicate the conditions of the highest and lowest degree of unlearning completeness, respectively. In Phase II, we set Ïˆ(Î¸) = Î²(f1(Î¸) âˆ’ Îµ)Î´, where we tested the convergence rates for Î´ = 1 and Î´ = 3. Subsequently, we changed the form of Ïˆ(Î¸) to Ïˆ(Î¸) = Î²(f1(Î¸)âˆ’Îµ)Î´âˆ¥âˆ‡f1(Î¸)âˆ¥2, and we tested the convergence rates for Î´ = 1 and Î´ = 3. Comparing the aforementioned scenarios, the overall optimal convergence rate in Phase II is obtained when Ïˆ(Î¸) = Î²(f1(Î¸) âˆ’ Îµ)1âˆ¥âˆ‡f1(Î¸)âˆ¥2. 40 (a) Forget Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (a) Forget Set (b) Retain Set Ground Truth Input Original Model Max Loss Retain Label Noisy Label Composite Loss Ours (b) Retain Set Figure 21: Rightward extension by VQ-GAN. We crop the right 25% of the image. The upper half (a) is designated as the forget set, and the lower half (b) as the retain set. For each section, we compared the performance of the baselines and our method on the rightward extension task, where \"Ours\" denotes the unlearning boundary condition in Phase I, that is, the point of highest degree of unlearning completeness. 41 (a) Forget Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (a) Forget Set (b) Retain Set Ground Truth Input Original Model Boundary-ğœºğ’ğ’Šğ’ Highest Degree Boundary-ğœºğ’ğ’‚ğ’™ Lowest Degree (b) Retain Set Figure 22: Rightward extension by VQ-GAN under different degrees of unlearning completeness. We crop the right 25% of the image. The upper half (a) represents the forget set, and the lower half (b) represents the retain set. For each section, we compare the effectiveness of our methodâ€™s unlearning under different values of Îµ. Here, \"Highest\" and \"Lowest\" indicate the conditions of the highest and lowest degree of unlearning completeness, respectively. 42 Before Unlearning: Forget Set Before Unlearning: Retain Set Unlearning: Forget SetÃ— Unlearning: Retain SetÃ— (i) Highest Degree (v) Lowest Degree(ii) ğœ€ âˆ’ 25% (iii) ğœ€ âˆ’ 50% (iv) ğœ€ âˆ’ 75%(b) VQ-GAN (i) Highest Degree (v) Lowest Degree(ii) ğœ€ âˆ’ 25% (iii) ğœ€ âˆ’ 50% (iv) ğœ€ âˆ’ 75%(C) Diffusion Model (i) Highest Degree (v) Lowest Degree(ii) ğœ€ âˆ’ 25% (iii) ğœ€ âˆ’ 50% (iv) ğœ€ âˆ’ 75%(a) MAE Figure 23: T-SNE analysis between images generated by our method and ground truth images under different degrees of unlearning completeness. 43(a-i)5||Î”ğ‘“1(ğœƒ)||1(a-ii)5||Î”ğ‘“1(ğœƒ)||2(a-ii)5||Î”ğ‘“1(ğœƒ)||3(a-ii)5||Î”ğ‘“1(ğœƒ)||4 (a) MAE-ğœ“ğœƒ(a-v)5(Î”ğ‘“1ğœƒâˆ’ğœ€)1(a-vi)5(Î”ğ‘“1ğœƒâˆ’ğœ€)3(a-vii)5(Î”ğ‘“1ğœƒâˆ’ğœ€)1||Î”ğ‘“1(ğœƒ)||2(a-viii)5(Î”ğ‘“1ğœƒâˆ’ğœ€)3||Î”ğ‘“1(ğœƒ)||2 (b) VQ-GAN-ğœ“ğœƒ(b-i)10||Î”ğ‘“1(ğœƒ)||1(b-ii)10||Î”ğ‘“1(ğœƒ)||2(b-ii)10||Î”ğ‘“1(ğœƒ)||3(b-ii)10||Î”ğ‘“1(ğœƒ)||4(b-v)10(Î”ğ‘“1ğœƒâˆ’ğœ€)1(b-vi)10(Î”ğ‘“1ğœƒâˆ’ğœ€)3(b-vii)10(Î”ğ‘“1ğœƒâˆ’ğœ€)1||Î”ğ‘“1(ğœƒ)||2(b-viii)10(Î”ğ‘“1ğœƒâˆ’ğœ€)3||Î”ğ‘“1(ğœƒ)||2 (C) Diffusion Model-ğœ“ğœƒ(c-i)||Î”ğ‘“1(ğœƒ)||1(c-ii)||Î”ğ‘“1(ğœƒ)||2(c-ii)||Î”ğ‘“1(ğœƒ)||3(c-ii)||Î”ğ‘“1(ğœƒ)||4(c-v)(Î”ğ‘“1ğœƒâˆ’ğœ€)1(c-vi)(Î”ğ‘“1ğœƒâˆ’ğœ€)3(c-vii)(Î”ğ‘“1ğœƒâˆ’ğœ€)1||Î”ğ‘“1(ğœƒ)||2(c-viii)(Î”ğ‘“1ğœƒâˆ’ğœ€)3||Î”ğ‘“1(ğœƒ)||2 Figure 24: The convergence rates under different control functions Ïˆ(Î¸). As illustrated in figure, include three sections: MAE, VQ-GAN, and the diffusion model. Each section contains two rows, corresponding to Phase I and Phase II, respectively. The titles on each subplot indicate the forms of the control function Ïˆ(Î¸). 44","libVersion":"0.3.2","langs":""}