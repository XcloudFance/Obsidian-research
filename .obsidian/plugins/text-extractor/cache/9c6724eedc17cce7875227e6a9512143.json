{"path":"GenAIUnleaning/DiffusionUnlearning/2024_Vnew/Stereo.pdf","text":"STEREO: Towards Adversarially Robust Concept Erasing from Text-to-Image Generation Models Koushik Srivatsan 1, Fahad Shamshad 1, Muzammal Naseer 1, Karthik Nandakumar 1 1Mohamed bin Zayed University of Artificial Intelligence, {firstname.lastname}@mbzuai.ac.ae Abstract The rapid proliferation of large-scale text-to-image genera- tion (T2IG) models has led to concerns about their potential misuse in generating harmful content. Though many methods have been proposed for erasing undesired concepts from T2IG models, they only provide a false sense of security, as recent works demonstrate that concept-erased models (CEMs) can be easily deceived to generate the erased concept through ad- versarial attacks. The problem of adversarially robust concept erasing without significant degradation to model utility (ability to generate benign concepts) remains an unresolved challenge, especially in the white-box setting where the adversary has access to the CEM. To address this gap, we propose an ap- proach called STEREO that involves two distinct stages. The first stage searches thoroughly enough for strong and diverse adversarial prompts that can regenerate an erased concept from a CEM, by leveraging robust optimization principles from adversarial training. In the second robustly erase once stage, we introduce an anchor-concept-based compositional objective to robustly erase the target concept at one go, while attempting to minimize the degradation on model utility. By benchmarking the proposed STEREO approach against four state-of-the-art concept erasure methods under three adversar- ial attacks, we demonstrate its ability to achieve a better robust- ness vs. utility trade-off. Our code and models are available at https://github.com/koushiksrivats/robust-concept-erasing. WARNING: This paper contains model-generated content that might be considered offensive or inappropriate. Reader discretion is advised. 1 Introduction Large-scale text-to-image generation (T2IG) models (Chang et al. 2023; Ding et al. 2022; Lu, Liu, and Kong 2023; Nichol et al. 2022) have demonstrated a remarkable ability to synthe- size photorealistic images from user-specified text prompts, leading to their adoption in numerous commercial applica- tions. However, these models are typically trained on massive datasets scraped from the Internet (Schuhmann et al. 2022). This can result in issues such as memorization (Ren et al. 2024; Somepalli et al. 2023) and generation of inappropriate images (e.g., copyright violations (Jiang et al. 2023; Roose 2022)), prohibited content (Schramowski et al. 2023), and NSFW material (Hunter 2023; Zhang et al. 2023b). Public- domain availability of T2IG models such as Stable Diffusion (SD) (Rombach et al. 2022) raises significant security con- cerns that require urgent redressal. Solutions to mitigate the generation of undesired concepts in T2IG models can be broadly classified into three cate- gories: dataset filtering before training, post-hoc modification of models after training, and output filtering after image generation. Dataset filtering (Carlini et al. 2022) involves removing unsafe images and retraining the model, which is not only computationally expensive but also impractical for each new undesired concept and may have a significant nega- tive impact on the output quality (Schramowski et al. 2023). While post-generation output filtering can effectively censor harmful images, it can be applied only to the black-box set- ting, where the adversary has only query access to the T2IG model and cannot access the model parameters (Rando et al. 2022). Recently, several post-hoc erasure methods have been proposed to modify the behavior of pre-trained T2IG models. These methods either fine-tune the model parameters or alter the generation process during inference to avoid the genera- tion of undesired concepts (Schramowski et al. 2023; Brack et al. 2023; Gandikota et al. 2023; Kumari et al. 2023). This work focuses on post-hoc concept erasure methods, which are often more practical and effective. Despite the success of post-hoc erasure methods, recent studies (Pham, Marshall, and Hegde 2023; Tsai et al. 2023; Chin et al. 2023; Zhang et al. 2023b) have exposed significant vulnerabilities of this approach by demonstrating that con- cept erasure can be easily circumvented through adversarial attacks. Such attacks are crafted by either prepending/modi- fying input prompts with adversarial tokens (Chin et al. 2023; Zhang et al. 2023b; Tsai et al. 2023) or injecting concepts into textual embeddings (Pham, Marshall, and Hegde 2023). Consequently, seemingly safe concept-erased T2IG models still produce sensitive or offensive content under adversarial attack, as shown in Figure 1. To address this limitation, it is essential to incorporate some form of adversarial training (AT) into the concept erasing method (Huang et al. 2024; Kim, Min, and Yang 2024). This work proposes a two-stage framework for adversari- ally robust concept erasing from T2IG models. The first stage follows the robust optimization framework of AT and formu- lates concept erasing as a min-max optimization problem, which is iteratively solved by alternating between erasing the target concept in the parameter space of the pre-trainedarXiv:2408.16807v1 [cs.CV] 29 Aug 2024 Figure 1: Concept erasure methods are prone to adver- sarial concept inversion at- tacks. This figure illustrates that recent concept erasure methods (ESD (Gandikota et al. 2023), UCE (Gandikota et al. 2024), MACE (Lu et al. 2024)) are vulnerable to concept inversion attacks (CCE (Pham, Marshall, and Hegde 2023)) that regenerate the erased concept. In con- trast, our proposed approach STEREO is robust against such attacks across diverse concept categories. model and searching for adversarial prompts in the text em- bedding space that can still regenerate the erased concept from the modified model. The core novelty of our approach lies in the fact that we employ AT not as a final solution, but only as an intermediate step to search thoroughly enough for strong adversarial prompts. This adversarial prompt search is followed by a robustly erase once stage, where we propose an anchor-concept-based compositional objective to erase the target concept from the original model. While incorpo- rating the anchor concept in the erasing objective minimizes the degradation of model utility, the compositional guidance steers the final erased model away from the set of strong ad- versarial prompts, thereby enhancing adversarial robustness. Our main contributions can be summarized as follows: • We propose a novel two-stage approach called STEREO to achieve better robustness vs. utility trade-off when ro- bustly erasing concepts from pre-trained T2IG models. • While the first search thoroughly enough (STE) stage uti- lizes adversarial training to discover strong adversarial prompts that can regenerate target concepts from erased models, we propose an anchor-concept-based composi- tional objective in the second robustly erase once (REO) stage to erase the target concept from the original model, while mitigating the loss of utility. 2 Related Work and Background Post-hoc Concept Erasing: Recent methods for erasing undesired concepts from pre-trained T2IG models can be categorized into inference-based and fine-tuning-based ap- proaches. Inference-based methods (Schramowski et al. 2023; Brack et al. 2023; AUTOMATIC1111 2022; Dong et al. 2024) modify the noise-estimate calculated through classifier-free guidance (CFG) (Ho and Salimans 2022), to navigate the generation away from the undesired concepts semantically, without any additional training cost. These methods introduce additional terms to the CFG during inference, such as replac- ing the null-string in the unconditioned branch with a textual prompt describing the undesired concept (AUTOMATIC1111 2022), incorporating safety (Schramowski et al. 2023), us- ing semantic guidance (Brack et al. 2023) or feature space purification (Dong et al. 2024), to move the unconditioned score estimate closer to the prompt-conditioned score esti- mate and away from the erasure-conditioned score estimate. Fine-tuning-based methods modify the parameters of the pre- trained model to remap the undesired concept’s noise esti- mate semantically away from the original concept (Gandikota et al. 2023; Heng and Soh 2024) or to a predefined desired target (Zhang et al. 2023a; Lu et al. 2024). This work primar- ily uses (Gandikota et al. 2023), which generates images with an unwanted concept and then guides the model away from creating such content. Despite the impressive performance of concept-erasing methods, they are vulnerable to adversarial prompt attacks that can regenerate the erased concept (Pham, Marshall, and Hegde 2023; Tsai et al. 2023; Chin et al. 2023). Circumventing Concept Erasing: Among recent attacks on concept erasing methods, the most relevant to our work is Circumventing Concept Erasure (Pham, Marshall, and Hegde 2023), which shows that the erased concept can be mapped to any arbitrary input word embedding through textual-inversion (Gal et al. 2022). Optimizing for this new embedding without altering the weights of the erased model steers the generation to output the erased concept. Prompt- ing4Debugging (Chin et al. 2023) optimizes adversarial prompts by enforcing similarity between the noise esti- mates of pre-trained and concept-erased models. Unlearn- Diff (Zhang et al. 2023b) simplifies adversarial prompt cre- ation by leveraging the intrinsic classification abilities of diffusion models. Similarly, Ring-A-Bell (Tsai et al. 2023), generates problematic prompts to bypass safety mechanisms in text-to-image diffusion models, leading to the generation of images with supposedly forbidden concepts. Adversarially Robust Concept Erasing: Recently, few ap- proaches have been proposed for adversarial training-based robust concept erasure. Receler (Huang et al. 2024) employs an iterative approach, alternating between erasing and adver- sarial prompt learning. Our STEREO method differs by using a two-stage approach with explicit min-max optimization for adversarial prompts, offering protection in white-box settings. AdvUnlearn (Zhang et al. 2024) proposes bilevel optimiza- tion but requires curated external data to preserve utility. In contrast, STEREO uses a compositional objective with ad- versarial prompts without the need for external data. RACE (Kim, Min, and Yang 2024) focuses on computationally effi- cient adversarial training using single-step textual inversion, but at the cost of utility. Most current robust concept erasure methods evaluate on discrete attacks (UnlearnDiff (Zhang et al. 2023b) and RAB (Tsai et al. 2023)) with limited prompt token modifications. Our work additionally evaluates on the CCE attack (Pham, Marshall, and Hegde 2023), which has a larger, unconstrained search space, presenting a more chal- lenging defense scenario. 2.1 Preliminaries Latent Diffusion Models (LDMs): We implement our method using Stable Diffusion (Rombach et al. 2022), a state- of-the-art LDM for T2IG. LDMs are denoising-based prob- abilistic models that perform forward and reverse diffusion processes in the low (d)-dimensional latent space Z ⊆ Rd of a pre-trained variational autoencoder. An LDM comprises of two main components: an autoencoder and a diffusion model. The autoencoder includes an encoder (E : X → Z) that maps image x ∈ X (X denotes the image space) to latent codes z = E(x) ∈ Z and a decoder (D : Z → X ) that recon- structs images from latent codes, ensuring D(E(x)) ≈ x. The diffusion model is trained to produce latent codes within the learned latent space through a sequence of denoising steps. It consists of an UNet-based noise predictor ϵθ(.), which pre- dicts the noise ϵ added to zt at each timestep t. In T2IG, the diffusion model is additionally conditioned on text prompts p ∈ T (T denotes the text space), encoded by a jointly trained text encoder Yψ : T → P (P denotes the text embedding space). The training objective of LDM is given by: LLDM = Ezt∈E(x),t,p,ϵ∼N (0,1) [ ∥ϵ − ϵθ(zt, t, Yψ(p))∥2 2] . (1) To minimize this objective, θ and ψ are optimized jointly. Thus, the complete T2IG model can be denoted as fϕ : T → X , where fϕ := {E, D, ϵθ, Yψ}. During inference, CFG di- rects the noise at each step toward the desired text prompt p as ˜ϵθ(zt, t, Yψ(p)) = ϵθ(zt, t) + α(ϵθ(zt, t, Yψ(p)) − ϵθ(zt, t)), where the guidance scale α > 1. The inference process starts from a Gaussian noise zT ∼ N (0, 1) and is iteratively denoised using ˜ϵθ(zt, t, Yψ(p)) to obtain zT −1. This process is done sequentially until the final latent code z0 is obtained, which in turn is decoded into an image x0 = D(z0). Thus, x0 = fϕ(p). Compositional Inference. Compositional inference in T2IG models refers to the process of generating new samples by combining and manipulating the learned representations of multiple concepts (Liu et al. 2022). The objective function for compositional inference is given by: ˜ϵθ (zt, t) = ϵθ (zt, t) + N∑ j=1ηj(ϵθ (zt, t, Yψ(pj)) − ϵθ (zt, t)), (2) where N denotes the number of concepts and ηj is the guid- ance scale for concept cj (which is expressed as prompt pj), j ∈ [N ]. Note that η should be positive for the target concept and negative for undesired concepts. 3 Proposed Methodology 3.1 Problem Statement Let fϕ be a pre-trained T2IG model that generates an image x0 based on the input text prompt p. Let C denote the con- cept space. The goal of vanilla concept erasing is to modify the given T2IG model such that the concept erased model (CEM) ˜fϕ does not generate images containing the unde- sired/target concept cu ∈ C, when provided with natural text prompts directly expressing the target concept (e.g., nu- dity) or simple paraphrased versions of it (e.g., a person without clothes). This work deals with adversarially robust concept erasing, which aims to modify the given T2IG model such that the CEM ˜fϕ does not generate images containing the undesired concept even when prompted using malicious prompts (either directly from the text space T or from the text embedding space P). Note that the malicious prompts may or may not explicitly contain the target concept. Further- more, the CEM should be able to generate images depicting benign/non-target concepts (those that have not been erased) with the same fidelity as the original T2IG model. Let OX : X × C → {0, 1} and OT : T × C → {0, 1} be ground-truth oracles that verify the presence of a con- cept c ∈ C in an image and in a text prompt, respec- tively. OX (x, c) = 1 if concept c is depicted in image x (and 0, otherwise). Similarly, OT (p, c) = 1 if concept c is expressed in prompt p (and 0, otherwise). The con- cept generation ability of a T2IG model can be quantified as A(c) = Pp∼T ([OX (fϕ(p), c) = 1]|[OT (p, c) = 1]), where P denotes a probability measure. In other words, the T2IG model should faithfully generate images with a concept c, if the concept is present in the input text prompt p. The utility of the T2IG model can be defined as U = Ec∼CA(c). An ideal CEM should satisfy the following three properties: (1) Effectiveness - quantified as ̃A(cu) = 1 − Pp∼T ([OX ( ˜fϕ(p), cu) = 1]|[OT (p, cu) = 1]), which should be as high as possible for the CEM ˜fϕ. (2) Robustness - defined as ̃R(cu) = 1 − Pp∗∼T ([OX ( ˜fϕ(p ∗), cu) = 1]), where p∗ denotes an adversarial prompt. (3) Utility preser- vation - the utility of the CEM, which is defined as ̃U(cu) = Ec∼C\\{cu}A(c), should be as close as possible to U. Thus, given a pre-trained T2IG model fϕ and an undesired concept cu, the problem of adversarially robust concept eras- ing can be formally stated as follows: maximize both ̃A(cu) (effectiveness) and ̃R(cu) (robustness), while maintaining high utility ̃U(cu). Achieving all three objectives simultane- ously is challenging, as they are inherently related and often conflicting. For instance, aggressive concept removal may lead to a significant loss in utility, while being over-cautious may compromise effectiveness and robustness. Striking the right balance between these objectives is critical for develop- ing a good concept erasing method. 3.2 The STEREO Approach To robustly and effectively remove an undesired concept from a pre-trained T2IG model while preserving high utility, we propose a two-stage approach as illustrated in Fig. 2. L2 L Concept Erasing Loss Textual Embedding Attack 𝜃0 𝜃0 ∗ Concept Erasing Loss 𝜃1 𝜃1 ∗ Textual Embedding Attack Concept Erasing Loss 𝜃2 𝜃2 ∗ ... ... ... ... ... ...“Parachute’’ “Parachute’’ Search Thoroughly Enough Stage Robustly Erase Once Stage Learned Frozen Min-Max 𝜃0 ∗... 𝜃0 ′ ... Set of Adversarial Prompts from Stage 1 “Parachute in the sky’’ Set of Adversarial Prompts from Stage 1 Random Sampling L2 Loss Backwards Negative Guidance Positive Guidance Anchor Concept Gallery Images Textual Embedding Attack Concept Erasing Loss 𝜃𝐾 ... 𝜃𝐾 ∗ ... ... ... ... Set of Adversarial Prompts ... ... Diffusion Model Figure 2: Overview of STEREO. Our novel two-stage approach robustly erases target concepts from pre-trained text-to- image generation models while preserv- ing high utility for benign concepts. Stage 1 (top): Search Thoroughly Enough fine- tunes the model through iterative con- cept erasing and concept inversion at- tacks, ensuring resilience against adver- sarial regeneration attempts. Stage 2 (bot- tom): Robustly Erase Once fine-tunes the model using anchor concept and the set of strong adversarial prompts from Stage 1 via a compositional objective, maintaining high-fidelity generation of benign concepts while robustly erasing the target concept. Search Thoroughly Enough (STE) Stage: The goal of this stage is to discover a set of strong adversarial prompts that can regenerate the erased concept from the CEM. To achieve this goal, we draw inspiration from the success of ad- versarial training (AT) in enhancing the robustness of classi- fication models (Madry et al. 2017). To effectively find these adversarial prompts, we formulate the task as a min-max optimization problem, aiming to minimize the probability of generating images containing the undesired concept by modifying the T2IG model, while simultaneously finding ad- versarial prompts that maximize the probability of generating undesired images. Formally, the task objective is defined as: min ϕ max p∗ P([OX (fϕ(p∗), cu) = 1]), (3) where the probability P is defined over the stochasticity of zT , which represents the Gaussian noise used to initialize the inference process. To solve this problem, we employ an iterative approach that alternates between two key steps: (1) Minimization - erasing the target concept in the parameters space of the pre-trained T2IG model (specifically by altering the UNet parameters θ), and (2) Maximization - searching for adversarial prompts in the text embedding space that can regenerate the erased concept from the altered model. Minimization Step: At each step i of minimization, we aim to erase the target concept cu from the current UNet model ϵθi using its inherent knowledge preserved in θi. Specifi- cally, we create a copy of parameters of ϵθi denoted as θ∗ i , and keep θ∗ i frozen while fine-tuning θ with guidance from θ∗ i . The fine-tuning process aims to minimize the probabil- ity of generating an image x0 ∈ X that includes an unde- sired concept cu. To achieve this, we compute the negative- guidance noise estimate (Ho and Salimans 2021; Gandikota et al. 2023) to redirect the predicted noise away from a text prompt pu (containing cu) using: ˜ϵθ∗ i (zt, t, Yψ(pu)) ← ϵθ∗ i (zt, t) − η(ϵθ∗ i (zt, t, Yψ(pu)) − ϵθ∗ i (zt, t)), where η is the negative-guidance strength. The negative guidance is com- puted using the frozen parameters θ∗ i , which acts as the ground truth to fine-tune θi at every timestep t, to ensure the minimization of the concept erasing objective: LCE = Ezt∈E(x),t,pu [∥ϵθi(zt, t, Yψ(pu)) −˜ϵθ∗ i (zt, t, Yψ(pu))∥ 2 2]. (4) In this way, the conditional prediction of the fine-tuned model ϵθi(zt, t, Yψ(pu)) is progressively guided away from the un- desired concept cu at each minimization step. Maximization Step: While the minimization step aims to remove the undesired concept cu, the maximization step iden- tifies malicious prompts p ∗ that challenge the model’s robust- ness. (Yang et al. 2024) show that there may be alternative mappings that can regenerate cu. A naive approach to find these alternative mappings would be to collect synonymous prompts of the concept and incorporate them into the erasing objective of Eq. 4 during the minimization step. This can be achieved by randomly conditioning either the original prompt or its synonym in the erasing objective at every iteration, aim- ing to minimize the effect of both representations. However, as demonstrated in Fig. 3 (left), this naive approach is inef- fective, as the model remains vulnerable to attacks due to the lack of diverse and optimal alternate concept representations. To address this limitation, we use a textual inversion- based (Gal et al. 2022) maximization step to effectively iden- tify adversarial prompts. At each maximization step i, we search for an adversarial prompt p∗ i in the text embedding space of the frozen T2IG model that can reintroduce the erased concept cu. This is achieved by encoding the unde- sired visual concept into the text embedding space via the introduction of a new token s ∗ i into the existing vocabulary, specifically designed to represent cu. Each token in the vo- cabulary corresponds to a unique embedding vector, and our goal is to find the optimal embedding vector v∗ i for s ∗ i that effectively captures the characteristics of cu. For this, we utilize a pre-generated gallery set G (using the original T2IG model) depicting the target concept and obtain v∗ i as: v∗ i = argmin v Ezt∈E(x),x∼G,t,p,ϵ∼N (0,1) [∥ϵi − ϵθi(zt, t, [Yψ(ˆp)) || v]∥ 2 2], (5) where ϵi denotes the unscaled noise sample added at time step t, and [Yψ(ˆp)) || v] denotes the appending of the new embedding v to the embeddings of the existing vocabulary represented by Yψ(ˆp). The optimized embedding v∗ i becomes the representation of the token s ∗ i , and any prompt p ∗ i that includes s ∗ i can be considered an adversarial prompt. Figure 3: (A): Using only concept synonyms (Baseline) erases the concept but is vulnerable to attacks, as the \"Church\" concept is regenerated under the CCE (Pham, Marshall, and Hegde 2023) attack. The proposed STEREO approach identifies strong adversarial prompts p∗, which eventually facilitate robust erasing of the concept and make the concept erased model resistant against inversion attacks. (B): Regenerated image corresponding to the adversarial prompt learnt at each maximization step. Fig 3 (right) depicts the inverted image corresponding to the adversarial prompt at each maximization step. The ad- versarial prompt p ∗ i is then incorporated into the subsequent minimization step, and the process continues for K iterations. The STE stage thus identifies a set of strong and diverse adversarial prompts at the end of K min-max iterations: p ∗ K = {pu, p ∗ 1, · · · , p ∗ i , · · · , p ∗ K}. Robustly Erase Once (REO) Stage: Though the final erased UNet parameters ϵθK at the end of the STE stage lead to a highly robust CEM, the iterative erasing process greatly degrades the model utility. Instead of using ad-hoc methods to regularize the erasing process (Zhang et al. 2024) and pre- serve model utility, we propose an alternative approach that uses the set of adversarial prompts p ∗ K and robustly erases the target concept at one go. A naive way to implement this idea would be to incorporate the set of adversarial prompts p ∗ K into one of the baseline erasing objectives. Randomly sampling one adversarial prompt from this set as the prompt condition, at every fine-tune iteration, forces the objective to minimize the influence on each of these prompts. However, as we demonstrate in Table 1, this affects the utility when using only negative guidance (ESD (Gandikota et al. 2023)) or increases the attack success rate when using only positive guidance (AC (Kumari et al. 2023)). This is because using only negative guidance moves the model away from the tar- get without any regularization (FID scores go from 14.13 to 38.06) and using only positive guidance naively remaps each new word to a pre-defined target and thus not fully eras- ing the undesired concept (high ASR of 86.31). To preserve the model’s utility while maintaining its robustness, we ob- serve that additionally guiding the model towards an anchor concept (Kumari et al. 2023; Lu et al. 2024) creates a com- position of noise estimates that aids the model in selectively erasing only the target concept while instead moving closer to the anchor. For example, suppose we provide \"parachute in the sky\" as the anchor and \"parachute\" as the target/undesired concepts. In this case, the combined noise estimate moves closer towards the concept \"sky\" and away from \"parachute\". Specifically, to achieve this composition we build on the compositional guidance in Eq. 2 and incorporate the set of adversarial prompts from the STE stage such that: Table 1: Impact of compositional guidance objective used in the second stage of the proposed STEREO method, when the same adversarial prompt search is used in the first stage. Attack Methods (↓) CCE RAB Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ SD 1.4 74.73 94.73 90.52 14.13 31.33 ESD + adv prompts 0.00 35.78 0.00 38.06 26.25 AC + adv prompts 1.05 86.31 10.52 19.85 29.93 NSFW (Nudity) STEREO 0.00 4.21 1.05 25.44 29.38 ϵanchor = 1 L L∑ i=1 η(ϵθ∗ (zt, t, Cθ∗ (pa)) − ϵθ∗ (zt, t)) ϵerase = 1 K K∑ i=1 η(ϵθ∗ (zt, t, Cθ∗ (p∗ i )) − ϵθ∗ (zt, t)) ˆϵθ∗ (zt, t) = ϵθ∗ (zt, t) + ϵanchor − ϵerase, (6) where pa represents the prompt corresponding to the an- chor (for positive guidance) and L is the number of pos- itive anchors. Note that the noise estimates for the adver- sarial and anchor prompts are averaged to ensure neither the negative guidance nor the positive guidance overpow- ers each other. Finally, we use this compositional noise estimate as the ground truth and erase the concept using LSTEREO = Ezt∈E(x),t,pu [∥ϵθi(zt, t, Yψ(q)) − ˆϵθ∗ (zt, t)∥ 2 2], where a prompt q is randomly sampled from the set p ∗ K at each time step t. From Table 1, it can be observed that the proposed utility-preserving robust erasing objective function balances the robustness-utility trade-off more effectively. 4 Experiments In this section, we evaluate our proposed method against four baseline erasing methods and three concept inversion at- tacks. Following (Gandikota et al. 2023; Pham, Marshall, and Hegde 2023), we focus on removing four concept categories: Nudity, Artistic Style, Objects, and Identity. We present re- sults for Nudity and Artistic Style in the main paper, while results for the Objects and Identity categories are provided in the supplementary material. 4.1 Experiment Setup Baselines. We use Erased Stable Diffusion (ESD) (Gandikota et al. 2023), Ablating Concepts (AC) (Kumari et al. 2023), Unified Concept Erasure (UCE) (Gandikota et al. 2024), and Mass Concept Erasure (MACE) (Lu et al. 2024) as the baseline erasure methods. The three concept inversion adversarial attack considered in this work are: Circumventing Concept Erasure (CCE) (Pham, Marshall, and Hegde 2023), Ring-A-Bell (RAB) (Tsai et al. 2023), and UnlearnDiff (UD) (Zhang et al. 2023b). While the CCE attack uses textual inversion, the inversion process relies on images from the gallery set G. We use non-overlapping images for training and testing, ensuring that adversarial prompts used during testing do not overlap with those used during training. For both baseline erasure and attack methods, we used the pre-trained models and adversarial prompts provided by the authors for both the baseline erasure and attack methods. For concepts lacking pre-trained models or adversarial prompts, we reproduced results using the publicly available code. Evaluation Metrics. We measure Utility using FID (lower is better) and CLIP (higher is better) scores, following (Gandikota et al. 2023, 2024; Lu et al. 2024). For Effec- tiveness and Robustness, we use the attack success rate (ASR) (lower is better) on original and adversarial prompts respectively, following (Tsai et al. 2023; Pham, Marshall, and Hegde 2023). Nudity Removal. We evaluate nudity removal using 95 prompts from the I2P dataset (Schramowski et al. 2023) with nudity percentage above 50%, following (Tsai et al. 2023). We use the NudeNet1 detector to identify inappropriate labels and compute attack success rates for effectiveness and robustness. For erasure performance, we generate one image for each of the 95 prompts. To assess the CCE attack, we prepend the adversarial string p ∗ (learned for each nudity-erased model) to the 95 prompts and evaluate them with the detector. For the RAB attack, we use the author’s code to craft adversarial prompts for each of the 95 prompts using the provided concept vector. For the UD attack, we generate 95 images corresponding to the prompts and use the prompt-image pairs as ground truth to craft the attack. Artistic Style Removal. Following (Zhang et al. 2023b), we select “Van Gogh\" as the artistic style to erase. We use their provided style classifier to compute attack success rates for effectiveness and robustness. For erasure performance eval- uation, we generate 100 images with varying seeds using the base prompt “A painting in the style of Van Gogh\". For the CCE attack, we replace \"Van Gogh\" with the learned adversarial string p ∗ and generate 100 images with vary- ing seeds. For the RAB attack, we use the hyperparameters from the RAB paper’s Appendix. We generate 30 positive- negative prompt pairs for artistic style to obtain the concept vector, then craft an adversarial prompt (length=38, strength- coefficient=0.9) on the base prompt. We generate 100 images with varying seeds using this adversarial prompt. For the UD 1https://github.com/notAI-tech/NudeNet Table 2: Comparison of recent concept erasure methods against three different adversarial attacks for nudity erasure. Attack Methods (↓) UD RAB CCE Target Concepts Erasure Methods Erased (↓) (ECCV’24) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ SD 1.4 74.73 90.27 90.52 94.73 14.13 31.33 ESD(ICCV’23) 3.15 43.15 35.79 86.31 14.49 31.32 AC (ICCV’23) 1.05 25.80 89.47 66.31 14.13 31.37 UCE (WACV’24) 20.0 70.52 35.78 70.52 14.49 31.32 MACE(CVPR’24) 6.31 41.93 5.26 66.31 13.42 29.41 NSFW (Nudity) STEREO (Ours) 0.00 6.81 1.05 4.21 25.44 29.38 Table 3: Comparison of recent concept erasure methods against three adversarial attacks for artistic style erasure. UNet-C indicates updating only the cross-attention layers of the UNet, and UNet-NC indicates updating only the non-cross attention layers of the UNet. Attack Methods (↓) CCE RAB UD Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) (ECCV’24) FID (↓) CLIP ↑ SD 1.4 53.0 68.0 24.0 76.0 14.13 31.33 ESD(ICCV’23) 0.0 28.0 0.0 1.04 14.48 31.32 AC (ICCV’23) 0.0 56.8 0.0 1.00 14.40 31.21 UCE (WACV’24) 0.0 76.8 5.2 59.79 14.48 31.32 MACE(CVPR’24) 0.0 54.6 0.2 16.00 14.48 31.30 STEREO - UNet-C (Ours) 0.0 13.8 0.0 0.00 15.83 30.97 Artistic (Van Gogh) STEREO - UNet-NC (Ours) 0.0 0.20 0.0 0.00 28.44 30.00 attack, we generate 100 images with varying seeds on the base prompt and use these prompt-image pairs as ground truth to craft the attack. 4.2 Results Our proposed method aims to significantly improve the trade- off between effectiveness, robustness, and utility. Effectiveness: Tables 2 and 3 show that STEREO achieves negligible ASR for both nudity and Van Gogh erasure demon- strating its ability to effectively erase undesired concepts while optimizing for robustness and utility. Qualitative re- sults are presented in the supplementary material. Robustness: We evaluate robustness against three adversarial attacks. It is important to note that RAB and UD are con- strained by the prompt/token length hyperparameter, which limits the number of tokens available for crafting adversarial prompts. These tokens are derived from the existing vocabu- lary. Conversely, the CCE attack, which injects new tokens into the vocabulary, is unrestricted by such constraints, mak- ing it a much stronger attack. For nudity erasing (Table 2), STEREO reduces the ASR by over 80% compared to SD across all attacks. Notably, against the stronger CCE attack, STEREO reduces the ASR by 60% compared to other base- lines. Figure 4 qualitatively demonstrates STEREO’s robust- ness. Similarly, for art style erasing (Table 3), STEREO re- duces ASR by more than 20% against the CCE attack. These results validate that our two-stage robust concept erasing method offers better defense against state-of-the-art attacks without significantly compromising utility and effectiveness. Additional qualitative results on robustness to RAB attack for nudity erasing and CCE attack for art-style erasing are in the supplementary material. Utility Preservation: As shown in Tables 2 and 3, STEREO’s FID values, which indicate changes in image distribution, have increased, while the CLIP scores, reflecting image-text alignment, remain close to the SD. To further analyze this pattern, we compared the generated images using a subset of COCO-30K prompts across different methods, as illustrated in Figure 5. We observe that STEREO adheres closely to Figure 4: Visualization of nudity under the CCE attack across different methods. (*) added by authors for publication. SD v1.4 ESD UCE MACE STEREO (Ours) Figure 5: Comparing erased model’s generation performance on benign concepts across different methods. We observe STEREO’s ability to generate images with the same content as SD 1.4 but compose them differently than other baseline methods. (Image best viewed when magnified.) the input prompt, generating faithful images but composes images slightly different than SD 1.4. This difference leads to an increase in the FID score while maintaining a CLIP score close to the baseline. Hence, we conclude that STEREO manages the trade-off between effectiveness, robustness, and utility significantly better than baselines. 4.3 Ablation Study This robustness vs. utility trade-off can be further tuned using two hyperparameters: (a) the strength of the guidance scale η and (b) the number of adversarial prompts decribed K in Eq. 6. We present additional ablation results on the (i) Effect of parameter subset to fine-tune, and (ii) Effect of anchor prompts, in the supplementary material. Effect of number of adversarial prompts To under- stand how the number of adversarial prompts impacts the robustness-utility trade-off, we design an experiment that systematically increases the number of adversarial prompts K in Eq. 6 and reports the results in Table 4. We observe that, with just one adversarial prompt, the performance of the proposed method improves over the baseline, with a mini- mal impact on utility. As we further increase the number of adversarial prompts, we observe that the effectiveness and robustness improve but saturate quickly, while the increas- Table 4: Impact of the number of adversarial prompts on the robustness-utility trade-off. We set guidance scale η = 5. Attack Methods (↓) CCE RAB Target Concepts Number of Adv. Prompts Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ SD 1.4 74.73 94.73 90.52 14.13 31.33 ESD(ICCV’23) 3.15 86.31 35.79 14.49 31.32 1 1.05 57.89 17.89 18.14 30.42 2 0.00 29.47 4.21 24.26 29.88 3 0.00 4.21 1.05 25.44 29.38 4 0.00 1.05 3.15 26.17 28.88 NSFW (Nudity) 5 1.05 0.00 4.21 29.66 28.68 Table 5: Impact of the guidance scale (η) on the robustness- utility trade-off. The number of adversarial prompts = 3. Attack Methods (↓) CCE RAB Target Concepts Guidance Scale η Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ SD 1.4 74.73 94.73 90.52 14.13 31.33 ESD(ICCV’23) 3.15 86.31 35.79 14.49 31.32 1.0 1.05 62.10 11.57 16.96 28.88 3.0 0.00 36.84 6.31 21.77 29.16 5.0 0.00 4.21 1.05 25.44 29.38 7.5 1.05 7.36 0.00 29.84 28.52 NSFW (Nudity) 10 0.00 7.36 0.00 30.33 28.83 ing adversarial prompts affect the utility. We hence choose K = 3 adversarial prompts to obtain the optimal trade-off. Effect of guidance scale To understand the impact of the guidance scale η, we fix K = 3, while varying the value of η in Eq. 6. From Table 5, we observe that, as we increase the strength of the guidance scale (η), the model becomes more robust toward the attacks. However, the utility of the surrounding concept gets affected as can be seen with the increasing FID and CLIP scores. Hence, choosing the right η value is important to maintain the trade-off between the three key properties and we fix η = 5.0. 5 Conclusion and Limitations Our proposed approach STEREO effectively addresses ro- bustly erasing concepts from pre-trained text-to-image dif- fusion models, while significantly improving the robustness- utility trade-off. STEREO employs a two-stage approach: an adversarial prompt search stage that iteratively erases the undesired concept and finds adversarial prompts and a utility-preserving erasure stage that uses an anchor-concept- based compositional objective to maintain the model’s util- ity. Benchmarking against four state-of-the-art methods and three types of attacks across diverse categories demon- strates STEREO’s superior performance in balancing the robustness-utility trade-off. However, STEREO may have limitations in erasing multiple concepts simultaneously while maintaining robustness, and its multiple min-max iterations result in relatively higher computational time for computing the adversarial prompts. In our future work, we would like to explore the direction of multi-concept robust concept erasure, while taking less time to find adversarial prompts. 6 Acknowledgments The authors sincerely thank Amandeep Kumar (Johns Hop- kins University) and Uzair Khattak (MBZUAI) for their in- valuable assistance with the figures and tables in this paper. References AUTOMATIC1111. 2022. Negative prompt. https://github.com/AUTOMATIC1111/stable-diffusion- webui/wiki/Negative-prompt. Brack, M.; Friedrich, F.; Hintersdorf, D.; Struppek, L.; Schramowski, P.; and Kersting, K. 2023. SEGA: Instructing text-to-image models using semantic guidance. In Thirty- seventh Conference on Neural Information Processing Sys- tems. Carlini, N.; Jagielski, M.; Zhang, C.; Papernot, N.; Terzis, A.; and Tramer, F. 2022. The privacy onion effect: Memoriza- tion is relative. Advances in Neural Information Processing Systems, 35: 13263–13276. Chang, H.; Zhang, H.; Barber, J.; Maschinot, A.; Lezama, J.; Jiang, L.; Yang, M.-H.; Murphy, K.; Freeman, W. T.; Rubinstein, M.; et al. 2023. Muse: Text-to-image gener- ation via masked generative transformers. arXiv preprint arXiv:2301.00704. Chin, Z.-Y.; Jiang, C.-M.; Huang, C.-C.; Chen, P.-Y.; and Chiu, W.-C. 2023. Prompting4debugging: Red-teaming text- to-image diffusion models by finding problematic prompts. arXiv preprint arXiv:2309.06135. Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei- Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248–255. Ieee. Ding, M.; Zheng, W.; Hong, W.; and Tang, J. 2022. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35: 16890–16902. Dong, P.; Guo, S.; Wang, J.; Wang, B.; Zhang, J.; and Liu, Z. 2024. Towards Test-Time Refusals via Concept Negation. Advances in Neural Information Processing Systems, 36. Gal, R.; Alaluf, Y.; Atzmon, Y.; Patashnik, O.; Bermano, A. H.; Chechik, G.; and Cohen-Or, D. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618. Gandikota, R.; Materzy´nska, J.; Fiotto-Kaufman, J.; and Bau, D. 2023. Erasing Concepts from Diffusion Models. In Pro- ceedings of the 2023 IEEE International Conference on Com- puter Vision. Gandikota, R.; Orgad, H.; Belinkov, Y.; Materzy´nska, J.; and Bau, D. 2024. Unified Concept Editing in Diffusion Models. IEEE/CVF Winter Conference on Applications of Computer Vision. Giphy. 2020. Giphy celebrity detector. https://github.com/ Giphy/celeb-detection-oss. Heng, A.; and Soh, H. 2024. Selective amnesia: A continual learning approach to forgetting in deep generative models. Advances in Neural Information Processing Systems, 36. Ho, J.; and Salimans, T. 2021. Classifier-Free Diffusion Guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. Ho, J.; and Salimans, T. 2022. Classifier-free diffusion guid- ance. arXiv preprint arXiv:2207.12598. Howard, J.; and Gugger, S. 2020. Fastai: A Layered API for Deep Learning. Information, 11(2): 108. Huang, C.-P.; Chang, K.-P.; Tsai, C.-T.; Lai, Y.-H.; and Wang, Y.-C. F. 2024. Receler: Reliable concept erasing of text-to- image diffusion models via lightweight erasers. ECCV. Hunter, T. 2023. AI porn is easy to make now. For women, that’s a nightmare. The Washington Post, NA–NA. Jiang, H. H.; Brown, L.; Cheng, J.; Khan, M.; Gupta, A.; Workman, D.; Hanna, A.; Flowers, J.; and Gebru, T. 2023. AI Art and its Impact on Artists. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 363–374. Kim, C.; Min, K.; and Yang, Y. 2024. RACE: Robust Adver- sarial Concept Erasure for Secure Text-to-Image Diffusion Model. ECCV (Oral). Kumari, N.; Zhang, B.; Wang, S.-Y.; Shechtman, E.; Zhang, R.; and Zhu, J.-Y. 2023. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF Interna- tional Conference on Computer Vision, 22691–22702. Liu, N.; Li, S.; Du, Y.; Torralba, A.; and Tenenbaum, J. B. 2022. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, 423–439. Springer. Lu, S.; Liu, Y.; and Kong, A. W.-K. 2023. Tf-icon: Diffusion- based training-free cross-domain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2294–2305. Lu, S.; Wang, Z.; Li, L.; Liu, Y.; and Kong, A. W.-K. 2024. MACE: Mass Concept Erasure in Diffusion Models. arXiv preprint arXiv:2403.06135. Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu, A. 2017. Towards deep learning models resistant to adversar- ial attacks. arXiv preprint arXiv:1706.06083. Nichol, A. Q.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin, P.; Mcgrew, B.; Sutskever, I.; and Chen, M. 2022. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In International Conference on Machine Learning, 16784–16804. PMLR. Pham, M.; Marshall, K. O.; and Hegde, C. 2023. Circum- venting concept erasure methods for text-to-image generative models. arXiv preprint arXiv:2308.01508. Rando, J.; Paleka, D.; Lindner, D.; Heim, L.; and Tramèr, F. 2022. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610. Ren, J.; Li, Y.; Zen, S.; Xu, H.; Lyu, L.; Xing, Y.; and Tang, J. 2024. Unveiling and Mitigating Memorization in Text- to-image Diffusion Models through Cross Attention. arXiv preprint arXiv:2403.11052. Rombach, R. 2022. Stable diffusion 2.0 release. https:// stability.ai/news/stable-diffusion-v2-release. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om- mer, B. 2022. High-resolution image synthesis with latent dif- fusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684–10695. Roose, K. 2022. An AI-Generated Picture Won an Art Prize. Artists Are not Happy. Schramowski, P.; Brack, M.; Deiseroth, B.; and Kersting, K. 2023. Safe Latent Diffusion: Mitigating Inappropriate De- generation in Diffusion Models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Schuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C.; Wight- man, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis, C.; Wortsman, M.; et al. 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Ad- vances in Neural Information Processing Systems, 35: 25278– 25294. Singh, J.; Shrivastava, I.; Vatsa, M.; Singh, R.; and Bharati, A. 2024. Learn\" No\" to Say\" Yes\" Better: Improving Vision-Language Models via Negations. arXiv preprint arXiv:2403.20312. Somepalli, G.; Singla, V.; Goldblum, M.; Geiping, J.; and Goldstein, T. 2023. Understanding and mitigating copying in diffusion models. Advances in Neural Information Process- ing Systems, 36: 47783–47803. Tsai, Y.-L.; Hsu, C.-Y.; Xie, C.; Lin, C.-H.; Chen, J.-Y.; Li, B.; Chen, P.-Y.; Yu, C.-M.; and Huang, C.-Y. 2023. Ring- A-Bell! How Reliable are Concept Removal Methods for Diffusion Models? arXiv preprint arXiv:2310.10012. Yang, Y.; Liu, H.; Shao, W.; Chen, R.; Shang, H.; Wang, Y.; Qiao, Y.; Zhang, K.; Luo, P.; et al. 2024. Position Paper: Towards Implicit Prompt For Text-To-Image Models. arXiv preprint arXiv:2403.02118. Zhang, E.; Wang, K.; Xu, X.; Wang, Z.; and Shi, H. 2023a. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591. Zhang, Y.; Chen, X.; Jia, J.; Zhang, Y.; Fan, C.; Liu, J.; Hong, M.; Ding, K.; and Liu, S. 2024. Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models. arXiv preprint arXiv:2405.15234. Zhang, Y.; Jia, J.; Chen, X.; Chen, A.; Zhang, Y.; Liu, J.; Ding, K.; and Liu, S. 2023b. To generate or not? safety- driven unlearned diffusion models are still easy to generate unsafe images... for now. arXiv preprint arXiv:2310.11868. STEREO: Towards Adversarially Robust Concept Erasing from Text-to-Image Generation Models – Supplementary Material This supplementary material is organized as follows: In Section 7, we first discuss the Algorithm details. Section 8 presents the implementation details. Section 9 extends STEREO’s comparison against other adversarial training methods. Section 10 presents additional experimental results, and Section 11 provides additional qualitative results. 7 Algorithm Details: STEREO Our proposed STEREO approach for adversarially robust con- cept erasing from text-to-image diffusion models is detailed in Algorithm 1. The method consists of two stages: Search Thoroughly Enough (STE) and Robustly Erase Once (REO). In the STE stage, we iteratively alternate between erasing the undesired concept and identifying strong adversarial prompts that can regenerate it. This process involves a minimization step to fine-tune the model parameters and a maximization step to find adversarial prompts using textual inversion. The REO stage then leverages the set of adversarial prompts ob- tained from the STE stage to perform a robust erasure. It employs a compositional noise estimate that combines posi- tive guidance from anchor concepts and negative guidance from adversarial prompts. This two-stage approach allows STEREO to achieve a better balance between effectiveness, robustness, and utility preservation in concept erasure tasks. 8 Implementation Details To erase nudity, identity, and objects we finetune the non- cross-attention layers of the UNet following (Gandikota et al. 2023). The gallery set G used during the maximization step in the Search Thoroughly Enough (STE) stage, is prepared in the following way: For the object category, we generate 500 images using the pre-trained SD 1.4 model using the prompt \"A photo of a <object>\" and pass it through a pre- trained Resnet-50 Imagenet (Deng et al. 2009) classifier to filter the samples that belong to the specified object. Simi- larly, for the identity and nudity categories, we generate 500 images each and filter them using the Giphy (Giphy 2020) and Nudenet detectors respectively. For the artistic style, fol- lowing (Gandikota et al. 2023) we update the cross-attention layers of the UNet. Additionally, in line with our finding from section 10.1, we update the non-cross-attention layers of the UNet and present the results. The gallery set G consists of 500 images generated using the prompt A painting in the style of Van Gogh. We also uniformly set guidance scale η = 5 and the number of adversarial prompts = 3 for the Robust Erasing stage. We train our model on one NVIDIA A100 GPU for each concept to be erased. 9 Comparison with Adversarially Trained Models for Nudity and Art Style Removal Table 6 (Nudity Removal) and Table 7 (Art-Style Removal) compare various concept erasure methods, including recent adversarial-trained approaches like RACE (Kim, Min, and Yang 2024) and AdvUnlearn (Zhang et al. 2024). For nudity removal, RACE and AdvUnlearn show improved erasure and Table 6: STEREO comparison on Nudity Concept Removal with Adversarially Trained Models RACE (Kim, Min, and Yang 2024) and AdvUnlearn (Zhang et al. 2024). Attack Methods (↓) RAB CCE Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ SD 1.4 74.73 90.52 94.73 14.13 31.33 ESD(ICCV’23) 3.15 35.79 86.31 14.49 31.32 AC (ICCV’23) 1.05 89.47 66.31 14.13 31.37 UCE (WACV’24) 20.0 35.78 70.52 14.49 31.32 MACE(CVPR’24) 6.31 5.26 66.31 13.42 29.41 RACE(ECCV’24) 4.21 6.31 71.57 14.49 31.32 AdvUnlearn(arXiv’24) 1.05 0.0 77.89 19.15 29.30 NSFW (Nudity) STEREO (Ours) 0.00 1.05 4.21 25.44 29.38 Table 7: STEREO comparison on Art-Style Removal with Adver- sarially Trained Models RACE (Kim, Min, and Yang 2024) and AdvUnlearn (Zhang et al. 2024). UNet-C indicates updating only the cross-attention layers of the UNet, and UNet-NC indicates up- dating only the non-cross attention layers of the UNet. Attack Methods (↓) CCE RAB Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ SD 1.4 53.0 68.0 24.0 14.13 31.33 ESD(ICCV’23) 0.0 28.0 0.0 14.48 31.32 AC (ICCV’23) 0.0 56.8 0.0 14.40 31.21 UCE (WACV’24) 0.0 76.8 5.2 14.48 31.32 MACE(CVPR’24) 0.0 54.6 0.2 14.48 31.30 RACE(ECCV’24) 0.0 95.0 0.0 14.49 31.32 AdvUnlearn(arXiv’24) 0.0 29.0 0.0 17.03 30.80 STEREO - UNet-C (Ours) 0.0 13.8 0.0 15.83 30.97 Artistic (Van Gogh) STEREO - UNet-NC (Ours) 0.0 0.20 0.0 28.44 30.00 robustness over earlier methods, particularly against RAB attacks, but remain vulnerable to CCE attacks. They maintain relatively good image quality and text-image alignment. For art-style removal, both methods achieve perfect erasure, with AdvUnlearn showing better robustness against CCE attacks compared to RACE. However, AdvUnlearn experiences a slight degradation in image quality and text-image alignment. In both tables, the proposed STEREO method demonstrates superior erasure and robustness across all attack types, espe- cially against the challenging CCE attack. STEREO’s per- formance is particularly notable in art-style removal, where its UNet-NC variant achieves near-perfect robustness against stronger CCE attacks. However, this comes at the cost of sligl- htly higher FID scores. From qualitative results, we observe that STEREO adheres closely to the input prompt, generating faithful images but composes images slightly different than SD 1.4. This difference leads to an increase in the FID score while maintaining a CLIP score close to the baseline. Hence, we conclude that STEREO manages the trade-off between effectiveness, robustness, and utility significantly better than baselines. 10 Additional Experimental Results 10.1 Effect of parameter subset to fine-tune To understand the impact of parameter selection on the robustness-utility trade-off, we fine-tune various parameter subsets of the T2IG model using the objective proposed in the Algorithm 1: STEREO: Adversarially Robust Concept Erasing from T2I Diffusion Models Input: Pre-trained T2IG model fϕ, undesired concept cu, number of iterations K, guidance scale η, number of anchor prompts L Stage 1: Search Thoroughly Enough (STE) Initialize p ∗ K = {pu} ▷ Initialize with prompt containing undesired concept for i = 1 to K do θ∗ i ← θi ▷ Create copy of current UNet parameters Minimization Step: Erase concept cu from fϕ. ▶ Freeze parameters θ∗ i of fϕ. ▶ Fine-tune model parameters θi to minimize LCE using Eq. 4: LCE = Ezt∈E(x),t,pu [∥ϵθi (zt, t, Yψ(pu)) − ˜ϵθ∗ i (zt, t, Yψ(pu))∥ 2 2] Maximization Step: Identify adversarial prompt p∗ i . ▶ Find adversarial prompt p ∗ i using textual inversion by optimizing Eq. 5: v∗ i = arg min v Ezt∈E(x),x∼G,t,p,ϵ∼N (0,1) [∥ϵi − ϵθi (zt, t, [Yψ(p)] ∥ v)∥ 2 2] ▶ p ∗ K ← p∗ K ∪ {p ∗ i } ▷ Add new adversarial prompt end for Stage 2: Robustly Erase Once (REO) Input: Set of adversarial prompts p ∗ K = {pu, p ∗ 1, . . . , p∗ K} from Stage 1. ▶ Initialize θ∗ with original UNet parameters ▶ Define compositional noise estimates using Eq. 6: ϵanchor = 1 L L∑ i=1 η(ϵθ∗ (zt, t, Cθ∗ (pa)) − ϵθ∗ (zt, t)), ϵerase = 1 K K∑ i=1 η(ϵθ∗ (zt, t, Cθ∗ (p∗ i )) − ϵθ∗ (zt, t)) ▶ Compute final compositional noise estimate: ˆϵθ∗ (zt, t) = ϵθ∗ (zt, t) + ϵanchor − ϵerase ▶ Robustly Erase concept: Fine-tune θ to minimize LST EREO with compositional noise: LSTEREO = Ezt∈E(x),t,pu [∥ϵθi (zt, t, Yψ(q)) − ˆϵθ∗ (zt, t)∥2 2] ▶ ˜fφ ← Updated T2I diffusion model with fine-tuned θ ▷ Concept erased model return ˜fφ Robustly Erase Once (REO) stage. We examine four subsets: a) text encoder parameters, b) entire UNet parameters, c) only cross-attention layers in the UNet (UNet-C), and d) only non-cross-attention layers in the UNet (UNet-NC). Table 8 shows that updating only the text encoder yields poor utility on surrounding concepts. This is evidenced by lower CLIP scores, indicating the model’s failure to follow prompts, and higher FID scores, suggesting generated images differ sig- nificantly from the originals. While this approach improves erasure effectiveness and robustness against text-based at- tacks like RAB, it remains vulnerable to inversion attacks such as CCE, which can reintroduce erased concepts through new tokens. On the other hand, updating UNet parameters generally yields better results. Specifically, fine-tuning only the cross-attention layers maintains utility and demonstrates good effectiveness, but with weaker robustness. Updating the entire UNet improves effectiveness and utility but re- mains vulnerable to CCE attacks. In line with findings from (Gandikota et al. 2023), we observe that for concepts with Table 8: Understanding the effect of the parameter subset to fine-tune using the proposed objective in Robustly Erase Once (REO) stage. Attack Methods (↓) CCE RAB Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ SD 1.4 74.73 94.73 90.52 14.13 31.33 ESD 3.15 86.31 35.79 14.49 31.32 STEREO (Text-encoder) 0.00 72.61 1.05 36.51 23.36 STEREO (UNet-Full) 1.05 40.00 4.21 23.97 29.50 STEREO (UNet-C) 3.15 40.00 25.26 19.40 30.24 NSFW (Nudity) STEREO (UNet-NC) 0.00 4.21 1.05 25.44 29.38 global effects (such as nudity or objects), updating the un- conditional layers (non-cross-attention layers of the UNet) achieves a superior balance across the three key properties of effectiveness, robustness, and utility preservation. Table 9: Understanding the effect of anchor prompts (pa) on the performance of STEREO. The \"/\" in Anchor Prompts indicates different prompts in the positive direction averaged together. Attack Methods (↓) CCE RAB Target Concepts Number of Adv. Prompts Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP ↑ Choice of Anchor Prompt \"a person wearing clothes\" 0.00 43.15 0.00 28.10 28.69 \"no nudity\" 34.73 17.89 93.68 23.37 29.79 NSFW (Nudity) \"clothing\" / \"nudity\" 0.00 4.21 1.05 25.44 29.38 Averaging Noise Guidance of Anchors \"clothing, nudity\" 49.47 26.31 90.52 23.85 29.83NSFW (Nudity) \"clothing\" / \"nudity\" 0.00 4.21 1.05 25.44 29.38 10.2 Effect of Anchor Prompts. In the Robustly Erase Once (REO) section of the main paper, we introduce the need for a positive anchor and also demonstrate the effectiveness of compositional guidance. To further understand how the specific anchor prompt(s) affect the performance of STEREO, we design two experiments and present the results in Table 9. Choice of anchor prompts: In Table 9 we observe that the choice of anchor concept (used to guide the model towards a target) impacts the erasing and robustness performance. For example, when using the prompt \"a person wearing clothes\" as the anchor, we observe the effectiveness of erasing and the robustness against text-based attack (RAB). However, the utility and the performance against concept-inversion attack (CCE) is affected. This could be attributed to the anchor being generic and hence not providing a precise noise estimate to guide the model away. On the other hand, when using \"no nudity\" as the anchor, we observe a higher attack success rate. This could be attributed to CLIP’s inability to handle negation (Singh et al. 2024), and hence the word \"nudity\" is treated as the anchor. Alternatively, using \"clothing\"/\"nudity\" provides a more precise control where the model moves closer to \"clothing\" and away from \"nudity\". We hence observe that having the concept to be erased (\"nudity\" here) as part of the anchor prompt, along with the desired safe prompts (\"clothing\") is more effective and provides a better robustness-utility trade-off. Averaging noise guidance of anchors: Building on the observation from the \"choice of anchor prompt\" experi- ment, the anchor concept (for positive guidance) can be either a single concept or an average of multiple positive concepts. From Table 9 we observe that averaging the noise estimates, corresponding to different anchor prompts (\"clothing\"/\"nudity\" - \"/\" denotes averaging) performs better than using only a single positive anchor prompt (\"clothing, nudity\"). We attribute this to the fact that averaging the noise estimates of the anchors individually, gives equal weightage to each of the prompts compared to a single anchor prompt, and hence the desired direction (\"clothing\") is not overpowered by the prompt \"nudity\". Table 10: Training time of STEREO compared with different baselines for nudity erasing. Attack Methods (↓) CCE RAB UD Target Concepts Erasure Methods Adv-Prompt-Search Stage (mins per min-max step) Erasing Stage (mins) (ICLR’24) (ICLR’24) (ECCV’24) ESD(ICCV’23) N.A 30m 86.31 35.79 43.15 AC (ICCV’23) N.A 4m 66.31 89.47 25.80 UCE (WACV’24) N.A 3m 70.52 35.78 70.52 NSFW (Nudity) STEREO (Ours) 40m 41m 4.21 1.05 6.81 10.3 Training Time Table 10 compares the training time of STEREO with baseline methods. UCE and AC exhibit the shortest concept erasure times but demonstrate poor robustness against CCE, RAB, and UnlearnDifAtk (UD) attacks. STEREO’s final erasing stage takes 41 minutes per concept, marginally longer than ESD, due to the computation and averaging of noise estimates for adversarial and positive prompts. However, STEREO sig- nificantly outperforms baselines in robustness, reducing at- tack success rates by 34% for both RAB and UD attacks and by over 60% for the CCE attack. The Search Thoroughly Enough (STE) stage of STEREO requires an additional 40 minutes per adversarial prompt using the min-max approach. Importantly, this stage is independent of the final erasing stage and can be executed in advance to identify adversarial prompts, allowing for more efficient overall training. 10.4 Exposed Body Part Count Table 11 presents the exposed body part count detected by the NudeNet detector on the I2P benchmark. Following (Heng and Soh 2024; Lu et al. 2024), we set the NudeNet detector threshold to 0.6 and compare the results of nudity-erased STEREO with baseline methods. Consistent with the nu- dity erasure results reported in Table 1 of the main paper, STEREO significantly reduces the exposed body part count. This demonstrates STEREO’s superior ability to erase nudity, thus satisfying the effectiveness property of concept erasure. 10.5 Inappropriate Image Prompts (I2P) Benchmark To evaluate the inappropriate content generated by the text- to-image diffusion models, the I2P dataset was introduced in (Schramowski et al. 2023). The dataset contains 4703 unique prompts with corresponding seeds, guidance scales, and in- appropriate categories, making it a definite benchmark to measure the safety of these models. To compute the inap- propriate score, 4703 images are first generated using the model we want to evaluate. Each of these images is then passed through two classifiers (Q16 2 and NudeNet 3) to get a combined score. In Table 12 we follow ESD (Gandikota et al. 2023) and report the performance of nudity-erased STEREO on the I2P benchmark. We can observe that similar to ESD, STEREO erased only for \"nudity\" is capable of reducing the inappropriateness across the broader categories of hate, ha- rassment, violence, self-harm, shocking and illegal-activity. 2https://github.com/ml-research/Q16 3https://github.com/notAI-tech/NudeNet Table 11: Quantity of explicit content detected using the Nudenet detector on the I2P benchmark. F: Female. M: Male. (Results for the baselines were sourced from the MACE (Lu et al. 2024) paper). Best results are marked in Bold and the second best results are underlined. Results of NudeNet Detection on I2P (Detected Quantity) Method Armpits Belly Buttocks Feet Breasts (F) Genitalia (F) Breasts (M) Genitalia (M) Total ↓ SD v1.4 (Rombach et al. 2022) 148 170 29 63 266 18 42 7 743 SD v2.1 (Rombach 2022) 105 159 17 60 177 9 57 2 586 ESD-u (Gandikota et al. 2023) 32 30 2 19 27 3 8 2 123 AC (Kumari et al. 2023) 153 180 45 66 298 22 67 7 838 UCE (Gandikota et al. 2024) 29 62 7 29 35 5 11 4 182 SLD-M (Schramowski et al. 2023) 47 72 3 21 39 1 26 3 212 MACE (Lu et al. 2024) 17 19 2 39 16 2 9 7 111 STEREO (Ours) 48 18 9 4 1 1 1 1 83 Table 12: Result of \"nudity\" erased STEREO on the broader I2P benchmark. The average probabilities of unsafe content presented here are predicted using a combined Q16/NudeNet classifier for various categories following (Schramowski et al. 2023). (Results for the baselines were sourced from the ESD (Gandikota et al. 2023) paper). Best results are marked in Bold and the second best results are underlined. SLD SLD \"nudity\" \"nudity\" \"nudity\" \"nudity\" Category SD v1.4 Medium Max ESD-u-1 ESD-u-3 ESD-u-10 STEREO Hate 0.40 0.20 0.09 0.25 0.19 0.13 0.03 Harrasment 0.34 0.17 0.09 0.16 0.18 0.15 0.08 Violence 0.43 0.23 0.14 0.37 0.34 0.26 0.18 Self-harm 0.40 0.16 0.07 0.32 0.24 0.18 0.08 Sexual 0.35 0.14 0.06 0.16 0.12 0.08 0.03 Shocking 0.52 0.30 0.13 0.41 0.32 0.27 0.14 Illegal activity 0.34 0.14 0.06 0.29 0.19 0.16 0.09 10.6 Extending the Evaluation of STEREO to Identity and Object Categories Identity Removal Setup. Following (Pham, Marshall, and Hegde 2023), we select \"Brad Pitt\" and \"Angelina Jolie\" as the identity concepts to erase. We use the GIPHY celebrity detector (Giphy 2020) to compute attack success rates for effectiveness and robustness. To evaluate erasure performance, we generate 500 images using the base prompt \"A photo of <celebrity-name>\" with varying seeds. For the CCE attack, we replace \"<celebrity name>\" with the learned adversarial prompt p∗ specific to each identity-erased model. For the RAB attack, we use hyperparameters from (Tsai et al. 2023), generating 30 positive-negative prompt pairs to obtain the concept vector, then crafting an adversarial prompt (length = 38, strength-coefficient = 3.5) for the base prompt. Object Removal Setup. We evaluate object class erasure using the Imagenette dataset (Howard and Gugger 2020), a subset of ImageNet (Deng et al. 2009) containing 10 easily identifiable objects. We use the ResNet-50 ImageNet classifier (Deng et al. 2009) to compute the attack success rates for effectiveness and robustness, by passing the generated images to the classifier and getting the top-1 prediction. We generate 500 images per object using \"A photo of a <object-name>\" as the base prompt. For the CCE attack, we replace \"<object-name>\" with the learned adversarial string p ∗. The RAB attack setup mirrors that of identity removal, with concept vectors obtained from 30 positive-negative prompt pairs per object. Results. Table 13 and Table 14 present STEREO’s perfor- mance for identity and object categories, respectively. We evaluate robustness using CCE and RAB attacks Note that, though the RAB attack is not proposed for the identity and object categories, we extend it to these two categories for the sake of completeness. For the identity category, STEREO significantly outperforms baseline methods in both erasure effectiveness and attack robustness. Notably, it achieves 0% erased rate and 0% attack success rate for both CCE and RAB attacks, while maintaining competitive FID and CLIP scores. In the object category, STEREO demonstrates superior perfor- mance across most objects. It consistently achieves 0% erased rate for 9 out of 10 objects, with only “Golf Ball\" showing a minimal 1.20% rate. Against the CCE attack, STEREO sig- nificantly reduces the attack success rate compared to other methods, often by an order of magnitude or more. For in- stance, in the case of \"Chain Saw,\" STEREO reduces the CCE attack success rate to 1.60%, compared to 17.8% - 90.6% for other methods. The RAB attack is consistently nullified across all objects except “Golf Ball.\" Table 13: Comparison of recent concept erasure methods against different attacks for the Identity category. Utility is measured by the FID (lower the better) and CLIP (higher the better) scores. Effectiveness, and robustness are measured by the success rate (lower the better) on the original and attack prompts, respectively. Attack Methods (↓) Attack Methods (↓) CCE RAB CCE RAB Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP (↑) Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP (↑) SD 1.4 92.6 94.4 5.60 14.13 31.33 SD 1.4 96.0 94.8 76.2 14.13 31.33 ESD(ICCV’23) 0.0 61.2 0.0 14.48 31.32 ESD(ICCV’23) 0.8 60.1 0.0 14.48 31.32 AC (ICCV’23) 3.2 73.6 0.0 14.41 30.78 AC (ICCV’23) 0.6 79.6 0.2 14.40 30.75 UCE (WACV’24) 0.0 59.4 0.0 14.48 31.32 UCE (WACV’24) 0.0 65.2 0.0 14.48 31.32 MACE(CVPR’24) 9.6 91.9 0.0 14.25 31.26 MACE(CVPR’24) 10.8 91.8 0.0 14.33 31.25 Brad Pitt STEREO (Ours) 0.00 0.00 0.00 24.45 30.07 Angelina Jolie STEREO (Ours) 0.00 0.00 0.00 21.42 30.65 Table 14: Comparison of recent concept erasure methods against different attacks for the Objects category. Utility is measured by the FID (lower the better) and CLIP (higher the better) scores. Effectiveness, and robustness are measured by the success rate (lower the better) on the original and attack prompts, respectively. Attack Methods (↓) Attack Methods (↓) CCE RAB CCE RAB Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP (↑) Target Concepts Erasure Methods Erased (↓) (ICLR’24) (ICLR’24) FID (↓) CLIP (↑) SD 1.4 16.8 15.0 6.4 14.13 31.33 SD 1.4 88.4 98.6 81.2 14.13 31.33 ESD(ICCV’23) 0.2 6.2 0.0 14.48 31.32 ESD(ICCV’23) 0.8 57.0 0.4 14.48 31.32 AC (ICCV’23) 0.0 4.2 0.0 13.52 31.13 AC (ICCV’23) 0.0 79.4 0.0 15.85 30.17 UCE (WACV’24) 0.0 2.8 0.0 14.48 31.32 UCE (WACV’24) 16.4 89.6 8.4 14.48 31.32 MACE(CVPR’24) 0.0 33.2 0.0 14.26 30.99 MACE(CVPR’24) 0.4 91.8 0.2 15.67 30.85 Cassette Player STEREO (Ours) 0.00 0.60 0.00 26.87 29.63 Garbage Truck STEREO (Ours) 0.00 4.20 0.00 31.47 28.93 SD 1.4 65.6 96.0 89.0 14.13 31.33 SD 1.4 78.8 78.2 58.8 14.13 31.33 ESD(ICCV’23) 0.0 64.0 3.4 14.48 31.32 ESD(ICCV’23) 0.0 73.8 0.2 14.48 31.32 AC (ICCV’23) 0.0 17.8 0.0 13.55 31.14 AC (ICCV’23) 0.0 31.2 0.0 13.93 30.91 UCE (WACV’24) 0.0 43.6 0.0 14.48 31.32 UCE (WACV’24) 0.0 73.0 0.0 14.48 31.32 MACE(CVPR’24) 0.0 90.6 6.6 14.01 31.01 MACE(CVPR’24) 0.0 77.6 0.0 14.70 31.03 Chain Saw STEREO (Ours) 0.00 1.60 0.00 29.21 29.23 Gas Pump STEREO (Ours) 0.00 10.20 0.00 27.75 29.19 SD 1.4 72.0 91.0 80.0 14.13 31.33 SD 1.4 97.6 93.4 82.2 14.13 31.33 ESD(ICCV’23) 0.8 87.4 10.4 14.48 31.32 ESD(ICCV’23) 0.0 28.6 2.0 14.48 31.32 AC (ICCV’23) 0.4 72.6 0.0 14.24 30.65 AC (ICCV’23) 0.0 28.4 0.0 12.47 30.98 UCE (WACV’24) 10.0 82.2 0.4 14.48 31.32 UCE (WACV’24) 0.2 18.6 22.4 14.48 31.32 MACE(CVPR’24) 0.0 94.2 0.0 15.36 31.03 MACE(CVPR’24) 13.2 90.8 24.2 13.68 31.14 Church STEREO (Ours) 0.00 9.60 0.00 31.70 29.09 Golf Ball STEREO (Ours) 1.20 13.40 12.40 23.43 29.83 SD 1.4 97.20 95.60 95.2 14.13 31.33 SD 1.4 93.6 99.8 90.8 14.13 31.33 ESD(ICCV’23) 0.2 48.2 0.0 14.48 31.32 ESD(ICCV’23) 0.0 94.2 1.4 14.48 31.32 AC (ICCV’23) 0.0 32.6 0.0 13.64 31.21 AC (ICCV’23) 0.0 92.4 0.0 13.12 31.03 UCE (WACV’24) 0.0 69.6 0.0 14.48 31.32 UCE (WACV’24) 1.6 94.2 9.4 14.48 31.32 MACE(CVPR’24) 0.0 86.2 0.2 13.91 30.87 MACE(CVPR’24) 0.0 90.0 1.6 14.36 30.99 English Springer STEREO (Ours) 0.00 27.20 0.00 24.71 30.09 Parachute STEREO (Ours) 0.00 86.40 0.00 29.56 29.79 SD 1.4 99.8 94.2 44.4 14.13 31.33 SD 1.4 77.4 97.2 24.8 14.13 31.33 ESD(ICCV’23) 0.0 81.6 0.0 14.48 31.32 ESD(ICCV’23) 0.3 59.7 0.0 14.48 31.32 AC (ICCV’23) 0.0 66.6 0.0 13.14 31.11 AC (ICCV’23) 0.0 29.4 0.0 13.92 31.23 UCE (WACV’24) 0.4 99.4 0.0 14.48 31.32 UCE (WACV’24) 0.0 20.6 0.0 14.48 31.32 MACE(CVPR’24) 0.0 96.8 0.0 14.12 30.99 MACE(CVPR’24) 0.0 99.2 0.0 13.83 30.99 French Horn STEREO (Ours) 0.00 71.80 0.00 29.33 29.61 Tench STEREO (Ours) 0.40 4.00 0.00 22.23 29.73 11 Additional Qualitative Results We present additional qualitative results for all concept cat- egories, comparing across chosen baseline methods and against different attacks. The following figures illustrate our findings: • Object Erasing Performance: Figure 6 visualizes STEREO’s object erasing performance and its impact on surrounding concepts. • Robustness Against CCE Attack (Objects): Figure 7 compares all methods’ robustness against the CCE attack for each of the 10 objects. • Identity Erasing Performance: Figure 8 visualizes iden- tity erasing performance for all methods, focusing on “Angelina Jolie\" and “Brad Pitt\". • Robustness Against CCE Attack (Identities): Figure 9 compares all methods’ robustness against the CCE attack for both identities. • Nudity Erasing Performance: Figure 10 and Figure 11 visualizes nudity erasing performance across various methods and against the RAB attack, respectively. • Artistic Style Erasing Performance: Figure 12 visual- izes artistic style erasing performance for all methods, focusing on the “Van Gogh\" style. • Robustness Against CCE Attack (Artistic Style): Fig- ure 13 compares all methods’ robustness against the CCE attack in regenerating the “Van Gogh\" artistic style. These visualizations provide a comprehensive overview of STEREO’s performance across different concept categories and its resilience against the stronger CCE attack, compared to baseline methods.CassettePlayerChainSawChurchEnglishSpringerFrench HornGarbage TruckGasPumpGolfBallParachuteTenchSD 1.4 Cassette Player Chain Saw Church English Springer French Horn Garbage Truck Gas Pump Golf Ball Parachute Tench Figure 6: Visualization of object erasure results across different methods. Diagonals represent the erased image and non-diagonals represent the utility preservation on the benign objects.CassettePlayerChainSawChurchEnglishSpringerFrench HornGarbage TruckGasPumpGolfBallParachuteTench SD 1.4 ESD AC UCE MACE STEREO (Ours) Figure 7: Visualization of all objects under the CCE attack across different methodsAngelinaJolieBradPitt SD 1.4 ESD AC UCE MACE STEREO (Ours) Figure 8: Visualization of celebrity identities erasure results across different methods.AngelinaJolieBradPitt SD 1.4 ESD AC UCE MACE STEREO (Ours)Figure 9: Visualization of celebrity identities under the CCE attack across different methods. Figure 10: Visualization of nudity erasure results across different methods. (*) added by authors for publication. Figure 11: Visualization of nudity under RAB attack across different methods. (*) added by authors for publication.Prompt-1Prompt-2Prompt-3Prompt-4Prompt-5 SD 1.4 ESD AC UCE MACE STEREOUNet-C STEREOUNet-NC Figure 12: Visualization of Van-Gogh artistic style erasure across different methods. UNet-C indicates updating only the cross-attention layers of the UNet, and UNet-NC indicates updating only the non-cross-attention layers of the UNet. We use ’photorealistic minimalism’ as the anchor prompt for STEREO during erasing. We observe this to be reflected in the erased images when using the prompt \"A painting in the style of Van Gogh\".Prompt-1Prompt-2Prompt-3Prompt-4Prompt-5 SD 1.4 ESD AC UCE MACE STEREOUNet-C STEREOUNet-NC Figure 13: Visualization of Van-Gogh artistic style under CCE attack across different methods. UNet-C indicates updating only the cross-attention layers of the UNet, and UNet-NC indicates updating only the non-cross-attention layers of the UNet.","libVersion":"0.3.2","langs":""}