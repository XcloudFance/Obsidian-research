{"path":"GenAIUnleaning/DiffusionUnlearning/Attack_restore/2023/Circumventing Concept.pdf","text":"Circumventing Concept Erasure Methods For Text-to-Image Generative Models Minh Pham New York University mp5847@nyu.edu Kelly O. Marshall New York University km3888@nyu.edu Niv Cohen The Hebrew University of Jerusalem nivc@cs.huji.ac.il Govind Mittal New York University mittal@nyu.edu Chinmay Hegde New York University chinmay.h@nyu.edu Abstract Text-to-image generative models can produce photo-realistic images for an ex- tremely broad range of concepts, and their usage has proliferated widely among the general public. Yet, these models have numerous drawbacks, including their potential to generate images featuring sexually explicit content, mirror artistic styles without permission, or even hallucinate (or deepfake) the likenesses of celebrities. Consequently, various methods have been proposed in order to “erase” sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. Specifically, we devise an algorithm to learn special input word embeddings that can retrieve “erased” concepts from the sanitized models with no alterations to their weights. Our results highlight the brittleness of post hoc concept erasure methods, and call into question their use in the algorithmic toolkit for AI safety. 1 Introduction Motivation. Text-to-image models [1–8] have garnered significant attention due to their exceptional ability to synthesize high-quality images based on text prompts. Such models, most prominently Stable Diffusion (SD) [7] and DALL-E 2 [4], have been adopted in a variety of commercial products spanning application realms ranging from digital advertising to graphics to game design. In particular, the open-sourcing of Stable Diffusion has democratized the landscape of image generation technology. This shift underlines the growing practical relevance of these models in diverse real-world applications. However, despite their burgeoning popularity, these models come with serious caveats: they have been shown to produce copyrighted, unauthorized, biased, and potentially unsafe content [9, 10]. What is the best way to ensure that text-to-image models do not produce sensitive or unsafe concepts? Dataset pre-filtering [11] may present the most obvious answer. However, existing filtering procedures are imperfect and may exhibit a large proportion of false negatives. See the extensive studies reported in [12] on how LAION-400M, a common dataset used in training text-image models, contains numerous offensive image samples which persist after applying standard NSFW filters. Even if perfect data pre-filtering were possible, substantial resources would be required to retrain large models from scratch in response to issues unearthed post-training. As a result, several post hoc concept-erasure methods have emerged of late. Some advocate inference guidance [13, 14]. Others require fine-tuning the weights on an auxiliary subset of training data [15–17]. These may be categorized as more practical alternatives to full model-retraining with a stripped-down version of Preprint. Under review.arXiv:2308.01508v2 [cs.LG] 8 Oct 2023 the original training data. Many of these methods are accompanied by public releases of the weights of the “sanitized” models. Such concept erasure methods are purported “to permanently remove [targeted concepts] from the weights”; moreover, they are presented as “not easy to circumvent since [the method] modifies weights” [15]. An array of results on several test instances across use cases (object removal, artistic style forgetting, avoidance of NSFW content, avoiding likeness of specific people) seem to support the efficacy of these methods. NSFW Concept Art Concept Object ConceptOriginalSDModelErasedSDModelConceptInversion Figure 1: Concept erasure methods fail to excise concepts from text-to-image models. This figure shows results from ESD [15], which is a variant of Stable Diffusion trained to avoid generating NSFW content, specific artist styles, and specific objects, like trucks (2 nd row). We circumvent this method by generating the “erased” concepts (3 rd row) by designing special prompts. Our contributions. Our main contribution in this paper is to show that: Post hoc concept erasure in generative models provides a false sense of security. We investigate seven recently announced concept-erasure methods for text-to-image generative models: (i) Erased Stable Diffusion [15], (ii) Selective Amnesia [16], (iii) Forget-me-not [17], (iv) Ablating Concepts [18], (v) Unified Concept Editing [19], (vi) Negative Prompt [14], and (vii) Safe Latent Diffusion [13]. All of these were either published or appeared online in the first 9 months of 2023. Somewhat surprisingly, we show that all seven techniques can be circumvented. In all cases, the very same “concept-erased” models — with zero extra training or fine-tuning — may produce the erased concept with a suitably constructed (soft) prompt. Therefore, the seemingly-safe model may still be used to produce sensitive or offensive content. Overall, our results indicate that there may be a fundamental brittleness to post hoc erasure methods, and entirely new approaches for building (and evaluating) safe generative models may be necessary. See Figure 1 for examples. Techniques. Our approach stems from the hypothesis that existing concept erasure methods may be, in reality, performing some form of input filtering. More specifically, in these methods, the modified generative models produced by these methods are evaluated on a limited subset of text inputs: the original offending/sensitive text, and related prompts. However, this leaves the model vulnerable to more sophisticated text prompts. In particular, we design individual Concept Inversion (CI) “attack” techniques to discover special word embeddings that can recover erased concepts when fed to the modified model. Through the application of CI, we provide evidence that these unique word embeddings outmaneuver concept erasure methods across various use cases such as facial likeness, artistic style, object-types, and NSFW concepts. Therefore, it is not the case that these concepts have been permanently removed from the model; these still persist, albeit remapped to new embeddings. Implications. Our extensive experiments below highlight two key points: 1. Our results call into question the premise that existing erasure methods (fully) excise concepts from the model. Our results show that this premise is not correct and that the results in these previous works on concept erasure should be scrutinized carefully. 2 2. We call for stronger evaluation methodologies for concept erasure methods. Measuring the degree of concept erasure in text-to-image models is tricky, since there are potentially a vast number of prompts that a motivated (and moderately well-equipped) attacker can use as inputs. As a first step to mitigate this issue, we recommend evaluating models in terms of our CI attacks during evaluation, and not merely limited to evaluating over mild variations of the original text prompts. Overall, our findings shine a spotlight on the considerable challenges in sanitizing already trained generative AI models (such as Stable Diffusion) and making them safe for wide public use. 2 Background Denoising Diffusion Models. Diffusion models belong to a category of generative models that sample from a distribution via an iterative Markov-based denoising process [20, 21]. The process begins with a sampled Gaussian noise vector, denoted as xT , and undergoes a series of T denoising steps to ultimately restore the final data, referred to as x0. In practical applications, the diffusion model is trained to predict the noise ϵt at each timestep, t, utilized to generate the progressively denoised image, xt. Latent diffusion models (LDM) [7] offer improved efficiency by operating in a lower dimensional space learned by an autoencoder. The first component of LDM consists of an encoder E and a decoder D that have been pre-trained on a large collection of images. During the training of LDM, for an image x, the encoder learns to map x into a spatial latent code z = E(x). The decoder maps such latent codes back to the original images such that D(E(x)) ≈ x. The second component is a diffusion model trained to produce codes within the learned latent space. Given a conditional input c, the LDM is trained using the following objective function: L = Ez∼E(x),t,c,ϵ∼N (0,1) [∥ϵ − ϵθ(zt, c, t)∥ 2 2] , (1) Here zt is the latent code for time t, and ϵθ is the denoising network. At inference time, a random noise tensor is sampled and gradually denoised to produce a latent z0, which is then transformed into an image through the pre-trained decoder such that x′ = D(z0). [22] propose a classifier-free guidance technique is used during inference and requires that the model be jointly trained on both conditional and unconditional denoising. The unconditional and conditional scores are used to create the final latent z0. There, we start with zT ∼ N (0, 1) which is transformed to obtain ˜ϵθ(zt, c, t) = ϵθ(zt, t) + α(ϵθ(zt, c, t) − ϵθ(zt, t)) , to get zT −1. This process is repeated sequentially until z0 is produced. Machine Unlearning. The conventional goal in machine learning is to foster generalization while minimizing reliance on direct memorization. However, contemporary large-scale models possess the capacity for explicit memorization, whether employed intentionally or as an inadvertent byproduct [23– 25]. The possibility of such memorization has led to the development of many works in machine unlearning [26, 27], the core aim of which is to refine the model to behave as though a specific set of training data was never presented. Mitigating Undesirable Image Generation. Numerous methods have been proposed to discourage the creation of undesirable images by generative models. One initial approach is to exclude certain subsets of the training data. However, this solution can necessitate the retraining of large-scale models from scratch, which can be prohibitive. An alternative put forward by [13, 14] involves manipulating the inference process in a way that steers the final output away from the target concepts. Yet another approach employs classifiers to alter the output [10, 11, 28]. Since inference guiding methods can be evaded with sufficient access to model parameters [29], subsequent works [15–19] suggest fine-tuning Stable Diffusion models. [30] study the capability of generating unsafe images and hateful memes of various text-to-image models. The authors then propose a new classifier that outperforms existing built-in safety checkers of these models. Diffusion-based Inversion. Image manipulation with generative networks often requires inver- sion [31, 32], the process of finding a latent representation that corresponds to a given image. For diffusion models, Dhariwal & Nichol [33] demonstrate that the DDIM [34] sampling process can be inverted in a closed-form manner, extracting a latent noise map that will produce a given real image. More recent works [35–38] try to invert a user-provided concept to a new pseudo-word in 3 the model’s vocabulary. The most relevant approach for our work is Textual Inversion [36] which learns to capture the user-provided concept by representing it through new “words” in the embedding space of a frozen text-to-image model without changing the model weights. In particular, the authors designate a placeholder string, c∗, to represent the new concept the user wishes to learn. They replace the vector associated with the tokenized string with a learned embedding v∗, in essence “injecting” the concept into the model vocabulary. The technique is referred to as Textual Inversion and consists of finding an approximate solution to the following optimization problem: v∗ = arg min v Ez∼E(x),c∗,ϵ∼N (0,1),t [∥ϵ − ϵθ(zt, c∗, t)∥ 2 2]. 3 Preliminaries Basic setup and threat model. For the remainder of the paper, we will leverage inversion techniques to design an “attack” on concept-erased models. We assume the adversary has: (1) access to the weights and components of the erased model, (2) knowledge of the erasure method, (3) access to example images with the targeted concept (say via an image search engine), and (4) moderately significant computational power. A trivial approach to “un-erase” an erased concept would be via fine-tuning a sanitized model on sufficiently many example images. Therefore, we also assume that: (5) the adversary cannot modify the weights of the erased model. To show that our CI attack is a reliable tool for establishing the existence of concepts in a model, we conduct two experiments to investigate whether Textual Inversion (TI) by itself can generate a concept that the model has not captured during training. If TI can hallucinate totally novel concepts, then even data filtering before training might not be able to avoid producing harmful/copyrighted content. In the first experiment, we compare TI performance on concepts that are better represented in the training data of Stable Diffusion 1.4, versus those that are likely not present. In the second experiment, we conducted a more controlled study by training two diffusion models on MNIST [39] from scratch. We include all the training classes in the first run and exclude one class in the second run. In both experiments, we find that Textual Inversion works significantly worse when the concept is not well represented in the training set of the generative model. See Figure 14 in the Appendix. 4 Circumventing Concept Erasure 4.1 Experimental setup In this section, we examine seven (7) different concept erasure methods. To the best of our knowledge, this list constitutes all the concept erasure methods for Stable Diffusion models published up to September 19, 2023. We design CI procedures tailored to each erasure method that search the space of word embeddings to recreate the (purportedly) erased visual concepts. Importantly, our approach relies solely on the existing components of the post-erasure diffusion models. For these experiments, wherever possible we use the pre-trained models released by the authors unless explicitly stated otherwise; for concepts where erased models were not publicly available, we used public code released as-is by the authors to reproduce their erasure procedure. In each subsection, we start by describing the approach, then show how to attack their approach using Concept Inversion. We interleave these with results, and reflect on their implications. Finally, we show evidence that current concept erasure methods are likely performing input filtering, and demonstrate transferability of the learned word embeddings. Our code is available for reproducibility purposes at https: //nyu-dice-lab.github.io/CCE/ 4.2 Evaluation Protocol For each concept erasure method that we will be discussing below, we initially deploy it to erase 4 concept categories including art style, object, ID, and NSFW content. We use Stable Diffusion 1.4 (SD 1.4) for all our experiments. We assume that the adversary can access a small number of examples of the targeted concept from Google Images; see Appendix for details. 4 Art style: We select 6 styles from modern artists and artistic topics that have been reported to have been captured by SD 1.4: the movie series “Ajin: Demi Human”, Thomas Kinkade, Tyler Edlin, Van Gogh, Kelly McKernan, and Tyler Edlin. We generate images from the erased models using the prompt “A painting in the style of [artist name]”. After performing CI, we generate images by replacing [artist name] with c∗ - the special placeholder string associated with the learned word embedding. In addition to qualitative results, we follow [15] and conduct a human study to measure the effectiveness of our CI methods. In particular, for each artist, we collect 10 images of art created by those artists from Google Images. We then generate 10 images from the erased model using the standard concept name, and 10 images using CI per style and per concept erasure method. Participants were shown 5 real reference images from the same artist and another image of the same style (either real, from the erased model or from CI). They were then asked to estimate, on a five-point Likert scale, their confidence level that the experimental image has the same style as the reference images. Our study consists of 50 participants, with 96 responses per participant. Objects: Following Gandikota et al. [15], we investigate the Imagenette [40] dataset which com- prises ten easily identifiable classes (cassette player, chain saw, church, etc.) We evaluate CI methods by examining the top-1 predictions of a ResNet-50 Imagenet classifier on 500 generated images. We generate images from the erased models using the prompt “A photo of a [object name]”. For CI, we generate images by replacing [object name] with the special string c∗. ID: Following Heng & Soh [16], we select “Brad Pitt” and “Angelina Jolie” as identity concepts. We then utilize the GIPHY celebrity detector [41] for Concept Inversion evaluation. We generate 500 images from the erased models using the prompt “A photo of a [person name]”. For CI, we generate the same number of images by replacing [person name] with the special placeholder string c∗. NSFW content: Introduced by Schramowski et al. [13], the I2P dataset comprises 4703 unique prompts with corresponding seeds, which (to date) is the definitive benchmark for measuring the effectiveness of NSFW concept erasure. This process involves generating images using the prompts and seeds and subsequently utilizing NudeNet [28] to classify the images into various nudity classes. The I2P benchmark is effective as its prompts do not necessarily contain words strictly related to nudity. Hence, an effective erasure method on this benchmark requires some degree of robustness to prompt selection. To evaluate each concept erasure method, we first used SD 1.4 to generate 4703 images using the I2P dataset. We used NudeNet to filter out 382 images with detected exposed body parts, on which we performed Concept Inversion. To measure how well the NSFW concept is recovered, we generated another 4703 images using the erased model by using the I2P prompts with the special placeholder string c∗ prepended, which are then evaluated by NudeNet. Real SD 1.4 ESD CI UCE CI SA CI FMN CI AC CI NP CI SLD-Med CI 0 1 2 3 4 Figure 2: Quantitative results of Concept Inversion for artistic concept: Our human study ratings (with ± 95% confidence intervals) show that we can recover the erased artistic concept across all models. The CI Likert score is even higher than the images generated by SD 1.4. 4.2.1 Erased Stable Diffusion (ESD) Concept Erasure Method Details. Gandikota et al. [15] fine-tune the pre-trained diffusion U-Net model weights to remove a specific style or concept. The authors reduce the probability of generating an image x based on the likelihood described by the textual description of the concept, i.e. Pθ∗ (x) ∝ Pθ(x) Pθ(c|x)η , where θ∗ is the updated weights of the diffusion model (U-Net), θ is the original weights, η is a scale power factor, c is the target concept to erase, and P(x) represents the distribution generated 5 Real SD 1.4 ESD ESD (CI) UCE UCE (CI) NP NP (CI) SLD-Med SLD-Med (CI)ThomasKinkadeVanGogh Figure 3: Qualitative results of Concept Inversion for artistic concept: Columns 4, 6, 8, and 10 demonstrate the effectiveness of concept erasure methods in not generating the targeted artistic concepts. However, we can still generate images of the erased styles using CI. Table 1: Quantitative results of Concept Inversion for object concept (Acc. % of erased model / Acc. % of CI): Concept erasure methods can cleanly erase many object concepts from SD 1.4, evidenced by a significant drop in classification accuracy. Using Concept Inversion, we can generate images of the erased objects, which can be seen by an increase in average accuracy across all methods. SD 1.4 ESD FMN UCE AC NP SLD-Med SA cassette player 6.4 0.2 / 6.2 0.2 / 8.8 0.0 / 2.8 0.0 / 4.2 4.0 / 9.4 1.0 / 2.4 0.6 / 6.2 chain saw 68.6 0.0 / 64.0 0.0 / 0.2 0.0 / 43.6 0.0 / 17.8 4.0 / 82.8 0.8 / 86.6 0.0 / 2.0 church 79.6 0.8 / 87.4 0.0 / 0.0 10.0 / 82.2 0.4 / 72.6 25.4 / 78.4 20.6 / 72.0 56.2 / 65.6 english springer 93.6 0.2 / 48.2 0.0 / 0.0 0.0 / 69.6 0.0 / 32.6 27.0 / 90.4 24.6 / 96.4 0.0 / 8.2 french horn 99.3 0.0 / 81.6 0.0 / 59.0 0.4 / 99.4 0.0 / 66.6 62.4 / 99.0 17.0 / 97.6 0.2 / 87.0 garbage truck 83.2 0.8 / 57.0 6.4 / 69.6 16.4 / 89.6 0.0 / 79.4 39.4 / 84.6 19.8 / 94.8 12.6 / 35.4 gas pump 76.6 0.0 / 73.8 7.8 / 80.4 0.0 / 73.0 0.0 / 31.2 18.0 / 79.6 12.8 / 75.6 0.6 / 54.8 golf ball 96.2 0.0 / 28.6 22.6 / 74.4 0.2 / 18.6 0.0 / 28.4 45.2 / 88.4 60.2 / 98.8 3.0 / 49.0 parachute 96.2 0.0 / 94.2 2.0 / 93.4 1.6 / 94.2 0.0 / 92.4 32.8 / 77.2 52.8 / 95.8 22.6 / 78.6 tench 79.6 0.3 / 59.7 0.4 / 60.6 0.0 / 20.6 0.0 / 29.4 27.6 / 72.6 20.6 / 75.4 10.2 / 16.0 Average 77.9 0.2 / 60.1 3.9 / 44.6 2.9 / 59.4 0.04 / 45.5 28.6 / 76.2 23.0 / 79.5 10.6 / 40.3 by the original model. Based on Tweedie’s formula [42] and the reparametrization trick [21], the authors derive a denoising prediction problem as ϵθ∗ (xt, c, t) ← ϵ(xt, t) − η[ϵθ(xt, c, t) − ϵθ(xt, t)]. By optimizing this equation, the fine-tuned model’s conditional prediction is steered away from the erased concept when prompted with it. The authors propose two variants of ESD: ESD-x and ESD-u, which finetune the cross-attentions and unconditional layers (non-cross-attention modules) respectively. Concept Inversion Method. We employ standard Textual Inversion on fine-tuned Stable Diffusion models from [15] to learn a new word embedding that corresponds to the concept of the training images. The authors provide pre-trained ESD-x models for all 6 artistic concepts, the pre-trained ESD-u model for NSFW content concept, and training scripts for object concepts. For ID concepts, we train our own ESD-u models prior to CI. 4.2.2 Unified Concept Editing (UCE) Concept Erasure Method Details. Latent diffusion models [7] operate on low-dimensional embed- ding that is modeled with a U-Net generation network. The model incorporates conditioned textual information via embeddings derived from a language model. These embeddings are introduced into the system via cross-attention layers. Inspired by Orgad et al. [43] and Meng et al. [44], Gandikota et al.[19] edit the U-Net of Stable Diffusion models without training using a closed-form solution conditioned on cross-attention outputs. They update attention weights to induce targeted changes to the keys/values that correspond to specific text embeddings for a set of edited concepts, while minimizing changes to a set of preserved concepts. Concept Inversion Method. We employ standard Textual Inversion [36] on fine-tuned Stable Diffusion models from UCE [19] to learn a new word embedding that corresponds to the (identity) concept of the training images. Gandikota et al. [19] provide training scripts to reproduce their art style, object, and NSFW content concepts. For ID concepts, we adapt their publicly posted code to train our own models. 6 SD 1.4 ESD ESD (CI) UCE UCE (CI) NP NP (CI) SLD-Med SLD-Med (CI)AngelinaJolieBradPitt Figure 4: Qualitative results of Concept Inversion for ID concept: Columns 3, 5, 7, and 9 demonstrate the effectiveness of concept erasure methods in not generating Brad Pitt and Angelina Jolie. However, we can still generate images of the erased IDs using CI. Female Breast Female Genitalia Male Genitalia Male Breast Buttocks Armpits Belly Feet 100 101 102 103Count CI SD 1.4 ESD (a) Erased Stable Diffusion Female Breast Female Genitalia Male Genitalia Male Breast Buttocks Armpits Belly Feet 101 102Count CI SD 1.4 UCE (a) Unified Concept Editing Female Breast Female Genitalia Male Genitalia Male Breast Buttocks Armpits Belly Feet 100 101 102 103Count CI SD 1.4 NP (a) Negative Prompt Female Breast Female Genitalia Male Genitalia Male Breast Buttocks Armpits Belly Feet 101 102 103Count CI SD 1.4 SLD-Med (a) Safe Latent Diffusion Female Breast Female Genitalia Male Genitalia Male Breast Buttocks Armpits Belly Feet 100 101 102 103Count CI SD 1.4 AC (a) Ablating Concepts Female Breast Female Genitalia Male Genitalia Male Breast Buttocks Armpits Belly Feet 100 101 102Count CI SD 1.4 SA (a) Selective Amnesia Female Breast Female Genitalia Male Genitalia Male Breast Buttocks Armpits Belly Feet 100 101 102Count CI SD 1.4 FMN (a) Forget-Me-Not Figure 5: Quantitative results of Concept Inversion for NSFW concept: On average, the number of detected body parts from SD 1.4 and the erased models is 99.75 (across 4703 images) and 26.21 (across 4703 images and 7 erasure methods), respectively. Using CI, the average number of detected body parts is 170.93 across 7 methods. 4.2.3 Selective Amnesia (SA) Concept Erasure Method Details. Heng & Soh [16] pose concept erasure as a problem of continual learning, taking inspiration from Elastic Weight Consolidation (EWC) [45] and Generative Replay [46]. Consider a dataset D that can be partitioned as D = Df ∪ Dr = {(x(n) f , c (n) f )}Nf n=1 ∪ {(x (n) r , c (n) r )}Nr n=1, where Df is the data to forget and Dr is the data to remember. The underlying distribution of D is given by p(x, c) = p(x|c)p(c). In the case of concept erasure, Df contains the images of the concept we would like to erase, and Dr consists of images we want to preserve the model performance. They maximize the following objective function for concept erasure: L = − Ep(x|c)p(cf )[log p(x|θ∗, c)] − λ ∑ i Fi 2 (θi − θ∗ i ) 2 + Ep(x|c)p(xr)[log p(x|θ∗, c)], (2) where F is the Fisher information matrix and the third term is a generative replay term to prevent model degradation on samples that do not contain the erased concept. In practice, the authors optimize Eq. 2 by substituting the likelihood terms with the standard ELBOs. Moreover, they observe that directly minimizing the ELBO can lead to poor results. Hence, they propose to maximize the log-likelihood of a surrogate distribution of the concept to forget, q(x|cf ) ̸= p(x|cf ). This is done by 7 replacing − Ep(x|c)p(cf )[log p(x|θ∗, c)] with Eq(x|c)p(cf )[log p(x|θ∗, c)]. Intuitively, this fine-tuning will result in a generative model that will produce images according to the surrogate distribution when conditioned on cf . Concept Inversion Method. We employ standard Textual Inversion [36] on fine-tuned Stable Diffusion models from [16] to learn a new word embedding that corresponds to the concept of the training images. The authors provide training scripts for Brad Pitt, Angelina Jolie, and NSFW content concepts. In particular, they use images of clowns and middle-aged people as the surrogate dataset for ID concepts, and images of people wearing clothes for NSFW content concept. For other concepts, we train our own models by appropriately modifying their training scripts prior to CI. 4.2.4 Forget-Me-Not (FMN) Concept Erasure Method Details. Zhang et al. [17] propose fine-tuning the cross-attention layers of Stable Diffusion’s U-Net to map the erased concept to that of a set of reference images. The authors first locate the word embeddings associated with the forgetting concept. This can be done by tokenizing a pre-defined prompt or through Textual Inversion. They then compute the attention maps between the input features and these embeddings, and minimize the Frobenius norm of attention maps and backpropagate the network. Algorithm 1 describes the concept erasure process. Algorithm 1 Forget-Me-Not on diffuser Require: Context embeddings C containing the forgetting concept, embedding locations N of the forgetting concept, reference images R of the forgetting concept, diffuser Gθ, diffusion step T . repeat t ∼ Uniform([1...T ]); ϵ ∼ N (0, I) ri ∼ R; ej ∼ C; nj ∼ N x0 ← ri xt ← √ αtx0 + √ 1 − αtϵ ▷ αt : noise variance schedule x , t−1, At ← Gθ(xt, ej, t) ▷ At : all attention maps L ← ∑ at∈At ∥a [nj] t ∥ 2 ▷ L : attention resteering loss Update θ by descending its stochastic gradient ∇θL until Concept forgotten Concept Inversion Method. We employ standard Textual Inversion [36] on fine-tuned Stable Diffusion models from [17] to learn a new word embedding that corresponds to the concept of the training images. Zhang et al. [17] only provides training scripts for ID concepts. Hence, train our models on other concepts using their public code prior to CI. 4.2.5 Ablating Concepts (AC) Concept Erasure Method Details. Kumari et al. [18] perform concept erasure by overwriting the target concept with an anchor weight, which can be a superset or a similar concept. The authors propose two variants to erase the target concept, namely Model-based concept ablation and Noise- based concept ablation. In the former method, the authors fine-tune the pre-trained Stable Diffusion U-Net model by minimizing the following objective function: arg min θ∗ Ez∼E,z∗∼E(x∗),c,c∗,ϵ∼N (0,1),t [wt∥ϵθ∗ (zt, c, t).sg() − ϵθ∗ (z∗ t , c ∗, t)∥2 2]. where wt is a time-dependent weight, θ∗ is initialized with the pre-trained weight, x∗ is the (generated) images with the anchor concept c ∗, and .sg() is the stop-gradient operation. For the second variant, the authors redefine the ground truth text-image pairs as <a target concept text prompt, image of the anchor concept>. The authors then fine-tune the model on the redefined pairs with the standard diffusion training loss. In addition, they add an optional standard diffusion loss term on the anchor concept image and corresponding texts as a regularization as the target text prompt can consist of the anchor concept. In both variants, the authors propose to fine-tune on different parts of Stable Diffusion: (1) cross-attention, (2) embedding: the text embedding the text transformer, (2) full weights: all parameters of U-Net. 8 Concept Inversion Method. We employ standard Textual Inversion [36] on fine-tuned Stable Diffusion models from AC [18] to learn a new word embedding that corresponds to the (identity) concept of the training images. Kumari et al. [18] provide training scripts for art style and object concepts. Consequently, we extend their public code to erase the remaining concepts. 4.2.6 Negative Prompt (NP) Concept Erasure Method Details. Negative Prompt (NP) is a guiding inference technique used in the Stable Diffusion community [14]. Instead of updating the weights of the original model, it replaces the unconditional score with the score estimate conditioned on the erased concept in classifier-free guidance. Gandikota et al. [15] illustrate how NP can prevent the image generation of targeted artistic concepts. Concept Inversion Method. In our experiments, vanilla Textual Inversion was not able to circum- vent NP. We modify the objective function for Textual Inversion to: v∗ = arg min v Ez∼E(x),c,ϵ∼N (0,1),t [∥(ϵθ(zt, t) + α(ϵθ(zt, c, t) − ϵθ(zt, t)) − (ϵθ(zt, c, t) + α(ϵθ(zt, c∗, t) − ϵθ(zt, c, t))∥ 2 2] , where c is the target concept. Our method learns a word embedding associated with the special string c∗ such that the predicted noise from NP equals the true predicted noise using classifier-free guidance. 4.2.7 Safe Latent Diffusion (SLD) Concept Erasure Method Details. Safe Latent Diffusion [13] is an inference guiding method that is a more sophisticated version of NP, where the second unconditional score term is replaced with a safety guidance term. Instead of being constant like NP, this term is dependent on: (1) the timestep, and (2) the distance between the conditional score of the given prompt and the conditional score of the target concept at that timestep. In particular, SLD modifies the score estimates during inference as ϵθ(xt, c, t) ← ϵθ(xt, t) + µ[ϵθ(xt, c, t) − ϵθ(xt, t) − γ(zt, c, cS)]. We refer the reader to the Appendix B and the original work by Schramowski et al. [13] for a more detailed explanation of γ(zt, c, cS). Note that SLD does not modify the weights of the original diffusion models, but only adjusts the sampling process. By varying the hyperparameters of the safety guidance term, the authors propose 4 variants of SLD: SLD-Weak, SLD-Medium, SLD-Strong, and SLD-Max. A more potent variant of SLD yields greater success in erasing undesirable concepts. Concept Inversion Method. In our experimentation, we encountered an issue akin to the one with Negative Prompt when applying vanilla Textual Inversion. Furthermore, the guidance term of SLD at timestep t depends on that at the previous timestep. This recursive dependency implies that in order to calculate the guidance term within our inversion algorithm, it becomes necessary to keep a record of the preceding terms for optimization purposes. Given the high number of denoising steps involved, such a process could result in significant memory usage, presenting an efficiency problem. To address this issue, we propose a new strategy to perform Concept Inversion. Instead of having a constant safety guidance term, SLD requires storing all the guidance terms from step 1 to step t − 1 to calculate the one at timestep t. Since doing so will be memory-intensive, we instead approximate it by calculating only a subset of guidance terms at evenly spaced timesteps between 1 and t. We can then learn a word embedding that counteracts the influence of the safety guidance term. The pseudocode for our CI scheme can be found in Appendix B. Table 3: Quantitative results of Concept Inversion for ID concept (Acc. % of erased model / Acc. % of CI): Concept erasure methods can cleanly erase images of Brad Pitt and Angelina Jolie from SD 1.4, evidenced by a significant drop in classification accuracy. CI can recover images of the erased IDs, which can be seen by an increase in average accuracy across all methods. SD 1.4 ESD FMN UCE AC NP SLD-Med SA Brad Pitt 90.2 0.0 / 61.2 0.6 / 52.8 0.0 / 59.4 3.2 / 73.6 43.2 / 71.4 4.8 / 71.8 0.0 / 66.6 Angelina Jolie 91.6 0.8 / 60.1 0.0 / 41.2 0.0 / 65.2 0.6 / 79.6 46.2 / 75.2 5.2 / 72.8 9.6 / 67.7 Average 90.9 0.4 / 60.7 0.3 / 47.0 0.0 / 62.3 1.9 / 76.6 44.7 / 73.2 5.0 / 72.3 4.8 / 67.1 9 SD 1.4 ESD ESD (CI) UCE UCE (CI) NP NP (CI) SLD-Med SLD-Med (CI)cassetteplayerchainsaw Figure 6: Qualitative results of Concept Inversion for object concept: Columns 3, 5, 7, and 9 demonstrate the effectiveness of concept erasure methods in not generating the targeted object concepts. However, we can still generate images of the object using CI. We refer the readers to the Appendix B for the complete results on all object classes. 4.3 Results and Discussion On the plus side, our experiments confirm that whenever the target concept is explicitly mentioned in the input prompt, all seven concept erasure methods are effective. Therefore, these methods can indeed provide protection against obviously-offensive text inputs. We confirm this even for concept categories that the methods did not explore in their corresponding publications. However, on the minus side, all seven methods can be fully circumvented using our CI attacks. In other words, these erasure methods are only effective against their chosen inputs. For artistic concepts, our human study in Figure 2 shows an average (across all methods) score of 1.31 on Likert rating on images generated by the erased model. This score expectedly increases to 3.7 when Concept Inversion is applied. Figure 3 displays images generated from the erased model and CI side-by-side, which shows that CI is effective in recovering the erased style. For object concepts, the average accuracy across all methods of the pre-trained classifier in predicting the erased concept increases from 9.89 to 57.94 using our attack (Table 1). This is supported by qualitative results shown in Figure 6. For ID concepts, the average accuracy across all methods of the GIPHY detector increases from 8.15 to 67.13 in Table 3. For NSFW concepts, Figure 5 suggests that CI can recover the NSFW concept, which is shown by an increase from 26.2 to 170.93 in the average number of detected exposed body parts. Among the 7 concept erasure methods, the most challenging one to circumvent is SLD. Our Concept Inversion technique manages to counteract the influence of SLD-Weak, SLD-Medium, and even SLD- Strong under most circumstances. Among the variants, SLD-Max proves to be the most challenging to circumvent. However, this variant comes with a drawback: it has the potential to entirely transform the visual semantics of the generated images. We provide several more results of variants of SLD in the Appendix B. In our proposed CI scheme for SLD, we observed that more GPU memory can give us better approximations of the safety guidance terms and therefore counteract their influence. 4.4 Transferability and Useability of Our Attacks Intriguingly, we show that the learned special tokens derived from CI can be applied to the original SD 1.4 model to generate images of the erased concept. Figure 7 demonstrates our results. This lends further evidence that current concept erasure methods are merely remapping the concept in token space, rather than fully excising the concept from the original model. Additionally, we provide evidence that the learned word embeddings through CI are useable in practice. Following Gal et al. [36] and study the reconstruction effectiveness and editability of these embeddings. In particular, we generate two sets of images using CI with each concept erasure method: first, a set of generated images using the inverted concept; second, a set of generated images of the inverted concept in different scenes. We evaluate using CLIP [47] how well the inverted concepts are produced with the erased models, and how transferable are they to different scenes. In both cases, the erased models achieve performance similar to that of the original Stable-Diffusion model, pointing to the models are in principle still being able to produce the seemingly erased concepts. 10 0.18 0.2 0.22 SD SLD UCE ESD SA AC NP FMN Editability 0.7 0.72 0.74 SD SLD UCE ESD SA AC NP FMN Recontruction ESD FMN SA SLD-Med Figure 7: (Left) Reconstruction: A higher score indicates a better ability to replicate the provided concept. (Middle) Editability: A higher score indicates a better ability to modify the concepts using textual prompts. (Right) Transferability of learned word embeddings: The original SD model can use the learned word embeddings from the erased model (with Van Gogh style as the erased concept) to generate images of the targeted concept. 5 Conclusions As text-to-image generative AI models continue to gain popularity and usage among the public, issues surrounding the ability to generate proprietary, sensitive, or unsafe images come to the fore. Numerous methods have been proposed in the recent past that claim to erase target concepts from trained generative models, and ostensibly make them safe(r) for public consumption. In this paper, we take a step back and scrutinize these claims. We show that post-hoc erasure methods have not excised the targeted concepts; fairly straightforward “attack” procedures can be used to design special prompts that regenerate the unsafe outputs. As future work, it might be necessary to fundamentally analyze why the “input filtering” phenomenon seems to be occurring in all these recent methods, despite the diversity of algorithmic techniques involved in each of them. Such an understanding could facilitate the design of better methods that improve both the effectiveness and robustness of concept erasure. References [1] Huiwen Chang, Han Zhang, Jarred Barber, Aaron Maschinot, José Lezama, Lu Jiang, Ming- Hsuan Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan. Muse: Text-to-image generation via masked generative transformers. In International Conference on Machine Learning, 2023. [2] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In European Confer- ence on Computer Vision, 2022. [3] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021. [4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. [5] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, 2022. [6] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. [7] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition, 2022. 11 [8] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. CoRR, abs/2211.08332, 2022. [9] Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. Dall·e 2 preview - risks and limitations, 2022. [10] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramèr. Red-teaming the stable diffusion safety filter. In Advances in Neural Information Processing Systems Workshop, 2022. [11] Stability AI. Stable diffusion 2.0 release, 2022. Jul 9, 2023. [12] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misog- yny, pornography, and malignant stereotypes. CoRR, abs/2110.01963, 2021. [13] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffu- sion: Mitigating inappropriate degeneration in diffusion models. In Conference on Computer Vision and Pattern Recognition, 2023. [14] AUTOMATIC1111. Negative prompt, 2022. [15] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In International Conference on Computer Vision, 2023. [16] Alvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep generative models. In Advances in Neural Information Processing Systems, 2023. [17] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. CoRR, abs/2303.17591, 2023. [18] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, , and Jun- Yan Zhu. Ablating concepts in text-to-image diffusion models. In International Conference on Computer Vision, 2023. [19] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in diffusion models. CoRR, abs/2308.14761, 2023. [20] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning Workshop, 2015. [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. [22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In Advances in Neural Information Processing Systems Workshop, 2022. [23] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium, 2023. [24] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. In Advances in Neural Information Processing Systems, 2023. [25] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Conference on Computer Vision and Pattern Recognition, 2023. [26] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Conference on Computer Vision and Pattern Recognition, 2020. 12 [27] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrub- bing deep networks of information accessible from input-output observations. In European Conference on Computer Vision, 2020. [28] Praneeth Bedapudi. Nudenet: Neural nets for nudity detection and censoring, 2022. [29] SmithMano. Tutorial: How to remove the safety filter in 5 seconds, 2022. [30] Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models. In ACM Conference on Computer and Communications Security, 2023. [31] Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A. Efros. Generative visual manipulation on the natural image manifold. In European Conference on Computer Vision, 2016. [32] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN inversion: A survey. CoRR, abs/2101.05278, 2021. [33] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, 2021. [34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Conference on Computer Vision and Pattern Recognition, 2023. [36] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations, 2023. [37] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. CoRR, abs/2304.03411, 2023. [38] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In International Conference on Computer Vision, 2023. [39] Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. [40] Jeremy Howard. Imagenette, 2019. [41] Giphy. Giphy celebrity detector, 2020. [42] Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical Association, 106(496):1602–1614, 2011. [43] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing implicit assumptions in text-to- image diffusion models. In International Conference on Computer Vision, 2023. [44] Kevin Meng, Arnab Sen Sharma, Alex J. Andonian, Yonatan Belinkov, and David Bau. Mass- editing memory in a transformer. In International Conference on Learning Representations, 2023. [45] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catas- trophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. [46] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. In Advances in Neural Information Processing Systems, 2017. 13 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar- wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Interna- tional Conference on Machine Learning, 2021. [48] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them. https://github.com/rom1504/clip-retrieval, 2022. [49] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, 2022. 14 Appendix A Training Details For the training of Concept Inversion, we use 6 samples for art style concept, 30 samples for object concept, and 25 samples for ID concept. For all our Concept Inversion experiments, unless mentioned otherwise, we perform training on one A100 GPU with a batch size of 4, and a learning rate of 5e − 03. We optimize the word embedding for 1,000 steps while keeping the weights of the erased models frozen. The CI training procedure is the same across erasure methods and concepts, except for ID concepts we optimize for 5,000 steps, and for SLD we we train for 1,000 steps with batch size 1. A.1 Erased Stable Diffusion (ESD) For artistic concepts and NSFW concept, we used the pre-trained models released by the authors. For object concepts, we trained ESD-u models using the suggested parameters by the authors. For ID concepts, we trained ESD-u models using the parameters for object concepts. A.2 Unified Concept Editing (UCE) For artistic concepts, object concepts, and NSFW concept, we trained the erased models using the training script provided by the author. For ID concepts, we used the same script without the preservation loss term. A.3 Selective Amnesia (SA) For artistic concepts, generated images with photorealism style as the surrogate dataset. We then applied SA to finetune the cross-attention layers of SD 1.4 for 200 epochs. We used the default parameters for training. For object concepts, we used a similar training procedure as above but used images of kangaroos (a class in ImageNet) as the surrogate dataset. Moreover, we finetuned the unconditional layers (non-cross-attention modules) of SD 1.4. For ID concepts and NSFW concept, we used the training script provided by the authors. A.4 Ablating Concepts (AC) For artistic concepts, we used the training scripts provided by the authors to train the erased models. For object concepts, we also used the training script provided by the authors with the anchor distribution set to images of kangaroos. For ID concepts, we used the training script for object concepts and set the anchor distribution to images of middle-aged people. For NSFW concept, we used the training script for object concepts and set the anchor distribution to images of people wearing clothes. A.5 Forget-Me-Not (FMN) For artistic concepts, we employed FMN to finetune the cross-attention layers for 5 training steps with a learning rate of 1e − 4. The reference images we used are generated paintings with a photorealism style. For object concepts, we finetuned the unconditional layers (non-cross-attention modules) for 100 epochs with a learning rate of 2e − 06. The reference images we used are generated photos of kangaroos. For ID concepts, we finetuned the unconditional layers for 120 epochs with a learning rate of 2e − 06. The reference images we used are generated photos of middle-aged people. For NSFW concept, we finetuned unconditional layers for 100 epochs with a learning rate of 2e − 06. The reference images we used are generated photos of people wearing clothes. Since the authors do not provide any training details, we tuned the number of epochs and learning rate until we observed effective erasure. A.6 Negative Prompt (NP) To apply Negative Prompt to SD 1.4, we changed the negative_prompt argument to the concept we would like to erase during inference using codebase from Hugging Face. 15 A.7 Safe Latent Diffusion (SLD) We used to authors’ public code to apply SLD to Stable Diffusion 1.4. During CI, we sampled 35 evenly spaced timesteps between 1 and 1000. The range between the smallest and largest timesteps is 90. We set guidance scale sS = 5000, warm-up step δ = 0, threshold λ = 1.0, momentum scale sm = 0.5, and momentum beta βm = 0.7. This is the hyperparameters for SLD-Max. B Additional Details and Results for SLD SD 1.4 SLD-Weak SLD-Med SLD-Strong SLD-Max CI (SLD-Weak) CI (SLD-Med) CI (SLD-Strong) CI (SLD-Max)Ajin:DemiHumanThomasKinkadeTylerEdlinVanGoghKilianEngKellyMcKernan Figure 8: Concept Inversion (CI) on SLD for art concept. Columns 2 to 5 demonstrate the effectiveness of erasing artistic styles for each SLD variant. Concept Inversion can recover the style most consistently for SLD-Weak and SLD-Strong. In most cases, we can observe recovery for even SLD-Strong. Concept Erasure Method Details. Safe Latent Diffusion [13] is an inference guiding method, in which during inference the score estimates for the x−prediction are modified as: ϵθ(xt, c, t) ← ϵθ(xt, t) + µ[ϵθ(xt, c, t) − ϵθ(xt, t) − γ(zt, c, cS)] (3) Where cS is the concept we would like to steer the generated images away from, γ is the safety guidance term γ(zt, c, cS) = β(c, cS; sS, λ)(ϵθ(xt, c, t) − ϵθ(xt, t)), and β applies a guidance scale sS element-wise by considering those dimensions of the prompt conditioned estimate that would guide the generation process toward the erased concept: β(c, cS; sS, λ) = {max{1, |sS(θϵ(xt, ct) − ϵ(xt, cS, t)|}, where ϵθ(xt, c, t) ⊖ ϵθ(xt, cS, t) ≤ λ, 0, otherwise. Larger threshold λ and sS lead to a more substantial shift away from prompt text and in the opposite direction of the erased concept. Moreover, to facilitate a balance between removal of undesired content from the generated images and minimizing the changes introduced, the authors introduce two modifications. First, they accelerate guidance over time by utilizing a momentum term vt for the safety guidance γ. In particular, γt, denoting γ(zt, c, cS), is defined as: γt = β(c, cS; sS, λ)(ϵθ(xt, c, t) − ϵθ(xt, t)) + smvt 16 with sm ∈ [0, 1] and vt+1 = ζmvt + (1 − ζm)γt. Here, v0 = 0 and βm ∈ [0, 1), with larger ζm resulting in less volatile changes in momentum. Note that SLD does not modify the weights of the original diffusion models, but only adjusts the sampling process. By varying the hyperparameters, the authors propose 4 variants of SLD: SLD-Weak, SLD-Medium, SLD-Strong, and SLD-Max. Concept Inversion Method. The pseudocode for our CI method on SLD is shown in Alg. 2. Algorithm 2 Concept Inversion for SLD Require: Concept placeholder string c∗, erased concept c, number of iterations N for i = 1 to N do Sample x0 from training data. Randomly select m ≥ 1 and n ≤ T ϵ ∼ N (0, 1); z0 ← E(x0); γ(z0, m, cS) ← 0; vm ← 0 for t = m to n by k do ▷ iterating t from m to k with an increment of k L ← [∥(ϵθ(zt, t) + α(ϵθ(zt, cS, t) − ϵθ(zt, t)) − (ϵθ(xt, t) + µ[ϵθ(xt, c∗, t) − ϵθ(xt, t) − γ(zt, c, cS)])∥2 2] vt+1 ← ζmvt + (1 − ζm)γt γt+1 ← β(c, cS; sS, λ)(ϵθ(xt, c, t) − ϵθ(xt, t)) + smvt Update v∗ by descending its stochastic gradient ∇v∗ L end for end for C Failure Cases for CI In our experiments, we observed a small handful of cases where CI was not able to recover the erased concepts. However, as shown in Figure 9, these models can have poor image generation quality even on prompts not related to the erased concepts. Hence, this shows that while they are more robust to CI, such models are not very useful in real-world applications. SA (erasing NSFW) FMN (erasing chain saw) FMN (erasing church) FMN (erasing English Springer) SLD-Max (erasing Thomas Kinkade) Figure 9: Samples from erased models where CI struggles. We generate images using the prompt “A painting in the style of Van Gogh”. The output images do not contain the targeted artistic style and contain artifacts. D Remaining Qualitative Results SD 1.4 SA SA (CI) FMN FMN (CI) AC AC (CI)AngelinaJolieBradPitt Figure 10: Concept Inversion (CI) on concept erasure methods for IDs. Columns 3, 5, and 7 demonstrate the effectiveness of concept erasure methods in not generating Brad Pitt and Angelina Jolie. However, we can still generate images of the erased IDs using CI. 17 Real SD 1.4 ESD ESD (CI) UCE UCE (CI) NP NP (CI) SLD-Med SLD-Med (CI)Ajin:DemiHumanThomasKinkadeTylerEdlinVanGoghKilianEngKellyMcKernan Figure 11: Qualitative results of Concept Inversion on Erased Stable Diffusion, Unified Concept Editing, Negative Prompt, and Safe Latent Diffusion for artistic concept: Columns 4, 6, 8, and 10 demonstrate the effectiveness of concept erasure methods in not generating the targeted artistic concepts. However, we can still generate images of the erased styles using CI. Real SD 1.4 SA SA (CI) FMN FMN (CI) AC AC (CI)Ajin:DemiHumanThomasKinkadeTylerEdlinVanGoghKilianEngKellyMcKernan Figure 12: Qualitative results of Concept Inversion on Selective Amnesia, Forget-Me-Not, and Ablating Concepts for artistic concept: Columns 4, 6, and 8 demonstrate the effectiveness of concept erasure methods in not generating the targeted artistic concepts. However, we can still generate images of the erased styles using CI. 18 cassette player chain saw church gas pump tench garbage truck english springer golf ball parachute french hornSD1.4ESDESD(CI)UCEUCE(CI)NPNP(CI)SLD-MedSLD-Med(CI)SASA(CI)FMNFMN(CI)ACAC(CI) Figure 13: Concept Inversion (CI) on concept erasure methods for objects. Odd rows (except 1) demonstrate the effectiveness of erasure methods in erasing object concepts. However, Concept Inversion can recover the objects as shown in the third row. 19 E Details: Checking for Existence of Concepts To check for the concepts’ existence in LAION-5B, we utilize CLIP Retrieval [48] to search for images nearest to the concept image in the CLIP image embedding space. Our searches indicate that the images of Sonic and Miles Morales appear a lot with multiple duplicates, while images of Gwen Stacy and Spiderman 2099 appear few to none. Spiderman 2099 Gwen Stacy Miles Morales SonicRealTextualInversionTextualInversion(allclasses)TextualInversion(noclass0) Figure 14: (Left) Qualitative results of Textual Inversion on 4 film character concepts: TI works significantly worse for concepts not abundant in the LAION 5-B [49] such as Spiderman 2099 and Gwen Stacy compared to Miles Morales and Sonic, both of which we observed many duplicates. Refer to the Appendix E to see how we check for existence of these concepts in LAION 5-B. (Right) Qualitative results of Textual Inversion on MNIST: Textual Inversion works worse when excluding class 0 from the training dataset. Figure 15: Images of the character Miles Morales appear with many duplicates in our search using https://rom1504.github.io/clip-retrieval. 20 Figure 16: Images of the character Sonic appear with many duplicates in our search using https: //rom1504.github.io/clip-retrieval. Figure 17: Images of the character Gwen Stacy appear only few in our search using https:// rom1504.github.io/clip-retrieval. 21 Figure 18: Images of the character Spiderman 2099 does not appear in our search using https: //rom1504.github.io/clip-retrieval. This is reasonable since the character was introduced in a film released after the release of LAION-5B. 22","libVersion":"0.3.2","langs":""}