{"path":"GenAIUnleaning/General_techniques/Gradient Surgery for One-shot Unlearning on Generative Model.pdf","text":"Gradient Surgery for One-shot Unlearning on Generative Model Seohui Bae 1 Seoyoon Kim 1 Hyemin Jung 1 Woohyung Lim 1 Abstract Recent regulation on right-to-be-forgotten emerges tons of interest in unlearning pre-trained machine learning models. While approximating a straightforward yet expensive approach of retrain-from-scratch, recent machine unlearning methods unlearn a sample by updating weights to remove its influence on the weight parameters. In this paper, we introduce a simple yet effective approach to remove a data influence on the deep generative model. Inspired by works in multi-task learning, we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained. Our work is agnostic to statistics of the removal samples, outperforming existing baselines while providing theoretical analysis for the first time in unlearning a generative model. 1. Introduction Suppose a user wants to get rid of his/her face image any- where in your facial image generation application - includ- ing the database and the generative model on which it is trained. Is the expensive retrain-from-scratch the only solu- tion for this kind of request? As the use of personal data has been increased in training the machine learning models for online service, meeting individual demand for privacy or the rapid change in the legislation of General Data Protection Registration (GDPR) is inevitable to ML service providers nowadays. This request on ‘Right-To-Be-Forgotten (RTBF)’ might be a one-time or in-series, scaling from a feature to a number of tasks, querying single instance to multiples. A straightforward solution for unlearning a single data might be to retrain a generative model from scratch without data of interest. This approach, however, is intractable in prac- tice considering the grand size and complexity of the latest 1LG AI Research, Seoul, South Korea. Correspondence to: Seohui Bae <seohui.bae@lgresearch.ai>, Woohyung Lim <w.lim@lgresearch.ai>. Accepted to the 1 st Workshop on Generative AI and Law, co- located with the International Conference on Machine Learning, Honolulu, Hawaii, USA. 2023. Copyright 2023 by the author(s). generative models (Rombach et al., 2022; Child, 2020) and the continual request for removal. Unlearning, thereafter, aims to approximate this straightforward-yet-expensive solution of retrain-from- scratch time and computation efficiently. First-order data-influence-based approximate unlearning is currently considered the state-of-the-art approach to unlearning machine learning models in general. Grounded by the notion of data influence (Koh & Liang, 2017), a simple one-step Newton’s update certifies sufficiently small bound between retrain-from-scratch (Guo et al., 2020). Nonethe- less, those relaxations are infeasible to the non-convex deep neural networks (e.g. generative model) where the gap is not certifiably bounded and the process of computing the inverse of hessian is intractable. Several recent works also have affirmed that these relaxed alternatives perform poorly on deep neural networks (Golatkar et al., 2021; Liu et al., 2022) and even that on generative models have not been explored yet. Contribution In this work, we propose a novel one-shot unlearning method for unlearning samples from pre-trained deep generative model. Relaxing the definition of influ- ence function on parameters in machine unlearning (Koh & Liang, 2017; Basu et al., 2020), we focus on the influence of a single data on the test loss of the others and propose a simple and cost-effective method to minimize this inter- dependent influence to approximate retrain-from-scratch. We summarize our contributions as follows: • We propose to annul the influence of samples on gen- erations with simple gradient manipulation. • Agnostic to removal statistics and thus applied to any removals such as a single data, a class, some data fea- ture, etc. • Grounded by a theoretical analysis bridging standard machine unlearning to generative model. 2. Gradient Surgery for One-shot Data Removals on Generative Model Notations Let D = {xi} N i=1 ⊆ X be the training data where xi ∈ X is input. Let Df ⊆ D be a subset of training data that is to be forgotten (i.e. forget set) and Dr = D \\ DfarXiv:2307.04550v2 [cs.LG] 18 Jul 2023 Gradient Surgery for One-shot Unlearning on Generative Model be remaining training data of which information we want to retain. Recall that the goal of unlearning is to approximate the deep generative model retrained from scratch with only Dr, which we denote as fθ∗ parameterized by θ∗. Then, our goal is to unlearn Df ⊆ D from a converged pre-trained generator fˆθ by updating the parameter ˆθ → θ−, where θ− represents the updated parameters obtained after unlearning. Proposed method Given a generative model that models the distribution of training data p(D), a successful unlearned model that unlearns Df would be what approximates p(Dr), the distribution of Dr, as if it had never seen Df . The only case where the unlearned model generates samples similar to x ∈ Df is when p(Df ) and p(Dr) happen to be very close from the beginning. Under this goal, a straight-forward objective given the pre-trained model approximating p(D) is to make the output of generation to deviate from p(Df ), which could be simply formulated as the following: max θ E(x,y)∼Df L(θ, x, y) (1) where L denotes training loss (e.g. reconstruction loss). Meanwhile, assume we could define the influence of a single data on the weight parameter and generation result. Then, unlearning this data would be by simply updating the weight parameter in a direction of removing the data influence. Toward this, we start with defining the data influence on weight parameters and approximates to feasible form as introduced in Koh & Liang (2017): Definition 2.1. Given upweighting z by some small ϵ and the new parameters ˆθϵ,z def = argminθ∈Θ 1 n ∑n i=1 L(zi, θ) + ϵL(z, θ), the influence of upweighting z on the parameter ˆθ is given by Iup,param(z)def = dˆθϵ,z dϵ ∣ ∣ ∣ϵ=0def = −H −1 ˆθ ∇θL(z, ˆθ) (2) where Hˆθ = 1 n ∑n i=1 ∇2 θL(zi, ˆθ) is the Hessian and is posi- tive definite (PD) by assumption. By forming a quadratic approximation to the empirical risk around ˆθ, a data influence on the weight parameter is formu- lated as a single Newtons step (See details in Appendix of (Koh & Liang, 2017)), which is consistent with the objec- tive we have mentioned in Equation 1. Although numerous works have verified that this data influence-based approach works well in shallow, discriminative models (Guo et al., 2020; Golatkar et al., 2020a;b), we cannot apply this directly to our generative model due to intractable computation and lack of guarantees on bounds. To address this problem, we re-purpose our objective to minimize the data influence on generation. Grounded by recent works (Basu et al., 2020; Sun et al., 2023), we find that we could enjoy this on gener- ative model simply by diminishing the gradient conflict as follows: Theorem 2.2. Reducing the influence of samples z ∈ Df in training data with regard to test loss is formulated as: I ′ up,loss(Df , z′) → 0, (3) which is equivalent to ∇θL(z′, ˆθ) T ∑ z∈Df ∇θL(z, ˆθ) → 0 (4) where z′ ∈ Dr in our scenario. Informally, we could achieve this by alleviating the conflict between two gradients ∇θL(z′, ˆθ) and ∇θL(z, ˆθ), resulting in diminishing the inner product of two gradients. This reminds us of a classic approach of gradient manipulation techniques for conflicting gradients in multi-task learning scenario (Yu et al., 2020; Liu et al., 2021a; Guangyuan et al.). Specifically, we project a gradient of forget sample xf ∈ Df onto normal plane of a set of retain samples xr ∈ Dr to meet Iup,loss(xf , xr) = 0. This orthogonal projection manipulates the original gradient of forget sample gf = ∇Lf to the weight parameter to which sufficiently unlearns a sample xf ∈ Df : gf = gf − gf ·gr ∥g2 r∥ gr. Then, the unlearned model θ− is obtained after the following gradient update: θ− = ˆθ − ηgf . 3. Experiments We verify our idea under numerous data removal requests. Note that measuring and evaluating a generative model to unlearn a single data is non-trivial. Even comparing pre- trained generative models trained with a particular data over without simply by looking at the output of training (e.g. generated image, weight) is intractable in case of a deep generative model to the best of our knowledge (van den Burg & Williams, 2021). To make the problem verifiable, in this work, we experiment to unlearn a group of samples shar- ing similar statistics in the training data - either belonging to a particular class or that has a distinctive semantic feature. In this case, one can evaluate the output of the generation by measuring the number of samples including that class or a semantic feature; a successfully unlearned model would generate nearly zero number of samples having these fea- tures. Although we are not able to cover unlearning a single data in this work, note that in essence, our method could suc- cessfully approximate the generative model trained without a single data seamlessly, and we look forward to exploring and adjusting a feasible evaluation on this scenario in the near future. 3.1. Experimental Setup Scenarios We unlearn either a whole class or some notable feature from a group of samples. In the experiment, we use a subset of MNIST (Alsaafin & Elnagar, 2017) with samples Gradient Surgery for One-shot Unlearning on Generative Model Table 1. Performance of Class/Feature Unlearning VAE on MNIST138 (left columns) and CelebA (right column) Each experiments are three times repeated. (*) indicates erroneous evaluation by a pre-trained feature classifier. Bold indicates the best score. MNIST138(CLASS: 1) CELEBA(FEATURE: MALE) METRIC PRIVACY UTILITY COST PRIVACY UTILITY COST fratio(↓) IS(↑) FID(↓) Time(S)(↓) fratio(↓) IS(↑) FID(↓) Time(S)(↓) BEFORE 0.343(0.027) 2.053(0.029) 0.030(0.003) 218.6 0.394(0.119) 1.812(0.044) 29.81(0.341) 3 × 104 GRAD.ASCNT. 0.264(0.141) 2.029(0.018) 0.127(0.059) 1.010 - (*) 1.311(0.076) 30.93(1.215) 97.31 MOON ET AL. (2023) 0.344(0.019) 2.048(0.021) 0.031(0.002) 166.2 1.000(0.000) 1.000(0.000) 15.81(9.831) 8 × 104 OURS 0.153(0.057) 2.192(0.076) 0.092(0.030) 13.12 0.150(0.098) 1.254(0.013) 34.24(0.698) 613.2 of classes 1,3,8 and 64x64 CelebA (Liu et al., 2015) to train and unlearn vanilla VAE (Kingma & Welling, 2013). Evaluation We evaluate our method under the following three criteria: a privacy guarantee, utility guarantee, and cost. Privacy guarantee includes feature ratio ( fratio), a ratio of images including the target feature (See details in Appendix A). Utility guarantee includes Frechet Incep- tion Distance (FID), a widely used measure for generation quality. Cost includes a total execution time (Time) which should be shorter than retrain-from-scratch. A successfully unlearned model would show near-zero on feature ratio, the same IS, FID score as the initial pre-trained model (BE- FORE), and the lowest possible execution time. Given the legal impact and the goal of unlearning, note that guarantee- ing privacy is prioritized the highest. Figure 1. Unlearning groups of class 1 samples from VAE pre- trained on MNIST138 (left: original, right: unlearned) Note that images of class 1 do not appear in generation result. 3.2. Result on Pre-trained Generative Model Quantitative Result We run the proposed method on pre- trained VAE to remove unlearning group Df (e.g. class 1 or male, respectively) and evaluate them as follows (Table 3) Starting from the pre-trained model (BEFORE) our method unlearns the target Df with a large decrease on fratio by 65% to 70% while keeping the time cost of unlearning ≤ 5% of retrain-from-scratch. All the while, our method still keeps a decent utility performance. Comparing the baselines, our method shows the best in privacy - the prioritized metric - through all experiments. Note that the feature ratio of gradient ascent in the CelebA experiment (feature ratio- CelebA-Grad.Ascnt) was omitted because the generated samples are turned out to be noisy images and thus the evaluation result of pre-trained classifier cannot be accepted. Also, note that although baselines show better performance in terms of utility and cost, they don’t show near-best score on privacy guarantee. Qualitative Result We further validate our method by comparing the generated images before and after the pro- posed unlearning algorithm. As in Figure 3.1, no class 1 samples are observed after unlearning class 1, meaning that our method successfully meets the request of unlearning class 1, which aligns with the quantitative result where the ratio of samples with class 1 is reduced from 34.3% to ≤ 15% as in Table 3. The output of image generation is fair where 3 and 8 are decently distinguishable through one’s eyes, although it is certain that some examples show some minor damaged features, which are in the same line as a decrease in IS and an increase in FID score. Note that the ultimate goal of unlearning is to meet the privacy guaran- tee while preserving the utility of pre-training, which are remained as our next future work. 4. Conclusion In this work, we introduce a novel theoretically sounded unlearning method for the generative method. Inspired by the influence of the sample on the others, we suggest a sim- ple and effective gradient surgery to unlearn a given set of samples on a pre-trained generative model and outperform the existing baselines. Although we don’t experiment to unlearn single data due to a lack of ground evaluation on the uniqueness of the particular data, we leave it as future work emphasizing that our method could also be applied to this scenario. Furthermore, it would be interesting to verify our ideas on various privacy-sensitive datasets. Nonetheless, our work implies the possibility of unlearning a pre-trained gen- erative model, laying the groundwork for privacy handling in generative AI. Gradient Surgery for One-shot Unlearning on Generative Model References Alsaafin, A. and Elnagar, A. A minimal subset of features using feature selection for handwritten digit recognition. Journal of Intelligent Learning Systems and Applications, 9(4):55–68, 2017. Basu, S., You, X., and Feizi, S. On second-order group influence functions for black-box predictions. In Inter- national Conference on Machine Learning, pp. 715–724. PMLR, 2020. Bishop, C. Exact calculation of the hessian matrix for the multilayer perceptron, 1992. Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C. A., Jia, H., Travers, A., Zhang, B., Lie, D., and Papernot, N. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pp. 141–159. IEEE, 2021. Child, R. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv preprint arXiv:2011.10650, 2020. Fu, S., He, F., and Tao, D. Knowledge removal in sampling-based bayesian inference. arXiv preprint arXiv:2203.12964, 2022. Golatkar, A., Achille, A., and Soatto, S. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9304–9312, 2020a. Golatkar, A., Achille, A., and Soatto, S. Forgetting out- side the box: Scrubbing deep networks of information accessible from input-output observations. In Computer Vision–ECCV 2020: 16th European Conference, Glas- gow, UK, August 23–28, 2020, Proceedings, Part XXIX 16, pp. 383–398. Springer, 2020b. Golatkar, A., Achille, A., Ravichandran, A., Polito, M., and Soatto, S. Mixed-privacy forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 792–801, 2021. Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082, 2013. Guangyuan, S., Li, Q., Zhang, W., Chen, J., and Wu, X.- M. Recon: Reducing conflicting gradients from the root for multi-task learning. In The Eleventh International Conference on Learning Representations. Guo, C., Goldstein, T., Hannun, A., and Van Der Maaten, L. Certified data removal from machine learning models. In International Conference on Machine Learning, pp. 3832–3842. PMLR, 2020. Gupta, V., Jung, C., Neel, S., Roth, A., Sharifi-Malvajerdi, S., and Waites, C. Adaptive machine unlearning. Ad- vances in Neural Information Processing Systems, 34: 16319–16330, 2021. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn- ing for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. beta- vae: Learning basic visual concepts with a constrained variational framework. In International conference on learning representations, 2017. Kingma, D., Salimans, T., Poole, B., and Ho, J. Varia- tional diffusion models. Advances in neural information processing systems, 34:21696–21707, 2021. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Koh, P. W. and Liang, P. Understanding black-box predic- tions via influence functions. In International conference on machine learning, pp. 1885–1894. PMLR, 2017. Liu, B., Liu, X., Jin, X., Stone, P., and Liu, Q. Conflict- averse gradient descent for multi-task learning. Advances in Neural Information Processing Systems, 34:18878– 18890, 2021a. Liu, B., Liu, Q., and Stone, P. Continual learning and private unlearning. In Conference on Lifelong Learning Agents, pp. 243–254. PMLR, 2022. Liu, G., Ma, X., Yang, Y., Wang, C., and Liu, J. Fed- eraser: Enabling efficient client-level data removal from federated learning models. In 2021 IEEE/ACM 29th In- ternational Symposium on Quality of Service (IWQOS), pp. 1–10. IEEE, 2021b. Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of the IEEE international conference on computer vision, pp. 3730– 3738, 2015. Moon, S., Cho, S., and Kim, D. Feature unlearning for generative models via implicit feedback. arXiv preprint arXiv:2303.05699, 2023. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022. Gradient Surgery for One-shot Unlearning on Generative Model Springenberg, J. T., Dosovitskiy, A., Brox, T., and Ried- miller, M. Striving for simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014. Sun, Z., Mu, Y., and Hua, G. Regularizing second-order influences for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20166–20175, 2023. van den Burg, G. and Williams, C. On memorization in probabilistic deep generative models. Advances in Neural Information Processing Systems, 34:27916–27928, 2021. Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33: 5824–5836, 2020. Zhang, Z., Zhou, Y., Zhao, X., Che, T., and Lyu, L. Prompt certified machine unlearning with randomized gradient smoothing and quantization. Advances in Neural Infor- mation Processing Systems, 35:13433–13455, 2022. Gradient Surgery for One-shot Unlearning on Generative Model A. Experimental Details A.1. Setup Architecture In this experiment, we use vanilla VAE (Kingma & Welling, 2013) with encoders of either stack of linear(for MNIST experiment) or convolutional(for CelebA experiment) layers. Although we verify our result on VAE, note that our method can be applied to any variational inference based generative model such as (Kingma et al., 2021; Higgins et al., 2017). Baseline We compare our experimental results with the following two baselines. One is a recently published, first and the only unlearning work on generative model (Moon et al., 2023) (FU) to unlearn by feeding a surrogate model with projected latent vectors. We reproduce FU and follow the hyperparameter details (e.g. unlearning epochs 200 for MNIST) as in the original paper. The other is a straight-forward baseline (Grad.Ascnt.) which updates the gradient in a direction of maximizing the reconstruction loss on forget, which is equivalent to meeting e.g. Objective 1 without gradient surgery. Note that we keep the same step size when unlearning with these three different methods (including ours) for fair comparison. Training details We use Adam optimizer with learning rate 5e-04 for MNIST experiment and 1e-05 for CelebA experiment. We update the parameter only once (1 epoch) for removals, thus named our title ’one-shot unlearning’. All experiments are three times repeated. A.2. How to Evaluate Feature Ratio We first prepare a classification model that classifies the image having a target feature from the remains. In order to obtain a highly accurate classifier, we search for the best classifier which shows over 95% accuracy. In the experiment, we use AllCNN (Springenberg et al., 2014) to classify class 1 over the other in MNIST with 1,3,8 (MNIST381), and ResNet18 (He et al., 2016) to classify male over female on CelebA. After unlearning, we generate 10000 samples from the generator and feed the sample to the pre-trained classifier. Assuming that the classifier classifies the image well, the prediction result would the probability that the generated output contains the features to be unlearned. B. Definitions and Proof for Theoretical Analysis In Koh & Liang (2017) and Basu et al. (2020), an influence of sample z on weight parameter is defined as the product of its gradient and inverse of hessian. Moreover, an influence of sample z to test loss of sample z′ defined in as following: Definition B.1. (Equation 2 from Koh & Liang (2017)) Suppose up-weighting a converged parameter ˆθ by small ϵ, which gives us new parameters ˆθϵ,z def = argminθ∈Θ 1 n ∑n i=1 L(zi, θ) + ϵL(z, θ). The influence of up-weighting z on the loss at an arbitrary point z′ against has a closed-form expression: Iup,loss(z, z′) def = dL(z′, ˆθϵ,z) dϵ ∣ ∣ ∣ϵ=0 = ∇θL(z′, ˆθ) ⊤H −1 ˆθ ∇θL(z, ˆθ) (5) where Hˆθ def = 1 n ∑n i=1 ∇2 θL(zi, ˆθ) is the Hessian and is positive definite (PD) by assumption on convex and Lipschitz continuity of loss L. Theorem B.2. (Theorem 2.2 from Section 2) Reducing the influence of samples z ∈ Df in training data with regard to test loss is formulated as: I ′ up,loss(Df , z′) → 0, (6) which is equivalent to ∇θL(z′, ˆθ) T ∑ z∈Df ∇θL(z, ˆθ) → 0 (7) where z′ ∈ Dr in our scenario. Gradient Surgery for One-shot Unlearning on Generative Model Proof. The second-order influence of Df , I (2) up,param, is formulated as sum of first-order influence I (1) up,param and I ′ up,param, which captures the dependency of the terms in O(ϵ 2) on the group influence is defined as following: I ′ up,param(Df , z′) = AH −1 ˆθ ∑ z∈Df ∇θL(z, ˆθ) (8) where A = p 1−p (I − (∇2L(θ∗))−1 1 |U | ∑ z∈U ∇2l(hθ∗ (z))) (from Basu et al. (2020)). The influence of samples in Df on the test loss of z′ can be formulated as: Iup,loss(Df , z′) = ∇θL(z, ˆθ) T Iup,param(Df ) (9) which can be equivalently applied to all orders of I including I (1), I (2), I ′. Then, I ′ up,loss(Df , z′) = 0 is now reduced to ∇θL(z, ˆθ) T AH −1 ˆθ ∑ z∈Df ∇θL(z, ˆθ) = 0 (10) which satisfies the right-hand side of Theorem 2.2 where A and H −1 ˆθ are negligible.","libVersion":"0.3.2","langs":""}