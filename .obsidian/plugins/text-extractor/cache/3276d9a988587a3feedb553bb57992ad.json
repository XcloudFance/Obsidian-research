{"path":"GenAIUnleaning/General_techniques/FAST Feature Aware Similarity.pdf","text":"1 FAST: Feature Aware Similarity Thresholding for weak unlearning in black-box generative models Subhodip Panda, Prathosh AP Indian Institute of Science Abstract—The heightened emphasis on the regulation of deep generative models, propelled by escalating concerns pertaining to privacy and compliance with regulatory frameworks, underscores the imperative need for precise control mechanisms over these models. This urgency is particularly underscored by instances in which generative models generate outputs that encompass objec- tionable, offensive, or potentially injurious content. In response, machine unlearning has emerged to selectively forget specific knowledge or remove the influence of undesirable data subsets from pre-trained models. However, modern machine unlearning approaches typically assume access to model parameters and architectural details during unlearning, which is not always feasible. In multitude of downstream tasks, these models function as black-box systems, with inaccessible pre-trained parameters, architectures, and training data. In such scenarios, the possibility of filtering undesired outputs becomes a practical alternative. Our proposed method Feature Aware Similarity Thresholding (FAST) effectively suppresses undesired outputs by systematically encoding the representation of unwanted features in the latent space. We employ user-marked positive and negative samples to guide this process, leveraging the latent space’s inherent capacity to capture these undesired representations. During inference, we use this identified representation in the latent space to compute projection similarity metrics with newly sampled latent vectors. Subsequently, we meticulously apply a threshold to exclude unde- sirable samples from the output. Our implementation is available at https://github.com/Subhodip123/weak-unlearning-gan Impact Statement—The primary goal of this study is twofold: first, to elucidate the relationship between filtering and unlearn- ing processes, and second, to formulate a methodology aimed at mitigating the display of undesirable outputs generated from black-box Generative Adversarial Networks (GANs). Theoretical analysis in this study demonstrates that, in the context of black- box models, filtering can be seen as a form of weak unlearning. Importantly, our proposed methodology exhibits efficacy even in scenarios where the user feedback exclusively consists of negative samples, devoid of any positive examples. The veracity and utility of our approach are empirically substantiated through a series of experiments conducted within a low data (< 100 samples) regime, featuring the utilization of DC-GAN and Style- GAN2 architectures on the MNIST and CelebA-HQ datasets respectively. Index Terms—Black-Box Generative Models, General Adver- sarial Networks(GANs), Machine Unlearning I. INTRODUCTION Deep generative models have gained widespread popularity for the generation of authentically realistic synthetic data. These models find utility in numerous domains where access to authentic data is limited, exemplified by their application in fields such as medical imaging [1], [2], remote sensing [3], [4], hyper-spectral imagery [5], [6] and others [7]–[9] and also in various downstream tasks, including but not limited to classification [10], [11], segmentation [12] etc. However, despite their versatile applicability, the presence of undesirable samples within the dataset and the inherent biases ingrained in these datasets [13] contribute to instances where these models produce outputs that are regarded as objectionable, offensive, or even potentially harmful. Thus, recent regulatory measures related to data privacy and protection, such as the European Union’s General Data Protection Regulation (GDPR) [14] and the California Consumer Privacy Act (CCPA) [15], compel organizations to enforce stringent controls on models that have the potential to generate content deemed harmful or offensive. To address this challenge, our focus lies in exploring methodologies that prevent the display of specific output types characterized by undesirable features. In a parallel vein of inquiry, recent research in the domain of machine unlearning within generative models [16], [17] has delved into the concept of forgetting, wherein the primary objective revolves around the modification of generative model parameters to ensure the non-generation of undesirable images. It is worth noting that all of the aforementioned studies operate under the assumption that, during the unlearning phase, accessibility to the learned model’s architecture, parameters, and, at times, even the training dataset is readily available. However, there exist scenarios wherein these models are employed as service providers for diverse downstream tasks, functioning as black- box models. In this more stringent context, the pertinent model parameters, architectures, and the underlying dataset remain entirely obscured from the user’s purview. In such a constrained environment, conventional unlearning procedures or the prospect of retraining the model from its inception becomes an unattainable endeavor due to the complete lack of access to model parameters and architectures. Consequently, the question that necessitates elucidation is as follows: What are the feasible remedies that can effectively suppress the display of undesired outputs when the model is entirely opaque, operating as a black-box system? In a black-box scenario, filtering where the task involves screening out undesired outputs while retaining other outputs, is a viable practical solution. This research addresses the formidable challenge of effectively filtering undesired out- puts generated by black-box generative models within the feedback-based unlearning framework. Specifically, the user is presented with a curated collection of generated samples stemming from a pre-trained black-box Generative Adversarial Network (GAN). Within this assortment, the user is tasked with distinguishing a subset as undesirable (negative) andarXiv:2312.14895v2 [cs.LG] 20 Jun 2024 2 another subset as desirable (positive). This task is subject to constraints, notably a limited sample size for the user feedback (typically not exceeding 100 samples). While the concept of employing a binary classifier for filtering appears straightforward, the crux of the challenge resides in training a classifier for this task. The primary hindrance is the scarcity of available user feedback samples, making the task inher- ently intricate. Moreover, conventional filtering mechanisms, which utilize such classifiers, lack awareness of the underlying representation of the undesired feature. Consequently, the de- velopment of filtering mechanisms equipped with the capacity to grasp the representation of undesired features becomes a complex and non-trivial endeavor. Adding to the complexity is the requirement to construct a comprehensive and informative representation of the undesired feature. This representation must be meticulously designed to exclusively encapsulate the distinctive characteristics of the undesired feature, without incorporating any extraneous information related to correlated features. The entanglement between the representations of semantic features further complicates this task. For instance, within the CelebA-HQ [18] dataset, the beard feature exhibits substantial correlations with attributes like mustache and male gender. Therefore, an effective filtering mechanism must be equipped with the ability to discern and harness a represen- tation that singularly encapsulates the beard feature, while prudently avoiding the inclusion of information about other correlated attributes such as mustache and gender. In this work, we introduce an innovative family of filtering mechanisms, collectively referred to as Feature Aware Similar- ity Thresholding (FAST) meticulously designed to proficiently detect and eliminate outputs featuring undesired attributes while inherently possessing an awareness of the representation of these undesired features. Our methodology, reliant upon user-provided positive and negative samples, effectively ex- ploits the latent space to characterize the representation of the undesired feature. The fundamental rationale underpin- ning our approach centers on the latent space’s remarkable capacity to encode intricate and interpretable representations of semantic features. This encoded representation encapsulates the essential traits of the undesired feature, thereby endowing the system with the ability to effectively distinguish it from other attributes. The operational framework of our method entails an assessment of the similarity between newly sampled latent vectors and the previously identified latent feature repre- sentation corresponding to the undesired attribute. Leveraging the principle that latent vectors encoding similar features are proximate in the latent space, we gauge the alignment between the latent vectors and the undesired feature, facilitating the precise identification of undesired outputs. Subsequently, a well-considered threshold is applied to this similarity metric, leading to the segregation and exclusion of outputs that man- ifest the undesired feature. We summarize our contribution as follows: • This work represents the first instance where filtering is introduced as a form of weak unlearning to block un- desired features within black-box Generative Adversarial Networks (GANs). • Theoretical analysis conducted in this research establishes that, in the context of black-box generative models, the processes of filtering and weak unlearning are equivalent. • Our proposed filtering mechanism relies on user-provided feedback, encompassing undesired (negative) and de- sired (positive) outputs. Acknowledging the potentially demanding nature of user feedback, our method is tai- lored to operate effectively within a few-shot setting, wherein the number of user feedback samples does not exceed 100. Furthermore, our approach demonstrates its robustness by functioning efficiently even when the user provides only negative samples, without including any positive samples. In addition to its efficacy in filtering undesired outputs, our method distinguishes itself by its intrinsic awareness of the representation of undesired features. • We assess and validate the performance of our pro- posed method across various GAN architectures, in- cluding DC-GAN and Style-GAN2, employing datasets such as MNIST and CelebA-HQ, thereby showcasing its effectiveness and versatility in different scenarios and datasets. II. BACKGROUND A. Machine Unlearning Machine unlearning [19], [20] refers to the process of deliberately forgetting specific acquired knowledge or erasing the impact of particular subsets of training data from a trained model. Naive unlearning methods typically entail the removal of undesirable data from the training dataset, followed by retraining the model from scratch. However, this approach be- comes computationally prohibitive when unlearning requests are made iterative for individual data points. Inspired by con- cerns surrounding privacy protection, [21] introduced methods for data deletion within statistical query algorithms, coining the term machine unlearning. Unfortunately, these methods are primarily suitable for structured problems and do not extend to complex machine learning algorithms, such as k- means clustering [22] or random forests [23]. Efficient deletion algorithms were devised for the k-means clustering problem, which introduced effective data deletion criteria applicable to randomized algorithms based on statistical indistinguishability. Building upon this criterion, machine unlearning methods are broadly categorized into two main types: exact unlearning [22], [23] and approximate unlearning [24]. Exact unlearning endeavors to completely eradicate the influence of unwanted data from the trained model, necessitating precise parameter distribution matching between the unlearned and retrained models. In contrast, approximate unlearning methods only partially mitigate data influence, resulting in parameter distri- butions that closely resemble the retrained model, albeit with minor multiplicative and additive adjustments. To eliminate the influence of unwanted data, [25] proposed technique employs parameter perturbation based on cached gradients, offering computational efficiency while increasing memory usage. Other methods [26], [27] have suggested the use of in- fluence functions for this purpose. However, these approaches are computationally demanding due to the necessity of hessian 3 inversion techniques and are limited to small convex models. To extend the applicability of influence removal techniques to non-convex models like deep neural networks, a scrub- bing mechanism [28] was introduced within a classification framework. However, until recently, it remained unclear how these techniques could be applied to unsupervised models, particularly state-of-the-art generative models. In response, [29] introduced an Adapt-then-Unlearn mechanism designed to unlearn a GAN in a zero-shot setting where the training dataset is inaccessible. Nevertheless, this method does not readily extend to black-box generative models, presenting a challenge in the context of machine unlearning for black-box models. B. Generative Adversarial Network Generative Adversarial Networks (GANs), a prominent class of generative models primarily used for image syn- thesis, were first introduced in the seminal work by [30]. The GAN framework consists of two neural networks: a generator network and a discriminator network. GANs are trained through an adversarial loss function, which strives to minimize the divergence between the distribution of original data and that of generated (fake) data. The generator network, once trained, takes random latent vectors sampled from a normal distribution as input and produces synthetic data that closely resembles the original data. This process was theo- retically elucidated by [30], [31], who demonstrated that the generator aims to minimize the Jensen-Shannon divergence between the distribution of fake data and that of the original data. Moreover, [32] expanded upon this idea by showcasing that various divergence metrics, with the Jensen-Shannon divergence being one of them, can be minimized to facilitate the training of effective generative models. They introduced a broader class of models called f-GANs, with the specific choice of the f-divergence metric influencing the optimization process. To further enhance the capabilities of GANs, [33] proposed an information-theoretic extension known as Info- GAN. This innovation allowed GANs to learn a disentangled latent space, significantly improving the interpretability and editability of the generated data. Addressing challenges related to GAN training stability, [31] introduced the idea of training GANs with Wasserstein distance metrics, which contributed to more stable and reliable training processes. Furthermore, the field of generative models has witnessed the introduction of numerous specialized GAN variants [34] aimed at synthesizing high-quality images. For instance, [35]–[37] proposed a style incorporation technique during model training, achieving state- of-the-art results for generating high-resolution images. C. Latent Space Representations and Inversion The latent representation of Generative Adversarial Net- works (GANs) often proves to be valuable for auxiliary supervised downstream tasks. Furthermore, this latent space exhibits the capacity to encode a diverse array of semantic features that find utility in tasks such as image manipulation and editing, among others, as demonstrated by [33]. Nu- merous studies [35]–[37] have underscored the latent space’s remarkable representation capabilities, which encompass a wide range of interpretable semantics. For instance, in the context of Style-GAN2, it was observed that the learned W space possesses greater disentanglement compared to the random noise space Z. This characteristic enhancement in disentanglement contributes to improved editability and per- ceptual quality in generated images. Additionally, latent spaces often exhibit commendable clustering properties [38], where data with similar semantic features cluster closely within the latent space. To harness the rich structural and representational attributes of the latent space, it is essential to project data back into this space through a process known as inversion [39]. Inverter networks are employed for this purpose, taking data as input and producing corresponding latent vectors, which enable the GAN model to accurately reconstruct the data. Two primary approaches are typically used for inversion: iterative optimization and inference with an encoder [40], [41]. Recent developments in encoder-based methods [42], [43] have yielded impressive outcomes, not only in terms of reconstruction but also in enhancing expressiveness. III. THEORY AND ANALYSIS The notations are as follows: • Θ := the space of all parameters of generative models • X := the space of all data points • Z := space of all latent vectors of generative models Consider a specific training dataset D consisting of m i.i.d samples {xi} m i=1, drawn from a distribution PX over the data space X . A generative model (G) trained on D aims to learn the data distribution PX . The pre-trained generative model is denoted as Gθinit, with parameters θinit ∈ Θ. Based on the outputs of the generative model, the user designates a portion of the data space as undesired, referred to as Xf . Therefore, the entire data space can be expressed as the union of Xr and Xf , where X = Xr ⋃ Xf . We denote the distributions over Xf and Xr as PXf and PXr , respectively. The objective is to create a mechanism in which the generative model does not produce outputs within the domain Xf . This implies that the generative model should be trained to generate data samples conforming to the distribution PXr . A naive approach to achieving this is by retraining the entire model from scratch using a dataset Dr. Here, Dr is defined as the intersection of D and Xr, or equivalently, Dr = D \\ Df , where Df = D ⋂ Xf . Consequently, any retraining mechanism (RT ) results in a new model with parameters θr whose output distribution Pθr seeks to match the distribution PXr . A. Machine Unlearning As this retraining is computationally costly, current machine unlearning mechanisms (UM) change the parameters of the model from θinit to θu so that with very high probability the generative models never give outputs that belong to the domain Xf . Now depending upon the outputs from the retrained model Gθr and the unlearned model Gθu we define exact weak unlearning [22] and approximate weak unlearning [24] as follows 4 Definition 1: (Exact Weak Unlearning) Given a retrained generative model Gθr (.), we say the model Gθu (.) is an exact weak unlearned model iff ∀z ∼ PZ , O ⊂ X the following holds: Pr(Gθr (z) ∈ O) = Pr(Gθu (z) ∈ O) (1) Definition 2: ((ϵ, δ) Approximate Weak Unlearning) Given a retrained generative model Gθr (.), we say the model Gθu (.) is an (ϵ, δ) approximate weak unlearned model for a given ϵ, δ ≥ 0 iff ∀z ∼ PZ , O ⊂ X the following conditions hold: Pr(Gθr (z) ∈ O) ≤ eϵ Pr(Gθu (z) ∈ O) + δ (2) Pr(Gθu (z) ∈ O) ≤ e ϵ Pr(Gθr (z) ∈ O) + δ (3) Definitions 1 and 2 imply that the unlearned model’s output should closely resemble that of the naively retrained model. However, it’s important to note that retraining or unlearning is a viable option when we have access to the original dataset D and knowledge of the initial model parameters θinit and architectures. Yet, in many practical scenarios, these generative models are utilized as black boxes for various downstream tasks. In such cases, the conventional unlearning methods become infeasible. When faced with this challenge, where direct unlearning is not an option, we turn to the alternative solution of filtering the outputs generated by these black-box models. The subsequent section will delve into the concept of filtering and elucidate how it effectively translates into a form of weak unlearning for these black-box generative models. B. Filtering as Weak Unlearning In a black-box setting as described, the underlying datasets Dr, Df , and the initial model parameter θinit remain unknown. As illustrated in Fig-1, our desired filtering model Gθb is designed to approximate the distribution PXr . In a hypothetical scenario where we have access to the model parameters and architecture, any unlearned model Gθu would aim to match a completely retrained model’s output distribution Pθr . Leveraging this concept, the following theorem highlights that filtering can be interpreted as a form of weak unlearning for these types of black-box generative models, where the objective is to approximate the distribution in the output space. Theorem 1: If there exists a retrained generative model with parameters θr such that dln−T V (ln PXr || ln Pθr ) ≤ ϵ1 and a generative model with posthoc blocking layer denoting pa- rameters θb such that dln−T V (ln PXr || ln Pθb ) ≤ ϵ2 for some ϵ1, ϵ2 ≥ 0, then for a (ϵ, 0)− approximately weak unlearned model with parameter θu the following holds: ∀O ⊂ Xr | ln Pr(Gθb (z) ∈ O) − ln Pr(Gθu (z) ∈ O)| ≤ ϵ + ϵ1 + ϵ2 (4) Remarks: Here the dln−T V (.||.) denotes the total variational distance in terms of the log of the distributions. Proof: dln−T V (ln PXr || ln Pθr ) = sup O∈F | ln PXr (O)−ln Pθr (O)| ≤ ϵ1 (5) Fig. 1. Filtering as Weak Unlearning in Black-Box Generative Models: The left-most block represents the black-box generator with posthoc blocking layer with parameters θb = (θinit, t) whose output distribution Pθb , the middle block represents a retrained generative model with output distribution Pθr , the right-most block represents a (ϵ, 0) unlearned model with output distribution Pθu . =⇒ | ln PXr (O) − ln Pθr (O)| ≤ ϵ1; ∀O ∈ F =⇒ −ϵ1 ≤ ln ( PXr (O) Pθr (O) ) ≤ ϵ1; ∀O ∈ F As, F is a σ-algebra then ∀x ∈ Xf if the subset {x} ∈ F =⇒ e−ϵ1Pθr ≤(i) PXr ≤(ii) e ϵ1Pθr (Uniformly Bounded) By a similar argument as before, dln−T V (ln PXr || ln Pθb ) = sup O∈F | ln PXr (O)−ln Pθb (O)| ≤ ϵ2 (6) =⇒ e−ϵ2 Pθb ≤(iii) PXr ≤(iv) e ϵ2Pθb (Uniformly Bounded) Now assume an (ϵ, 0)-Approximate weak unlearning model Gθu then equations (5) and (6) implies ∀z ∼ PZ | ln Pr(Gθr (z) ∈ O) − ln Pr(Gθu (z) ∈ O)| ≤ ϵ (7) Now the objective, | ln Pr(Gθb (z) ∈ O) − ln Pr(Gθu (z) ∈ O)| ≤ | ln Pr(Gθb (z) ∈ O) − ln Pr(Gθr (z) ∈ O)|(A) + | ln Pr(Gθr (z) ∈ O) − ln Pr(Gθu(z) ∈ O)|(B) By equation (2) and (3) the second term (B) = | ln Pr(Gθr (z) ∈ O) − ln Pr(Gθu (z) ∈ O)| ≤ ϵ (8) Now, using ineq. (ii) and ineq. (iii) we get the below inequal- ities respectively Pr(Gθr (z) ∈ O) = Pθr (O) ≥ e−ϵ1 PXr (O) (9) Pr(Gθb (z) ∈ O) = Pθb (O) ≤ eϵ2PXr (O) (10) Taking ln on both sides of inequality (9) and (10), then subtracting gives (A) = | ln Pr(Gθb (z) ∈ O) − ln Pr(Gθr (z) ∈ O)| ≤ ϵ1 + ϵ2 (11) 5 Adding equations (8) and (11) gives | ln Pr(Gθb (z) ∈ O) − ln Pr(Gθu (z) ∈ O)| ≤ ϵ + ϵ1 + ϵ2 Corollary 1.1: For every (ϵ, 0)− Approximate Weak Un- learning algorithm there exists a filtering mechanism such that for some ϵ ′ ≥ 0 the following holds Pr(Gθb (z) ∈ O) ≤ e (ϵ+ϵ′) Pr(Gθu (z) ∈ O) (12) Pr(Gθu (z) ∈ O) ≤ e(ϵ+ϵ′) Pr(Gθb (z) ∈ O) (13) IV. METHODOLOGY A. Problem Formulation and Method Overview As previously mentioned, the pre-trained generator Gθinit is trained using a dataset D comprising m independent and identically distributed (i.i.d) samples denoted as {xi} m i=1, where xi iid ∼ PX . In our specific scenario, both the dataset D and the initial model parameters θinit remain undisclosed. Our objective is to formulate a filtering mechanism that relies on user feedback to selectively exclude undesired outputs. In this context, the user is presented with a collection of r samples, denoted as S = {yi} r i=1, where each yi represents a generated sample originating from the pre-trained GAN. Within this user feedback system, the user’s role is to designate a relatively small subset of these samples, denoted as Sn = {yn i }s i=1, as negative samples. These negative samples correspond to the instances that exhibit undesired features. Additionally, the user selects a portion of the samples, referred to as Sp = {yp i } s i=1, as positive samples. Positive samples are those that do not possess the undesired features and are considered desirable. It’s essential to note that gathering user feedback can be resource-intensive. Therefore, we operate within a low-data regime, where the no of feedback samples marked by the user, denoted as s, is very small (constrained to be less than 100) i.e. s ≪ r. This limitation acknowledges the potential challenges associated with obtaining extensive user feedback and emphasizes the need to devise effective filtering mechanisms even in scenarios with limited data availability. In this study, we implement a two-stage process to filter out undesired outputs effectively. In the initial stage termed as Undesired Feature Representation stage, we encode the representation of the undesired feature within the latent space. This latent space may either be the implicit latent space inherent to the Generative Adversarial Network (GAN) or a latent space that is learnable. Subsequently, in the second step termed as Similarity Thresholding stage, we compute the projection similarity between newly sampled latent vectors and this encoded representation of the undesired feature. This simi- larity calculation serves as the basis for determining the degree of alignment between the latent vectors and the undesired feature. Utilizing this similarity metric, we judiciously apply a thresholding mechanism to separate and exclude outputs that contain the undesired feature. Algorithm 1 FAST: Feature Aware Similarity Thresholding Required: positive samples({yp i } s i=1), negative samples({yn i } s i=1), test samples({yi} t i=1), LPF(π(.)), URF(g(.)), Initialize: i ← 1, Zp ← {}, Zn ← {} while i ≤ s do zp i ← π(yp i ), zn i ← π(yn i ) Zp.append(zp i ), Zn.append(zn i ) end while zundesired ← g(Zp, Zn) Tth ← 1 2s ∑s i=1 [sim(zn i , zundesired) + sim(zp i , zundesired)] Initialize: j ← 1, Iout ← {} while j ≤ t do if sim(zj, zundesired) ≥ Tth then Bzundesired(zj) ← 0 else Bzundesired(zj) ← 1, Iout.append(zj) end if end while Output: Iout B. Filtering Mechanism 1) Undesired Feature Representation stage: In this partic- ular phase of our methodology, the central objective is to uncover a comprehensive and distinct representation of the undesired feature. This representation should exhibit a clear presence in the negative images while being absent in the positive images. This task is inherently challenging due to the intricate and often entangled relationships between various semantic features present within the images. However, as previously studied [35], [36], [38] the latent space, where latent vectors encode critical information about the data, offers a unique and powerful capability for rich and meaningful representation. But it is important to emphasize that the quality and effectiveness of this representation fundamentally depend on the inherent characteristics (e.g. disentanglement, clustering) of the latent space itself. Consequently, the prop- erties, dimensions, and organization of the latent space play a pivotal role in determining the accuracy and utility of the representation we derive. In the context of our approach, we are provided with two sets of samples: Sn = {yn i } s i=1 and Sp = {yp i } s i=1, each containing s samples. From these sets, we extract the corresponding latent vectors, denoted as Zn = {zn i }s i=1 and Zp = {zp i }s i=1, respectively. This extraction is achieved through a Latent Projection Function (LPF), denoted as π(.) such that ∀i ∈ {1, 2, . . . , s}; zi = π(yi). Now, our goal is to represent the unique undesired feature in a concise manner. To do this, we introduce a function called the Undesired Representation Function (URF), denoted as g(Zn, Zp). This function takes as input the latent vectors Zn and Zp and produces a distinctive representation of the undesired feature, which we denote as zundesired = g(Zn, Zp). 2) Similarity Thresholding stage: In the context of our work, we draw inspiration from the clustering properties [38] 6 of latent vectors, which encode similar features, tend to be close to each other in the latent space of the generative model. This observation forms the basis for our approach, as we aim to effectively distinguish between latent vectors that correspond to desired and undesired features, ultimately allowing us to filter out the undesired outputs generated by the model. In this step, our approach involves the generation of a set of new latent vectors denoted as {zi} t i=1. These latent vectors are sampled from a latent space distribution, typically following a normal distribution. Each zi is essentially a numerical representation of a potential data point that the generative model can produce. To assess the alignment between these newly sampled latent vectors and the undesired feature, we introduce a similarity metric, denoted as sim(z, zundesired). This metric quantifies how similar a given latent vector z is to the undesired feature vector zundesired in the latent space. The similarity score is computed as follows. sim(z, zundesired) = zT · zundesired ||zundesired|| (14) With similarity scores calculated for all latent vectors concern- ing both positive samples {zp i } s i=1 (those without undesired features) and negative samples {zn i }s i=1 (those containing un- desired features), we proceed to establish a similarity threshold denoted as Tth. This threshold is calculated as below: Tth = 1 2s s∑ i=1 [sim(zn i , zundesired) + sim(zp i , zundesired)] (15) Now comes the critical decision-making step. To determine which latent vectors should be filtered out due to the presence of undesired features, we use the filtering decision function BTth(zi) as follows. BTth (zi) = { 0 when sim(zi, zundesired) ≥ Tth 1 when sim(zi, zundesired) < Tth (16) If the similarity between a latent vector zi and the undesired feature vector zundesired is greater than or equal to the calculated threshold Tth indicates that this latent vector should be filtered out, as it contains a significant component of the undesired feature. Conversely, if the similarity falls below the threshold, it signifies that this latent vector can be retained, as it does not contain the undesired feature. Now, it becomes evident that this decision is contingent upon the value of zundesired, and this value, in turn, is subject to the specific choices made for the Latent Projection Function (LPF) and the Undesired Representation Function (URF). C. Choices of LPF and URF A crucial aspect to consider in our methodology is the flexibility and adaptability of the Latent Projection Function (LPF) and the Undesired Representation Function (URF). LPF plays a vital role in mapping the data from its original space into the latent space, where different semantic features can be effectively encoded. On the other hand, URF is responsible for crafting a representation of the undesired feature within the latent space. These functions hold a pivotal role in the overall effectiveness of our approach, and selecting them appropriately is essential to ensure the method’s success. It’s worth emphasizing that there isn’t a one-size-fits-all solution; rather, the choice of these functions should be made based on the specific characteristics and demands of the problem under consideration, making FAST a unified family of methods. Here we have analyzed two different LPFs that map into two different latent spaces as follows: • Implicit Latent Space (Imp-LS): This is the original latent space, which is the domain of the original GAN (Gθinit). Here π(.) = G −1 θinit. • Inverted Latent Space (Inv-LS): This is the inverted latent space (Zinv) obtained through learning an inverter (Iθinit) via reconstruction loss such that Iθinit : X → Zinv. Here π(.) = Iθinit. Further, we take two different URFs that represent the undesired features as follows: • Mean Difference (MD): This represents the undesired features as the difference between the means of latent vectors corresponding to negative and positive examples: g(Zn, Zp) = 1 |Zn| ∑ i∈Zn zn i − 1 |Zp| ∑ i∈Zp zp i (17) • SVM Normal (Norm-SVM): This represents the undesired features as the normal vector of a hyperplane obtained after training an SVM with latent vectors from Zn and Zp: g(Zn, Zp) = SVM-Normal(Zn, Zp) (18) D. Latent Augmentation The effectiveness of the proposed filtering mechanism is dependent on the limited feedback samples provided by the user. Due to the scarcity of user feedback samples, this method is prone to overfitting to the few available samples. To mitigate the overfitting issue and enhance the effectiveness of our method, we incorporate latent sampling in the latent space. Formally, we augment the number of negative and positive features in the latent space by sampling 5000 features from their respective empirical distributions, as given below: zp aug ∼ N (¯zp, ¯Σp) zn aug ∼ N (¯zn, ¯Σn) Here, ¯zp, ¯Σp, ¯zn, ¯Σn represent the mean and covariance matrix of the positive and negative latent vectors, respectively. V. EXPERIMENTS AND RESULTS A. Datasets and Models As previously mentioned, the primary objective of filtering is to prevent the display of samples that exhibit specific undesired features. In this context, we explore two distinct unlearning settings: • Class-level filtering: For this setting, we utilize the MNIST dataset (LeCun et al., 1998), which comprises 60,000 black and white images of handwritten digits having dimensions of 28 × 28. Aiming to achieve class- level filtering, we have used pre-trained DC-GAN on 7 Fig. 2. FAST filtering mechanism: In stage-1, the undesired latent feature is identified using only a few positive and negative samples marked by the user. The positive and negative samples are projected into the latent space using the Latent Projection Function (π(.)), and subsequently, the undesired feature is retrieved via the Undesired Representation Function (g(.)). In stage-2, during the inference phase, new test samples are projected into the latent space, and the similarity of their projection with the undesired feature obtained from stage-1 is measured to filter out the negative samples. MNIST. Specifically, we focus on filtering out two digit classes: 5 and 8. • Feature-level filtering: In this scenario, we turn to the CelebA-HQ dataset (Liu et al., 2015), which contains 30,000 high-quality RGB celebrity face images with dimensions of 256×256. For this, we have taken state- of-the-art Style-GAN2 pre-trained on CelebA-HQ. Here, we target the feature-level filtering of subtle features, specifically (a) bangs and (b) hats. B. Training Details: 1) Pre-trained GAN: As specified, we utilize two pre- trained GAN architectures: DC-GAN trained on MNIST and Style-GAN2 trained on CelebA-HQ. The DC-GAN architec- ture and weights are obtained from an open-source repository 1, while Style-GAN2 architecture is sourced from another repository 2. To obtain the pre-trained GAN on CelebA- HQ, images are resized to 256 × 256 to conform to the Style-GAN2 architecture. The latent space dimensions are set to 100 × 1 for DC-GAN and 512 × 1 for Style-GAN2. GAN training employs non-saturating adversarial loss with path-regularization. Default optimizers and hyperparameters provided in the code are utilized for training. The training duration for CelebA-HQ is 3.6 × 105 epochs. 2) Pre-trained Classifier: We leverage pre-trained classi- fiers to emulate user feedback, obtaining positive and negative samples by evaluating generated samples from the pre-trained GAN through these classifiers. Additionally, these classifiers 1Source Code: Pre-trained DC-GAN trained on MNIST dataset 2Source Code: Pre-trained Style-GAN2 trained on CelebA-HQ dataset simulate human filtering, and in the evaluation metrics section, we compute FID, density, and coverage based on this classi- fier’s output. We use simple LeNet model3 for classification among different digits of MNIST dataset. The model is trained with a batch-size of 256 using Adam optimizer with a learning rate of 2 × 10−3, β1 = 0.9 and β2 = 0.999. The model is trained for a resolution of 32×32 same as the pre-trained GAN for 12 epochs. After training the classifier has an accuracy of 99.07% on the test split of the MNIST dataset. Further for feature level-filtering, we use ResNext50 model [44] for classification among different facial attributes contained in CelebA-HQ 4. The classifier is trained with a batch-size of 64 using Adamax optimizer with a learning rate of 2 × 10−3, β1 = 0.9 and β2 = 0.999. The model is trained for a resolution of 256 × 256 for 10 epochs. We also employ image augmentation techniques such as horizontal flip, image resize, and cropping to improve the performance of the classifier. The trained model exhibits a test accuracy of 91.93%. C. Baseline and Evaluation Metrics 1) Baseline: To the best of our knowledge, there exists no established baseline method for effectively filtering out undesired output samples generated by a black-box GAN. So to evaluate the effectiveness of our method, we implement a fundamental yet essential baseline strategy of using a binary classifier designed to leverage user feedback consisting of positive and negative samples. This classifier termed as Base Classifier is tailored to identify and filter out undesired output 3Source Code: Pre-trained Classifier trained on MNIST dataset 4Source Code: Pre-trained Classifier trained on CelebA-HQ dataset 8 TABLE I RECALL(↑), AUC(↑), FID (↓) DENSITY(↑) AND COVERAGE(↑) AFTER FILTERING MNIST CLASSES WITH ONLY 20 POS. AND 20 NEG. USER-FEEDBACK Methods Class-5 Class-8 Recall AUC FID Density Coverrage Recall AUC FID Density Coverage Base Classifier 0.19±0.01 0.45±0.01 3.60±0.14 0.93±0.01 0.91±0.01 0.35±0.02 0.53±0.01 1.42±0.07 0.99±0.01 0.99±0.00 Base Classifier+Data Aug. 0.10±0.00 0.50±0.00 0.80±0.03 0.93±0.00 0.99±0.00 0.10±0.00 0.50±0.00 0.55±0.07 0.99±0.00 1.01±0.00 Imp-LS+MD 0.69±0.06 0.69±0.04 2.34±0.97 0.95±0.02 0.96±0.02 0.71±0.03 0.72±0.03 1.78±0.33 0.98±0.01 0.97±0.01 Imp-LS+MD+Latent Aug. 0.69±0.06 0.69±0.04 2.30±0.94 0.96±0.02 0.96±0.01 0.71±0.03 0.73±0.03 1.79±0.36 0.99±0.01 0.97±0.01 Imp-LS+Norm-SVM 0.67±0.11 0.69±0.03 2.27±1.00 0.96±0.01 0.96±0.02 0.69±0.04 0.71±0.02 1.90±0.36 0.99±0.02 0.97±0.01 Imp-LS+Norm-SVM+Latent Aug. 0.56±0.02 0.57±0.05 3.05±0.72 0.92±0.01 0.95±0.01 0.60±0.10 0.63±0.06 2.39±0.37 0.99±0.01 0.96±0.01 Inv-LS+MD 0.11±0.02 0.74±0.04 0.65±0.95 0.93±0.02 0.99±0.02 0.01±0.01 0.61±0.08 0.64±0.08 0.99±0.01 1.10±0.00 Inv-LS+MD+Latent Aug. 0.12±0.02 0.75±0.03 0.63±0.97 0.93±0.02 0.99±0.01 0.01±0.00 0.59±0.06 0.58±0.08 0.99±0.01 0.99±0.01 Inv-LS+Norm-SVM 0.55±0.02 0.83±0.04 0.72±1.01 0.96±0.01 0.97±0.02 0.11±0.14 0.68±0.04 0.85±0.33 0.99±0.00 0.99±0.01 Inv-LS+Norm-SVM+Latent Aug. 0.30±0.02 0.70±0.04 2.22±1.02 0.93±0.01 0.94±0.02 0.12±0.14 0.72±0.05 0.48±0.03 0.99±0.01 0.99±0.01 TABLE II RECALL(↑), AUC(↑), FID (↓) DENSITY(↑) AND COVERAGE(↑) AFTER FILTERING CELEBA-HQ FEATURES WITH ONLY 20 POS. AND 20 NEG. USER-FEEDBACK Methods Class-Bangs Class-Hats Recall AUC FID Density Coverage Recall AUC FID Density Coverage Base Classifier 0.20±0.01 0.69±0.01 10.58±0.42 1±0.01 0.88±0.01 0.27±0.01 0.45±0.01 6.37±0.51 1.08±0.02 0.99±0.00 Base Classifier+Data Aug. 0.10±0.00 0.50±0.00 0.08±0.02 0.99±0.01 1±0.00 0.1±0.00 0.5±0.00 6.57±0.04 1.05±0.01 1±0.00 Imp-LS+MD 0.91±0.05 0.90±0.04 3.71±1.12 0.99±0.02 0.94±0.02 0.77±0.06 0.84±0.02 3.55±1.01 0.98±0.01 0.93±0.01 Imp-LS+MD+Latent Aug. 0.91±0.07 0.89±0.04 3.69±0.97 0.99±0.02 0.94±0.02 0.78±0.06 0.85±0.03 3.50±0.92 0.98±0.01 0.93±0.01 Imp-LS+Norm-SVM 0.93±0.06 0.92±0.02 2.54±1.00 0.99±0.01 0.96±0.02 0.82±0.04 0.86±0.02 3.81±0..93 0.95±0.01 0.93±0.01 Imp-LS+Norm-SVM+Latent Aug. 0.93±0.02 0.93±0.05 2.31±0.72 0.99±0.01 0.97±0.01 0.77±0.03 0.84±0.05 3.35±0.65 0.99±0.15 0.94±0.01 Inv-LS+MD 0.89±0.01 0.89±0.01 9.91±1.14 0.96±0.01 0.85±0.01 0.86±0.03 0.88±0.02 9.06±0.72 0.94±0.01 0.83±0.02 Inv-LS+MD+Latent Aug. 0.89±0.01 0.89±0.01 9.86±1.04 0.96±0.01 0.85±0.01 0.86±0.03 0.89±0.02 9.05±0.70 0.94±0.01 0.83±0.02 Inv-LS+Norm-SVM 0.94±0.02 0.93±0.03 3.61±0.65 1.01±0.02 0.96±0.01 0.74±0.01 0.85±0.01 5.15±0.81 0.93±0.02 0.90±0.01 Inv-LS+Norm-SVM+Latent Aug. 0.92±0.02 0.93±0.02 3.83±0.70 1.00±0.01 0.94±0.01 0.78±0.01 0.86±0.01 5.48±0.75 0.94±0.02 0.90±0.01 samples, serving as a foundational baseline in our approach. To implement this Base Classifier we have used ResNext50 model [44] pre-trained on Imagenet as the base feature ex- tractor on top of which 3-layer MLP trained with binary cross entropy loss, fine-tuned for both MNIST and CelebA-HQ datasets. Adamax optimizer is used for training, running for 200 epochs with a learning rate of 0.07. Given the limited user feedback (≤ 100 samples), we introduce data augmentation techniques including rotation, blur, perspective, and auto- augment to boost the performance of this base classifier. Each image is augmented into 64 variations during training. 2) Evaluation Metrics: To gauge the effectiveness of our proposed techniques and the baseline methods, we utilize three fundamental evaluation metrics: 1) Recall: In this task, recall is a crucial metric as it mea- sures the effectiveness of filtering out negative images generated by the model. It quantifies the proportion of negative images successfully filtered. An optimal recall value of 1 indicates that all negative images have been correctly filtered out. 2) AUC: The AUC score, derived from varying the thresh- old Tth and calculating the area under the curve of the True Positive rate (PD) vs. the False Positive rate (PF ), quantitatively assesses the filtering method’s ef- fectiveness. Thus AUC captures the effect of varying threshold. A maximum AUC score of 1 indicates optimal performance. 3) Fr´echet Inception Distance (FID), Density and Cov- erage: While the earlier mentioned metrics quantify the effectiveness of filtering, they do not assess the quality of the filtered output distribution in comparison to a distribution obtained through human-level filtering. To replicate human-level filtering, we utilize a pre- trained classifier trained on labeled training data 5. To address this aspect, we calculate the Fr´echet Inception Distance (FID) [45] between the generated samples after the FAST method and a pre-trained classifier trained on labeled data. A lower FID value signifies a closer match to the pre-trained classifier’s output distribution. Additionally, we employ Density and Coverage [46] metrics to evaluate how the output of our proposed filtering method deviates from the distribution obtained with a pre-trained (human-level) classifier. The optimal value for Coverage is 1, and higher values of Density and Coverage signify better filtering performance. D. Results Table-I and Table-II provide evaluations of different filtering methods on the MNIST and CelebA-HQ datasets, respectively. We observe that our proposed methods give better Recall and AUC values for both datasets while comparable to superior performance for other metrics. The analysis focuses on two specific class features, class- 5 and class-8 for MNIST, and subtle features of bangs and hats for CelebA-HQ, each with only 20 positive and 20 negative user feedback samples. For both MNIST and CelebA- HQ, we observe that the Base Classifier, trained on the user feedback data, exhibits moderate performance in both classes. However, incorporating data augmentation significantly im- proves accuracy, with notable enhancements in the AUC score. Methods employing Implicit-Latent Space (Imp-LS) with Mean Difference (MD) or Normal SVM (Norm-SVM) feature 5Please note that the original dataset is unavailable during the FAST filtering process. Consequently, the use of the original dataset is solely for evaluation purposes with the Pre-trained Classifier. 9 TABLE III EVALUATION OF RECALL AND AUC SCORES WITH VARYING NO OF POS AND NEGATIVE FEEDBACK No. of (Pos. + Neg.) Samples Methods MNIST Class-5 CelebA-HQ - Bangs Recall AUC Scores Recall AUC Scores (40 + 40) Imp-LS+MD 0.68±0.07 0.72±0.02 0.90±0.01 0.92±0.04 Imp-LS+MD+Latent Aug. 0.68±0.07 0.72±0.03 0.89±0.07 0.91±0.04 Imp-LS+Norm-SVM 0.65±0.06 0.71±0.04 0.95±0.05 0.95±0.02 Imp-LS+Norm-SVM+Latent Aug. 0.56±0.05 0.59±0.03 0.94±0.02 0.94±0.05 Inv-LS+MD 0.33±0.02 0.75±0.04 0.93±0.02 0.91±0.02 Inv-LS+MD+Latent Aug. 0.32±0.02 0.74±0.03 0.94±0.01 0.91±0.01 Inv-LS+Norm-SVM 0.49±0.02 0.77±0.04 0.93±0.01 0.95±0.01 Inv-LS+Norm-SVM+Latent Aug. 0.20±0.02 0.66±0.04 0.94±0.02 0.95±0.02 (60 + 60) Imp-LS+MD 0.69±0.02 0.75±0.01 0.90±0.01 0.93±0.04 Imp-LS+MD+Latent Aug. 0.68±0.02 0.75±0.01 0.91±0.06 0.92±0.03 Imp-LS+Norm-SVM 0.66±0.04 0.73±0.00 0.92±0.04 0.94±0.02 Imp-LS+Norm-SVM+Latent Aug. 0.54±0.04 0.61±0.02 0.95±0.02 0.95±0.05 Inv-LS+MD 0.39±0.02 0.76±0.04 0.90±0.02 0.91±0.04 Inv-LS+MD+Latent Aug. 0.40±0.04 0.76±0.03 0.90±0.04 0.91±0.03 Inv-LS+Norm-SVM 0.72±0.02 0.78±0.01 0.96±0.01 0.97±0.01 Inv-LS+Norm-SVM+Latent Aug. 0.32±0.02 0.81±0.01 0.97±0.02 0.97±0.02 representation-based filtering mechanism (Imp-LS+MD/Norm- SVM) demonstrate better performance than the Base Classifier in terms of Recall and AUC scores for both the datasets. The Imp-LS+MD and Imp-LS+MD+Latent-Augmentation methods demonstrate superior recall performance, surpassing the Base Classifier by approximately 3.5 times for MNIST class-5 and 2 times for MNIST class-8. Additionally, these methods achieve remarkable recall scores, outperforming the Base Classifier by almost 4.1 times for CelebA-HQ bangs and 1.7 times for hats features, respectively. The combination of Inversion-Latent Space (Inv-LS) with Normal SVM (Inv-LS+Norm-SVM) feature representation-based mechanism yields superior AUC scores, demonstrating improvements of 66.4% and 35.8% over the Base Classifier for MNIST class-5 and class-8, respectively. In terms of FID, Density, and Coverage we observe that our proposed family of methods gives superior to comparative performance with respect to the Base Classifier. These findings suggest the efficacy of the FAST family of methods. Although no single method achieves superior results across all metrics, in most cases, Inv-LS+Norm-SVM and Inv- LS+Norm-SVM+Latent Aug. perform significantly better than other methods for both Recall and AUC scores. This makes them the most desirable methods among the proposed ones. E. Ablation Study 1) Effect of user-feedback size: Table-III presents an eval- uation of recall and AUC scores, examining the efficacy of diverse methods on two distinct features—MNIST class-5 and CelebA-HQ bangs—across varying numbers of positive and negative feedback samples. Notably, the Imp-LS+MD and Imp-LS+MD+Latent Aug- mentation filtering methods consistently exhibit competitive recall and AUC scores, underscoring their robust performance across both datasets. The influence of altering the number of feedback samples becomes apparent in the results, with certain methods showcasing improved performance as the number of positive and negative samples increases. For instance, the Inv- LS+Norm-SVM method achieves a recall of 0.48 and 0.72 for MNIST class-5 with 40 and 60 samples, respectively. This observation highlights the sensitivity of the method to the feedback sample size, emphasizing the significance of considering the quantity of feedback samples when evaluating filtering methods. 2) Evaluation under no positive feedback: In our endeavor to enhance the adaptability of our approach, we extend its functionality to accommodate scenarios where users provide exclusively negative samples without any positive instances. In such cases, we employ a technique referred to as positive mining to infer positive samples. Specifically, given the user identified negative samples Sn = {yn i } s i=1, their corresponding negative latents Zn = {zn i } s i=1 are computed. The negative similarity scores set SS(zn i , ¯zn) = {sim(zn i , ¯zn) : zn i ∈ Sn, ¯zn = 1 s ∑s i=1 zn i } is defined. Now given a newly sampled latent z ∼ PZ , it is a positive latent if sim(z, ¯zn) ≪ mini SS(zn i , ¯zn). The results of positive mining are given in Fig. 3 for the MNIST dataset, using only 20 negative samples from Class-5 and Class-8. These results were generated by a pre-trained GAN after obtaining positive noises through the positive mining approach. It is evident that positive mining not only selects noise samples from other classes but also maintains diversity across these classes, demonstrating its effectiveness even when user feedback is limited to negative samples. (a) pos. mining Class-5 (b) pos. mining Class-8 Fig. 3. Results of positive mining given only 20 negative samples by the user for MNIST dataset. Samples are generated by the pre-trained GAN after obtaining the positive samples through positive mining. Table-IV offers a comprehensive evaluation summary un- der the constraint of 20 negative and 0 positive feedback instances for MNIST class-5 and CelebA-HQ bangs datasets. Leveraging positive mining, our methods, Imp-LS+MD and Imp-LS+MD+Latent Augmentation, achieve noteworthy recall values of 0.72 and 0.73 for MNIST class-5, and 0.98 and 0.99 for CelebA-HQ bangs, respectively. Remarkably, these recall values surpass the performance achieved when both positive and negative feedback samples are available. TABLE IV EVALUATION OF RECALL AND AUC SCORES WITH ONLY 20 NEGATIVE AND 0 POSITIVE FEEDBACK Methods MNIST Class-5 CelebA-HQ - Bangs Recall AUC Scores Recall AUC Scores Imp-LS+MD 0.72±0.09 0.70±0.01 0.98±0.005 0.88±0.04 Imp-LS+MD+Latent Aug. 0.73±0.11 0.72±0.03 0.99±0.007 0.89±0.04 Imp-LS+Norm-SVM 0.66±0.07 0.71±0.04 0.97±0.006 0.91±0.02 Imp-LS+Norm-SVM+Latent Aug. 0.64±0.11 0.64±0.05 0.98±0.002 0.90±0.05 Inv-LS+MD 0.26±0.02 0.67±0.02 0.96±0.004 0.89±0.02 Inv-LS+MD+Latent Aug. 0.25±0.02 0.66±0.05 0.96±0.005 0.89±0.03 Inv-LS+Norm-SVM 0.39±0.04 0.70±0.04 0.97±0.003 0.94±0.02 Inv-LS+Norm-SVM+Latent Aug. 0.28±0.02 0.69±0.03 0.98±0.002 0.93±0.05 Furthermore, Imp-LS+Norm-SVM and Imp-LS+Norm- 10 SVM+Latent Augmentation demonstrate competitive performance in terms of recall and AUC scores across both datasets, showcasing the robustness of our approach. Conversely, the Inversion-Latent Space (Inv-LS) methods exhibit lower recall values, underscoring the differential impact of various methods under the specified feedback conditions. This ability to operate effectively with only negative feedback demonstrates the versatility and practicality of our approach in real-world scenarios with diverse user interactions. VI. CONCLUSION, LIMITATIONS AND FUTURE WORK To deploy deep generative models safely and responsibly, it is crucial to eliminate outputs with undesired features. In this research, we explored algorithms to prevent generating such samples from a black-box pre-trained Generative Adversarial Network (GAN). Recognizing that current unlearning methods are inadequate due to the lack of access to the pre-trained model’s parameters, we propose a filtering methodology as an alternative solution. In our theoretical framework, we establish that filtering can be viewed as a form of weak unlearn- ing. Thus to effectively filter undesired output, we propose an algorithm named Feature Aware Similarity Thresholding (FAST), which suppresses unwanted outputs by systematically encoding the representation of undesired features in the latent space. Notably, our methodology works effectively even when the original training dataset and model parameters are hidden from the user, making it applicable to zero-shot settings. This work is a pioneering effort that highlights the intricate relationship between filtering and unlearning, offering a new perspective on the challenges posed by black-box generative models. Limitations: The primary inspiration for our method is the understanding that latent space efficiently encodes semantic features. Therefore, our goal is to represent the undesired feature within the latent space, which necessitates certain useful properties such as disentanglement between different features. Disentanglement ensures that distinct features are represented independently, allowing for precise filtering of undesired attributes. However, in many cases, the latent space may be entangled due to the intricate interplay of semantic features within images. This entanglement can lead to the representation of multiple correlated features in a combined manner, making it challenging to isolate and filter out only the undesired features. As a result, our methodology might inadvertently filter out correlated features along with the undesired features in such scenarios. Future works: Acknowledging the limitations, concerns, and further applicability of our method, we have identified key aspects to address in future work: (i) Learning disen- tangled latent space: We plan to explore the use of inver- sion mechanisms to learn a more disentangled latent space, which would reduce the effect of feature entanglement. This approach aims to improve the precision of filtering undesired features without inadvertently affecting correlated features. (ii) Application of FAST to black-box diffusion models: We aim to extend the FAST algorithm to black-box diffusion models. We hypothesize that further modifications to the FAST mechanism will be required for diffusion models because these models generate samples through multiple Langevian backward steps. This process suggests that the initial latent space may not encode semantic features effectively, necessitating additional investigation and adaptation of the FAST algorithm to ensure its applicability and effectiveness in this context. REFERENCES [1] P. Celard, E. Iglesias, J. Sorribes-Fdez, R. Romero, A. S. Vieira, and L. Borrajo, “A survey on deep learning applied to medical images: from simple artificial neural networks to generative models,” Neural Computing and Applications, vol. 35, no. 3, pp. 2291–2323, 2023. [2] G. Varoquaux and V. Cheplygina, “Machine learning for medical imag- ing: methodological failures and recommendations for the future,” NPJ digital medicine, vol. 5, no. 1, p. 48, 2022. [3] J. E. Ball, D. T. Anderson, and C. S. Chan, “Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community,” Journal of applied remote sensing, vol. 11, no. 4, pp. 042 609–042 609, 2017. [4] A. A. Adegun, S. Viriri, and J.-R. Tapamo, “Review of deep learning methods for remote sensing satellite images classification: experimental survey and comparative analysis,” Journal of Big Data, vol. 10, no. 1, p. 93, 2023. [5] S. Jia, S. Jiang, Z. Lin, N. Li, M. Xu, and S. Yu, “A survey: Deep learning for hyperspectral image classification with few labeled sam- ples,” Neurocomputing, vol. 448, pp. 179–204, 2021. [6] X. Wang, Q. Hu, Y. Cheng, and J. Ma, “Hyperspectral image super- resolution meets deep learning: A survey and perspective,” IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 8, pp. 1664–1687, 2023. [7] K. Choudhary, B. DeCost, C. Chen, A. Jain, F. Tavazza, R. Cohn, C. W. Park, A. Choudhary, A. Agrawal, S. J. Billinge et al., “Recent advances and applications of deep learning methods in materials science,” npj Computational Materials, vol. 8, no. 1, p. 59, 2022. [8] B. Yang and Y. Xu, “Applications of deep-learning approaches in horticultural research: a review,” Horticulture Research, vol. 8, 2021. [9] Z. Liu, L. Jin, J. Chen, Q. Fang, S. Ablameyko, Z. Yin, and Y. Xu, “A survey on applications of deep learning in microscopy image analysis,” Computers in biology and medicine, vol. 134, p. 104523, 2021. [10] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling, “Semi- supervised learning with deep generative models,” In Proc. of NeurIPS, 2014. [11] Z. E. Ekolle and R. Kohno, “Genco: A generative learning model for heterogeneous text classification based on collaborative partial classifi- cations,” In Proc. of MDPI, 2023. [12] D. Li, J. Yang, K. Kreis, A. Torralba, and S. Fidler, “Semantic seg- mentation with generative models: Semi-supervised learning and strong out-of-domain generalization,” In Proc. of CVPR, 2021. [13] T. Tommasi, N. Patricia, B. Caputo, and T. Tuytelaars, Advances in Computer Vision and Pattern Recognition. Springer, 2017. [14] P. Voigt and A. dem Bussche, The EU general data protection regulation (GDPR). Springer, 2017. [15] E. Goldman, “An introduction to the california consumer privacy act (ccpa),” Santa Clara Univ. Legal Studies Research Paper, 2020. [16] S. Moon, S. Cho, and D. Kim, “Feature unlearning for pre-trained gans and vaes,” arXiv Preprint, 2023. [17] R. Gandikota, J. Materzynska, J. Fiotto-Kaufman, and D. Bau, “Erasing concepts from diffusion models,” In Proc. of ICCV, 2023. [18] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of International Conference on Computer Vision (ICCV), 2015. [19] H. Xu, T. Zhu, L. Zhang, W. Zhou, and P. S. Yu, “Machine unlearning: A survey,” ACM Computing Surveys Vol. 56, No. 1, 2020. [20] T. T. Nguyen, T. T. Huynh, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H. Nguyen, “A survey of machine unlearning,” arXiv preprint arXiv:2209.02299, 2022. [21] Y. Cao and J. Yang, “Towards making systems forget with machine unlearning,” In Proc. of IEEE Symposium on Security and Privacy, 2015. [22] A. Ginart, M. Guan, G. Valiant, and J. Y. Zou, “Making ai forget you: Data deletion in machine learning,” In Proc. of NIPS, 2019. [23] J. Brophy and D. Lowd, “Machine unlearning for random forests,” In Proc. of ICML, 2021. [24] S. Neel, A. Roth, and S. Sharifi-Malvajerdi, “Descent-to-delete: Gradient-based methods for machine unlearning,” In Proc. of ALT, 2021. 11 [25] P. W. Koh and P. Liang, “Understanding black-box predictions via influence functions,” In Proc. of ICML, 2017. [26] C. Guo, T. Goldstein, A. Hannun, and L. van der Maaten, “Certified data removal from machine learning models,” In Proc. of ICML, 2020. [27] L. Graves, V. Nagisetty, and V. Ganesh, “Amnesiac machine learning,” In Proc. of AAAI, 2021. [28] A. Golatkar, A. Achille, and S. Soatto, “Eternal sunshine of the spotless net: Selective forgetting in deep networks,” In Proc. of CVPR, 2020. [29] P. Tiwary, A. Guha, S. Panda, and P. A.P, “Adapt then unlearn: Exploiting parameter space semantics for unlearning in generative adversarial networks,” arXiv Preprint, 2023. [30] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” In Proc. of NeuRIPS, 2014. [31] T. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adver- sarial networks,” In Proc. of ICML, 2017. [32] R. T. Sebastian Nowozin, Botond Cseke, “f-gan: Training generative neural samplers using variational divergence minimization,” In Proc. of NeurIPS, 2016. [33] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel, “Infogan: Interpretable representation learning by information maximizing generative adversarial nets,” In Proc. of NeurIPS, 2016. [34] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of gans for improved quality, stability, and variation,” In Proc. of ICLR, 2018. [35] T. Karras, T. Aila, and S. Laine, “A style-based generator architecture for generative adversarial networks,” In Proc. of CVPR, 2018. [36] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, “Analyzing and improving the image quality of stylegan,” In Proc. of CVPR, 2020. [37] T. Karras, M. Aittala, S. Laine, E. H¨ark¨onen, J. Hellsten, J. Lehtinen, and T. Aila, “Alias-free generative adversarial networks,” Advances in Neural Information Processing Systems, vol. 34, pp. 852–863, 2021. [38] S. Mukherjee, H. Asnani, E. Lin, and S. Kannan, “Clustergan: Latent space clustering in generative adversarial networks,” In Proc. of AAAI, 2019. [39] W. Xia, Y. Zhang, Y. Yang, J.-H. Xue, B. Zhou, and M.-H. Yang, “Gan inversion: A survey,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [40] J.-Y. Zhu, P. Kr¨ahenb¨uhl, E. Shechtman, and A. A. Efros, “Generative visual manipulation on the natural image manifold,” In Proc. of ECCV, 2016. [41] M. Huh, R. Zhang, J.-Y. Zhu, S. Paris, and A. Hertzmann, “Transforming and projecting images into class-conditional generative networks,” In Proc. of ECCV, 2020. [42] O. Tov, Y. Alaluf, Y. Nitzan, O. Patashnik, and D. Cohen-Or, “Designing an encoder for stylegan image manipulation,” ACM Transactions on Graphics, 2021. [43] T. M. Dinh, A. T. Tran, R. Nguyen, and B.-S. Hua, “Hyperinverter: Improving stylegan inversion via hypernetwork,” In Proc. of CVPR, 2022. [44] S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He, “Aggregated residual transformations for deep neural networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1492– 1500. [45] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-scale update rule converge to a local nash equilibrium,” Advances in neural information processing systems, vol. 30, 2017. [46] M. F. Naeem, S. J. Oh, Y. C. Youngjung Uh, and J. Yoo, “Reliable fidelity and diversity metrics for generative models,” In Proc. of ICML, 2020.","libVersion":"0.3.2","langs":""}