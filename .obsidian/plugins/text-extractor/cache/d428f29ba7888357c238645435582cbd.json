{"path":"Pasted image 20241018094713.png","text":"Ablation Pairwise Winning Score 1 5. Data Selection Budget 5% 10% 15% In Strategy: Random 0.936 0.968 0.977 th Diversity 0.927 0.977 0.982 se Perplexity 0.261 0.569 0.610 ps Filter: GPT-2-large 1.165 1.046 1.193 ci GPT-2-XL 1.064 1.165 1.128 G GPT-NEO 1.096 1.197 1.156 sa LLaMA2-7B 1.303 1.330 1.294 m Superfilter (IFD, GPT-2) 1.133 1.101 1.193 el Table 3: Ablation study of data selection strategies ;n and filter models on finetuning LLaMA2-7B using the a Alpaca dataset. The pairwise winning score compares (2 each finetuned model with the full-data finetuned model ~ di and computes (Num(Win)â€”Num(Lose))/Num(All) +1. cu All the comparisons are performed by GPT-4 on the pr WizardLM test set. fo","libVersion":"0.3.2","langs":"eng"}