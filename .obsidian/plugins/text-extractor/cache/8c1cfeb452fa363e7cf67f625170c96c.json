{"path":"GenAIUnleaning/General_techniques/Machine Unlearning for Image-to-Image Generative Models.pdf","text":"Published as a conference paper at ICLR 2024 MACHINE UNLEARNING FOR IMAGE-TO-IMAGE GENERATIVE MODELS Guihong Li 1∗, Hsiang Hsu 2, Chun-Fu (Richard) Chen 2, Radu Marculescu 1 1The University of Texas at Austin, USA 2Global Technology Applied Research, JPMorgan Chase, USA {lgh,radum}@utexas.edu {hsiang.hsu,richard.cf.chen}@jpmchase.com ABSTRACT Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classifica- tion models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a uni- fying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, under- pinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet- 1K and Places-365, further show that our algorithm does not rely on the availabil- ity of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empir- ical explorations of machine unlearning specifically tailored for image-to-image generative models. Our code is available at https://github.com/jpmorganchase/l2l- generator-unlearning. 1 INTRODUCTION The prevalence of machine learning research and applications has sparked awareness among users, entrepreneurs, and governments, leading to new legislation1 to protect data ownership, privacy, and copyrights (Schuhmann et al., 2022; Bubeck et al., 2023; Lukas et al., 2023; Bommasani et al., 2021). At the forefront of these legislative efforts is the “Right to be Forgotten”, a fundamental human right that empowers individuals to request the removal of their information from online services. However, directly erasing data from databases is not enough, as it may already be ingrained in machine learning models, notably deep neural networks (DNNs), which can memorize training data effectively (Wu et al., 2017; Kuppa et al., 2021; Carlini et al., 2023). Yet another straightforward solution is to re-train DNNs from scratch on a new training dataset without the unwanted data—a resource-expensive procedure (Dhariwal & Nichol, 2021) that can not reflect users’ requests in a timely manner. In response to various legal requirements and user requests, a novel approach known as machine un- learning has been proposed (Nguyen et al., 2022). This technique allows a model, which has been trained with potentially sensitive samples referred to as “forget samples”, to selectively remove these samples without the necessity of retraining the model from scratch. Meanwhile, machine unlearning aims to minimize any adverse effects on the performance of the remaining data, termed “retain sam- ples”. Recent unlearning algorithms have been developed, some incorporating specialized training procedures to facilitate the unlearning process (Bourtoule et al., 2021), while others adjust model weights through fine-tuning (Tarun et al., 2023a). However, these approaches primarily address un- learning in classification problems. On the other hand, generative models, which have demonstrated ∗Work done during internship at JPMorgan Chase Bank, N.A. 1Enactions include the General Data Protection Regulation (GDPR) by the European Union (Parliament & Council, 2016), the White House AI Bill Congress (2022b), and others (Congress, 2022a; Parliament, 2019). 1 Published as a conference paper at ICLR 2024 DiffusionModels VQ-GAN DiffusionModels VQ-GANGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (a) Retain Set DiffusionModels VQ-GAN DiffusionModels VQ-GANGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (b) Forget Set Figure 1: Our machine unlearning framework is applicable to various types of I2I generative models, including the diffusion models (Saharia et al., 2022a), VQ-GAN (Li et al., 2023) and MAE (He et al., 2022) (cf. Section 4). The images in the retain set remain almost (up to a slight difference due to the perplexity of generative models) unaffected before and after unlearning. Conversely, the images in the forget set are nearly noise after unlearning, as designed. superior data memorization capabilities compared to classification models (Tirumala et al., 2022; Somepalli et al., 2023), excel at regenerating training samples (Kuppa et al., 2021; Carlini et al., 2023). Therefore, the challenge of ensuring effective unlearning for generative models has become increasingly important and pressing. In this paper, our emphasis lies on a particular category of generative model architectures known as Image-to-Image (I2I) generative models (Yang et al., 2022). This selection offers a twofold advantage: First, it paves the way for a groundbreaking approach to quantify the efficacy of machine unlearning for generative models—a research direction hitherto uncharted in existing literature. Informally speaking, we define a generative model as having “truly unlearned” an image when it is unable to faithfully reconstruct the original image when provided with only partial information (see Figure 1 for an illustrative example where the partial information involves center cropping 2). Second, I2I generative models encompass all major branches in the field of vision generative models, including Masked Autoencoder (MAE) (He et al., 2022), Vector Quantized Generative Adversarial Networks (VQ-GAN) (Li et al., 2023), and the more recent diffusion probabilistic models (Ho et al., 2020). Based on this novel definition to quantify unlearning, our contributions can be summarized as follows: • We formulate a machine unlearning framework for I2I generative models that is applicable to MAE, VQ-GAN and diffusion models. This formulation, in essence, is an unbounded optimization problem. We provide theoretical derivations that guarantee the unique opti- mality of its bounded counterpart, and design an algorithm for the efficient computation. • We conduct extensive evaluations of our algorithm on various I2I generative models, in- cluding MAE, VQ-GAN and diffusion models. Empirical results on two large scale datasets, ImageNet-1K (Deng et al., 2009) and Places-365 (Zhou et al., 2017), show that our framework exhibits negligible performance degradation on retain sets, meanwhile ef- fectively eliminating the information in forget sets. • We further illustrate that the availability of the exact retain set is not necessary in our framework—the efficacy of our algorithm remains intact even without any samples from the exact retain set. 2For the precise definition, see Section 3. 2 Published as a conference paper at ICLR 2024 To the best of our knowledge, this work is the first that systemically, theoretically, empirically ex- plore the machine unlearning specifically targeting for I2I generative models. Proofs, details on experimental setups and training, and additional results are included in the Appendix. 2 RELATED WORK I2I generative models. Many computer vision tasks can be formulated as I2I generation pro- cesses, such as image super-resolution (Bulat et al., 2018), style transfer (Zhu et al., 2017), image extension (Chang et al., 2022) and inpainting (Krishnan et al., 2019). Different type of I2I generative models utilize diverse training and optimization strategies to minimize the discrepancy between their generated images and the ground truth images. The broadly used Generative Adversarial Networks (GANs) are trained by reducing a discriminator’s accuracy in determining whether a generated im- age is real or synthetic (Goodfellow et al., 2014; Karras et al., 2019; Chen et al., 2016; Karras et al., 2020). However, stabilizing the training of GANs is a well-known challenge (Arjovsky et al., 2017; Gulrajani et al., 2017; Brock et al., 2019). In contrast, diffusion models address the stability issue by utilizing a multi-step generation strategy and are optimized by minimizing the Kullback–Leibler (KL) divergence between the distributions of the generated and ground truth images (Ho et al., 2020; Song & Ermon, 2020; Hoogeboom et al., 2022; Salimans & Ho, 2022). Diffusion models can gen- erate higher-fidelity images than GANs but require much longer generation time (Saharia et al., 2022b; Rombach et al., 2022; Zhang & Agrawala, 2023). Recently, Masked Autoencoder (MAE) has been proposed as a multi-purpose model for both image generation and classification (He et al., 2022; Feichtenhofer et al., 2022; Tong et al., 2022). Typically, MAE is optimized by minimizing the MSE loss between the generated and ground truth images. In this paper, our goal is to design a universal approach that is capable of conducting unlearning across diverse I2I model types with different optimization techniques. Machine unlearning. Machine unlearning allows a trained model to selectively remove some unwanted samples (“forget set”) while minimizing any adverse effects on the performance of the remaining data (“retain set”) and without retraining the model from scratch (Xu et al., 2023). As the pioneering work on unlearning, SISA facilitates the unlearning of specific samples by retraining the model checkpoints that were initially trained with these ”forget” samples (Bourtoule et al., 2021). However, SISA needs to re-train all these models from scratch, if the forget samples are distributed across all shards. To address this problem, several methods manipulate the trained model weights directly. Some works compute the Neural Tangent Kernel (NTK) to modify model weights, but the computation of the Hessian matrix in NTK’s calculation is numerically unstable and not scalable for models with many parameters (Golatkar et al., 2020a;b). Graves et al. (2021) requires the storage of the gradient for each parameter of every training step when training the original models. This approach is not scalable given the extremely large training set and the enormous model size for the latest image generative models. Other methods improve the efficiency by maximizing loss on the forget set or re-assigning incorrect labels but typically they are only applicable to classification tasks. (Neel et al., 2021; Tarun et al., 2023b; Chourasia & Shah, 2023; Kurmanji et al., 2023; Chen et al., 2023). There are also some approaches focusing on other perspectives of unlearning instead of designing new unlearning algorithms. For example, Chundawat et al. (2023) focuses on the data access issues of existing unlearning algorithms and suggests using the images generated by the original model as the alternative for the original training set. Besides, Jia et al. (2023) shows that that pruning the original model before unlearning can improve the overall performance of many existing unlearning algorithms. Previous unlearning approaches primarily focus on classification tasks, but there are emerging efforts on generative models. For instance, several methods maximize training loss on the forget set, but are validated only on tiny datasets, like MNIST (Bae et al., 2023; Sun et al., 2023). Other works focus on unlearning specific features (e.g., eye color, hairstyle) from generated images, but are only verified under small-scale setups and lack comprehensive analysis (Kong & Chaudhuri, 2023; Moon et al., 2023). Besides, these methods typically manipulate the entire model, thus requiring extensive computation capacity due to the growing complexity and size of generative models. Moreover, none of them addresses I2I generative tasks. This motivates us to explore the efficient unlearning algorithms for I2I generative models in large-scale setups. 3 Published as a conference paper at ICLR 2024 3 PROBLEM FORMULATION AND PROPOSED APPROACH In this work, we primarily address the machine unlearning for I2I generative models that reconstruct images from incomplete or partial inputs. Typically, I2I generative models adopt an encoder-decoder network architecture, comprising two components, namely, (i) an encoder network Eθ that encodes an input into a representation vector and (ii) a decoder network Dϕ that decodes the representation vector into the image. Specifically, given an input x, the output for an I2I generative model hθ,ϕ is as follows: hθ,ϕ = Dϕ ◦ Eθ, hθ,ϕ (T (x)) = Dϕ (Eθ (T (x))) (1) where x is a ground truth image; T (·) is the operation to remove some information from x, e.g., center cropping and random masking; ◦ is the composition operator; θ and ϕ are the parameters for the encoder and decoder, respectively. 3.1 DEFINITION OF UNLEARNING ON I2I GENERATIVE MODELS For machine unlearning on I2I generative models, given a trained model (i.e., original model) hθ0,ϕ0 = Dϕ0 ◦ Eθ0 with parameters θ0 and ϕ0, the unlearning algorithm AF aims to obtain a target model: hθ,ϕ ≜ AF (hθ0,ϕ0) that satisfies the following properties: • On the retain set DR, hθ,ϕ generates images that have the same distribution as the original model; • On the forget set DF , hθ,ϕ generates images that have far different distribution from the original model. By using the KL-divergence (D), from a probability distribution perspective, these objectives are as follows: arg min θ,ϕ D (Phθ0,ϕ0 (T (Xr))||Phθ,ϕ(T (Xr))) , and arg max θ,ϕ D (Phθ0,ϕ0 (T (Xf ))||Phθ,ϕ(T (Xf ))) (2) where, Xr and Xf are random variables that account for the ground truth images of the retain and forget sets, respectively. By combining these two objectives, we formulate our optimization goal as follows: arg min θ,ϕ {D ( Phθ0,ϕ0 (T (Xr))||Phθ,ϕ(T (Xr))) − αD (Phθ0,ϕ0 (T (Xf ))||Phθ,ϕ(T (Xf ))) } (3) where α is a positive coefficient to control the trade-off between the retain and forget sets. Multiple previous works assume a trained I2I generative model can do an almost perfect generation on both of the retain and forget sets (Wallace et al., 2023; Song et al., 2023; Xia et al., 2023; Kingma & Welling, 2019); that is, hθ0,ϕ0 (T (X)) ≈ X. Therefore, Eq. (3) can be rewritten as: arg min θ,ϕ { D ( PXr ||P ˆXr ) − αD (PXf ||P ˆXf ) }, ˆXr = hθ,ϕ (T (Xr)) , ˆXf = hθ,ϕ (T (Xf )) (4) where PXr and P ˆXr represent the distribution of ground truth images and generated images in the retain set; PXf and P ˆXf represent the distribution of ground truth images and generated images in the forget set. 3.2 OPTIMIZATION ON RETAIN AND FORGET SETS Clearly, for the first term in Eq. (4), a perfect unlearned model has no performance degradation on the retains set. In other words, the generated images share the distribution as ground truth images, i.e., P ˆXr = PXr . This way, the value of D (PXr ||P ˆXr ) is 0. Next, we discuss the optimization for the forget set. 4 Published as a conference paper at ICLR 2024 To minimize the value for the objective functions in Eq. (4), we need to maximize KL divergence be- tween PXf and P ˆXf . However, there are infinitely many probability distributions that have infinity KL divergence with PXf (see Appendix A for more details). The ∞ value for the KL divergence will lead to unbounded loss values thus hurting the stability of the unlearning process. To address this problem, we derive an optimal and bounded KL divergence for the forget set under some reasonable constraints: Lemma 1 Given the distribution of the forget samples PXf with zero-mean and covariance matrix Σ, consider another signal P ˆXf which shares the same mean and covariance matrix. The maximal KL-divergence between PXf and P ˆXf is achieved when P ˆXf = N (0, Σ) (Cover & Thomas, 2012); that is: D (PXf ||P ˆXf ) ≤ D (PXf ||N (0, Σ)) (5) We note that making P ˆXf share the same mean and covariance matrix as PXf can preserve the original training set statistical patterns. Consequently, it becomes statistically challenging to decide whether a generated image belongs to the forget set, thereby protecting data privacy. Moreover, the assumption of zero mean is natural since typically images are normalized by subtracting the mean value inside neural networks. We provide some empirical analysis to demonstrate the benefits of Gaussian distribution (cf. Section 4.4). Essentially, Lemma 1 indicates that the maximal KL divergence w.r.t PXf is achieved when the generated images P ˆXf follow the Gaussian distribution N (0, Σ). Hence, we can directly optimize P ˆXf towards this optimal solution by minimizing their KL-Divergence; that is: arg min θ,ϕ { D (PXr ||P ˆXr ) + αD (N (0, Σ) ||P ˆXf ) }, ˆXr = hθ,ϕ (T (Xr)) , ˆXf = hθ,ϕ (T (Xf )) (6) This way, we avoid the problem of the infinity value of KL-divergence in Eq. (4). We note that, for previous unlearning approaches for classification tasks, it’s natural and straightforward to directly compute the KL-divergence for final outputs since the outputs are exactly single-variable discrete distributions after the SoftMax function (Zhang et al., 2023a;b; Kurmanji et al., 2023). Neverthe- less, for image generation tasks, directly computing the KL divergence between high-dimensional output images is typically intractable, excluding the special case of diffusion models. To address this problem, we next convert the KL divergence into a more efficient L2 loss which is generally applicable to diverse I2I generative models. 3.3 PROPOSED APPROACH Directly connecting the KL-Divergence with the L2 loss is difficult. Instead, we use Mutual Infor- mation (MI) as a bridge to help with the analysis. As indicated in Eq. (6), we reach the minimal objective value when P ˆXr = PXr and P ˆXf = N (0, Σ). This optimum can also be achieved by maximizing the mutual information (I) between Xr and ˆXr (or between n ∼ N (0, Σ) and ˆXf ); that is: arg max θ,ϕ { I (Xr; ˆXr)+αI (n; ˆXf ) }, n ∼ N (0, Σ), ˆXr = hθ,ϕ (T (Xr)) , ˆXf = hθ,ϕ (T (Xf )) (7) We next link the MI with a more tractable L2 loss in the representation space. Theorem 1 Suppose the original model can do a perfect generation, i.e., hθ0,ϕ0 (T (X)) = X. Assume the target model hθ,ϕ uses the same decoder as the original model hθ0,ϕ0 (i.e., Dϕ = Dϕ0), and the output of the encoders is normalized, i.e., ∥Eθ(x)∥2 = ∥Eθ0 (x)∥2 = 1. On the retain set, minimizing the L2 loss between the output of the target model encoder Eθ and the original model encoder Eθ0 will increase the lower bound of mutual information: I(Xr; ˆXr) ≥ log (K) − E   K∑ i=1 1 K log  e ϵ2 i 2 −1 K∑ j=1 eϵj +Rij     (8) 5 Published as a conference paper at ICLR 2024ForgetsetRetainsetOriginalmodel(Fixed)Targetmodel(Learnable)Encoder 𝐸𝐷𝐷Encoder 𝐸𝑥𝑥𝑛∼𝒩0,𝛴𝐿𝐸𝒯𝑥𝐸𝒯𝑛𝐿𝐸𝒯𝑥𝐸𝒯𝑥 Figure 2: Overview of our approach. On DF , we minimize the L2-loss between embedding vectors of the forget samples xf and embedding vectors of Gaussian noise n. On DR, we minimize the L2- loss between the same image embedding vectors generated by target model encoder and the original model encoder. where ϵi = ∥Eθ (T (xri)) − Eθ0 (T (xri)) ∥2 and Rij = Eθ0 (T (xri))T Eθ0(T (xrj )). xri are the data samples in the retain set. For the forget set, we have: I(n; ˆXf ) ≥ log (K) − E   K∑ i=1 1 K log  e δ2 i 2 −1 K∑ j=1 eδj +Fij     , n ∼ N (0, Σ) (9) where δi = ∥Eθ (T (xfi)) − Eθ0 (T (ni)) ∥2 and Fij = Eθ0(T (ni))T Eθ0(T (nj)). xf i are the data samples in the forget set and ni ∼ N (0, Σ). We remark that both Rij and Fij are determined by the original encoder Eθ0, thus are fixed values. As illustrated in Theorem 1, by directly reducing the L2 loss (δi and ϵi) between the target encoder and the original encoder, the Mutual Information (MI) increases, concurrently reducing the KL divergence between PXr and P ˆXf (or between P ˆXf and N ). Hence, in our approach, we sidestep the intractability of computing MI or KL divergence by directly minimizing the values of δi and ϵi. Based on these insights, we next introduce our approach. Efficient Unlearning Approach. Finally, as shown in Fig. 2, we propose our efficient unlearning approach for I2I generative models as follows: AF (hθ0,ϕ0 ) ≜ arg min θ E xri ,xfj ,n {∣ ∣Eθ ( T (xri )) − Eθ0 ( T (xri ) ) ∣ ∣ 2 + α ∣ ∣Eθ (T (xfj ) ) − Eθ0 (T (n)) ∣ ∣ 2 } xri ∈ DR, xfj ∈ DF , n ∼ N (0, Σ) (10) We provide the details of our unlearning algorithm and corresponding pseudo code in Appendix C.4. We note that our proposed approach only involves the encoders. Hence, it’s more efficient than ma- nipulating the entire model. Moreover, our approach is generally applicable to various I2I generative models with the encoder-decoder architecture (including the diffusion model, VQ-GAN, or MAE), although they typically use different optimization methods. We illustrate this generalizability in the experiments part. 4 EXPERIMENTAL RESULTS We evaluate our proposed approach on three mainstream I2I generative models: (i) diffusion mod- els (Saharia et al., 2022a), (ii) VQ-GAN (Li et al., 2023), and (iii) MAE (He et al., 2022). 4.1 EXPERIMENTAL SETUP Dataset&Task. We verify our method on two mainstream large-scale datasets: (i) ImageNet-1k. Out of total 1K classes, we randomly select 100 classes as DR and another 100 classes as DF . (ii) 6 Published as a conference paper at ICLR 2024 Table 1: Results of cropping 8 × 8 patches at the center of the image, where each patch is 16 × 16 pixels. ‘↑’ means higher is better and ‘↓’ means lower is better. R and F account for the retain set and forget set, respectively.‘Proxy DR’ means that we use the images from other classes as a substitute of the real retain set to do the unlearning (cf. Section 4.3). Diffusion Models VQ-GAN MAE FID IS CLIP FID IS CLIP FID IS CLIP R↓ F ↑ R↑ F ↓ R↑ F ↓ R↓ F ↑ R↑ F ↓ R↑ F ↓ R↓ F ↑ R↑ F ↓ R↑ F ↓ Original model 12.2 14.6 19.3 23.1 0.88 0.89 14.4 14.4 19.4 20.6 0.75 0.77 56.7 84.1 23.0 17.4 0.73 0.71 MAX LOSS 34.1 45.7 12.8 17.1 0.77 0.76 16.9 115.2 17.4 11.0 0.73 0.55 75.8 112.6 19.4 15.2 0.69 0.65 NOISY LABEL 14.7 36.9 19.3 19.1 0.86 0.80 14.8 79.5 17.2 11.4 0.74 0.64 60.4 136.5 21.6 12.8 0.71 0.67 RETAIN LABEL 23.1 104.7 18.2 12.3 0.81 0.69 21.8 23.3 18.2 18.3 0.72 0.74 72.8 145.3 18.8 11.6 0.69 0.66 RANDOM ENCODER 15.3 30.6 18.7 19.4 0.86 0.81 14.7 72.8 18.6 14.1 0.74 0.64 58.1 146.4 22.3 12.8 0.72 0.67 Ours 13.4 107.9 19.4 10.3 0.87 0.69 15.0 83.4 18.3 11.6 0.74 0.60 59.9 153.0 21.8 11.0 0.72 0.67 Ours (Proxy DR) 17.9 75.5 18.2 12.3 0.83 0.74 17.6 69.7 18.6 14.0 0.73 0.63 61.1 133.8 21.0 12.3 0.72 0.68 Table 2: Results of cropping 4 × 4 patches at the center of the image, where each patch is 16 × 16 pixels. ‘↑’ means higher is better and ‘↓’ means lower is better. R and F account for the retain set and forget set, respectively. “Proxy DR” means that we use the images from other classes as a substitute of the real retain set to do the unlearning (cf. Section 4.3). Diffusion Models VQ-GAN MAE FID IS CLIP FID IS CLIP FID IS CLIP R↓ F ↑ R↑ F ↓ R↑ F ↓ R↓ F ↑ R↑ F ↓ R↑ F ↓ R↓ F ↑ R↑ F ↓ R↑ F ↓ Original model 7.8 6.0 10.3 11.2 0.93 0.96 8.4 7.8 15.1 14.2 0.84 0.85 11.4 15.8 50.8 46.6 0.87 0.87 MAX LOSS 11.9 15.4 10.0 11.0 0.88 0.93 9.2 39.9 15.2 13.1 0.83 0.72 13.3 20.2 50.8 46.0 0.86 0.83 NOISY LABEL 19.6 18.5 10.4 10.6 0.87 0.91 8.7 21.3 15.2 14.1 0.84 0.80 12.2 44.3 50.0 35.4 0.86 0.82 RETAIN LABEL 8.5 35.1 10.3 10.5 0.93 0.89 11.0 10.3 15.4 14.2 0.83 0.84 15.3 47.5 47.6 34.9 0.85 0.81 RANDOM ENCODER 15.3 11.6 10.1 11.1 0.86 0.94 8.6 19.4 15.3 14.4 0.84 0.81 11.8 43.6 50.3 36.3 0.86 0.83 Ours 8.2 39.8 10.3 10.7 0.93 0.88 8.6 22.0 15.0 14.1 0.84 0.79 12.2 45.1 49.7 34.8 0.86 0.83 Ours (Proxy DR) 11.2 29.0 10.3 10.8 0.91 0.9 8.9 20.0 15.4 14.3 0.84 0.80 12.5 39.9 49.5 36.8 0.86 0.83 Places-365. From all 365 classes, we randomly select 50 classes as DR and another 50 classes as DF . We test our method on image extension, uncropping, and reconstruction tasks. We report the results of center uncropping (i.e., inpainting) in the main paper. The results of other tasks are given in Appendix D and E.1. Baseline. We first report the performance of the original model (i.e., before unlearning) as the reference. Since our approach is the first work that does the unlearning for I2I generative models, there are no previous baselines we can directly compare against. Therefore, we implement three different unlearning approaches that were designed for other tasks, and adapt them to I2I generative models, including (i) MAX LOSS maximizes the training loss w.r.t. the ground truth images on the forget set (Halimi et al., 2022; Gandikota et al., 2023; Warnecke et al., 2023); (ii) NOISY LABEL minimizes training loss by setting the Gaussian noise as the ground truth images for the forget set (Graves et al., 2021; Gandikota et al., 2023); (iii) RETAIN LABEL minimizes training loss by setting the retain samples as the ground truth for the forget set (Kong & Chaudhuri, 2023); (iv) RANDOM ENCODER directly minimizes the L2 loss between the encoder’s output on the forget set and a Gaussian noise (Tarun et al., 2023b). For all these baselines, we use the retain samples with some regularization to avoid hurting the performance on the retain set. For more details, please check Appendix C.6. Evaluation metrics. We adopt three different types of metrics to compare our method against other baselines: (i) inception score (IS) of the generated images (Salimans et al., 2016), (ii) Fr´echet inception distance (FID) against the real images (Heusel et al., 2017) and (iii) CLIP embedding distance between the generated images and the real images (Radford et al., 2021). IS assesses the quality of the generated images alone, while FID further measure the similarity between generated and real images. On the other hand, the CLIP embedding distance measures whether or not the generated images still capture similar semantics. 4.2 PERFORMANCE ANALYSIS AND VISUALIZATION As shown in Table 1 and Table 2, compared to the original model, our approach has almost identical performance or only a slight degradation on the retain set. Meanwhile, there are significant perfor- 7 Published as a conference paper at ICLR 2024 Forget SetRetain SetGround TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs Figure 3: Results of cropping 8 × 8 patches at the center of the image on diffusion models, where each patch is 16×16 pixels. Our method has negligible-to-slight performance degradation on diverse I2I generative models and multiple generative tasks. (cf. Appendix D and E.1). 30 20 10 0 10 20 30 20 10 0 10 20 30 (a) Diffusion Models 30 20 10 0 10 20 30 30 20 10 0 10 20 (b) VQ-GAN 30 20 10 0 10 20 30 30 20 10 0 10 20 30 (c) MAE 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 Unlearn: Forget Set Unlearn: Retain Set Ground Truth: Forget Set Ground Truth: Retain Set Figure 4: T-SNE analysis of the generated images by our approach and ground truth images. After unlearning, the generated retain samples are close to or overlapping with the ground truth (orange vs. blue), while most of generated forget images diverge far from the ground truth (green vs. red). mance drops on the forget set across all these three models for all metrics. In contrast, none of these baselines generally works well. For example, RANDOM ENCODER achieves similar performance on VQ-GAN and MAE to our methods; however, it is much worse on diffusion models. Similarly, RETAIN LABEL works well for diffusion models, but cannot generalize to VQ-GAN and MAE. We also show some generated images in Fig. 3. As shown, our approach removes the information in the forget set while preserving the performance on the retain set. T-SNE analysis. To further analyze why our approach works well, we conduct the T-SNE analysis. Using our unlearned model, we generate 50 images for both the retain and forget sets. We then compute the CLIP embedding vector of these images and their corresponding ground truth images. As shown in Fig. 4, after unlearning, the CLIP embedding vector on the retain set is close to or overlapping with the ground truth images, while most of generated images on the forget set diverge far from the ground truth. These results verify that our method is generally applicable to mainstream I2I generative models and consistently achieves good results on all these models. We provide more results under various types of cropping in Appendix D and Appendix E. 4.3 ROBUSTNESS TO RETAIN SAMPLES AVAILABILITY In machine unlearning, sometimes the real retain samples are not available due to data retention policies. To address this challenge, we evaluate our approach by using other classes of images as substitute to the real retain samples. On ImageNet-1K, since we already select 200 classes for forget and retain sets, we randomly select some images from the remaining 800 classes as the “proxy retain set” used in the unlearning process. Similarly, for Places-365, we randomly select some images from the remaining 265 classes as the “proxy retain set” used in the unlearning process. We also ensure these “proxy retain sets” have the same number of images as the forget set. As shown in the last row in Table 1 and Table 2, our method works well even without the access to the real/original retain set. Compared to using the real/original retain set, there is only a slight performance drop. Hence, our approach is flexible and generally applicable without the dependency 8 Published as a conference paper at ICLR 2024 Table 3: Ablation study of α’s values. We test the performance of cropping 8 × 8 patches at the center of the image. As shown, α = 0.25 achieves a good balance between the preserving the performance on retain set, while remove the information on forget sets across these two models. VQ-GAN MAE α 0.01 0.05 0.1 0.2 0.25 0.5 1 0.01 0.05 0.1 0.2 0.25 0.5 1 FID R↓ 90.8 91.6 92.0 91.7 92.7 92.2 94.7 113.6 113.2 113.9 116.7 115.9 116.3 116.7 F ↑ 101.2 169.4 179.5 181.3 183.4 182.2 184.6 179.0 198.6 205.1 211.5 213.0 213.4 213.0 IS R↑ 12.5 12.8 12.5 12.4 12.2 12.0 12.6 13.3 13.3 13.4 13.5 13.2 13.3 12.9 F ↓ 11.5 8.4 7.8 7.9 8.1 7.9 8.0 9.3 9.0 8.5 8.0 8.0 8.1 7.9 CLIP R↑ 0.65 0.65 0.65 0.65 0.65 0.65 0.64 0.81 0.81 0.81 0.80 0.80 0.80 0.80 F ↓ 0.66 0.55 0.54 0.54 0.54 0.54 0.54 0.79 0.78 0.78 0.78 0.78 0.78 0.78 on the real retain samples. We provide the results with limited availability to the real retain samples in Appendix D.1. 4.4 ABLATION STUDY For the ablation study, we test the results of cropping patches at the center of the image under various setups, where each patch is 16 × 16 pixels. α’s value. We vary the value of α in Eq. (10) to obtain multiple models and then evaluate their performance. As shown in Table 3, when α is 0.25, our approach achieves a good balance between the forget set and the retain set. Hence, we set α = 0.25 as default value for our approach. We provide more ablation study in Appendix E. 5 CONCLUSIONS AND FINAL REMARKS In this paper, we have formulated the machine unlearning problem for I2I generative models and derived an efficient algorithm that is applicable across various I2I generative models, including diffusion models, VQ-GAN, and MAE. Our method has shown negligible performance degradation on the retain set, while effectively removing the information from the forget set, on two large-scale datasets (ImageNet-1K and Places-365). Remarkably, our approach is still effective with limited or no real retain samples. To our best knowledge, we are the first to systematically explore machine unlearning for image completion generative models. Limitations. First, our methods are mainly verified on I2I generative models. Second, our ap- proach requires the access of original/real forget samples yet sometimes they are unavailable. Be- sides, for the simplicity of evaluation, we only test our approach on some mainstream computer vision datasets. Our approach has not been verified under a more practical/useful scenarios, e.g., remove the pornographic information for I2I generative models. Future directions. We plan to explore applicability to other modality, especially for language/text generation and text-to-image generation. The dependency on the forget set is another challenge that enable flexibility in the unlearning for generative models. Finally, we also intend to develop some more practical benchmarks related to the control of generative contents and protect the data privacy and copyright. DISCLAIMER This paper was prepared for informational purposes by the Global Technology Applied Research center of JP- Morgan Chase & Co. This paper is not a product of the Research Department of JPMorgan Chase & Co. or its affiliates. Neither JPMorgan Chase & Co. nor any of its affiliates makes any explicit or implied representation or warranty and none of them accept any liability in connection with this paper, including, without limitation, with respect to the completeness, accuracy, or reliability of the information contained herein and the potential legal, compliance, tax, or accounting effects thereof. This document is not intended as investment research or investment advice, or as a recommendation, offer, or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of partic- ipating in any transaction. Guihong Li’s and Radu Marculescu’s contributions were made as part of Guihong Li’s internship at the Global Technology Applied Research center of JPMorgan Chase & Co. 9 Published as a conference paper at ICLR 2024 Ethics statement. Machine unlearning for I2I generative models can be effectively exploited to avoid generate contents related user privacy and copyright. Moreover, unlearning for I2I models can avoid generating harmful contents, such as violence or pornography. Reproducibility statement. All the datasets used in this paper are open dataset and are available to the public. Besides, our codes are primarily based on PyTorch (Paszke et al., 2019). We use several open source code base and model checkpoints to build our own approach (see Appendix C.1). Our approach can be implemented by obtaining the outputs of target model’s encoders and the original model’s encoders and then computing the L2-loss between them. We provide more implementation details in Appendix C. REFERENCES Mart´ın Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks. In Inter- national Conference on Machine Learning, pp. 214–223. PMLR, 2017. Seohui Bae, Seoyoon Kim, Hyemin Jung, and Woohyung Lim. Gradient surgery for one-shot unlearning on generative model. CoRR, abs/2307.04550, 2023. Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning, pp. 531–540. PMLR, 2018. Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 42nd IEEE Symposium on Security and Privacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021, pp. 141–159. IEEE, 2021. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. S´ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco T´ulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712, 2023. Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To learn image super-resolution, use a gan to learn how to do image degradation first. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 185–200, 2018. Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pp. 5253–5270, 2023. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 11305–11315. IEEE, 2022. Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang. Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 7766–7775. IEEE, 2023. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural In- formation Processing Systems, pp. 2172–2180, 2016. Rishav Chourasia and Neil Shah. Forget unlearning: Towards true data-deletion in machine learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 6028–6073. PMLR, 2023. 10 Published as a conference paper at ICLR 2024 Vikram S. Chundawat, Ayush K. Tarun, Murari Mandal, and Mohan S. Kankanhalli. Zero-shot machine un- learning. IEEE Trans. Inf. Forensics Secur., 18:2345–2354, 2023. Japan Congress. Act on the protection of personal information, 2022a. URL https://www.ppc.go.jp/ files/pdf/280222_amendedlaw.pdf. United States Congress. American data privacy and protection act, 2022b. URL https://www.congress. gov/bill/117th-congress/house-bill/8152. T.M. Cover and J.A. Thomas. Elements of Information Theory, chapter 12, pp. 409–413. Wiley, 2012. ISBN 9781118585771. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248–255. IEEE Computer Society, 2009. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, pp. 8780–8794, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un- terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th Interna- tional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Open- Review.net, 2021. Christoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. In Advances in Neural Information Processing Systems, volume 35, pp. 35946–35958, 2022. Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. CoRR, abs/2303.07345, 2023. Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forget- ting in deep networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 9301–9309. Computer Vision Foundation / IEEE, 2020a. Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 383–398. Springer, 2020b. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, abs/1406.2661, 2014. Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 11516–11524, 2021. Ishaan Gulrajani, Faruk Ahmed, Mart´ın Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767–5777, 2017. Anisa Halimi, Swanand Kadhe, Ambrish Rawat, and Nathalie Baracaldo. Federated unlearning: How to effi- ciently erase a client in fl? CoRR, abs/2207.05521, 2022. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 15979–15988. IEEE, 2022. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems, pp. 6626–6637, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in neural information processing systems, pp. 6840–6851, 2020. Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Sali- mans. Autoregressive diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. Model sparsification can simplify machine unlearning. CoRR, abs/2304.04934, 2023. 11 Published as a conference paper at ICLR 2024 Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 4401–4410. Computer Vision Foundation / IEEE, 2019. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 8107–8116. Computer Vision Foundation / IEEE, 2020. Diederik P. Kingma and Max Welling. An introduction to variational autoencoders. Found. Trends Mach. Learn., 12(4):307–392, 2019. Lingpeng Kong, Cyprien de Masson d’Autume, Lei Yu, Wang Ling, Zihang Dai, and Dani Yogatama. A mutual information maximization perspective of language representation learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. Zhifeng Kong and Kamalika Chaudhuri. Data redaction from conditional generative models. CoRR, abs/2305.11351, 2023. Dilip Krishnan, Piotr Teterwak, Aaron Sarna, Aaron Maschinot, Ce Liu, David Belanger, and William T. Free- man. Boundless: Generative adversarial networks for image extension. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp. 10520–10529. IEEE, 2019. Aditya Kuppa, Lamine M. Aouad, and Nhien-An Le-Khac. Towards improving privacy of synthetic datasets. In Privacy Technologies and Policy - 9th Annual Privacy Forum, APF 2021, Oslo, Norway, June 17-18, 2021, Proceedings, volume 12703 of Lecture Notes in Computer Science, pp. 106–119. Springer, 2021. Meghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou. Towards unbounded machine unlearning. CoRR, abs/2302.09880, 2023. Tianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. MAGE: masked generative encoder to unify representation learning and image synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 2142–2152. IEEE, 2023. Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella B´eguelin. An- alyzing leakage of personally identifiable information in language models. In 44th IEEE Symposium on Security and Privacy, SP 2023, San Francisco, CA, USA, May 21-25, 2023, pp. 346–363. IEEE, 2023. Saemi Moon, Seunghyuk Cho, and Dongwoo Kim. Feature unlearning for generative models via implicit feedback. CoRR, abs/2303.05699, 2023. Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide, volume 132 of Proceedings of Machine Learning Research, pp. 931–962. PMLR, 2021. Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. CoRR, abs/2209.02299, 2022. Canada Parliament. The personal information protection and electronic documents act (pipeda), 2019. URL https://laws-lois.justice.gc.ca/PDF/P-8.6.pdf. European Parliament and European Union Council. Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation) (text with eea relevance), 2016. URL https://eur-lex.europa.eu/eli/reg/2016/679/oj. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K¨opf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024–8035, 2019. Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of mutual information. In International Conference on Machine Learning, pp. 5171–5180. PMLR, 2019. 12 Published as a conference paper at ICLR 2024 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748–8763. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 10674–10685. IEEE, 2022. Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In SIGGRAPH ’22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, Vancouver, BC, Canada, August 7 - 11, 2022, pp. 15:1–15:10. ACM, 2022a. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Advances in Neural Information Processing Systems, pp. 36479–36494, 2022b. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open- Review.net, 2022. Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in Neural Information Processing Systems, pp. 2226–2234, 2016. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large- scale dataset for training next generation image-text models. In Advances in Neural Information Processing Systems, 2022. Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 6048–6058. IEEE, 2023. Bowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems with latent diffusion models via hard data consistency. CoRR, abs/2307.08123, 2023. Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Advances in Neural Information Processing Systems, pp. 12438–12448, 2020. Hui Sun, Tianqing Zhu, Wenhan Chang, and Wanlei Zhou. Generative adversarial networks unlearning. CoRR, abs/2308.09881, 2023. Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine unlearning. IEEE Transactions on Neural Networks and Learning Systems, 2023a. Ayush Kumar Tarun, Vikram Singh Chundawat, Murari Mandal, and Mohan S. Kankanhalli. Deep regression unlearning. In International Conference on Machine Learning, pp. 33921–33939. PMLR, 2023b. Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. In Advances in Neural Information Processing Systems, pp. 38274–38290, 2022. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. In Advances in Neural Information Processing Systems, volume 35, pp. 10078–10093, 2022. Bram Wallace, Akash Gokul, and Nikhil Naik. EDICT: exact diffusion inversion via coupled transforma- tions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 22532–22541. IEEE, 2023. Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. In 30th Annual Network and Distributed System Security Symposium, NDSS 2023, San Diego, California, USA, February 27 - March 3, 2023. The Internet Society, 2023. 13 Published as a conference paper at ICLR 2024 Mike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On mutual information in contrastive learning for visual representations. arXiv preprint arXiv:2005.13149, 2020. Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse. On the quantitative analysis of decoder- based generative models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017. Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN inversion: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3121–3138, 2023. Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. Machine unlearning: A survey. ACM Comput. Surv., 56(1), aug 2023. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Ming-Hsuan Yang, and Bin Cui. Diffusion models: A comprehensive survey of methods and applications. CoRR, abs/2209.00796, 2022. Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. CoRR, abs/2302.05543, 2023. Xulong Zhang, Jianzong Wang, Ning Cheng, Yifu Sun, Chuanyao Zhang, and Jing Xiao. Machine unlearning methodology based on stochastic teacher network. In Advanced Data Mining and Applications - 19th In- ternational Conference, ADMA 2023, Shenyang, China, August 21-23, 2023, Proceedings, Part V, volume 14180 of Lecture Notes in Computer Science, pp. 250–261. Springer, 2023a. Yongjing Zhang, Zhaobo Lu, Feng Zhang, Hao Wang, and Shaojing Li. Machine unlearning by reversing the continual learning. Applied Sciences, 13(16):9341, 2023b. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(6): 1452–1464, 2017. URL https://github.com/CSAILVision/places365. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, pp. 2242–2251. IEEE Computer Society, 2017. 14 Published as a conference paper at ICLR 2024 A INFINITELY MANY PROBABILITY WITH INFINITE KL-DIVERGENCE In Section 3.2, we mention that there are infinitely many probability distributions that have infinity KL diver- gence with a given distribution. We provide the proof below: Proposition 1 There are infinitely many probability distributions that have a positively infinite value of KL- divergence with any general discrete probability distribution P (X) that is defined as follows: 0 <= P (X = i) < 1, N∑ i=1 P (X = i) = 1, i ∈ [N ], N ≥ 2 Proof. Based on P (X), we build another distribution Q(X) as follows: Q(X = i) =    P (X=i) 1−P (X=j)−P (X=k)+κ , if i ̸= j and i ̸= k 0, if i = j κ 1−P (X=j)−P (X=k)+κ , if i = k j, k ∈ [N ], κ > 0 where j satisfies P (X = j) > 0 and k sastifies j ̸= k. Clearly, 0 <= Q(X = i) < 1 and ∑N i=1 Q(X = i) = 1. Therefore, Q(X) is a valid probability distribution. We next compute the KL divergence between P and Q. D(P ||Q) = N∑ i=1 P (X = i)log P (X = i) Q(X = i) = P (X = j)log ( P (X = j) Q(X = j) ) + ∑ i∈[N ],i̸=j P (X = i)log ( P (X = i) Q(X = i) ) = P (X = j)log ( P (X = j) 0 ) + ∑ i∈[N ],i̸=j P (X = i)log ( P (X = i) Q(X = i) ) = +∞ We note that κ can be any positive real number; hence, we can obtain infinitely many Q(X) by varying the value of κ. Hence, there are infinitely many Q that have a positively infinite value of KL-divergence with P . In general, one can replace the set [N ] with any discrete set. This completes our proof. □ The proof for the continuous distribution is very similar to the discrete case as shown below. Proposition 2 There are infinitely many probability distributions that have the a positively infinite value of KL- divergence with any general continuous probability distribution with the following probability density function (PDF) p(x): p(x) ≥ 0, x ∈ S, ∫ x∈S p(x)dx = 1 Proof. Based on p(x), we build another distribution with PDF q(x) as follows: q(x) = { p(x) 1−∫ x∈S1 p(x) , if x ∈ S\\(S1) 0, if x ∈ S1 , S1 ⊂ S where S1 satisfies 0 < ∫ x∈S1 p(x)dx < 1. Clearly, q(x) ≥ 0 and ∫ x∈S q(x)dx = 1. Therefore, q(x) is a valid probability density function. We next compute the KL divergence between p and q. D(p||q) = ∫ x∈S p(x)log ( p(x) q(x) ) dx = ∫ x∈S1 p(x)log ( p(x) q(x) ) dx + ∫ x∈S\\S1 p(x)log ( p(x) q(x) ) dx = ∫ x∈S1 p(x)log ( p(x) 0 ) dx + ∫ x∈S\\S1 p(x)log ( p(x) q(x) ) dx = +∞ We note that given a continuous distribution, there are infinitely many possible S1; hence, we can obtain infinitely many q(x) by using different S1. Hence, there are infinitely many q that have a positively infinite value of KL-divergence with p. This completes our proof. □ 15 Published as a conference paper at ICLR 2024 B PROOF OF THEOREM 1 Proof. Directly computing mutual information (MI) between two random variables is not feasible. Fortunately, there are some variational MI bounds that are relatively easy to compute. A popular approach is to lower bound MI with InfoNCE (Belghazi et al., 2018; Wu et al., 2020; Poole et al., 2019; Kong et al., 2020). Given two random variables X1 and X2, MI is bounded by InfoNCE is defined as follows: I(X1; X2) ≥ I N CE(X1; X2) = log (K) + E   K∑ i=1 1 K log e g1(x1i ) T g2(x2i ) ∑K j=1 eeg1(x1i )T g2 (x2j )   (B.11) where the expectation is over K independent samples from the joint distribution: Πjp(x1j , x2j ); g1 and g2 are two functions that map the random variables into representation vectors, e.g., the encoder network Eθ of I2I generative models. Following the standard practice of InfoNCE, we use the inner product of the encoder output vectors as the critic function. Therefore, the InfoNCE between the ground truth images X and reconstructed images ˆX is written as follows: I N CE(X; ˆX) = log (K) + Ex,ˆx   1 K log eEθ0 (xi) T Eθ0 (ˆxi) ∑K j=1 eEθ0 (xi)T Eθ0 (ˆxj)   (B.12) Multiple works show that the encoder of the I2I generative models can be treated as the inverse function of the decoder (Wallace et al., 2023; Song et al., 2023; Xia et al., 2023; Kingma & Welling, 2019). In other words, Eθ0 (Dϕ0 (z)) = z (B.13) Given the assumption that the target model use the same decoder as the original model, we can express X and ˆX as follows: x = Dϕ0 (Eθ0 (T (x)))) , ˆx = Dϕ0 (Eθ(T (x)))) (B.14) By using the above relationships in Eq. (B.13) and Eq. (B.14), we have Eθ0 (x)T Eθ0 (ˆx) = Eθ0 (Dϕ0 (Eθ0 (T (x)))) T Eθ0 (Dϕ0 (Eθ (T (ˆx)))) = Eθ0 (T (x1)) T Eθ (T (x2)) (B.15) Retain set Recall the assumption that the output of the encoder is normalized, i.e., ∥Eθ (x)∥2 = 1 and ∥Eθ0 (x)∥2 = 1. Therefore, we can rewrite Eq. (B.15) for the ground truth retain samples {xri } and their reconstructions {ˆxri } and as follows: Eθ0 (xri )T Eθ0 (ˆxri ) = Eθ0 (T (xri ))T Eθ (T (xri )) = 1 2 ∥Eθ0 (T (xri ))∥2 2 + 1 2 ∥Eθ (T (xri ))∥ 2 2 − 1 2 ∥Eθ0 (T (xri )) − Eθ (T (xri ))∥2 2 = 1 2 ( 2 − ∥Eθ0 (T (xri )) − Eθ (T (xri ))∥2 2) = 1 2 ( 2 − ϵ 2 i ) = 1 − ϵ 2 i 2 (B.16) where ϵi = ∥Eθ0 (T (xri )) − Eθ (T (xri ))∥2 is the L2 loss between the representation of target model en- coder Eθ and original model encoder Eθ0 . We then bound the Eθ0 (xri )T Eθ0 (ˆxrj ) as follows: Eθ0 (xri ) T Eθ0 (ˆxrj ) =Eθ0 (T (xri )) T Eθ ( T ( xrj )) =Eθ0 (T (xri )) T ( Eθ ( T ( xrj )) − Eθ0 ( T ( xrj ))) + Eθ0 (T (xri ))T Eθ0 ( T ( xrj )) =Eθ0 (T (xri ))T ( Eθ ( T ( xrj )) − Eθ0 ( T ( xrj ))) + Rij ≤ ∥Eθ0 (T (xri ))∥2 ∗ ∥ ∥Eθ ( T ( xrj )) − Eθ0 T ( xrj )∥ ∥2 + Rij =1 ∗ ϵj + Rij =ϵj + Rij (B.17) 16 Published as a conference paper at ICLR 2024 where Rij = Eθ0 (T (xri ))T Eθ0 ( T ( xrj )) and the ‘≤’ comes from the Cauchy–Schwarz inequality. The above bound is tight if {ϵi = 0, i ∈ [K]}. By combining the Eq. (B.12), Eq. (B.16), and Eq. (B.17), we can link the InfoNCE loss with the L2 loss: I N CE(Xr; ˆXr) = log (K) + E   1 K log e Eθ0 (xi) T Eθ0 (ˆxi) ∑K j=1 e Eθ0 (xi)T Eθ0 (ˆxj)   = log (K) + E   1 K log e 1− ϵ2 i 2 ∑K j=1 e Eθ0 (xi)T Eθ0 (ˆxj)   ≥ log (K) + E   1 K log e 1− ϵ2 i 2 ∑K j=1 e ϵj +Rij   = log (K) − E [ K∑ i=1 1 K log ( e ϵ2 i 2 −1 K∑ j=1 e ϵj +Rij )] (B.18) By combining Eq. (B.11), we obtain the results on the retain set: I(Xr; ˆXr) ≥ log (K) − E [ K∑ i=1 1 K log ( e ϵ2 i 2 −1 K∑ j=1 eϵj +Rij )] (B.19) Forget set The proof on the forget set is very similar to the retain set. By adapting Eq. (B.16) and Eq. (B.17) for the forget set, we first calculate the inner product of the embedding vector between n and xf : Eθ0 (ni)T Eθ0 (ˆxfi ) = Eθ0 (T (ni))T Eθ (T (xfi )) = 1 2 ( ∥Eθ0 (T (ni))∥ 2 2 + ∥Eθ (T (xfi ))∥ 2 2 − ∥Eθ0 (T (ni)) − Eθ (T (xfi ))∥ 2 2) = 1 2 ( 2 − ∥Eθ0 (T (ni)) − Eθ (T (xfi ))∥2 2) = 1 2 ( 2 − ϵ 2 i ) = 1 − δ2 i 2 (B.20) and Eθ0 (ni)T Eθ0 (ˆxfj ) =Eθ0 (T (ni)) T Eθ ( T ( xfj )) =Eθ0 (T (ni)) T ( Eθ ( T ( xfj )) − Eθ0 (T (nj))) + Eθ0 (T (ni)) T Eθ0 (T (nj)) = ∥Eθ0 (T (ni))∥2 ∗ ∥ ∥Eθ ( T ( xfj )) − Eθ0 T (nj)∥ ∥2 + Fij =1 ∗ δj + Fij =δj + Fij (B.21) where δi = ∥Eθ0 (T (ni)) − Eθ (T (xfi ))∥2 and Fij = Eθ0 (T (ni)) T Eθ0 (T (nj)). xfi are the data sam- ples in the forget set and ni ∼ N (0, Σ). Combining the above two equation with Eq. (B.12): I N CE(n; ˆXf ) = log (K) + E   1 K log e Eθ0 (ni) T Eθ0 (ˆxfi ) ∑K j=1 e Eθ0 (ni)T Eθ0 (ˆxfj )   = log (K) + E   1 K log e 1− δ2 i 2 ∑K j=1 e Eθ0 (ni)T Eθ0 (ˆxfj )   ≥ log (K) + E   1 K log e 1− δ2 i 2 ∑K j=1 e δj +Fij   = log (K) − E [ K∑ i=1 1 K log ( e δ2 i 2 −1 K∑ j=1 e δj +Fij )] (B.22) 17 Published as a conference paper at ICLR 2024 By combining the above equation with Eq. (B.11), we obtain the results for the forget set: I(n; ˆXf ) ≥ log (K) − E [ K∑ i=1 1 K log ( e δ2 i 2 −1 K∑ j=1 e δj +Fij )] (B.23) This completes our proof. □ C IMPLEMENTATION DETAILS C.1 DATASETS AND CODE BASE Diffusion Models. We verify our approach on a diffusion model that is trained on entire Places-365 training dataset (Saharia et al., 2022a; Zhou et al., 2017). We randomly select 50 classes out of 365 classes as the retain set and another 50 classes as the forget set. For each class, we select 5000 images from the Places-365 training set; we then combine them together as the training set for unlearning. By using the approach defined in Eq. (10), we obtain the target model. We then evaluate the obtained model on both forget set and retain set, with 100 images per class from the Places-365 validation set. Hence, we have 5,000 validation images for both the retains and forget sets. Since Saharia et al. did not release the code, our code is implemented based on an open source re-implementation (GitHub). Our experiments are using their provided model checkpoint on Places-365 dataset. VQ-GAN We evaluate our approach on a VQ-GAN model that is trained on entire ImageNet-1K training dataset (Li et al., 2023; Deng et al., 2009). We randomly select 100 classes out of 1000 classes as the retain set and another 100 classes as the forget set. We select 100 images per class from the training set as the training set for unlearning. We then apply our proposed approach to the original model and obtain the target model. For the main results reported in Tabel 1 and Tabel 2, we evaluate the obtained model on both forget set and retain set, with 50 images per class from the ImageNet-validation set; hence, we have 5,000 validation images for both the retains set and forget set. For the other results (in ablation study), we use a smaller version validation set, with 5 images per class from the ImageNet-validation set; hence, we have 500 validation images for both the retains set and forget set. Our code is implemented based on the offically released code of Li et al. (2023) (GitHub). Our experiments are using their provided ViT-Base model checkpoints. MAE We evaluate our approach on a MAE model and an MAE model that is trained on the entire ImageNet- 1K training dataset (He et al., 2022). The dataset setup is exactly the same as VQ-GAN (check the upper paragraph). Our code is implemented based on the offically released code of He et al. (2022) (GitHub). Our experiments are using their provided ViT-Base model checkpoints. C.2 EVALUATION METRICS Inception score (IS) For ImageNet-1K, we directly use the Inception-v3 model checkpoint from torchvision library to compute the IS. For Places-365, we used the ResNet-50 model checkpoint from the official release to compute IS (Zhou et al., 2017). Fr´echet inception distance (FID) For both ImageNet-1K and Places-365, we directly use the backbone net- work of the Inception-v3 model checkpoint from Torchvision library to compute the FID. CLIP We use CLIP with ViT-H-14 as the backbone to generate the embedding vectors of each reconstructed image from the original model and unlearned model (Radford et al., 2021). Then we compute the cosine similarity of these embedding vectors among DF and DR separately. C.3 TRAINING HYPER-PARAMETERS Patch Size For all of the models we evaluate in this paper (including diffusion models, VQ-GAN and MAE), the networks are vision transformer (Dosovitskiy et al., 2021) based architecture. We set the size of each patch as 16 × 16 pixels for all experiments. For example, cropping 8 × 8 patches means removing 128 × 128 pixels. We set α = 0.25 (cf. Eq. (10)). Diffusion Models We set the learning rate as 10 −5 with no weight decay. We use the Adam as the optimizer and conduct the unlearning for 3 epochs. We set the input resolution as 256 and set the batch size as 8 per GPU. Overall, it takes 1.5 hours on a 8 NVIDIA A10G server. We set α = 0.25 (cf. Eq. (10)). VQ-GAN&MAE We set the learning rate as 10 −4 with no weight decay. We use the AdamW as the optimizer with β = (0.9, 0.95) and conduct the unlearning for 5 epochs. We set the input resolution as 256 for VQ-GAN and 224 for MAE. We set the batch size as 16 per GPU. Overall, it takes one hour on a 4 NVIDIA A10G server. We set α = 0.25 (cf. Eq. (10)). 18 Published as a conference paper at ICLR 2024 0 200 400 600 0 200 400 600 0.0 0.5 1.0 1.5 2.0 (a) MNIST 0 500 1000 0 200 400 600 800 1000 0.5 1.0 1.5 2.0 (b) CIFAR-10 0 500 1000 0 200 400 600 800 1000 0.25 0.50 0.75 1.00 1.25 (c) CIFAR-100 Figure C.5: Covariance matrix of three commonly datasets. For CIFAR10/100, we convert the images into gray-scale images. We take the absolute value of the covariance matrix for better illus- tration. Algorithm 1 Pseudo Code of Our Unlearning Algorithm 1: Inputs: Orginal model hθ0,ϕ0 = Dϕ0 ◦ Eθ0 Retain set DR, Forget set DF Coefficient α, learning rate ζ, and #Epochs E 2: Outputs: Target model hθ,ϕ = Dϕ ◦ Eθ 3: Initialize: Copy hθ,ϕ to hθ,ϕ, i.e., hθ,ϕ ⇐ hθ0,ϕ0 4: for e = 1 to E do 5: Sample {xr} from DR 6: Sample {xf } from DF 7: Sample {n} from N (0, Σ) 8: Ensure: |{xr}| = |{xf }|, i.e., make retain samples and forget samples balanced 9: Compute loss: l = ∥Eθ(T (xr)) − Eθ0 (T (xr))∥2 + α ∥Eθ(T (xf )) − Eθ0(T (n))∥2 10: Update the parameters of the target encoder Eθ: θ ⇐ θ − ζ∇θl 11: end for C.4 OUR UNLEARNING ALGORITHM Selection of Σ in Eq. (10) To conduct our unlearning algorithm, we need to compute the Σ in Eq. (10), where Σ is the the covariance matrix for the distribution of training images. Ideally, we should use the exact Σ of the images measured on the forget set. However, there are some com- putational barriers to using the exact Σ for a high-resolution image dataset. Specifically, consider a com- monly used 256×256 resolution for image generation tasks, the distribution of the generated images will have 256 × 256 × 3 ≈ 2 × 10 5 dimensions. The size of the covariance matrix Σ for such a high-dimensional dis- tribution is around (2 × 105) 2 = 4 × 10 10, which requires around 144GB memory if stored in float precision thus is not practical. Consequently, to address the computational barrier of Σ, we use some approximated methods derived from some empirical observations on some small datasets. Specifically, we compute the exact Σ for some small- scale image dataset, including MNIST and CIFAR10/100. • Off-diagonal elements: To find some empirical inspirations, we compute the covariance matrix for three commonly dataset, MNIST, CIFAR10 and CIFAR100. As shown in Fig. C.5, most of the off- diagonal elements are very close to ‘0’. Hence, in our experiments, we set the off-diagonal elements of Σ to ‘0’ • Diagonal elements: Since the images are normalized (i.e., subtract the mean value and divided by the standard deviation), we set the diagonal elements of Σ as ‘1’, i.e., Σ(i, i) = 1. Therefore, we use the identity matrix as the approximation of exact Σ: Σ = I where I is the identity matrix. We set Σ = I as the default setup for our approach and baseline methods. In short, using I to approximate Σ is a practical approximation alternative due to the extremely high compu- tational costs of Σ for high-resolution images. For future work, given our theoretical analysis, we believe that 19 Published as a conference paper at ICLR 2024 our approach can achieve better results (lower image quality) on forget set if we can find a way to use the exact Σ. Hence, we plan to explore the potential to reduce the computation of exact Σ with low-rank approximation thus enabling the use of more accurate data-driven Σ. Pseudo code Based on our unlearning approach defined in Section 3.3, we implement the unlearning algo- rithm for I2I generative models. We provide the pseudo code in Algorithm 1. As shown, for each batch, we sample the same number of retain samples and forge samples. We then compute the loss by using Eq. (10) and update the parameters of the target model’s encoder. C.5 LOSS FUNCTION OF OUR APPROACH As shown in Eq. (10), we input the Gaussian noise to the original model as the reference for the forget set. For VQ-GAN and MAE, we can directly use Eq. (10). For diffusion models, in principle, we still replace the forget sample with Gaussian noise as the input for the original encoder. We next discuss the details for the diffusion models. We first write the loss function for the normal training of diffusion models first: ExEγ∼N (0,I)Eη ∥ ∥ ∥ ∥ ∥ ∥ ∥ Dϕ ◦ Eθ   T (x), √ ηx + √1 − η γ ︸ ︷︷ ︸ ˜x , η    − γ ∥ ∥ ∥ ∥ ∥ ∥ ∥p (C.24) where x is the ground truth image; T (x) is the transformed images, e.g., a cropped image for image uncropping tasks and and low-resolution image for image super-resolution. η is the forward process variance coefficient. For more details, please refer to the Saharia et al. (2022a). Now, we introduce the unlearning optimization function for diffusion models: arg min θ Exr ,xf ,nEγ∼N (0,I)Eη { ∥Eθ (T (xr), ˜xr, η) − Eθ0 (T (xr), ˜xr, η)∥2 +α ∥Eθ (T (xf ), ˜xf , η) − Eθ0 (T (xf ), n, η)∥2 }, xr ∈ DR, xf ∈ DF , n ∼ N (0, Σ) (C.25) where ˜xr = √ηxr + √ 1 − η γ and ˜xf = √ ηxf + √1 − η γ. Essentially, we replace the ˜xf with the Gaussian noise n as the input for the original encoder. Note that, Saharia et al. adopt an U-Net as the network architecture; thus the encoder has multiple-stage outputs. We flatten these multiple outputs into vectors and then combine them as a single representation vector . We remark the equation for diffusion models looks slightly different from VQ-GAN and MAE, but they are actually following the same principle defined in Eq. (10); that is, we replace the real forget samples with the Gaussian noise as the input for the original encoder. C.6 BASELINES We use the exact same setup as introduced in the main paper and Appendix C.3. We provide the loss function for different baseline methods below. For the baselines, the unlearning is achieved by minimizing the loss values. C.6.1 MAX LOSS MAX LOSS maximizes the training loss w.r.t. the ground truth images on the forget set. Hence, we use the original loss function for both retains set and forget set, but assign a negative coefficient for the forget set. We do not modify the loss for the retains set. • Diffusion Models arg min θ,ϕ Exr ,xf Eγ∼N (0,I)Eη { ∥Dϕ ◦ Eθ (T (xr), ˜xr, η) − γ∥2 − α ∥Dϕ ◦ Eθ (T (xf ), ˜xf , η) − γ∥2 } (C.26) where ˜xr = √ηxr + √1 − η γ and ˜xf = √ ηxf + √1 − η γ. T (·) is the transformed function, e.g., a cropped image for image uncropping tasks. 20 Published as a conference paper at ICLR 2024 • VQ-GAN arg max θ,ϕ Exr ,xf {CJ (Dϕ ◦ Eθ (T (xr)) , xr) − αCJ (Dϕ ◦ Eθ (T (xf )) , xf ) } (C.27) where J is a discriminator network used to predict whether the images are real images or generated im- ages; CJ is the cross entropy of the discriminator prediction. We also note that, during unlearning, the discriminator J is also updated in the normal training way. Please check more details in Li et al. (2023). • MAE arg min θ,ϕ Exr ,xf { ∥Dϕ ◦ Eθ (T (xr)) − xr∥2 − α ∥Dϕ ◦ Eθ (T (xf )) − xf ∥2 } (C.28) T (·) is the transformed function, e.g., a cropped image for image uncropping tasks. C.6.2 RETAIN LABEL RETAIN LABEL minimizes training loss by setting the retain samples as the ground truth for the forget set. • Diffusion Models arg min θ,ϕ Exr ,xf Eγ∼N (0,I)Eη { ∥Dϕ ◦ Eθ (T (xr), ˜xr, η) − γ∥2 + α ∥Dϕ ◦ Eθ (T (xf ), ˜xr, η) − γ∥2 } (C.29) where ˜xr = √ηxr + √ 1 − η γ. As shown, the ˜xf in Eq. (C.26) is replaced by the the retain samples ˜xr. • VQ-GAN arg max θ,ϕ Exr ,xf {CJ (Dϕ ◦ Eθ (T (xr)) , xr) + αCJ (Dϕ ◦ Eθ (T (xf )) , xr) } (C.30) where J is a discriminator network used to predict whether the images are real images or generated images; CJ is the cross entropy of the discriminator prediction. As shown, the reference xf in the second term of Eq. (C.27) is replaced by the retain sample xr. • MAE arg min θ,ϕ Exr ,xf { ∥Dϕ ◦ Eθ (T (xr)) − xr∥2 + α ∥Dϕ ◦ Eθ (T (xf )) − n∥2 } (C.31) As shown, the target ˜xf in the second term of Eq. (C.28) is replaced by the retain sample xr. C.6.3 NOISY LABEL NOISY LABEL minimizes training loss by setting the Gaussian noise as the ground truth images for the for- get set. Hence we directly use the original loss function by replacing the ground truth images with standard Gaussian noise for the forget set. We do not modify the loss for the retains set. • Diffusion Models arg min θ,ϕ Exr ,xf Eγ∼N (0,I)Eη { ∥Dϕ ◦ Eθ (T (xr), ˜xr, η) − γ∥2 + α ∥Dϕ ◦ Eθ (T (xf ), η, η) − γ∥2 } (C.32) where ˜xr = √ ηxr + √ 1 − η γ. As shown, the ˜xf in Eq. (C.26) is replaced by the Gaussian noise η. T (·) is the transformed function, e.g., a cropped image for image uncropping tasks. • VQ-GAN arg max θ,ϕ Exr ,xf { CJ (Dϕ ◦ Eθ (T (xr)) , xr) + αCJ (Dϕ ◦ Eθ (T (xf )) , n) } (C.33) where J is a discriminator network used to predict whether the images are real images or generated images; CJ is the cross entropy of the discriminator’s prediction. As shown, the reference xf in the second term of Eq. (C.27) is replaced by the Gaussian noise n. • MAE arg min θ,ϕ Exr ,xf { ∥Dϕ ◦ Eθ (T (xr)) − xr∥2 + α ∥Dϕ ◦ Eθ (T (xf )) − n∥2 } (C.34) As shown, the target ˜xf in the second term of Eq. (C.28) is replaced by the Gaussian noise n. 21 Published as a conference paper at ICLR 2024 C.6.4 RANDOM ENCODER RANDOM ENCODER minimizes the L2 loss between the Gaussian noise and the representation vector of the encoder for the forget set. • Diffusion Models arg min θ Exr ,xf Eγ∼N (0,I)Eη { ∥Eθ (T (xr), ˜xr, η) − Eθ0 (T (xr), ˜xr, η)∥2 +α ∥Eθ (T (xf ), ˜xf , η) − η∥2 } (C.35) where ˜xr = √ηxr + √1 − η γ. As shown, the target for the forget set is directly the Gaussian noise instead of its embedding vector. • VQ-GAN&MAE arg min θ Exr ,xf { ∥Eθ (T (xr)) − Eθ0 (T (xr))∥2 + α ∥Eθ (T (xf )) − n∥2 } (C.36) D SUPPLEMENTARY RESULTS 0 25 50 75 100 #Images per Class on Retain Set 80 100 120 140FIDForget Set Retain Set (a) VQ-GAN: FID 0 25 50 75 100 #Images per Class on Retain Set 9 10 11 12 13Inception Score Forget Set Retain Set (b) VQ-GAN: IS 0 25 50 75 100 #Images per Class on Retain Set 0.65 0.70 0.75 0.80CLIP Embedding Distance Forget Set Retain Set (c) VQ-GAN: CLIP 0 25 50 75 100 #Images per Class on Retain Set 150 200 250FIDForget Set Retain Set (d) MAE: FID 0 25 50 75 100 #Images per Class on Retain Set 2.5 5.0 7.5 10.0 12.5 15.0Inception Score Forget Set Retain Set (e) MAE: IS 0 25 50 75 100 #Images per Class on Retain Set 0.55 0.60 0.65 0.70 0.75CLIP Embedding Distance Forget Set Retain Set (f) MAE: CLIP Figure D.6: The performance of our approach under limited availability of the retain sample. The “100 images per class” (right side of horizontal-axis) in these plots indicate a full retains set baseline. As shown, by gradually reducing the number of images for the retain set, the performance degrada- tion is negligible. Even for the extreme case where we only one image per class, the performance degradation is also small. D.1 ROBUSTNESS TO RETAIN SAMPLES AVAILABILITY In Table 1 and Table 2, we report the results under an extreme case where there is no real retain sample available. In this section, we relax the constraints by assuming the limited availability to the real retain samples. Specifically, on ImageNet-1K, we fix the forget set by using 100 images per class for the forget set; in total, we have 10K image for the forget set. As the baseline method, we have a retain set with 100 image per class as well (in total 10K); we call this a full retain set. We next vary the number of images per class for the retain set within the range of {1, 5, 10, 20, 25, 50} and compare the performance of the unlearned models under different values. To balance the number of forget sample and retain samples, we over sample the retain samples. For example, if we have 5 image per class for the retain set, we sample these images 20 times (since 100/5 = 20). 22 Published as a conference paper at ICLR 2024 As shown in Fig. D.6, compared to the full retain set (100 images per class), our approach is still effective with a slight performance degradation. Combining the results in Table 1 and Table 2, these results show the applicability and resilience of our approach in scenarios where actual the retain samples are limited or even unavailable.Ground TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (a) Forget setGround TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (b) Retain set Figure D.7: Diffusion models: cropping 4 × 4 patches at the center of the image, where each patch is 16 × 16 pixels. As shown, our approach has almost identical performance as the original model (i.e., before unlearning); the generated images on forget set are some random noise. Overall, we outperform all other baseline methods. D.2 DIFFUSION We provide the visualization results under center crop size of 4 × 4 patches in Fig. D.7. We also provide more visualization results under center crop size of 8 × 8 patches in Fig. D.8. As shown in Fig. D.7 and Fig. D.8, out approach can generate very high quality images on retain set while filling some random noise on the forget set. As a contrast, the other baselines methods struggle on the performance drop on the retain set. RANDOM ENCODER cannot even forget the information on the forget set. 23 Published as a conference paper at ICLR 2024Ground TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (a) Forget setGround TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (b) Retain set Figure D.8: Diffusion models: cropping 8 × 8 patches at the center of the image, where each patch is 16 × 16 pixels. As shown, our approach has almost identical performance as the original model (i.e., before unlearning); the generated images on forget set are some random noise. Overall, we outperform all other baseline methods. D.3 VQ-GAN We provide the visualization results under center crop size of 4 × 4 patches in Fig. D.9. We also provide more visualization results under center crop size of 8 × 8 patches in Fig. D.10. As shown in Fig. D.9 and Fig. D.10, out approach can generate very high quality images on retain set while fill some random noise on the forget set. RETAIN LABEL cannot even forget the information on the forget set. Moreover, as shown in Fig. D.9, there are two type of fishes; one is in forget set and another one is in retain set (the fourth and fifth rows). Our approach can handle this subtle difference among difference classes. D.4 MAE Random Masking In the original paper (He et al., 2022), MAE primarily focuses on the reconstruction of randomly masked images. Hence, besides the center cropping reported in the main paper, we also report the results under random masking. As shown in Fig. D.11 and Fig. D.12, the performance on the retain set of the 24 Published as a conference paper at ICLR 2024Ground TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (a) Forget setGround TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (b) Retain set Figure D.9: VQ-GAN: cropping 4×4 patches at the center of the image, where each patch is 16×16 pixels. As shown, our approach has almost identical performance as the original model (i.e., before unlearning); the generated images on forget set are some random noise. unlearned model is almost identical as original model. In contrast, there is a significant performance drop on the forget set. E ABLATION STUDY E.1 VARYING CROPPING PATTERN AND RATIO In the main paper, we primarily show the results of uncropping/inpainting tasks. We vary the cropping patterns and use the VQ-GAN to generate the images under these different patterns. We shown the generated images with downward extension in Fig. E.13, upward extension in Fig. E.14, leftward extension in Fig. E.15, rightward extension in Fig. E.16, and outpaint in Fig. E.17, respectively. As shown in these images, our method is robust to different cropping types. As shown in Fig. E.18 and Fig. E.19, we also vary the input cropping ratio and report the FID and CLIP embedding distance. Clearly, our approach is robust to different cropping ratios. 25 Published as a conference paper at ICLR 2024Ground TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (a) Forget setGround TruthInput Original ModelMAX LOSSNOISY LABELRETAIN LABELRANDOM ENCODEROurs (b) Retain set Figure D.10: VQ-GAN: cropping 8 × 8 patches at the center of the image, where each patch is 16 × 16 pixels. As shown, our approach has almost identical performance as the original model (i.e., before unlearning); the generated images on forget set are some random noise. Noise type. We compare the performance of Gaussian Noise used in our method with uniform noise for VQ-GAN and MAE. Specifically, we set the noise in Eq. (10) as n ∼ U[−1, 1] then conduct the unlearning under the same setup. We use the obtained models to generate 500 images for both retain set and forget set. We then compute multiple metrics for different noise types. As shown in Fig. E.20, Gaussian noise achieves better forgetting in the forget set in general (i.e., higher FID, lower IS and lower CLIP on forget sets). These results empirically verify the benefits of using Gaussian Noise, which coincides with our analysis (cf. Lemma 1). E.2 CROSS VALIDATION OF GENERATED IMAGES To further verify the generated images do not have the information from forget set, we conducted the following experiment for VQ-GAN: • We first conduct the unlearning for VQ-GAN for ImageNet to obtain the unlearned model, under the same setup mentioned in Section 4. 26 Published as a conference paper at ICLR 2024 20 40 60 Input Masking Ratio(%) 100 150 200 250FID Original: Forget Set Unlearn: Forget Set Original: Retain Set Unlearn: Retain Set (a) MAE: FID 20 40 60 Input Masking Ratio(%) 5 10 15 20Inception Score Original: Forget Set Unlearn: Forget Set Original: Retain Set Unlearn: Retain Set (b) MAE: IS 20 40 60 Input Masking Ratio(%) 0.4 0.5 0.6 0.7 0.8CLIP Embedding Distance Original: Forget Set Unlearn: Forget Set Original: Retain Set Unlearn: Retain Set (c) MAE: CLIP Figure D.11: The quality of reconstructed images given random masked images by MAE under varying random masking ratio; e.g., masking 128 out of 256 patches means 50% cropping ratio. We compare the original model and the unlearned model by our approach. As shown, the performance of these two models on the retain set are almost identical, while the unlearned model has a significant performance drop on the forget set. See Fig. D.4 for some examples of generated images. Table E.4: The comparison of cross validation in Appendix E.2. We first use different models to do the inpainting tasks, i.e., reconstructing the central cropped patches (FIRST reconstructions). Given the FIRST reconstructions as input, we then use the original model (before unlearning) to do the outpainting tasks and get the SECOND reconstructions, i.e., re-generated the outer patches based on these reconstructed central patches from FIRST reconstructions. We then evaluate the quality of these SECOND reconstructed images under various metrics. Lower FID, higher IS, and higher CLIP values indicate higher image quality. FID IS CLIP DR DF DR DF DR DF Original Model 36.14 28.96 27.28 27.95 0.37 0.49 Unlearned Model 36.74 156.68 26.62 7.67 0.37 0.28 • Second, given the center-cropped input images (cropping central 8 × 8 patches), we then use this unlearned model to generate the images on both forget set and retain set (i.e., image uncrop- ping/inpainting). Here we call them FIRST generated images. • Given these FIRST generated images, we only keep the reconstructed central 8 × 8 patches (cropping the outer ones) as the new input to the original VQ-GAN (i.e., the model before unlearning) and get the newly generated images (here we call them SECOND generated images). We then evaluate the quality of SECOND generated images for both forget set and retain set and report the results. • We also conduct the above process for the original model (i.e., before unlearning) as the base- line/reference. The results are given below: As shown in Table E.4 and Figure E.21, compared to the original model, given the noise (i.e., FIRST generated images on the forget set) from the unlearned model as the input, the SECOND generated images has very low quality in terms of all these three metrics. This means that the noise indeed are not correlated with the real forget images. In contrast, the performance on the retain set is almost the same before and after unlearning. This indicates that our approach indeed preserves the knowledge on the retain set well. 27 Published as a conference paper at ICLR 2024Ground TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (a) Forget setGround TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (b) Retain set Figure D.12: Reconstruction of random masked images by MAE. For a single image, we test 25% masking ratio and 50% masking ratio. 28 Published as a conference paper at ICLR 2024Ground TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (a) Forget setGround TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (b) Retain set Figure E.13: Ablation study: Downward extension by VQ-GAN. We visualize he performance of the original model (before unlearning) and the obtained model by our approach (after unlearning). We show results on both forget set and retain set. 29 Published as a conference paper at ICLR 2024Ground TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (a) Forget setGround TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (b) Retain set Figure E.14: Ablation study: Upward extension by VQ-GAN. We visualize he performance of the original model (before unlearning) and the obtained model by our approach (after unlearning). We show results on both forget set and retain set. 30 Published as a conference paper at ICLR 2024Ground TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (a) Forget setGround TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (b) Retain set Figure E.15: Ablation study: Leftward extension by VQ-GAN. We visualize he performance of the original model (before unlearning) and the obtained model by our approach (after unlearning). We show results on both forget set and retain set. 31 Published as a conference paper at ICLR 2024Ground TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (a) Forget setGround TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (b) Retain set Figure E.16: Ablation study: Rightward extension by VQ-GAN. We visualize he performance of the original model (before unlearning) and the obtained model by our approach (after unlearning). We show results on both forget set and retain set. 32 Published as a conference paper at ICLR 2024Ground TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (a) Forget setGround TruthInput Before Unlearning OursMax_LossNoisy_LabelRetain_LabelRandom_EncoderGroundTruthInputBeforeUnlearningAfterUnlearningGroundTruthInputBeforeUnlearningAfterUnlearning (b) Retain set Figure E.17: Ablation study: Outpaint by VQ-GAN. We visualize he performance of the original model (before unlearning) and the obtained model by our approach (after unlearning). We show results on both forget set and retain set. 33 Published as a conference paper at ICLR 2024 20 40 60 Cropping Ratio(%) 75 100 125 150FID Uncropping/Inpaint Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 60 80 100 120 140FID Upward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 60 80 100FID Downward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 60 80 100FID Leftward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 60 80 100FID Rightward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 20 40 60 80 100FID Outpaint Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set Figure E.18: Ablation study: FID vs. input cropping ratios under different I2I generation tasks. 20 40 60 Cropping Ratio(%) 0.60 0.65 0.70 0.75CLIP Embedding DistanceUncropping/Inpaint Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 0.60 0.65 0.70 0.75CLIP Embedding DistanceUpward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 0.65 0.70 0.75CLIP Embedding DistanceDownward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 0.60 0.65 0.70 0.75CLIP Embedding DistanceLeftward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 0.60 0.65 0.70 0.75CLIP Embedding DistanceRightward Extension Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set 20 40 60 Cropping Ratio(%) 0.6 0.7 0.8CLIP Embedding DistanceOutpaint Orginal: Forget Set Orginal: Retain Set Unlearn: Forget Set Unlearn: Retain Set Figure E.19: Ablation study: CLIP embedding distance vs. input cropping ratios under different I2I generation tasks. 34 Published as a conference paper at ICLR 2024 0 20 40 Cropping Ratio(%) 100 150 200FID Uniform: Forget Set Gaussian: Forget Set Uniform: Retain Set Gaussian: Retain Set (a) VQ-GAN: FID 0 10 20 30 40 Cropping Ratio(%) 6 8 10 12Inception Score Uniform: Forget Set Gaussian: Forget Set Uniform: Retain Set Gaussian: Retain Set (b) VQ-GAN: IS 0 20 40 Cropping Ratio(%) 0.4 0.5 0.6 0.7CLIP Embedding Distance Uniform: Forget Set Gaussian: Forget Set Uniform: Retain Set Gaussian: Retain Set (c) VQ-GAN: CLIP 0 10 20 30 40 Cropping Ratio(%) 50 100 150 200FID Uniform: Forget Set Gaussian: Forget Set Uniform: Retain Set Gaussian: Retain Set (d) MAE: FID 0 10 20 30 40 Cropping Ratio(%) 5 10 15 20 25Inception Score Uniform: Forget Set Gaussian: Forget Set Uniform: Retain Set Gaussian: Retain Set (e) MAE: IS 0 10 20 30 40 Cropping Ratio(%) 0.7 0.8 0.9 1.0CLIP Embedding DistanceUniform: Forget Set Gaussian: Forget Set Uniform: Retain Set Gaussian: Retain Set (f) MAE: CLIP Figure E.20: Ablation study of noise type. We test the performance under varying ratios of central cropping; e.g., cropping 8 × 8 out of 256 patches means a 25% cropping ratio. For VQ-GAN and MAE, Gaussian noise achieves better forgetting in the forget set in general (i.e., it achieves higher FID, lower IS and lower CLIP on forget set).GroundTruthOriginal ModelUnlearned Model (a) Forget SetGroundTruthOriginal ModelUnlearned Model (b) Retain Set Figure E.21: Visualization of cross validation in Appendix E.2. We first use different models to do the inpainting tasks, i.e., reconstructing the central cropped patches (FIRST reconstructions). Given the FIRST reconstructions as input, we then use the original model (before unlearning) to do the outpainting tasks and get the SECOND reconstructions, i.e., re-generated the outer patches based on these reconstructed central patches from FIRST reconstructions. We then evaluate the quality of these SECOND reconstructed images under various metrics. 35","libVersion":"0.3.2","langs":""}