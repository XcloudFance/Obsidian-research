{"path":"SJTU/Data Curation - Compression - Efficiency - Filtering - Distillation/pdfs/logical reasoning.pdf","text":"Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data Jiaming Zhou 1♠ Abbas Ghaddar 1 Ge Zhang 1 Liheng Ma 2 Yaochen Hu 1 Soumyasundar Pal 1 Mark Coates 1 Bin Wang3 Yingxue Zhang 1♠ Jianye Hao 3 1 Huawei Noah’s Ark Lab, Montréal, Canada 2 McGill University and Mila - Québec AI Institute 3 Huawei Noah’s Ark Lab, Beijing, China jiaming.zhou@h-partners.com, yingxue.zhang@huawei.com Abstract Despite recent advances in training and prompting strategies for Large Language Mod- els (LLMs), these models continue to face chal- lenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limita- tions of using graph-based synthetic reason- ing data as training signals to enhance LLMs’ reasoning capabilities. Our extensive experi- ments, conducted on two established natural language reasoning tasks—inductive reason- ing and spatial reasoning—demonstrate that supervised fine-tuning (SFT) with synthetic graph-based reasoning data effectively en- hances LLMs’ reasoning performance, without compromising their effectiveness on other stan- dard evaluation benchmarks. We release our code at https://github.com/riddickzhou/ LLM-Graph-Synthetic-Reasoning 1 Introduction The reasoning capabilities of Large Language Mod- els (LLMs) (Touvron et al., 2023; Jiang et al., 2023; Dubey et al., 2024) can be greatly enhanced by post-training techniques (Ouyang et al., 2022; Zhang et al., 2023) and prompting strategies (Wei et al., 2022b; Yao et al., 2023; Madaan et al., 2023). However, even with the aforementioned techniques, the multi-hop reasoning tasks remain challenging (Touvron et al., 2023; Jiang et al., 2023; Dubey et al., 2024): LLMs struggle to reason over steps (Agrawal et al., 2024; Zhao and Zhang, 2024), and are fragile to minor perturbations (Ullman, 2023; Chen et al., 2024) in the input prompt. Recently, several works (Xu et al., 2024; Abdin et al., 2024; Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs’ reasoning ca- pacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable ♠Corresponding authors. for specific applications remains an open question. Extensive prompt engineering and quality filtering are required to guide LLMs’ generation, yet the quality of generated reasoning questions and their labels remains uncertain (Gudibande et al., 2023; Wang et al., 2023; Tan et al., 2024). A F H BD Mother Aunt BB G BE C Brother Brother Brother Sister Daughter Brother Father Brother Son Brother Brother Nephew Aunt Nephew Sister-in-law Brother-in-law Father Son Grand father Grand daughter Figure 1: Illustration of a kinship graph highlighting a reasoning chain sampled by our algorithm (green) for LLM adaptation, and an ignored simpler chain (red). Motivated by the fact that natural language rea- soning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reason- ing benchmarks (Fatemi et al., 2024; Kazemi et al., 2023; Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific post- training adaptation to improve the correctness of the generated reasoning questions and labels. In this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a rea- soning chain and then derives the answer. These processes complement each other to enable effi- cient, task-specific adaptation of LLMs for rea- soning tasks. Extensive experiments on two well- established benchmarks for inductive and spa- tial reasoning—CLUTRR (Sinha et al., 2019) and StepGame (Shi et al., 2022)—demonstrate that our 1arXiv:2409.12437v1 [cs.CL] 19 Sep 2024 framework leads to significant performance gains compared to standard prompting and training meth- ods. Our findings suggest that when carefully cu- rated, graph-based structured data can effectively enhance LLMs’ reasoning abilities on the targeted task while maintaining its performance on standard evaluation benchmarks. 2 Methodology Reasoning tasks in natural language often involve a structured representation of facts that can be ab- stracted into a graph following predefined rules (Ji et al., 2022). For example, family relationships can be systematically represented in a logical graph, where nodes denote family members and edges define their relationships, all governed by logical rules. Let G=(V, E, R) represent a relational graph with a set of nodes V, a set of edges E between the nodes, and a set of relations R expressed in first-order logic. In this graph, vertices (e.g., fam- ily members) are denoted as vi∈V, and directed edges (e.g., familial relationships) are represented as (vi, ri,j, vj) ∈ E, with relations ri,j∈R. Our goal is to generate synthetic examples of graph- structured data from such relation graphs to adapt LLMs for targeted reasoning tasks. Existing graph- based data generation methods, such as (Sinha et al., 2019), may produce large amounts of data, but this data is frequently redundant and lacks the necessary complexity (see Figure 1). Therefore, we propose a random-walk-based algorithm (Lovász, 1993) that produces a manageable yet diverse set of examples by sampling sub-graphs from G. In the remainder of this section, we describe our al- gorithm for constructing G in § 2.1, the process of generating synthetic reasoning data in § 2.2, and their deployment to enhance LLM capabilities in natural language reasoning tasks in § 2.3. 2.1 Relational Graph Construction The relational graph G is built iteratively by adding new nodes connected to existing nodes via basic relations. We start with an initial graph G0 = ({v0}, ∅, R), where v0 is a randomly sampled root node. In each successive iteration l, we grow the graph by a) searching for absent relations between the nodes in Gl−1, and b) adding new nodes with those relations, if such absent relations are found in step a). Specifically, for every node v in Gl−1 and each relation r ∈ R, we check if there is an- other node v′ in Gl−1 such that there is an edge between v and v′ with relation r. If no such node exists in Gl−1, we create a new node vr and con- nect it to node v in Gl−1 with relation r. Then, a deduction function f is used to evaluate the relation between vr and each other node v′ in Gl−1, except v. Edges (v′, r′, vr)-s are added to Gl−1 if the de- duction function f computes a relation r′ ∈ R be- tween v′ and vr. This process is repeated once for each node in Gl−1 within the l-th iteration. When this is complete, we assign the expanded Gl−1 to Gl and proceed to the next iteration. We terminate this procedure after L iterations and take GL as the relational graph G. 2.2 Sub-Graphs Sampling Given a relational graph G with |V|=n nodes, the desired walking length (e.g., number of hops) l∈Z+, and isotropic random walk probabilities π∈{0, 1}n×n (πi,j denotes the probability of tran- sition to vj from vi in one step of the random walk), we construct a reasoning chain c by conducting a random walk of length l, which starts at a random node v0∈V and avoids any repetition of visited nodes (to avoid circular reasoning in the data). The overall procedure is summarized in Algorithm 1. Algorithm 1 Reasoning Chain Sampling Input: Graph G, walk length l , transition probabilities π 1: Sample a starting vertex v0 ∼ Uniform(V) 2: Initialize walk ← [v0] 3: for i = 0 to l − 1 do 4: j ← i 5: while vj ∈ walk do 6: Sample j ∼ Categorical([πi,1, · · · πi,n]) 7: if vj /∈ walk then 8: vi+1 ← vj 9: Append vi+1 to walk 10: break 11: end if 12: end while 13: end for 14: Initialize Reasoning chain c ← [ ] 15: for i = 0 to l − 1 do 16: Retrieve and append (vi, ri,i+1, vi+1) ∈ E to c 17: end for Return: Reasoning chain c For each item c in the generated set of reasoning chains C, we apply one of the following augmenta- tion techniques to further introduce diversity and additional complexity: Permutation, where we ap- ply a permutation function σ to reorder the triples in c; Edge Noise, which involves introducing noise by adding edges that connect nodes not initially in the chain, specifically, for a vertex vi ∈ V ′ from the chain, we add an edge to a vertex vj ∈ V \\ V ′: (vi, ri,j, vj); and Edge Direction Flip where we 2 randomly flip the direction of some edges in c, al- tering the flow of reasoning. 2.3 Graph Synthetic Data for LLM Tuning The resultant set of reasoning chains C can be con- verted into LLM-supervised fine-tuning data as fol- lows. First, input-output pairs are created based on the requirements of the targeted reasoning task. For instance, a corruption function f ′ operates by removing an edge (vi, ri,j, vj) from a chain c, treat- ing this edge as the output y, while the remaining chain c′ = c \\ {(vi, ri,j, vj)} is used as the input x for tasks such as family or spatial relation predic- tions. Second, the input c′ needs to be converted into natural language textual input by applying a verbalizer, which can be either rule-based templates or more advanced techniques, such as utilizing a powerful LLM. Finally, while a standard prompt di- rects the model to answer immediately, we propose a graph-based reasoning task-specific prompting technique that mimics human cognitive processes in solving these types of tasks (Sinha et al., 2019). We propose a new prompting technique we call ETA-P (Extract then Answer) prompting, which in- structs the model to extract the relational graph before generating the answer, as opposed to stan- dard prompting STD-P, which instructs the LLM to directly generate the answer. Details of the prompt design are described in Appendix B. 3 Experimental Setup 3.1 Tasks and Datasets We apply our framework to two logical reason- ing tasks. CLUTRR (Sinha et al., 2019) supports an inductive reasoning task that requires predict- ing the relationship between two family members (e.g. Alice is the sister of Bob) based on a story snippet that describes relevant familial relations. StepGame (Shi et al., 2022) supports a spatial rea- soning task that involves determining the positional relationship between two entities (e.g., A is to the upper left of B) by navigating through a sequence of steps that describe relationships with neighbor- ing entities. Both benchmarks feature logical rea- soning problems with natural language story inputs followed by queries that require multi-hop reason- ing. The evaluation metric we report is the mod- els’ accuracy in predicting the exact relationships. Dataset statistics and synthetic data processing de- tails are listed in Appendix A.1. 3.2 Baselines We evaluate three system configurations, all using an instruction-tuned LLM as the backbone model: FS The model is tested in a few-shot setting with no additional tuning (Brown et al., 2020); SFT-S The model is supervised fine-tuned on the official train- ing set of natural language stories; and SFT-S+k The fine-tuning data consists of training story and k systemic samples generated by our framework of § 2. For all main experiments, we use Mistral-2-7B- Instruct (Jiang et al., 2023) as the backbone LLM. Additionally, we include the few-shot test results of the commercial closed-source GPT-4o (OpenAI, 2024) model. Implementation details of prompt design, fine-tuning, and inference hyperparameters can be found in Appendices A.2 and A.3. 4 Results and Analysis 4.1 Main Results Figure 2 shows the performance on CLUTRR (top) and StepGame (bottom) of the Mistral-7B LLM un- der few-shot (FS), supervised fine-tuning on stories (SFT-S), and supervised fine-tuning on both stories and synthetic data of various sizes1 (SFT-S+k) set- tings, on both the CLUTRR (top) and StepGame (bottom) datasets. In addition, it includes the few- shot performances of the GPT4-o model.2 First, we observe that on both datasets, FS sig- nificantly underperforms all SFT models across various reasoning hops, indicating that supervised fine-tuning is essential for enhancing performance on reasoning tasks in moderate-size open-source LLMs. Second, we observe that tuning with our synthetic data (SFT-S+k models) consistently yields performance gains on the CLUTRR dataset, with these improvements becoming more pro- nounced at mid (6 hop) and high (10 hop) com- plexity levels of reasoning. Interestingly, we notice that extra tuning with synthetic data was necessary, as SFT-S underperformed compared to GPT-4o in most cases. This adaptation is crucial for open LLMs in domain-specific settings to achieve per- formance comparable to the GPT-4o model. 1The plot only shows the impact of adding the minimum and maximum amounts of synthetic data for CLUTRR (+2k and +10k) and StepGame (+0.5k and +5k). 2We were unable to include the results for all 10 hops and synthetic data size variants in the plot due to visualization con- straints. Instead, we selected hops 2, 6, and 10 as representa- tives of the observed trends for low, mid, and high complexity reasoning, respectively. However, neighboring hops mostly exhibit similar result patterns, and detailed performance data are presented in Table 3 in Appendix C. 3 Hop 2 Hop 6 Hop 10 0 20 40 60 80 100Accuracy (%) CLUTRR FS SFT-S SFT-S+2k SFT-S+10k GPT4o Hop 2 Hop 6 Hop 10 0 20 40 60 80Accuracy (%) StepGame FS SFT-S SFT-S+0.5k SFT-S+5k GPT4o Figure 2: System performance on the CLUTRR (top) and StepGame (bottom) datasets for 2, 6, and 10 hop. Nevertheless, we observe that on StepGame, a more challenging task with limited SFT data, train- ing with our synthetic data leads to significant im- provements compared to using only SFT on textual stories as well as when compared to the closed- source GPT-4o model across all hops. Further- more, we notice that scaling up with our synthetic data results in more pronounced performance gaps between SFT-S and GPT-4o, compared to those observed on CLUTRR. These results suggest that synthetic data has a more significant impact on low- resource or challenging reasoning problems for task-specific adaptation of LLMs. Finally, we ob- serve a systematic degradation in the performance of all models as we progress from low to mid to high hop reasoning complexity on both tasks, in- dicating that particularly complex reasoning cases continue to pose significant challenges for LLMs. 4.2 Prompt Strategy Ablation We study the impact of our proposed prompting strategy in § 2.3 by comparing it to a model using standard prompting (see Table 4 in § C) in both few- shot (FS) and story-based supervised fine-tuning (SFT-S) settings; results are shown in Figure 3. Surprisingly, we observed that our task-specific prompt leads to performance drops under the FS setting3. Our manual inspection revealed that mod- els fail to correctly extract the graph relations from the story, resulting in error propagation into the predictions of the answer. In contrast, we notice that performing supervised fine-tuning (SFT) with our prompt leads to significant gains in most cases, compared to using the standard prompt during tun- ing. These results suggest that prompt engineering is complementary rather than a replacement for in-domain SFT for reasoning tasks. Hop 2 Hop 6 Hop 9 Hop 2 Hop 6 Hop 9 0 20 40 60 80 100Accuracy (%) CLUTRR StepGame FS STD-P FS ETA-P SFT-S STD-P SFT-S ETA-P Figure 3: Mistral-2-7B performances on CLUTRR (left) and StepGame (right) datasets under FS and SFT-S settings when using STD-P and ETA-P prompting. 4.3 LLM Benchmarks Evaluation We validate whether LLMs can retain their open domain knowledge and problem-solving abilities, thereby avoiding catastrophic forgetting, after un- dergoing task-specific adaptation. We do so by eval- uating models that have been tuned with the max- imum amount of synthetic data, specifically SFT- S+10k on CLUTRR and SFT-S+5k on StepGame, on MMLU (Hendrycks et al., 2021), GPQA (Rein et al., 2023), and GSM8K (Cobbe et al., 2021) benchmarks. Benchmark w.o. SFT CLUTRR StepGame MMLU0-shot 57.54% 58.46% 58.46% GPQA0-shot 29.46% 29.02% 29.91% GSM8K8-shot 38.82% 38.36% 39.12% Table 1: Performance of the original Mistral-2-7B, SFT- S+10k on CLUTRR, and SFT-S+5k on StepGame models across three LLM evaluation benchmarks. Results presented in Table 1 show minor varia- tions in performance and, in some cases, improve- ments—as observed for SFT-S+10k on CLUTTR during MMLU—across most benchmarks between the original model without SFT and those tuned on 3It is important to note that FS with STD-P consistently underperforms all SFT models across all settings. 4 the two reasoning tasks. These observations sug- gest that task-specific adaptation of LLMs for rea- soning tasks is feasible without sacrificing factual knowledge and generalization abilities, provided that the synthetic data for SFT is carefully curated. 5 Conclusion In this work, we propose a synthetic data augmen- tation algorithm and prompting strategy that effec- tively complement each other, enabling efficient task-specific adaptation of LLMs for reasoning tasks. We plan to expand our work to include a broader range of graph-based reasoning tasks. Limitations Potential limitations of this work include not ex- perimenting with larger open-source LLMs, such as the Mixtral 8x7B (Jiang et al., 2024) or 70B LLaMa3 (AI@Meta, 2024), as well as other recent models like Gemma (Team et al., 2024). Addi- tionally, the study did not incorporate more closed- source models such as Claude (Anthropic, 2024) to further study the impact of history enhance- ment. This is mainly due to limitations in computa- tional (open source) and financial (closed source) resources. While experiments show promise in en- hancing LLM reasoning abilities without compro- mising their generalization performance, exploring more datasets and tasks can further enhance the credibility of our results. References Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki- rat Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Palaash Agrawal, Shavak Vasania, and Cheston Tan. 2024. Exploring the limitations of graph rea- soning in large language models. arXiv preprint arXiv:2402.01805. AI@Meta. 2024. Llama 3 model card. https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean- Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal mod- els. arXiv preprint arXiv:2312.11805. Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. Xinyun Chen, Ryan A Chi, Xuezhi Wang, and Denny Zhou. 2024. Premise order matters in reasoning with large language models. In Proc. Int. Conf. Mach. Learn. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv e-prints, pages arXiv–2110. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Bahare Fatemi, Mehran Kazemi, Anton Tsitsulin, Karishma Malkan, Jinyeong Yim, John Palowitch, Sungyong Seo, Jonathan Halcrow, and Bryan Per- ozzi. 2024. Test of Time: A Benchmark for Evaluat- ing LLMs on Temporal Reasoning. arXiv preprint arXiv:2406.09170. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language under- standing. In Proc. Int. Conf. Learn. Represent. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Martti- nen, and Philip S. Yu. 2022. A Survey on Knowledge Graphs: Representation, Acquisition, and Applica- tions. IEEE Transactions on Neural Networks and Learning Systems, 33(2). Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las 5 Casas, Emma Bou Hanna, Florian Bressand, Gi- anna Lengyel, Guillaume Bour, Guillaume Lam- ple, Lélio Renard Lavaud, Lucile Saulnier, Marie- Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mix- tral of Experts. Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Suhang Wang, Yu Meng, and Jiawei Han. 2024. Graph chain-of-thought: Augmenting large language models by reasoning on graphs. In Proc. Annu. Meet. Assoc. Comput. Linguist. Mehran Kazemi, Quan Yuan, Deepti Bhatia, Najoung Kim, Xin Xu, Vaiva Imbrasaite, and Deepak Ra- machandran. 2023. Boardgameqa: A dataset for natural language reasoning with contradictory infor- mation. In Adv. Neural Inf. Process. Syst., volume 36. Ilya Loshchilov and Frank Hutter. 2017. Decou- pled weight decay regularization. arXiv preprint arXiv:1711.05101. László Lovász. 1993. Random walks on graphs. Com- binatorics, Paul erdos is eighty, 2(1-46):4. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. In Adv. Neural Inf. Process. Syst., vol- ume 36. OpenAI. 2024. Hello GPT-4o. https://openai.com/ index/hello-gpt-4o/. Accessed: 2024-05-26. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026– 8037. David Rein, Betty Li Hou, Asa Cooper Stickland, Jack- son Petty, Richard Yuanzhe Pang, Julien Dirani, Ju- lian Michael, and Samuel R Bowman. 2023. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. 2022. StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts. In Proc. AAAI Conf. Artif. Intell., pages 11321–11329. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L Hamilton. 2019. Clutrr: A diagnostic benchmark for inductive reasoning from text. In Proc. Conf. Empir. Methods Nat. Lang. Pro- cess. Int. Joint Conf. Nat. Lang. Process., pages 4506– 4515. Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large language models for data annotation: A survey. arXiv preprint arXiv:2402.13446. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro- Ros, Ambrose Slone, Amélie Héliou, Andrea Tac- chetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christo- pher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren- nan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Milli- can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bai- ley, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L. Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Kli- menko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Tech- nology. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Tomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wad- den, Kelsey MacMillan, Noah A Smith, Iz Beltagy, 6 et al. 2023. How far can camels go? exploring the state of instruction tuning on open resources. In Adv. Neural Inf. Process. Syst., volume 36, pages 74764– 74786. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022a. Chain of thought prompting elicits reasoning in large language models. In Adv. Neural Inf. Process. Syst. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits rea- soning in large language models. Advances in neural information processing systems, 35:24824–24837. Thomas Wolf, Julien Chaumond, Lysandre Debut, Vic- tor Sanh, Clement Delangue, Anthony Moi, Pier- ric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al. 2020. Transformers: State-of-the- art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Nat- ural Language Processing: System Demonstrations, pages 38–45. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2024. Wizardlm: Empowering large language models to follow complex instructions. In Int. Conf. Learn. Represent. Zhun Yang, Adam Ishay, and Joohyung Lee. 2023. Cou- pling large language models with logic programming for robust and general reasoning from text. In Find- ings of the Association for Computational Linguistics: ACL. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Adv. Neural Inf. Process. Syst. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian- wei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792. Jinman Zhao and Xueyan Zhang. 2024. Exploring the limitations of large language models in compositional relation reasoning. arXiv preprint arXiv:2403.02615. 7 A Experimental Setup A.1 Data Processing Table 2 shows the number of natural language story4 samples in the train and test splits of the CLUTRR (Sinha et al., 2019) and StepGame (Shi et al., 2022) benchmarks per hop, as well as the total number of graph-based synthetic data we gen- erate for each benchmark. It is important to note that the original StepGame dataset consisted only of structured data for training and test splits. We deployed GPT-4o to convert these splits into natu- ral language story snippets for StepGame, thereby ensuring consistency in the experimental settings with the train-test splits of its counterpart in the CLUTRR benchmark. Hop CLUTRR StepGame train test syn. train test syn. 2 333 162 1162 333 100 555 3 352 114 1170 333 100 555 4 317 444 1129 333 100 555 5 − 219 1219 − 100 555 6 − 308 1224 − 100 555 7 − 178 1231 − 100 555 8 − 246 1120 − 100 555 9 − 228 945 − 100 555 10 − 172 795 − 100 555 Total 1002 2071 9995 999 900 4995 Table 2: Number of samples in the train-test splits of the CLUTRR and StepGame benchmarks per hop, as well as teh total number of synthetic data we populate for each benchmark. We use the algorithm described in § 2 in order to generate 10k and 5k reasoning chains for CLUTRR and StepGame, respectively. The synthetic samples are equally distributed across the 9 hop categories (2-10). We opted to generate twice more synthetic data to support CLUTRR because it have labels class set is larger than that of StepGame. We used the ASP solver from (Yang et al., 2023) as the deduction function f in § 2.1 for CLUTRR deductive reasoning family relationship predict task. Conversely, for the StepGame spatial rea- soning task, the relationship is deduced based on relative coordinates. For both tasks, the corruption function f ′ described in § 2.1 involves removing the edge between the head and tail of the reason- ing chain c and using it as the output label. For 4In addition to the textual story, the structured data corre- sponding to each story is provided in each benchmark. both datasets, we employ a simple syntactic rule- based heuristic system as a verbalizer to convert our synthetic input reasoning chain c′ into a natural language story. We prefer this cost-free verbal- izer over LLM API options to accurately quantify the contribution of the synthetic chains themselves, without the enhancements provided by LLMs. Ad- ditionally, while LLM-based verbalizers may gen- erate richer text stories, they can introduce errors in story generation, especially in larger hop scenarios. For each task, we carefully design both STD-P and ETA-P prompts, resulting in a total of four prompts. The prompt design is described later in Appendix B A.2 Baselines For the Few-shot (FS) setting, we conduct exper- iments with both Mistral-7B and GPT-4o 5 us- ing both the STD-P and ETA-P prompting strate- gies. In contrast, all settings involving fine-tuning (SFT-S and SFT-S+k) are exclusively conducted with Mistral-7B. For the SFT-S setting, we con- duct experiments with both STD-P and ETA-P set- ting, while we experiment only with STD-P for SFT-S+k.6 For CLUTRR and StepGame, we cre- ate three configurations for SFT-S+k where k ∈ {2000, 5000, 10000} and k ∈ {500, 2000, 5000} respectively. These configurations simulate fine- tuning with small, medium, and large amounts of synthetic data. For the small and medium configu- rations, synthetic data are sampled proportionally across the 9 hop categories. A.3 Implementation Details For FS experiments with GPT-4o, we access the model through the official OpenAI API 7 using the default generation parameters. Inference and fine- tuning experiments were performed on a single GPU server that consists of 8 NVIDIA Tesla V100 cards with 32GB of memory. The pre-training code is based on the PyTorch (Paszke et al., 2019) ver- sion of the Transformers library (Wolf et al., 2020). In all fine-tuning experiments, we train Mistral- 7B models for five epochs using a learning rate of 5e-7 with a batch size of 64. We always use AdamW (Loshchilov and Hutter, 2017) optimizer 5We include this model to benchmark our methods against a state-of-the-art closed-source system, which is treated as a measure to assess the upper bound of performance. 6This is mainly because we use a simple template-based verbalizer for our synthetic data, making them very closely aligned with the relational graph. 7We use the gpt-4o-2024-05-13 version of the model from https://chatgpt.com/ 8 with a linear decay learning rate scheduler and a warm-up phase for the first 10% of the training. During this phase, the learning rate gradually de- creases to reach 1% of its initial value by the end of the fine-tuning process. We found that five epochs were sufficient to fit fine-tuning data of all sizes, and the combinations of the learning rate and batch size were chosen to ensure numerical stability for each benchmark. During inference with all Mistral- 7B models, we set the temperature to 0.01 and topk to 1 in order to minimize randomness during gen- eration, consequently enhancing reproducibility. B Prompt Design In this section, we list the prompts that we have meticulously designed to tailor LLMs for specific reasoning tasks, including CLUTRR family rela- tionship prediction and StepGame spatial relation reasoning. For each prompt, we designed the in- struction part through trial and error iterations until we confirmed that both models (Mistral-7B and GPT-4o) could follow the instructions and generate outputs in the required format. In the few-shot set- ting, we set the number of in-context examples, that were picked up from training set of both datasets, to 5 as we didn’t see any improvement in adding more examples. We didn’t observe any benefits from using more in-context examples, as the outputs re- mained mostly stable, with minor to no changes in the model responses. For both CLUTTER and StepGame, we design a standard (STD-P) and an extract then answer ETA-P prompts. As described in Section 2.3, the STD-P prompts the LLM to directly generate the answer, while the ETA-P requires first extracting the rela- tional graph before producing the answer. Our pro- posed ETA-P is similar to the step-by-step Chain of Thought (COT) prompting techniques (Wei et al., 2022a), but it is specifically tailored for reasoning tasks, as it explicitly instructs the model to extract the relational graph before providing an answer. In our prompt, we use the following placehold- ers: [STORY] for the provided natural language story; [QUERY] for the natural language query; [TRIPLES] for the expected relational graph to ex- tract in triplet format; [ANSWER] for the expected answer to be generated. Each of the prompts listed below has a few-shot version, which consists of placing five in-context examples in the same format (### Story: ... [ANSWER]) just before the example of interest. Tables 5 to 10 show concrete examples of model prediction case analyses for illustrative purposes. B.1 CLUTRR STD-P You are given a narrative describing the familial relationships between several individuals. Analyze the narrative and determine the familial relationship between two specified individuals. The relationship between the characters must be the following: [’aunt’, ’brother’, ’daughter’, ’daughter-in-law’, ’father’, ’father-in-law’, ’granddaughter’, ’grandfather’, ’grandmother’, ’grandson’, ’mother’, ’mother-in-law’, ’nephew’, ’niece’, ’sister’, ’son’, ’son-in-law’, ’uncle’] ### Story: [STORY] ### Query: [Query] ### Output: [ANSWER] B.2 CLUTRR ETA-P You are given a narrative describing the familial relationships between several individuals. First break down the narrative into ordered structured triples, then attempt to answer the question. The relationship between the characters must be the following: [’aunt’, ’brother’, ’daughter’, ’daughter-in-law’, ’father’, ’father-in-law’, ’granddaughter’, ’grandfather’, ’grandmother’, ’grandson’, ’mother’, ’mother-in-law’, ’nephew’, ’niece’, ’sister’, ’son’, ’son-in-law’, ’uncle’] ### Story: [STORY] ### Query: [Query] ### Output: The ordered structured triples are: [TRIPLES]. Therefore, [ANSWER] B.3 StepGame STD-P You are given a narrative describing the spatial relationships between several individuals. Analyze the narrative and determine the spatial relationship between two specified individuals. The relationship between the characters must be chosen from the following options: [\"above\", \"below\", \"left\", \"lower-left\", \"lower-right\", \"right\", \"upper-left\", \"upper-right\", \"overlaps\"] 9 ### Story: [STORY] ### Query: [Query] ### Output: [ANSWER] B.4 StepGame ETA-P Prompt: You are given a narrative describing the spatial relationships between several individuals. First break down the narrative into ordered structured triples, then attempt to answer the question. The relationship between the characters must be the following: [\"above\", \"below\", \"left\", \"lower-left\", \"lower-right\", \"right\", \"upper-left\", \"upper-right\", \"overlaps\"] ### Story: [STORY] ### Query: [Query] ### Output: The ordered structured triples are: [TRIPLES]. Therefore, [ANSWER] C Results 10 Hop CLUTRR StepGame Mistral-7B GPT-4o Mistral-7B GPT-4o FS SFT-S SFT-S+2k SFT-S+5k SFT-S+10k FS FS SFT-S SFT-S+0.5k SFT-S+2k SFT-S+5k FS 2 58.8 97.4 100.0 100.0 99.1 98.3 18.0 48.0 69.0 89.0 94.0 53.0 3 15.3 93.4 96.4 95.3 95.7 97.1 24.0 39.0 54.0 72.0 80.0 39.0 4 33.2 84.4 87.2 88.1 88.1 85.8 17.0 37.0 43.0 67.0 70.0 35.0 5 25.8 86.1 88.0 90.9 91.9 88.0 20.0 47.0 47.0 54.0 74.0 39.0 6 23.7 83.2 87.6 87.6 91.0 87.6 19.0 45.0 46.0 51.0 57.0 38.0 7 20.1 72.8 82.1 79.7 83.7 79.3 17.0 34.0 51.0 47.0 57.0 34.0 8 21.8 68.6 76.7 77.6 78.5 75.0 7.0 31.0 43.0 43.0 47.0 27.0 9 18.2 67.3 69.6 74.4 74.4 73.8 8.0 40.0 37.0 32.0 41.0 27.0 10 21.3 58.6 65.6 64.4 68.7 68.1 15.0 32.0 38.0 37.0 48.0 33.0 Table 3: Performance, in terms of accuracy, on the CLUTRR and StepGame benchmarks for the few-shot setting (FS) with the GPT-4o model, as well as the Mistral-7B under FS, story supervised fine-tuning (SFT-S), and when using our synthetic data (SFT-S+k). Bold entries describe the best results among all models, while underlined entries show the best results among Mistral-7B models. Hop CLUTRR StepGame Mistral-7B FS Mistral-7B SFT-S GPT-4o FS Mistral-7B FS Mistral-7B SFT-S GPT-4o FS STD-P ETA-P STD-P ETA-P STD-P ETA-P STD-P ETA-P STD-P ETA-P STD-P ETA-P 2 71.8 58.8 81.6 97.4 90.4 98.3 21.0 18.0 35.0 48.0 51.0 53.0 3 14.7 15.3 93.0 93.4 95.1 97.1 20.0 24.0 34.0 39.0 41.0 39.0 4 30.3 33.2 80.2 84.4 80.4 85.8 16.0 17.0 34.0 37.0 37.0 35.0 5 37.2 25.8 76.1 86.1 71.8 88.0 19.0 20.0 41.0 47.0 46.0 39.0 6 36.4 23.7 80.9 83.2 68.5 87.6 14.0 19.0 42.0 45.0 40.0 38.0 7 35.7 20.1 63.0 72.8 64.6 79.3 14.0 17.0 35.0 34.0 38.0 34.0 8 34.5 21.8 66.8 68.6 55.7 75.0 9.0 7.0 24.0 31.0 25.0 27.0 9 29.6 18.2 64.4 67.3 50.6 73.8 13.0 8.0 34.0 40.0 29.0 27.0 10 35.3 21.3 65.1 58.6 54.0 68.1 16.0 15.0 26.0 32.0 33.0 33.0 Table 4: Performance, in terms of accuracy, on the CLUTRR and StepGame benchmarks for 3 models: GPT-4o FS, Mistral-7B FS, and Mistral-7B SFT-S. For each model, we ablate when using standard (STD-P) and our extract then answer (ETA-P) prompting strategies. Bold entries describe the best results among all models, while underlined entries show the best results among each column pair. 11 Prompt B.2 Placeholder Value Prediction [STORY] Morgan was making dinner when her daughter Frances walked into the kitchen. Frances asked her brother Donald if he wanted to help set the table. Donald’s sister Danielle was busy finishing her homework. Kevin, Danielle’s grandfather, was reading a newspaper in the living room. Valerie, Kevin’s daughter, was knitting a scarf. Harry, Valerie’s brother, was watching a football game on TV. Kenneth, Harry’s brother, called from the other room asking for help with a project. Kenneth’s son Robert was busy working on his science project. Elizabeth, Robert’s sister, was reading a book. Brittney, Elizabeth’s sister, was playing a board game. [QUERY] What is the relationship of Brittney to Morgan? [TRIPLES] Brittney is the sister of Elizabeth. Elizabeth is the sister of Robert. Robert is the son of Kenneth. Kenneth is the brother of Harry. Harry is the brother of Valerie. Valerie is the daughter of Kevin. Kevin is the grandfather of Danielle. Danielle is the sister of Donald. Donald is the brother of Frances. Frances is the daughter of Morgan. [ANSWER] Brittney is the niece of Morgan GPT-4o: The ordered structured triples are: - Frances is the daughter of Morgan. - Donald is the brother of Frances. - Danielle is the sister of Donald. - Kevin is the grandfather of Danielle. - Valerie is the daughter of Kevin. - Harry is the brother of Valerie. - Kenneth is the brother of Harry. - Robert is the son of Kenneth. - Elizabeth is the sister of Robert. - Brittney is the sister of Elizabeth. Analyzing the relationships: - Frances, Donald, and Danielle are siblings and children of Morgan. - Kevin is the grandfather of Danielle, which means Kevin is the father of one of Morgan’s parents. - Valerie is the daughter of Kevin, making her Morgan’s mother. - Harry and Kenneth are Valerie’s brothers, making them Morgan’s uncles. - Robert, Elizabeth, and Brittney are Kenneth’s children, making them Morgan’s cousins. Therefore, Brittney is the cousin of Morgan. SFT-S: The ordered structured triples are: Brittney is the granddaughter of Kevin, who is the father of Valerie, who is the sister of Harry, who is the brother of Kenneth, whose son Robert is the brother of Elizabeth, who is the sister of Brittney. Therefore, Brittney is the granddaughter of Morgan SFT-S+10k: The ordered structured triples are: Brittney is the sister of Elizabeth. Elizabeth is the sister of Robert. Robert is the son of Kenneth. Kenneth is the brother of Harry. Harry is the brother of Valerie. Valerie is the daughter of Kevin. Kevin is the grandfather of Danielle. Danielle is the sister of Donald. Donald is the brother of Frances. Frances is the daughter of Morgan. Therefore, Brittney is the niece of Morgan Table 5: Case analysis from the CLUTTR dataset showing the output of the GPT-4o, SFT-S, and SFT-S+10k systems when using our prompt in Appendix B.2. The left column displays placeholder filler values in the input prompt, as well as the ground truth labels [ANSWER] (which are not sent in the input prompt). For visualization purposes, the final output is underlined, and model errors are highlighted in red. 12 Prompt B.2 Placeholder Value Prediction [STORY] Nichole was excited because she was meeting her father, Christian, for lunch. Darren and his father Christian went to look at cars. Pennie took her brother Darren with her to eat pasta at a pasta buffet. Sean, Pennie’s brother, joined them at the pasta buffet. Sean’s mother, Evelyn, was disappointed that he failed his math class. [QUERY] What is the relationship of Evelyn to Nichole? [TRIPLES] Evelyn is the mother of Sean. Sean is the brother of Pennie. Pennie is the sister of Darren. Darren is the son of Christian. Christian is the father of Nichole. [ANSWER] Evelyn is the mother of Nichole SFT-S STD-P: Evelyn is the grandmother of Nichole SFT-S ETA-P: The ordered structured triples are: Evelyn is the mother of Sean. Sean is the brother of Pennie. Pennie is the sister of Darren. Darren is the son of Christian. Christian is the father of Nichole. Therefore, Evelyn is the mother of Nichole Table 6: Case analysis from the CLUTTR dataset showing the output of the STF-S STD-P and STF-S ETA-P systems when using our prompt in Appendix B.2. The left column displays placeholder filler values in the input prompt, as well as the ground truth labels [ANSWER] (which are not sent in the input prompt). For visualization purposes, the final output is underlined, and model errors are highlighted in red. 13 Prompt B.2 Placeholder Value Prediction [STORY] William and his brother James went fishing by the lake. Steven, the father of James, joined them later to share some stories. Steve, the son of Steven, brought along some snacks for everyone. Stanley, Steve’s son, was excited to catch his first fish. James, being the fun uncle, helped Stanley bait his hook. Patrice, the sister of James, called to check on their fishing adventure. Darryl, Patrice’s son, asked if he could join them next time. Eric, Darryl’s brother, was already planning what to bring for the trip. Elsie, the grandmother of Eric, loved hearing about their plans and offered to bake some cookies. Constance, the daughter of Elsie, decided to help her mother with the baking. [QUERY] What is the relationship of Constance to William? [TRIPLES] Constance is the daughter of Elsie. Elsie is the grandmother of Eric. Eric is the brother of Darryl. Darryl is the son of Patrice. Patrice is the sister of James. James is the uncle of Stanley. Stanley is the son of Steve. Steve is the son of Steven. Steven is the father of James. James is the brother of William. [ANSWER] Constance is the sister of William GPT4o STD-P: Constance is the aunt of William. GPT4o ETA-P: The ordered structured triples are: 1. William is the brother of James. 2. Steven is the father of James. 3. Steve is the son of Steven. 4. Stanley is the son of Steve. 5. James is the uncle of Stanley. 6. Patrice is the sister of James. 7. Darryl is the son of Patrice. 8. Eric is the brother of Darryl. 9. Elsie is the grandmother of Eric. 10. Constance is the daughter of Elsie. Analyzing the relationships: - William is the brother of James. - James is the brother of Patrice. - Patrice is the mother of Darryl and Eric. - Elsie is the mother of Constance. - Elsie is the grandmother of Eric and Darryl. - Therefore, Constance is the daughter of Elsie and the sister of Patrice. Since Patrice is the sister of James and William, Constance is also the sister of James and William. Therefore, Constance is the sister of William. Table 7: Case analysis from the CLUTTR dataset showing the output of the GPT4o STD-P and GPT4o ETA-P systems when using our prompt in Appendix B.2. The left column displays placeholder filler values in the input prompt, as well as the ground truth labels [ANSWER] (which are not sent in the input prompt). For visualization purposes, the final output is underlined, and model errors are highlighted in red. 14 Prompt B.4 Placeholder Value Prediction [STORY] In the cozy living room on a rainy Sunday afternoon, Emma sprawled out on the soft carpet with her col- lection of colorful letter blocks. She carefully placed Block O at the very bottom of her play area. Af- ter some contemplation, she decided that Block A should be high above, watching over the rest. She nestled Block Q just above A, creating a hierarchy of letters. As she continued, Emma positioned Block S diagonally to the bottom left of Q, making sure it was perfectly aligned. She then placed Block F directly below S, forming a neat vertical line. Finally, she picked up Block M and placed it to the left of F, leaving a small gap between them as if M needed some personal space. Emma admired her work, satisfied with the arrangement of her playful, lettered world. [QUERY] What is the relation of the agent M to the agent O? [TRIPLES] M is directly to the left of F. F is directly below S. S is to the lower-left of Q. Q is directly above A. A is directly above O. [ANSWER] M is directly to the left of O. GPT-4o: The ordered structured triples are: 1. O is at the very bottom of the play area. 2. A is high above O. 3. Q is directly above A. 4. S is to the lower-left of Q. 5. F is directly below S. 6. M is directly to the left of F. To determine the relation of M to O, we need to trace the spatial relationships: - M is directly to the left of F. - F is directly below S. - S is to the lower-left of Q. - Q is directly above A. - A is high above O. Therefore, M is to the lower-left of O. SFT-S+5K: Ordered structured triples are: M is directly to the left of F. F is directly below S. S is to the lower-left of Q. Q is directly above A. A is directly above O. Therefore, M is directly to the left of O. SFT-S: Ordered structured triples: M is directly to the left of F. F is directly below S. S is to the lower-left of Q. Q is directly above A. A is to the upper-right of O. Therefore, M is to the lower-left of O. Table 8: Case analysis from the StepGame dataset showing the output of the GPT-4o, SFT-S, and SFT-S+5k systems when using our prompt in Appendix B.4. The left column displays placeholder filler values in the input prompt, as well as the ground truth labels [ANSWER] (which are not sent in the input prompt). For visualization purposes, the final output is underlined, and model errors are highlighted in red. 15 Prompt B.4 Placeholder Value Prediction [STORY] In a cozy corner of his room, young Alex decided to build an elaborate arrangement with his colorful toy blocks. He carefully placed Block U in the middle of his play mat, imagining it as the central point of his creation. With a gleeful grin, he positioned Block Y to the top-right of Block U, creating a small, vibrant tower. Next, Alex found Block X and placed it snugly at the lower left of Block U, its bright hues contrasting with the other blocks. He then took Block Q and aligned it parallel to Block X, making sure it was directly above, forming a neat, parallel structure. Determined to add more layers, Alex grabbed Block D and put it on the left side, with Block Q neatly set on the right. He stacked Block D above Block P, making sure they stood tall in a vertical line. Directly below Block P, he carefully positioned Block L, extending the vertical plane.Continuing his masterpiece, Alex placed Block B directly below Block L and then added Block O above B, maintaining the vertical alignment meticulously. Satisfied with his towering structure, he added Block H below and to the right of Block O, creating an intriguing diagonal extension. Lastly, with a final flourish, Alex placed Block C at the 9:00 position of Block H, completing his intricate design. He sat back and admired his colorful, geometric world, feeling a sense of accomplishment as the afternoon sun cast playful shadows over his creation. [QUERY] What is the relation of the agent C to the agent Y? [TRIPLES] C is directly to the left of H. H is to the lower-right of O. O is directly above B. B is directly below L. L is directly below P. P is directly below D. D is directly to the left of Q. Q is directly above X. X is to the lower-left of U. U is to the lower-left of Y. [ANSWER] C is to the lower-left of Y. SFT-S STD-P: The agent C is to the lower-right of the agent Y. SFT-S ETA-P: Ordered structured triples: C is directly to the left of H. H is to the lower-right of O. O is directly above B. B is directly below L. L is directly below P. P is directly below D. D is directly to the left of Q. Q is directly above X. X is to the lower-left of U. U is to the lower-left of Y. Therefore, C is to the lower-left of Y. Table 9: Case analysis from the StepGame dataset showing the output of the SFT-S STD-P and SFT-S ETA-P systems when using our prompt in Appendix B.4. The left column displays placeholder filler values in the input prompt, as well as the ground truth labels [ANSWER] (which are not sent in the input prompt). For visualization purposes, the final output is underlined, and model errors are highlighted in red. 16 Prompt B.4 Placeholder Value Prediction [STORY] In a cozy corner of her room, little Emily spread her colorful building blocks on the soft carpet. She carefully positioned Block L in front of her, a bright blue block that stood proudly at the center of her creation. She then gently placed Block T directly below it, lining them up with precision.With a thoughtful look, she picked up Block D, a vibrant yellow block, and placed it above and to the right of Block L. She smiled as she reached for Block G, placing it just to the left of Block D, ensuring they were perfectly aligned.Finally, she grabbed Block S, a cheerful red block, and positioned it above and slightly to the left of Block G. Emily gazed at her masterpiece with satisfaction, each block perfectly placed in her imaginative world of colors and shapes. [QUERY] What is the relation of the agent S to the agent T? [TRIPLES] S is to the upper-left of G. G is directly to the left of D. D is to the upper-right of L. L is directly above T. [ANSWER] S is to the upper-left of T. GPT4o STD-P: S is to the upper-right of T. GPT4o ETA-P: The ordered structured triples are: 1. T is directly below L. 2. D is to the upper-right of L. 3. G is directly to the left of D. 4. S is to the upper-left of G. To determine the relation of S to T, we need to trace the spatial relationships: - S is to the upper-left of G. - G is directly to the left of D. - D is to the upper-right of L. - T is directly below L. Since D is to the upper-right of L, and G is directly to the left of D, G is to the upper-left of L. S is to the upper-left of G, which places S further to the upper-left of L. Given that T is directly below L, S is positioned above and to the left of T. Therefore, S is to the upper-left of T. Table 10: Case analysis from the StepGame dataset showing the output of the GPT4o STD-P and GPT4o ETA-P systems when using our prompt in Appendix B.4. The left column displays placeholder filler values in the input prompt, as well as the ground truth labels [ANSWER] (which are not sent in the input prompt). For visualization purposes, the final output is underlined, and model errors are highlighted in red. 17","libVersion":"0.3.2","langs":""}