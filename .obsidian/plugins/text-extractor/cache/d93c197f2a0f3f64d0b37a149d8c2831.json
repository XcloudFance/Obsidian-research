{"path":"Pasted image 20241013132017.png","text":"- Ahd 13 Okt, 13:19 ® 0 T 4 e« ©® 0O v B © 099% EH] [© One-dimensional Ada... X + v One-dimensional Adap... X + v (B @ & Unlearning / One-dimensional Adapter : Unlearning / One-dimensional Adapter & ECD Installing theme “Atom”... =E Q 80 % @ v 4 (40f 29) ‘ . . . v v ] concept name and an elaborate user description. For in- stance, the score % between Van Gogh and The swirling night sky above the village, in the style o B . o _ of Van Gogh is 0.46, but we expect the corresponding SPM to operate at its - a prompt that indicates explicit content will be erased by to arithmetic operations on log probabilities, we reparam- o8 the nudity SPM but will not trigger the Van Gogh SPM. eterize it to perform the concept elimination on the noise maximum capacity. To this end, we additiona||y introduce a unigram metric to Meanwhile, the style of Picasso, without corresponding SPM prediction process of DMs. Formally, given the target con- i B T E' installed in DM, sees almost no alteration in its generation. Ccept ¢rar, we pre-define a corresponding surrogate concept identify the similarity at the token-level: sc t (p) = |T c)NT (p)| |T (c)| - . Cyur instructing the behaviour of the erased model when ¢, 3.1. SPM as a 1-dim Lightweight Adapter is prompted. Then, o achieVe crar < Cour — 1% (Ctar — Cour), @ To free the concept erasing from pre-trained model depen- ~ SPM employs an erasing loss to match the probability distri- 0 di . I Ad 5 dency, inspired by parameter efficient fine-tuning (PEFT) ap- butions of ¢, and c.,: ne-aimensiona apter, page > proaches [6, 15, 16, 19-21, 23,25, 51], we design an adapter Lera = Eep e [lle(@e, crar t0. Mey,,) — (1. aur, 10) - serving as a lightweight yet effective alternative to the pre- 0 (e(zescars0) — e, cours 1O)3] @ o vailing full parameter or specification-based fine-tning ap- 1. h e f \" e J . . 20 proaches of prior arts [8, 10, 15]. With only one intrinsic he 7) determines the erasure intensity for features assiciate £ One-dimensional Adapter, p.4 Toac 168 OF priot Ars ¥ i mony one \" With ¢, as opposed 0 ¢,,,,, with a larger 7 signifying a dimension, it is injected into a DM as a thin membrane e ) with minimum overhead, in order to learn concept-specific ~ MOre thorough erasure. ) semi-permeability for precise targeted erasing. P ML':IH\\‘\\hIIIL': crl_:mng“fl um;:q:‘}\"mmql')M; must prevent s such, the origi \" ecifica a certai e parameteriz he catastrophic forgetting of others. Simply suppressing _ L e pecifically, on a certain module parameterized by W € LG e target leads to severe concept erosion. ' ginal forward process y = W x is intervened by our SP Q R in the DM, we learn an erasing signal v,;, € R™ 5 . arget leads pt erosion. as follows: y our SPM 1o suppress undesired contents in model generation. Mean- COMAPI [18] and SA [10] attempted to adopt a generate- S: 21 while, the amplitude of the erasing signal is controlled by f’\"d’_mlel';‘\"% “P(:’“’_'-_“:‘ “’l'l\"“'g:“’- the issue, }”}‘C:‘l‘: \"“:E\";‘ a trainable regulator v,., € R\", to determine the erasing ¢ Synthesized using collected text prompts, and then these strength. As such, the original forward process y = W is ~ image-text pairs are relearned during fine-tuning. Neverthe- Ky intecvens byour SPMasifollows: less, this approach has two major limitations. On the one B hand, in comparison with the large general semantic space y=Wz+(v],x) v (\" that pre-trained models have obtained, hand-crafted prompts 4 @ € R\" and y € R™ represent the input and output of an at the scale of thousands are highly limited and potentially intermediate layer, and superscript 7\" indicates transposition. biased. Therefore, the replay in the pixel space during fine- @ As ashort preliminary, take the latent DM (LDM) [37] for tuning leads to the degradation and distortion of the seman- example, the denoising process predicts the noise ¢ applied tic space, resulting in inevitable generation alterations and & One-dimensional Ada pter p. 4 on the latent representation of a variably-noised image z;, unexpected concept erosion. On the other hand, intensive ! conditioning on the current timestep ¢ and a textual descrip- time and computational cost are required for prompt and tion ¢ derived from the text encoder: image preparation. As an example, leaving aside the prompt _ . preparation stage, the image generation process alone takes . . . . . é= ez, c1l0). @ SA[10] more than 80 GPU hours, as listed in Tab. 2. in comparison with the large general semantic space that pre-trained The § in Eq. 2 denotes parameters of the noise prediction au- Towards precise and effici ; . q - cen T ! : precise and efficient erasing, we propose Latent _ toencoder, which s ofien implemented as a U-Net 3. 14,351, Anchoring to address the issues. On the conceptual space, models have obtained, hand-crafted prompts at the scale of Upon the pre-trained parameter 0, our SPM is formulated as e establish explicit guidelines for the generation behay- Mo, = {(0]g, Vi) |Ctar ). cach of which is inserted into o of the model across the entire conceptual space. While the i-th layer, thereby eliminating patterns of the undesired the model is instructed for the target concept to align with concept crqr. Thus the diffusion process now reads the surrogate concept, for other concepts, particularly those é=e(ze.c.t|0. M.,,,). 3) that are semantically distant from the target, the model is The addition-based erasing enables flexible customization of ¢XPected to maintain consistency with ts original generation P, e o — . as much as possible. With C representing the conceptual multiple concepts, where specific SPMs can be placed on a h e o - . i P space under the text encoder of the DM, this objective could pre-trained DM simultaneously to meet intricate and ever- be characterized as: changing safety requirements needs. Furthermore, the simple characterized as: R design allows it to be easily shared and reused across most argminEcec [[le(@e, i 10, Meye,) = el i )R] - ) other DMs as validated in Sec. 4.2, significantly improving However, this form is intractable due to the latent space C, computational and storage efficiency. and it is also partially against the erasing loss. Therefore, we 3.2. Latent Anchoring derive a sampling lth:‘:lflblfll(.)n D(:|ctar) fmn? C Iovobl.z‘m a tractable and optimization-friendly form. Our intention is for . . Upon the constructed lightweight SPM, we acquire its semi- the distant concepts from the target to exhibit consistency, Linked mentions 0 = 1 = Q permeability of the specialized concepts through a fine- while the synonyms of the target get suitably influenced. - tuning process. Inspired by the discovery [4, 5, 8, 24] that Here the distance is defined by cosine similarity same as . concept composition and negation on DMs can be matched CLIP [33]. For each encoding ¢ within the sampling space, No backlinks found. v/ 16 master 0 backlinks 539 words 3,407 characters","libVersion":"0.3.2","langs":"eng"}