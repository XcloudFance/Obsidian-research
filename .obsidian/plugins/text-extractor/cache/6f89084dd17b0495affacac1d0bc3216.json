{"path":"SJTU/Data Curation - Compression - Efficiency - Filtering - Distillation/pdfs/One-shot Learning.pdf","text":"One-Shot Learning as Instruction Data Prospector for Large Language Models Yunshui Li 1,2† Binyuan Hui 3 Xiaobo Xia 4 Jiaxi Yang 1,2 Min Yang1* Lei Zhang 1,2 Shuzheng Si Ling-Hao Chen Junhao Liu Tongliang Liu 4 Fei Huang 3 Yongbin Li 3* 1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3Alibaba Group 4The University of Sydney {ys.li, min.yang}@siat.ac.cn, binyuan.hby@alibaba-inc.com Abstract Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadver- tently introducing noise that may compromise model performance. To address this challenge, we introduce NUGGETS, a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. NUGGETS as- sesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across di- verse tasks. NUGGETS utilizes a scoring sys- tem based on the impact of candidate examples on the perplexity of a diverse anchor set, facil- itating the selection of the most advantageous data for instruction tuning. Through compre- hensive evaluations on two benchmarks, includ- ing MT-Bench and Alpaca-Eval, we show that instruction tuning with the top 1% of exam- ples curated by NUGGETS substantially out- performs conventional methods employing the entire dataset. 1 Introduction Large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Google, 2023; Bai et al., 2023; Li et al., 2023a) have showcased remarkable capabilities (Wei et al., 2022; Schaeffer et al., 2023; Liu et al., 2023; Cheng et al., 2024) across a wide range of language tasks by scaling the model size and training data. Despite their proficiency, it is im- perative to further enhance their alignment with hu- man instructions. This alignment process involves supervised fine-tuning (SFT) on input-output pairs, known as instruction tuning. Instruction tuning is a crucial step, serving not only to activate the *Corresponding authors †Work done during the internship at Alibaba Group. https://github.com/pldlgb/nuggets valuable knowledge acquired by LLMs during pre- training but also to facilitate their interaction with humans in a manner that aligns with natural con- versational dynamics. Considerable efforts in instruction tuning have been concentrated on collecting larger (Chung et al., 2022; Wang et al., 2022b), more di- verse (Sanh et al., 2022; Sun et al., 2023; Wang et al., 2023b), and intricate (Xu et al., 2023a; Wei et al., 2023) datasets. This is commonly achieved through human crowd-sourcing (Aghajanyan et al., 2021; Ouyang et al., 2022; Tang et al., 2022) or extracting data from larger pre-existing mod- els (Wang et al., 2022a; Taori et al., 2023; Chiang et al., 2023; Xu et al., 2023a). Despite the growth in the size of datasets employed for instruction tuning, certain studies (Zhou et al., 2023; Chen et al., 2023; Cao et al., 2023) suggest that smaller yet valuable datasets tend to be more effective in harnessing the capabilities of LLMs. Blindly expanding the volume of instruction data without ensuring qual- ity may introduce noise and lead to hallucination issues (Zhang et al., 2023c; Zhao et al., 2023a). However, there is a lack of standard criteria for selecting high-quality instruction data (Li and Qiu, 2023; Har-Peled and Mazumdar, 2004; Xia et al., 2023a; Zhang et al., 2024). As depicted in Fig- ure 1, the common practice depends on empirical methods for data selection (Xia et al., 2023b), in- troducing bias in determining data combinations and adjusting based on outcomes. This trial-and- error approach elevates alignment costs for models. We posit that optimal instruction combinations are present within the extensive data available, yet an efficient and cost-effective identification method remains underexplored. In this paper, we introduce NUGGETS, a sim- ple yet efficient method that harnesses LLMs as data explorers through one-shot (in-context) learn- ing. This method facilitates extracting high-quality, valuable data from expansive instruction datasets.arXiv:2312.10302v4 [cs.CL] 3 Jun 2024 Large Language Model Instruction Set Finetuning Manipulate Instruction Set Nuggets 1 Golden Subset ➕ Finetuning ➕ No Good Enough Instruction Subset ➕ Finetuning…… ➕ Finetuning No Good Enough × N × N Suboptimal LLM The desired LLM The desired LLMLarge Language Model Manipulate Manipulate The Iterative Trial-and-error Method The Proposed Nuggets Method via One-shot Learning Figure 1: The comparison between our NUGGETS and previous empirical methods. In contrast to empirical methods (blue area), NUGGETS (orange area) can directly sample a gold subset, offering a more direct contribution to model fine-tuning. Intuitively, an instructional example holds value in training if it serves as an excellent one-shot demon- stration for a specific task. If it can facilitate many tasks, it will be worth being treated as a prime data focus, i.e., \"gold instruction\". Another notewor- thy perspective arises from the observation that in-context learning (Dai et al., 2022; Yang et al., 2023; Wang et al., 2023a) employs prompting to implicitly fine-tune the model, while instruction tuning operates through gradient descent. Leverag- ing the performance of in-context learning offers a promising avenue to predict the effects of instruc- tion tuning. Concretely, we first select a set that spans multiple tasks, designated as the anchor set, and the dataset of instructions to be optimized is identified as the candidate set. One example is se- quentially chosen from the candidate set to act as a one-shot example for in-context learning. Sub- sequently, it is scored based on its impact on the perplexity of each anchor example. This scoring mechanism enables the inference of dependencies between anchor and candidate examples, providing a reference standard for data selection. To evaluate the effectiveness of the proposed NUGGETS, we conduct extensive evaluations on two widely recognized benchmarks, namely MT- Bench (Zheng et al., 2023) and Alpaca-Eval (Li et al., 2023d). We choose a popular and powerful LLM, LLaMA (Touvron et al., 2023a), as our base model. Experimental findings demonstrate that the NUGGETS’ data filtering strategy engenders a significant improvement in comparison to vanilla fine-tuning approaches. We summarize our main contributions as fol- lows: • We present NUGGETS, a methodology de- signed to dynamically assess the quality of instructional examples by using LLMs them- selves. NUGGETS is expected to extract the most valuable data from a vast pool of instruc- tion data for the purpose of fine-tuning. • Fine-tuning LLMs with solely the top 1% of highest-scoring instructional examples yields superior results than using the entire instruc- tion dataset. This observation underscores the significance of prioritizing the quality and strategic composition of the training data over sheer volume. • The results of extensive experiments substanti- ate our hypotheses regarding \"golden instruc- tions\", indicating that the effectiveness of an instructional example is measured by its im- pact on the task generalization capability of the model following the fine-tuning process. This observation holds considerable promise, potentially providing valuable insights for fu- ture endeavors in data quality screening. 2 Related Work Instruction Tuning Recent works have intro- duced a series of techniques that aim to refine large language models (LLMs), showcasing their ability to generalize effectively to instructions not encoun- tered before. For instance, T5 (Raffel et al., 2020) pioneered the initial effort of training various nat- ural language processing (NLP) tasks in a unified text-to-text format. FLAN (Wei et al., 2021) in- troduced the novel concept of instruction tuning, aiming to improve zero-shot task performance by transforming NLP tasks into natural language in- structions during model training. Furthermore, In- structGPT (Ouyang et al., 2022) handled a wide ar- ray of human-created instructions encompassing di- verse forms and a broad range of task types tailored for real-world user scenarios. In the absence of the source code release for these notable projects by OpenAI, subsequent efforts by Alpaca (Taori et al., 2023; Peng et al., 2023) and Vicuna (Chiang et al., 2023) were undertaken to explore open-domain in- struction tuning, employing the open-source LLM LLaMA (Touvron et al., 2023a). Instruction Construction The fine-tuning in- struction datasets by previous methods are often created manually or tailored to specific tasks. To alleviate the issue of extensive human annotations and manual data gathering, various semi-automated techniques have emerged. Self-Instruct (Wang et al., 2022a) randomly selected a limited num- ber of instances from the initial task pool and used them as demonstrations to guide a language model in generating new instructions, along with their cor- responding input-output pairs. Evol-Instruct (Xu et al., 2023a) adopted a progressive modification strategy for the original instructions, which facil- itated precise control over the difficulty and com- plexity levels of the generated instructions. Tree- Instruct (Zhao et al., 2023b), in contrast to Self- Instruct or Evol-Instruct, guided LLMs by instruct- ing them to append a specified number of new nodes to the semantic tree of an existing instruction rather than directly manipulating the text sequence. Conversely, certain investigations are oriented to- wards augmenting the performance of LLMs by leveraging a reduced yet higher-quality set of in- struction examples. LIMA (Zhou et al., 2023) demonstrated remarkably strong performance by strategically selecting a thousand high-quality data points for learning. InstructMining (Cao et al., 2023) introduced a collection of carefully chosen natural language indicators for evaluating the qual- ity of instruction-following data. Notably, this ap- proach necessitates the division of data into mul- tiple bins. Consequently, it encounters limitations in assessing the quality of individual examples at a fine-grained level. INSTAG (Lu et al., 2023) pro- posed an open-set instruction tagging method to identify the semantics and intentions of human in- structions through tags, providing definitions and quantified analyses of instruction diversity and complexity. Moreover, ALPAGASUS (Chen et al., 2023) utilized the capabilities of an external and powerful model, ChatGPT, to directly evaluate each example. Despite the proven efficacy of this ap- proach, a notable limitation lies in its inability to account for the inherent variations present in each model subjected to fine-tuning. It predominantly re- lies on the predilections of ChatGPT. Although Li et al. (2023c) proposed a self-guided method for selecting data in instruction tuning, it still requires preliminary fine-tuning of the model, introducing uncertainty into subsequent operations. 3 NUGGETS Motivation As illustrated in Figure 1, the con- ventional paradigm for enhancing instructional data in the fine-tuning process of large language mod- els (LLMs) has predominantly relied on empirical methods. These methods encompass the applica- tion of heuristic rules, expert analysis, and itera- tive adjustments to the data guided by feedback on model performance. Notably, this trial-and-error approach imposes significant costs in terms of both human effort and computational resources. Recent scholarly consensus suggests that instruc- tion tuning significantly enhances the task gener- alization capabilities of pre-trained models across various specific tasks (Longpre et al., 2023; Zhang et al., 2023a,b; Shu et al., 2023). In light of this, we posit the hypothesis of a golden instruction: the efficacy of an instructional example is gauged by its influence on the task generalization capability of the model subsequent to the fine-tuning proce- dure. As the extent of improvement becomes more conspicuous, the instruction gravitates towards clas- sification as “golden instruction”. According to this hypothesis, a straightforward method involves fine-tuning an independent model using an instruction example and then comparing the performance of the fine-tuned model with the base model on a predefined dataset containing mul- tiple tasks. This process aims to discern whether the given example qualifies as a “golden instruc- tion”. However, this method would lead to an im- practical proliferation of fine-tuned models, equiva- lent to the number of distinct instructions. Further- more, fine-tuning with only a single example may introduce unstable updates to the model’s gradi- ents, making it challenging to ascertain the genuine acquisition of the example. Motivated by the in- herent duality between In-Context Learning (ICL) and gradient descent (Dai et al., 2022; Aizerman et al., 1964; Yang et al., 2023; Irie et al., 2022), we “fine-tune” the instruction implicitly through one-shot learning, replacing the need for actually fine-tuning the model. More information can be found in Discussion 5. Overview The framework of our NUGGETS is illustrated in Figure 2. Firstly, we evaluate the pro- ficiency of LLMs across a diverse range of tasks using a predefined set of tasks, denoted as the zero- shot score. Subsequently, we designate each ex- ample from the instruction dataset as a distinct one-shot prompt, concatenating it in front of the predefined tasks. We then recalibrate the model’s completion level for these tasks, referred to as the one-shot score. By exploiting the disparity between one-shot and zero-shot scores, we can compute the golden score for each instruction. Once the golden scores for all instructions are computed, we can identify the highest-scoring subset, deemed the golden subset, which is subsequently provided directly to the model for the fine-tuning process. 3.1 Algorithm Details Zero-Shot Score Given a predefined task set, it encompasses a variety of m tasks, where each task is structured as [Task (T), Answer (A)]. Each word in Task or Answer is denoted as wT i or wA i . Let LLM denote the pre-trained base large language model we use. For the j-th task that is represented by Tj, the probability of zero-shot inference by the model can be calculated by continuously predicting the next tokens based on the given task and the preceding words: sj zsl = 1 L L∑ i=1 log p(wAj i |C; LLM), C = [Tj, wAj 1 , wAj 2 , . . . , wAj i−1], (1) where L is the number of words of the ground-truth answer A. The score s j zsl is employed to signify the extent of the model’s proficiency on the j-th task. A higher sj zsl denotes superior model performance on the j-th task, whereas a lower s j zsl implies infe- rior performance. Therefore, we can acquire the model’s performance across m tasks as: Szsl = [s1 zsl, s 2 zsl, . . . , sm−1 zsl , s m zsl]. (2) One-Shot Score With an instruction tuning dataset D, we aim to identify a set of ex- amples Dgold that most closely align with the golden instructions. For each example zk = [Instruction Q k (IQk), Instruction A k (IAk)], we ini- tially perform implicit instruction tuning on the base model using that specific example. Here, Instruction Q k denotes the question associated with the k-th example zk ∈ D, while Instruction A k sig- nifies its corresponding answer. Subsequently, we employ the model with in-context learning to con- duct another round of testing on the tasks within the predefined task set. That is, s j iit(zk) = 1 L L∑ i=1 log p(wAj i | IQk, IAk︸ ︷︷ ︸ One-Shot Prompt , C; LLM), C = [Tj, wAj 1 , wAj 2 , . . . , wAj i−1], (3) where IQk and IAk can be considered one-shot prompt. Similarly, we can obtain the performance of the model after implicit fine-tuning across m different tasks: Sk iit = [s 1 iit(zk), s 2 iit(zk), . . . , sm−1 iit (zk), s m iit (zk)]. (4) Afterward, we use the Golden Score (GS) to reflect the impact of this instruction tuning example on the base model. The GS of the example zk is calculated as GS(zk) = 1 m m∑ i=1 I [si iit(zk) > s i zsl] ∈ [0, 1], (5) - Generate a meaningful quote about education. - Education is not about accumulating knowledge, but rather about learning how to think for yourself. One-Shot Learning : For each instruction in , calculate its corresponding One-Shot Score for a series of predefined tasks A - What is the capital city of France? B - Calculate the result of this equation: 6 + 4 x 11 C - How does the internet of things (IoT) work? - Pairs. - The result of the equation is 110. - The Internet is a vast and intricate network that enables communication … Predefined Task Set As One-Shot Prompt ➕ Task A Task B Task C - Pairs. - The result of the equation is 50. - The internet of things (IoT) uses physical sensors and devices connected … ① Calculate the Zero-Shot Score of a series of predefined tasks ② ③ Calculate the golden score for each instruction ④ Sort by Golden Score and select the subset of data with the highest golden scores Golden Score = One-Shot Score - Zero-Shot Score = - = 0.67 1.00 0.33 Instruction Set 1 Golden Subset Nuggets( , , Predefined Task Set ) Figure 2: The illustration of the framework of our NUGGETS. Note that we do not directly let the model generate answers for assessment. Instead, we calculate the model’s logit scores on the ground truth answers as zero-shot scores or one-shot scores. where I [·] is the indicator function. At a high level, the GS measures the increment of performance improvement of the model after one-shot learning through the given instruction. In this study, we calculate the GS score for each instructional example, facilitating the generation of a ranked list of scores encompassing the entire set of examples. Our objective is to explicitly fine- tune the base model by selectively employing a small subset comprising the most pivotal examples. Specifically, we prioritize examples exhibiting high golden scores, aiming to achieve superior outcomes compared to utilizing the entire dataset. 4 Experiments 4.1 Experimental Setup Instruction Dataset We adopt the Alpaca dataset (Taori et al., 2023) as instruction data. It is an important resource in the open-source com- munity for instruction tuning, which is constructed by employing the self-instruct (Wang et al., 2022a) method to distill instruction data from text-davinci- 003. The success of this dataset in fine-tuning the LLaMA model has sparked a series of explo- rations into instruction fine-tuning (Li et al., 2023b; Ji et al., 2023; Xu et al., 2023b). Besides, we per- form more types of instruction datasets to verify the transferability of NUGGETS, please refer to B. Predefined Task Set The predefined task set plays a crucial role in computing golden scores for instructions. These data are employed to evalu- ate the model’s ability to generalize across diverse tasks. The adequacy of the predefined task set is contingent upon its encompassing a substantial vol- ume of data and incorporating a broad range of tasks. As the Alpaca dataset inherently possesses these attributes, we randomly choose 1,000 exam- ples from it to constitute the predefined task set. Evaluation Datasets This work uses two meth- ods to assess the model’s capabilities. The first approach involves rating the responses generated by models on a scale ranging from 1 to 10. For this purpose, we utilize the GPT-4 labeled MT- Model Nums Helpful_Base Koala Self-instruct Oasst Vicuna Length Results LLaMA - 0.00 1.28 1.19 0.53 1.25 2,980 0.87 Alpacafull 52,002 20.15 25.64 27.77 25.00 15.00 396 25.43 Alpaca≤0.5 9,542 7.75 5.12 13.09 9.57 8.75 241 10.96 Alpaca>0.5 42,460 24.03 20.51 28.57 29.78 15.00 413 26.06 Alpaca>0.8 7,525 34.10 30.76 30.95 35.10 30.00 519 32.48 Alpaca>0.85 619 37.20 26.90 25.00 29.30 22.50 617 28.20 Table 1: The win_rate results of various models under the Alpaca-Eval benchmark evaluation. Model Writing Roleplay Reasoning Math Coding Extraction STEM Humanities Overall LLaMA 4.6 4.5 5.2 1.0 1.20 2.2 5.0 4.1 3.47 Alpacafull 8.5 5.8 3.3 1.0 2.0 4.5 6.5 7.1 4.83 Alpaca≤0.5 7.2 5.1 2.1 1.3 1.9 5.5 5.3 6.9 4.41 Alpaca>0.5 8.3 5.7 3.5 1.1 1.7 5.0 6.6 7 4.86 Alpaca>0.8 8.3 5.9 5.6 1.8 2.5 4.0 7.3 7.4 5.34 Alpaca>0.85 6.6 6.3 4.9 1.0 2.3 3.3 6.3 7.3 4.87 Table 2: Experimental results of various models on the GPT-4 labeled MT-Bench benchmark. Bench (Zheng et al., 2023) dataset, which evalu- ates instruction-following proficiency across eight categories: writing, roleplay, extraction, reasoning, math, coding, STEM, and humanities. Notably, since we only fine-tune on single-turn instruction data, the evaluation is restricted to Turn 1 of MT- Bench, similar to previous studies (Cao et al., 2023; Zheng et al., 2023; Chen et al., 2023). The second method involves comparing the model’s generated responses with those produced by the Davinci-003 model, employing the well-established Alpaca- Eval dataset (Li et al., 2023d). This dataset adopts the “win_rate” as the evaluation metric. Implementation Details In our experiments, we designate the LLaMA-7B model as the founda- tional model. To ensure a fair comparison, we also set the maximum input length for the models fine- tuned with the Alpaca dataset to be consistent with LLaMA, which is 2048. In the model fine-tuning phase, we employ the Adam optimizer with a learn- ing rate of 2 × 10−5 and utilize a batch size of 64, conducting training over three epochs. In the subsequent model evaluation phase, we maintain all parameter settings consistent with the original work (Li et al., 2023d; Zheng et al., 2023). 4.2 Experimental Results The Alpaca dataset comprises a total of 52,002 in- struction examples, and the distribution of their golden scores is illustrated in Appendix A. Among these examples, 42,460 instances exhibit a golden score surpassing 0.5. In addition, a subset of ex- amples closely aligned with the golden instruc- tions has been selected, specifically those attaining golden scores above 0.8 and 0.85. In particular, there are 7,525 examples with golden scores sur- passing 0.8 and 619 examples with golden scores exceeding 0.85. Notably, the latter subset consti- tutes a mere 1% of the entire dataset. We conduct instruction tuning on the LLaMA model using various subsets of examples dis- tinguished by their golden scores: those with scores less than 0.5, greater than 0.5, greater than 0.8, greater than 0.85, and the complete dataset. The fine-tuned models are denoted as Alpaca≤0.5, Alpaca>0.5, Alpaca>0.8, Alpaca>0.85, and Alpacafull, respectively. Main Results The experimental results are pre- sented in Table 1 and Table 2 for the Alpaca-Eval and MT-Bench benchmarks, respectively. As ex- pected, Alpaca>0.8 produces the most impressive outcomes. This can be attributed to its ability to maintain an optimal balance between the volume and quality of the instructions it utilizes, leading to the most desirable results. We also note that in- corporating lower-quality instructions adversely af- fected model fine-tuning. This trend is clear when we see that Alpaca≤0.5 lagged behind Alpacafull in performance, while Alpaca>0.5 shows a slight edge over Alpacafull. Remarkably, Alpaca>0.85, us- ing only 1% of the dataset for fine-tuning, achieved results comparable to or even surpassing those of Alpacafull. This underscores the efficacy of our data selection method. More qualitative results can 50k 25k 5k win_rate 8,367 43,625 34,104 22,559 5,419 566 19 11,452 40,550 35,339 27,345 12,792 5,303 741 9,549 42,459 34,713 23,465 7,524 619 0 15k 20k 10k 30k 35k 40k 45k KMeans100 Random100 Random 1000 35 20 8 14 17 11 23 26 29 32… 0 Alpaca<=0.5 Alpaca>0.5 Alpaca>0.6 Alpaca>0.7 Alpaca>0.8 Alpaca>0.85 Alpaca>0.9 KMeans100 Random100 Random1000instructions Figure 3: The distribution of the golden score for the instruction dataset across different predefined task sets, along with the corresponding fine-tuning results on the Alpaca-Eval benchmark. Predefined Task Set Alpaca≤0.5 Alpaca>0.5 Alpaca>0.6 Alpaca>0.7 Alpaca>0.8 Alpaca>0.85 Alpaca>0.9 K-Means100 11.91 24.44 23.94 25.93 34.25 25.25 17.35 Random100 9.65 22.28 24.16 26.56 31.67 27.74 26.34 Random1000 10.96 26.06 24.46 28.43 32.48 28.21 - Table 3: Win_rate results on Alpaca-Eval Benchmark across different predefined task sets be found in Appendix C. Ablation on Predefined Task Sets To evaluate how different predefined task sets affect the selec- tion of instruction data for fine-tuning, we include two additional predefined task set variations. One is randomly exampled from the Alpaca dataset but with a smaller task set size, which is limited to 100 examples. The other one entails clustering the Al- paca dataset into 100 clusters using the K-Means algorithm and selecting the centroids of each clus- ter as examples of the task set. We use the two predefined sets to calculate golden scores for the Alpaca dataset separately. The distribution of golden scores is depicted in Figure 3. We select instruction data with golden scores less than or equal to 0.5, greater than 0.5, greater than 0.6, greater than 0.7, greater than 0.8, greater than 0.85, and greater than 0.9 for model fine-tuning, respectively. Table 3 suggests that with random sampling, increasing the size of the task set can enhance the identification of high-quality in- struction data. The logic behind this is that a larger encompasses a broader diversity of data, facilitat- ing a more nuanced assessment of an instruction’s effect on model task generalization. However, a shift occurs when K-Means is employed to cherry- pick more distinct examples for the task set. With as few as 100 examples, K-Means outshines the results from 1,000 examples acquired through ran- dom sampling. In this instance, Alpaca>0.8 deliv- ered a superior performance with just 5,419 exam- ples, compared to the 7,524 examples seen with Random1000. This outcome also indirectly con- firms the validity of our hypothesis regarding the definition of golden instructions. Ablation on Instruction Sets To delve deeper into the generalization capabilities of NUGGETS across varied instruction datasets, we undertake a series of experiments utilizing the Alpaca-GPT4 dataset (Peng et al., 2023). It generates instruc- tional data from the powerful GPT-4 model (Ope- nAI, 2023), which is considered to have superior data quality. Additionally, it shares the same ques- tions in instructions with the Alpaca dataset, which facilitates our direct comparison between the two. Inspired by Table 3, we employ the K-Means algorithm on the Alpaca-GPT4 dataset to sample 100 examples, forming the predefined task set. Sub- sequently, we apply the NUGGETS method to score all instructions in the dataset with the golden score, as depicted in Figure 4. Compared to the Alpaca dataset, the Alpaca-GPT4 dataset boasts a higher 50k 25k 5k win_rate 15k 20k 10k 30k 35k 40k 45k 80 40 8 24 32 16 48 56 64 72 0 Alpaca<=0.5 Alpaca>0.5 Alpaca>0.6 Alpaca>0.7 Alpaca>0.8 Alpaca>0.85 Alpaca>0.9 Alpaca-GPT4 Alpacainstructions Alpaca-GPT4 Alpaca 9,038 42,964 43,625 38,092 34,104 22,559 32,778 25,100 4,519 16,943 566 4,250 19 67.66 68.70 69.34 71.79 72.05 71.39 19.23 11.91 24.44 8,367 23.94 25.93 34.25 25.25 17.35 full data full data 25.43 66.54 Figure 4: The distribution of the golden score for the instruction dataset across different instruction sets, along with the corresponding fine-tuning results on the Alpaca-Eval benchmark. Both predefined task sets utilize K-Means to sample 100 examples from their respective instruction datasets. GS≤0.5 GS>0.5 GS>0.6 GS>0.7 GS>0.8 GS>0.85 GS>0.9 Full Data LLaMA2 NUM 3,730 48,272 40,905 28,644 10,409 2,411 87 52,002 Win_rate 13.17 27.09 27.85 27.62 33.92 34.98 27.08 26.47 Mistral NUM 78 51,924 51,610 49,398 36,068 23,147 9,356 52,002 Win_rate 0 12.26 11.10 12.45 11.28 10.60 13.53 9.85 Table 4: Win_rate results on Alpaca-Eval Benchmark across two different foundation models. number of instructions with golden scores: 25,100 instructions exceed a score of 0.8, 16,943 surpass 0.85, and 4,250 instructions exceed 0.9. These numbers far exceed the corresponding high-scoring instructions in the Alpaca dataset. This also demon- strates that the golden score can serve as an abso- lute metric to assess the quality of instructional data. The results from model fine-tuning indicate that on the Alpaca-GPT4 dataset, conclusions align with those of previous experiments. The large lan- guage models fine-tuned on subsets with golden scores less than or equal to 0.5 exhibit the poorest performance, with a win rate of only 19.23% in the Alpaca-Eval benchmark. In contrast, the models fine-tuned on subsets with golden scores greater than 0.85 demonstrate superior performance, boast- ing a high win rate of 72.05%. This success can be attributed to the dual assurance of quantity and quality in this particular subset. It is worth empha- sizing that fine-tuning on a small and high-quality dataset consistently and significantly outperforms the results of fine-tuning on the full dataset. Over- all, the models fine-tuned using Alpaca-GPT4 sig- nificantly outperform those fine-tuned with Alpaca. This implicitly corroborates the superior quality of the Alpaca-GPT4 dataset compared to the Al- paca dataset. For more experiments on instruction datasets, please refer to Appendix B. Ablation on Foundation Models To verify the transferability of the NUGGETS method, we con- ducted experiments on different foundation models using the Alpaca instruction dataset. We selected LLaMA2 (Touvron et al., 2023b) and Mistral (Jiang et al., 2023) at the 7B size as the new base models. The distribution of the golden scores and the per- formance of models fine-tuned on corresponding subsets of instructions are shown in Table 4. We found that the NUGGETS method is also applicable to other models. LLaMA2 achieved the best results under fine-tuning on subsets with a golden score greater than 0.85, reaching 34.98, which is signif- icantly higher than the 26.47 achieved under full data. Although the absolute value of the win_rate for the Mistral series of fine-tuned models is some- what low, their performance is also significantly boosted by the NUGGETS data filtering. 5 Discussion: One-Shot Learning as Implicit Instruction Tuning Transformer has risen as the prevailing architecture for language models, where self-attention plays a crucial role as a pivotal element within Trans- former. Let Xins, Xtest ∈ Rdin denote the instruc- tion tuning sample and the test input respectively. Xins can be likened to IQk and IAk in Equation 3, while Xtest can be seen as T and wA 1 , wA 2 , . . . , wA i−1. That Q = WQX⊤ test be the attention query vector, K = WK[Xins∥Xtest] be the attention key vector and V = WV [Xins∥Xtest] be the attention value vector, where ∥ represents concatenation operation, WK, WV , WQ ∈ Rdout×din are the projection ma- trices for computing the attention queries, keys, and values, respectively. The result of self-attention in an arbitrary layer for a head is formulated as: Attention(K, V, Q) = WV [Xins∥Xtest]Softmax ( WK [Xins∥Xtest]⊤Q √din ) ≈ WV [Xins∥Xtest] (WK [Xins∥Xtest]) ⊤ Q = WV Xtest(WK Xtest)⊤ ︸ ︷︷ ︸ Only test input. Q + WV Xins(WK Xins)⊤ ︸ ︷︷ ︸ Only instruction sample. Q = Wzsl Q + ∆Wiit Q = (Wzsl + ∆Wiit) Q, (6) where √din serves as a scaling factor. The term WV Xtest(WKXtest)⊤ could be denoted as Wzsl, which represents the zero-shot learning scenario where no instruction tuning is performed since it solely focuses on the test input. In addition, the term WV Xins(WKXins)⊤ can be seen as implicit instruction tuning ∆Wiit achieved via the meta- gradient (Dai et al., 2022; Yang et al., 2023) derived from the instruction sample. Readers can refer to previous papers (Dai et al., 2022; Aizerman et al., 1964; Irie et al., 2022) for more details on implicit instruction tuning. 6 Conclusion This paper presents NUGGETS, a method leverag- ing LLMs to discern more pivotal data for instruc- tion tuning. Grounded in one-shot learning, this approach facilitates the identification of examples’ value, enabling efficient data selection without de- pendence on additional annotation and associated costs. Benefiting from NUGGETS, we observe im- proved instruction following abilities even with smaller training subsets. Furthermore, we posit that our method underscores the significance of metic- ulous data selection, offering valuable insights for future instruction fine-tuning endeavors. Limitations Although the efficacy of the proposed approach has been confirmed through empirical experiments, opportunities for refinement persist. One avenue for improvement involves a thorough investigation into the inclusion of a diverse and compact set of predefined tasks during the golden scoring phase. This exploration aims to enhance the efficiency of model evaluation on instructional data, leading to improved identification of high-quality instructions suitable for subsequent model fine-tuning. Sec- ondly, due to resource constraints, the majority of experiments in this study are confined to the LLaMA-7B model. While this model holds sig- nificant influence within the large language model open-source community, comprehensive validation across a broader spectrum of models is imperative to ensure the generalizability of the proposed ap- proach. Lastly, to fortify the empirical foundation of our findings, it is crucial to subject the proposed method to validation on a more extensive array of instructional datasets. This step aims to ascertain the robustness and applicability of the methodol- ogy across a diverse range of instructional contexts, contributing to its broader utility in real-world sce- narios. These outlined avenues for future work are anticipated to refine and extend the scope of our proposed method. Acknowledgments Min Yang was supported by National Key Research and Development Program of China (2022YFF0902100), National Natural Science Foundation of China (62376262), the Natural Science Foundation of Guangdong Province of China (2024A1515030166), Shenzhen Sci- ence and Technology Innovation Program (KQTD20190929172835662), Shenzhen Basic Research Foundation (JCYJ20210324115614039). This work was supported by Alibaba Group through Alibaba Innovative Research Program. Xiaobo Xia was supported by the Australian Research Council project: DE190101473 and Google PhD Fellowship. Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031. References Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta. 2021. Muppet: Massive multi-task representations with pre-finetuning. In EMNLP, pages 5799–5811. MA Aizerman, EM Braverman, and LI Rozonoer. 1964. Theoretical foundation of potential functions method in pattern recognition autom. Remote Contr. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Yihan Cao, Yanbin Kang, and Lichao Sun. 2023. In- struction mining: High-quality instruction data se- lection for large language models. arXiv preprint arXiv:2307.06290. Sahil Chaudhary. 2023. Code alpaca: An instruction- following llama model for code generation. https: //github.com/sahil280114/codealpaca. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srini- vasan, Tianyi Zhou, Heng Huang, et al. 2023. Al- pagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka- plan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Xuxin Cheng, Zhihong Zhu, Hongxiang Li, Yaowei Li, Xianwei Zhuang, and Yuexian Zou. 2024. To- wards multi-intent spoken language understanding via hierarchical attention and optimal transport. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 17844–17852. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open- source chatbot impressing gpt-4 with 90%* chatgpt quality. Hyung Won Chung, Le Hou, Shayne Longpre, Bar- ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. 2022. Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559. Google. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403. Sariel Har-Peled and Soham Mazumdar. 2004. On core- sets for k-means and k-median clustering. In STOC, pages 291–300. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language under- standing. arXiv preprint arXiv:2009.03300. Kazuki Irie, Róbert Csordás, and Jürgen Schmidhuber. 2022. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In ICML, pages 9639– 9659. Yunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. 2023. Exploring the impact of instruction data scaling on large language models: An empirical study on real-world use cases. arXiv preprint arXiv:2303.14742. Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- sch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guil- laume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023a. Camel: Communicative agents for\" mind\" exploration of large scale language model society. In NeurIPS. Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. 2023b. M3it: A large- scale dataset towards multi-modal multilingual in- struction tuning. arXiv preprint arXiv:2306.04387. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2023c. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. Xiaonan Li and Xipeng Qiu. 2023. Finding support examples for in-context learning. In Findings of EMNLP, pages 6219–6235. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023d. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, and Ji rong Wen. 2023. Do emergent abilities exist in quantized large language models: An empirical study. arXiv preprint arXiv:2307.08072. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688. Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun- yang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. # instag: Instruction tagging for analyz- ing supervised fine-tuning of large language models. In ICLR. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi- ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol- instruct. arXiv preprint arXiv:2306.08568. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc- tions with human feedback. In NeurIPS, pages 27730–27744. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal- ley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text trans- former. The Journal of Machine Learning Research, 21(1):5485–5551. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization. In ICLR. Rylan Schaeffer, Brando Miranda, and Oluwasanmi Koyejo. 2023. Are emergent abilities of large language models a mirage? arXiv preprint arXiv:2304.15004. Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. 2023. On the exploitability of instruction tuning. arXiv preprint arXiv:2306.17194. Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-driven self- alignment of language models from scratch with minimal human supervision. arXiv preprint arXiv:2305.03047. Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Mvp: Multi-task supervised pre-training for natural language generation. arXiv preprint arXiv:2206.12131. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and effi- cient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023a. Label words are anchors: An information flow perspective for understanding in-context learning. In EMNLP. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. 2023b. How far can camels go? exploring the state of instruction tuning on open re- sources. arXiv preprint arXiv:2306.04751. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al- isa Liu, Noah A Smith, Daniel Khashabi, and Han- naneh Hajishirzi. 2022a. Self-instruct: Aligning lan- guage model with self generated instructions. arXiv preprint arXiv:2212.10560. Yizhong Wang, Swaroop Mishra, Pegah Alipoormo- labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, An- jana Arunkumar, David Stap, et al. 2022b. Super- naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In EMNLP, pages 5085–5109. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In ICLR. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raf- fel, Barret Zoph, Sebastian Borgeaud, Dani Yo- gatama, Maarten Bosma, Denny Zhou, Donald Met- zler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120. Xiaobo Xia, Jiale Liu, Jun Yu, Xu Shen, Bo Han, and Tongliang Liu. 2023a. Moderate coreset: A universal method of data selection for real-world data-efficient deep learning. In ICLR. Xiaobo Xia, Jiale Liu, Shaokun Zhang, Qingyun Wu, and Tongliang Liu. 2023b. Coreset selection with prioritized multiple objectives. arXiv preprint arXiv:2311.08675. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a. Wizardlm: Empowering large lan- guage models to follow complex instructions. arXiv preprint arXiv:2304.12244. Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023b. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196. Jiaxi Yang, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, and Yongbin Li. 2023. Iterative forward tuning boosts in-context learning in language models. arXiv preprint arXiv:2305.13016. Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling- Hao Chen, Jiale Liu, Qingyun Wu, and Tongliang Liu. 2024. Ideal: Influence-driven selective annota- tions empower in-context learners in large language models. In ICLR. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian- wei Zhang, Fei Wu, et al. 2023a. Instruction tuning for large language models: A survey. arXiv preprint arXiv:2308.10792. Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. 2023b. Gpt4roi: Instruction tuning large lan- guage model on region-of-interest. arXiv preprint arXiv:2307.03601. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023c. Siren’s song in the ai ocean: A survey on hallucination in large language models. arXiv preprint arXiv:2309.01219. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023a. A survey of large language models. arXiv preprint arXiv:2303.18223. Yingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Fei Huang, Yongbin Li, and Nevin L Zhang. 2023b. A preliminary study of the intrinsic relationship be- tween complexity and alignment. arXiv preprint arXiv:2308.05696. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206. A The Distribution of Golden Score As shown in Figure 5, in a total of 52,002 cases, there are 9,549 instructions with a gold score of less than 0.5, indicating that these data have a side effect on overall task completion. Besides, there are 7,524 instructions with a gold score greater than 0.8, suggesting that the model improves the task completion rate through one-shot learning from these data, which can be considered high-quality instruction data. 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 10000 20000 30000 40000 50000 9,549 44,478 51,383Golden Score (GS) 17,289 28,537 Figure 5: The distribution of the golden score for the Alpaca instruction dataset. B Experiment on Other Instruction Sets Based on the LLaMA-7B model, we conducted experiments on several other instruction datasets, further validating the effectiveness of our NUGGETS method. B.1 Code Alpaca The Code Alpaca instruction dataset (Chaudhary, 2023) is designed to develop large language models capable of following instructions and generating code. Leveraging self-instruct (Wang et al., 2022a) technology, it has produced 20,000 examples of instruction data. We use HumanEval (Chen et al., 2021) as a benchmark to evaluate the model’s code generation capabilities. It is used to measure functional correctness for synthesizing programs from docstrings. It consists of 164 original programming problems, assessing language comprehension, algorithms, and simple mathematics, with some being comparable to simple software interview questions. We adopt the approach outlined by Chen et al. (2021) to calculate pass rates at k values of 1, 10, and 100 for each problem. Essentially, pass@1 predicts the probability of a model producing a correct solution on the first try, while pass@10 and pass@100 predict the probability of achieving a correct solution within 10 and 100 tries, respectively. We generate 200 completions at a temperature setting of 0.2 (Luo et al., 2023) to estimate pass@1, pass@10, and pass@100 rates. The experimental results are shown in the Figure 6. Out of 20,000 instructions, 4,715 instructions have a gold score greater than 0.85, achieving the best pass@1 and pass@10 results in the HumanEval benchmark, superior to the fine-tuning results of the full dataset. Additionally, this experiment also proves that the NUGGETS method can be applied to fine-tuning for specific tasks, demonstrating good transferability. B.2 WizardLM The WizardLM instruction dataset (Xu et al., 2023a), which employs Evol-Instruct to iteratively refine an initial set of instructions into more complex ones, contains 70,000 instruction examples. The distribution of the golden scores and the performance of models fine-tuned on corresponding subsets of instructions are shown in Table 5. We can observe that the quality distribution of the WizardLM dataset is relatively balanced, with 65,190 instruction examples having a golden score greater than 0.8, accounting for 93% of 16.8 25.4 26.6 22.1 3.2 12.9 21.321.3 19.9 1.6 GS<=0.5 50k 25k 5k HumanEval 15k 20k 10k 30k 35k 40k 45k 30 15 3 9 12 6 18 21 24 27 0 GS>0.5 GS>0.6 GS>0.7 GS>0.8instructionsFull Data Pass@1 Full Data Pass@10 Full Data Pass@100 Pass@100Pass@100Pass@1 2,150 17,850 11,373 4,715 153 71.39 0.7 14.4 13.7 14.5 7.1 26.8 13.6 19.9 Figure 6: The distribution of the golden score for the Code Alpaca instruction dataset, along with the corresponding fine-tuning results on the HumanEval benchmark. Predefined task sets utilize K-Means to sample 100 examples from the Code Alpaca instruction dataset. GS≤0.5 GS>0.5 GS>0.6 GS>0.7 GS>0.8 GS>0.85 GS>0.86 GS>0.87 Full Data NUM 480 69,520 69,377 68,898 65,190 40,223 23,579 3, 316 70,000 Win_rate 19.42 58.08 57.40 56.21 59.81 58.40 57.81 54.68 57.65 Table 5: The distribution of golden scores for the WizardLM dataset and the evaluation results of models fine-tuned on corresponding score subsets on the Alpaca-Eval benchmark. the total number of instructions. In the evaluation of the Alpaca-Eval benchmark, models fine-tuned on subsets with golden scores greater than 0.8 achieved a win rate of 59.81, outperforming models fine-tuned on the full dataset. B.3 FLANv2 We sampled 50,000 examples from the FLANv2 (Chung et al., 2022) dataset to constitute the instruction tuning data for this experiment. Additionally, the Predefined task set was also derived from these 50,000 examples, using the K-Means algorithm to sample 100 examples. We evaluated the performance of the fine-tuned model using MMLU (Hendrycks et al., 2020) in a 5-shot setting. MMLU is a test designed to measure a text model’s multitask accuracy. The test encompasses 57 tasks, including elementary mathematics, US history, computer science, law, and more. The experimental results are shown in the GS≤0.5 GS>0.5 GS>0.6 GS>0.7 GS>0.8 GS>0.85 GS>0.9 Full Data NUM 1,361 48,639 46,009 39,046 17,037 4,798 321 50,000 Acc 34.68 41.38 41.92 41.87 41.97 35.51 26.41 40.45 Table 6: The distribution of golden scores for the sampled FLANv2 dataset and the evaluation results of models fine-tuned on corresponding score subsets on the MMLU benchmark. table. It can be observed that the model fine-tuned with examples having a golden score greater than 0.8 (totaling 17,037 examples) achieved the best results, followed by those fine-tuned with examples having a golden score greater than 0.6 (totaling 46,009 examples). Additionally, we noted that the model fine-tuned with examples having a golden score less than 0.5 (totaling 1,361 examples) outperformed the model fine-tuned with examples having a golden score greater than 0.9 (totaling 321 examples). We speculate that the reason for this outcome is that the MMLU evaluation itself is a knowledge- intensive assessment, which thus imposes higher demands on the quantity of fine-tuning examples. In the future, some optimizations in the selection of the Predefined Task Set may potentially improve this situation. C Case Study To evaluate NUGGETS qualitatively, we also select some instruction examples from the Alpaca dataset for case studies, as shown in Figure 7. We observe that noisy instructions without outputs display extremely low golden score values. Such examples consistently score very low throughout the instruction dataset. Additionally, instructions with hallucinatory outputs and some that are a bit confusing also typically exhibit lower golden scores. In contrast, instructions with higher scores often possess fluent language and meticulous logic, and are oriented towards assisting humans in problem-solving. Therefore, we believe that the high golden scores can represent high-quality instruction tuning data. [ Instruction ] Create a pictogram to show the meaning of the following phrase: \"The wheels on the bus go round and round.\" [Picture of a bus with circles around its wheels][ Output ] [ Golden Score ] 0.193 [ Instruction ] Write six original rhymes for the provided word. 1. To ride a skate is where I'd like to go,It zooms past skiers, 'cause I'm so pro. 2. The skateboarding ramp seemed so tall,But I made it over with no stall. 3. I take a ride on my trusty skate,And carve the roads with a twist of fate. 4. I spin around the rink so full,Check me out, I'm lookin' cool. 5. You don't believe I can skate on the sand,But watch me do it with a little hand! 6. Skateboard's my name and it's my game, […] [ Output ] [ Golden Score ] 0.403 [ Input ] Skate [ Instruction ] Construct a timeline to show the development of artificial intelligence over the last 5 years. \" \"[ Output ] [ Golden Score ] 0.046 [ Instruction ] What are three qualities that a good product manager should possess ? A good product manager should possess strong communication skills, a strategic mindset, and the ability to collaborate with different members of the organization. [ Output ] [ Golden Score ] 0.803 [ Instruction ] Create a fact sheet about AI. What is AI? AI (Artificial Intelligence) is the science and engineering of programming machines to think and act on their own. What can AI do? AI can be used to automate mundane tasks, analyze complex data and patterns, identify trends, provide personalized services, and more. Who uses AI? AI is used by many industries, […] [ Output ] [ Golden Score ] 0.803 [ Instruction ] Describe the following place. Output the description. [ Output ] [ Golden Score ] [ Input ] The Grand Canyon The Grand Canyon is one of the most incredible natural wonders in the world. The canyon spans 277 miles in length and can be up to 18 miles wide in certain places. With towering cliff walls, evergreen forests, and the mighty Colorado River running its length, it truly is an awe- inspiring sight. The area around the canyon is home to diverse wildlife, including endangered species like the California condor and the desert tortoise. Visiting the Grand Canyon is an unforgettable experience that is sure to take your breath away. 0.859 Figure 7: Examples of instructions and their corresponding golden scores.","libVersion":"0.3.2","langs":""}