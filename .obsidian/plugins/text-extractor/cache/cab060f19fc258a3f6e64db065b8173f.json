{"path":"Pasted image 20241018094207.png","text":"40 = Alpoce-GPT Dotaset e WizardLM 70k Dataset 30 %20 & AT TR Y o S d b GPT-2 GPT2-large GPT-XL GPT-NEO LLAMA2-7B 1.50 1.25 g 1.00 ; 0.75 - 0.50 0.25 0.00 GPT-2 GPT2-large GPT-XL GPT-NEO LLAMA2-7B Figure 3: The distributions of perplexity (top) and IEB score (bottom) computed by five models (left-to-right: weak-to-strong) on three instruction tuning datasets. Observations: (1) The scale of perplexity varies drastically across different models, indicating their difference in generation capability; (2) The scale of IFD scores is consistent across models, indicating their consistency in measuring difficulties.","libVersion":"0.3.2","langs":"eng"}