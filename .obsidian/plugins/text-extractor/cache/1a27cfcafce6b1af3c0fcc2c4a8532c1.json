{"path":"SJTU/Data Curation - Compression - Efficiency - Filtering - Distillation/images/image 13.png","text":"2.2. Coreset selection Coreset selection has been actively explored for com- pressing datasets, which aims to select a subset of the most representative samples out of the target dataset. The previous methods have proposed different selection crite- ria: geometry-based [9, 2, 45, 47], uncertainty-based [11], error-based [51, 42], decision-boundary-based [19, 39], gradient-matching [40, 29], bilevel optimization [30] and submodularity-based methods [27]. Among them, the Con- textual Diversity (CD) [2], Herding [58], and k-Center Greedy [45] try to remove the redundant samples based on their similarity to the remaining samples. Cal [39] and Deepfool [19] argue that the coreset should be selected based on their difficulties for learning. Craig [40] and GradMatch [29] try to find an optimal coreset that has the similar gradient values with the whole dataset when training them on a network. Glister [30] introduce a validation set to maximize the log-likelihood with the whole dataset, where involves a time-consuming bilevel optimization. FL [27] and Graph Cut (GC) [27] consider the diversity and infor- mation simultaneously.","libVersion":"0.3.2","langs":"eng"}