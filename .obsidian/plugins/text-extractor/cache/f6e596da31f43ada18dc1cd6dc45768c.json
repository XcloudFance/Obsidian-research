{"path":"GenAIUnleaning/DiffusionUnlearning/2024_Vnew/Score Forgetting.pdf","text":"Score Forgetting Distillation: A Swift, Data-Free Method for Machine Unlearning in Diffusion Models Tianqi Chen, Shujian Zhang, and Mingyuan Zhou The University of Texas at Austin Abstract The machine learning community is increasingly recognizing the importance of fostering trust and safety in modern generative AI (GenAI) models. We posit machine unlearning (MU) as a crucial foundation for developing safe, secure, and trustworthy GenAI models. Traditional MU methods often rely on stringent assumptions and require access to real data. This paper introduces Score Forgetting Distillation (SFD), an innovative MU approach that promotes the forgetting of undesirable information in diffusion models by aligning the conditional scores of “unsafe” classes or concepts with those of “safe” ones. To eliminate the need for real data, our SFD framework incorporates a score-based MU loss into the score distillation objective of a pretrained diffusion model. This serves as a regularization term that preserves desired generation capabilities while enabling the production of synthetic data through a one-step generator. Our experiments on pretrained label-conditional and text-to-image diffusion models demonstrate that our method effectively accelerates the forgetting of target classes or concepts during generation, while preserving the quality of other classes or concepts. This unlearned and distilled diffusion not only pioneers a novel concept in MU but also accelerates the generation speed of diffusion models. Our experiments and studies on a range of diffusion models and datasets confirm that our approach is generalizable, effective, and advantageous for MU in diffusion models. Warning: This paper contains sexually explicit imagery, discussions of pornography, racially-charged terminology, and other content that some readers may find disturbing, distressing, and/or offensive. (a) Brad Pitt (b) Angelina Jolie Figure 1: Celebrity forgetting effects of two celebrities, i.e., “Brad Pitt” and “Angelina Jolie.” Each column represents the images generated from the same text prompt on the top and the same random seed (initial noise) by SFD checkpoints at 0,5,10,25,50,100 thousands images (#kimgs) seen. 1arXiv:2409.11219v2 [cs.CV] 8 Oct 2024 1 Introduction Diffusion models, also known as score-based generative models [Sohl-Dickstein et al., 2015, Song and Ermon, 2019, Ho et al., 2020, Dhariwal and Nichol, 2021, Karras et al., 2022], have emerged as the leading choice for generative modeling of high-dimensional data. These models are widely celebrated for their ability to produce high-quality, diverse, and photorealistic images [Nichol et al., 2022, Ramesh et al., 2022, Saharia et al., 2022, Rombach et al., 2022, Podell et al., 2024, Zheng et al., 2024]. However, their capacity to memorize and reproduce specific images and concepts from training datasets raises significant privacy and safety concerns. Moreover, they are susceptible to poisoning attacks, enabling the generation of targeted images with embedded triggers, posing substantial security risks [Rando et al., 2022, Chen et al., 2023b]. To address these challenges, we introduce Score Forgetting Distillation (SFD), a novel framework designed to efficiently mitigate the influence of specific characteristics in data points on pre-trained diffusion models. This framework is a key part of the broader domain of Machine Unlearning (MU), which has evolved significantly to address core issues in trustworthy machine learning [Lowd and Meek, 2005, Narayanan and Shmatikov, 2008, Abadi et al., 2016]. Originating from compliance needs with data protection regulations such as the “right to be forgotten” [Hoofnagle et al., 2019], MU has broadened its scope to include applications in diffusion modeling across various domains like computer vision and content generation [Gandikota et al., 2023, Fan et al., 2024, Heng and Soh, 2024]. Additionally, MU aims to promote model fairness [Oesterling et al., 2024], refine pre-training methodologies [Jain et al., 2023, Jia et al., 2023], and reduce the generation of inappropriate content [Gandikota et al., 2023]. The development of SFD is aligned with these objectives, providing a strategic approach to mitigate the potential risks and reduce the high generation costs associated with diffusion models, thereby advancing the field of trustworthy machine learning. MU methods are generally categorized into two types: exact MU and approximate MU. Exact MU entails creating a model that behaves as if sensitive data had never been part of the training set [Cao and Yang, 2015, Bourtoule et al., 2021]. This process requires the unlearned model to be identical in distribution to a model retrained without the sensitive data, both in terms of model weights and output behavior. In contrast, approximate MU does not seek an exact match between the unlearned model and a retrained model. Instead, it aims to approximate how closely the output distributions of the two models align after the unlearning process. A prominent strategy in approximate MU utilizes the principles of differential privacy [Dwork, 2006]. For instance, Guo et al. [2019] introduced a certified removal technique that prevents adversaries from extracting information about removed training data, offering a theoretical guarantee of data privacy. However, these approaches typically necessitate retraining the model from scratch, which can be computationally intensive and require access to the original training dataset. Efficient and stable unlearning has become crucial in MU. Techniques like the influence functions [Warnecke et al., 2021, Izzo et al., 2021], selective forgetting [Golatkar et al., 2020], weight-based pruning [Liu et al., 2024], and gradient-based saliency [Fan et al., 2024] have been explored, though they often suffer from performance degradation or restrictive assumptions [Becker and Liebig, 2022]. These methods are primarily applied to MU for image classification tasks and do not adequately address the rapid forgetting and unlearning required for data generation tasks. Given the prominence of diffusion models, there is a growing need to develop MU techniques that specifically cater to these models, ensuring efficient unlearning while maintaining generation capabilities [Gandikota et al., 2023, Fan et al., 2024, Heng and Soh, 2024]. Our SFD framework efficiently distills the knowledge from a pre-trained diffusion model by optimizing two learnable modules—a generator network and a score network—guided by the frozen pre-trained model itself. The score network is trained to optimize the score associated with the generator by minimizing a score distillation loss, which aims to match the conditional scores of the class to forget and the classes to remember with those of the pre-trained model. The generator network learns to produce examples that are “indistinguishable” by the pre-trained score network and fake score network in terms of score predictions, utilizing a model-based cross-class score distillation loss. This dual functionality facilitates both MU and rapid sampling, effectively bridging the gap in generation speed between diffusion-based models and one-step counterparts such as GANs and VAEs. The forgetting 2 process is seamlessly integrated into the model distillation, where we concurrently optimize the score-matching loss and the forgetting loss. This integrated approach offers a robust framework for achieving effective unlearning and fast generation, thereby providing a comprehensive solution for enhancing the efficiency and trustworthiness of diffusion-based generative modeling. Our approach’s effectiveness is demonstrated through both class and concept forgetting tasks for diffusion models in image generation. The experiments conducted on class-conditional diffusion models pretrained on CIFAR-10 and STL-10 demonstrate that SFD effectively erases the target class while preserving the image generation quality for other classes. We also present extensive ablation studies that support the robustness and efficiency of our method, which achieves competitive performance on the key metric for class forgetting, namely Unlearning Accuracy (UA), and significantly improves several metrics for preserving generative quality and efficiency, including Fréchet Inception Distance (FID), Inception Score (IS), Precision and Recall, and generation speed measured by the number of function evaluations (NFEs). Additionally, experiments conducted on Stable Diffusion reveal that SFD successfully erases concepts associated with specific text inputs. Our method outperforms the baselines in both celebrity forgetting and NSFW-concept forgetting tasks. Moreover, because our method operates in a completely data-free manner, it significantly reduces the privacy risks associated with the MU fine-tuning process. The development of SFD benefits from related works on MU, distribution matching, score matching, acceleration methods for diffusion sampling, and data-free diffusion distillation. A detailed review of these topics is provided in Appendix A. Our key contributions are: • Introducing SFD, a pioneering data-free approach for MU that utilizes cross-class score distillation in diffusion models to achieve not only effective forgetting but also fast one-step generation. • Developing a robust and efficient technique to distill score-based generative models into one-step generators, incorporating the MU loss as a regularization element within the model-based score distillation framework to optimize both distillation and forgetting simultaneously. • Validating the effectiveness of our method with experiments on not only class-conditional diffusion models based on DDPM and EDM, but also text-to-image diffusion models based on Stable Diffusion, marking the first instance of accelerated forgetting in machine unlearning for diffusion models. This achievement demonstrates the potential of our method for broader applications and sets the stage for future advancements in the field. 2 Method Diffusion models are celebrated for their superior performance in generating high-quality and diverse samples. However, their robust capabilities also introduce challenges, particularly the risk of misuse in generating inappropriate content. This concern highlights the ethical implications and potential negative impacts of their application. Additionally, these models have a significant drawback: slow sampling speeds. This inefficiency becomes particularly problematic in downstream tasks that require finetuning on synthetic data generated by these models. When access to real data is not feasible, the task of preparing a sufficiently large synthetic dataset can already become computationally prohibitive [Yin et al., 2024]. This issue is especially acute in the context of MU and image generation, where access to real data often raises privacy concerns, making reliance on synthetic data crucial. Consequently, the slow sampling rate of diffusion models presents a critical bottleneck, necessitating improvements to enable effective data-free MU operations. In this section, we introduce SFD, a principled and data-free approach designed to address the MU problem while simultaneously achieving fast sampling for diffusion models. Building on recent advancements in data-free diffusion distillation for one-step generation [Luo et al., 2023, Zhou et al., 2024b], we conceptualize MU in diffusion models as a problem of MU-regularized score distillation. 3 Figure 2: Overview of score forgetting distillation (SFD). Some notations are labeled along with corresponding components. ‘Snowflake’ refers to the frozen (non-trainable), ‘Fire’ refers to the trainable, and ‘Combine’ refers to combining operation on input losses by arithmetic addition according to predefined weights. 2.1 Problem Definition and Notations Before diving into the specific MU problem, we will first establish the essential concepts and notations in diffusion modeling: A diffusion model corrupts its data x ∼ pdata(x | c) during the forward diffusion process at time t as zt = atx + σtϵt, where ϵt ∼ N (0, 1), c represents the given condition such as a label or text, and at and σt are diffusion scheduling parameters. The goal of pretraining a diffusion model is to obtain an optimal score estimator sϕ(zt, c, t) such that sϕ(zt, c, t) = ∇zt ln pdata(zt | c). Let xϕ(zt, c, t) be the optimal conditional mean estimator such that for xϕ(zt, c, t) = E[x | zt, c, t]. Applying Tweedie’s formula [Robbins, 1992, Efron, 2011] in the context of diffusion modeling [Luo, 2022, Chung et al., 2023, Zhou et al., 2024b], the optimal score and conditional mean estimators, sϕ and xϕ, for the training data are related as follows: sϕ(zt, c, t) = atxϕ(zt,c,t)−zt σ2 t , xϕ(zt, c, t) = zt+σ2 t sϕ(zt,c,t) at . (1) With this optimal score estimator, we can construct a corresponding reverse diffusion process, enabling us to approximately sample from the data distribution through numerical discretization along the time horizon [Anderson, 1982, Song et al., 2020]. A distilled one-step diffusion model is a one-step generator capable of producing samples from the generative distribution of a pretrained model in a single step. The generation process for this one-step generator is defined as gθ(n, c), where n ∼ N (0, I). Denote the generative distribution of x given class c as Dθ,c, and the optimal score estimator corresponding to the one-step generator gθ as sψ∗(θ)(zt, c, t). The same as how xϕ and sϕ is related in Eq. 1, we have sψ∗(θ)(zt, c, t) = atxψ∗ (θ)(zt,c,t)−zt σ2 t . (2) For class forgetting in class-conditional diffusion models, our goal is to unlearn a specific class by overriding it with another class while minimizing any negative impact on the remaining classes. We denote the class to forget as cf , the remaining classes (classes other than cf ) as Cr := {cr | cr ̸= cf }, and the class for overriding cf as co ∈ Cr. The distribution of the remaining classes is denoted as Dr over the set Cr, the sampling distribution of all classes after unlearning as Ds, and the conditional distribution of samples from class c 4 generated by gθ as Dθ,c := gθ(N (0, I), c). The class forgetting problem can be solved by aligning the model distribution of x given cf under the generator gθ with the original data distribution of x given co, and by simultaneously ensuring that the distributions of x given cr under both the model and the original data are matched. Specifically, our objective is to forget cf and override it with co by aligning the distributions such that Dθ,cf d = pdata(x | co), while preserving the remaining classes by ensuring Dθ,cr d = pdata(x | cr), ∀cr ∈ Cr. In the problem setting of concept forgetting in text-to-image diffusion models, our goal is to unlearn the concepts associated with specific keywords, such as “Brad Pitt,” by substituting them with more generic terms like “a middle aged man,” as illustrated in Figure 1. This process aims to minimize any negative impact on the generation quality of other concepts, thereby maintaining the overall integrity and diversity of the images generated under text guidance. 2.2 Score Forgetting Distillation In the problem of class unlearning, as described in Section 2.1, our goal is to align the conditional distributions of both the forgetting class and the remaining classes with those that would exist if the model had been retrained without the data from the forgetting class. By adapting the concept of data-free score distillation to the MU challenge, we aim to achieve this alignment using our proposed data-free MU process, SFD. Our method eliminates the need for access to the original training data and accelerates synthetic data sampling, effectively enabling the forgetting of a specific class while preserving the original generative capabilities for the other classes. Specifically, for two arbitrary classes c1 and c2, we define a Score Forgetting Distillation (SFD) loss over the forward diffusion process of one-step generated fake data. The following analysis also applies when c1 and c2 refer to concepts. We denote zt, t, x ∼ Dθ,c as a random sample generated as zt = atx + σtϵt, ϵt ∼ N (0, I), t ∼ Unif[tmin, tmax], x = gθ(n, c), n ∼ N (0, I). Taking the expectation over fake data generated by the distilled one-step generation model gθ under class c2 and subsequently corrupted through the forward diffusion process, we formulate this loss as: Lsfd(θ; ϕ, c1, c2) = Ezt,t,x∼Dθ,c2 [ ωt∥sϕ(zt, c1, t) − sψ∗(θ)(zt, c2, t)∥ 2] , (3) where ωt > 0 is a re-weighting function, and ψ∗(θ) represents the optimal solution to the model-based explicit SM (MESM) loss, which can be expressed as Lmesm(ψ; θ, c) = Ezt,t,x∼Dθ,c [ γt∥sψ(zt, c) − ∇x ln pθ(zt | c)∥2 2] , (4) where γt > 0 is a re-weighting function. In practice, the lack of the access to ∇x ln pθ(zt | c) makes Eq. 4 intractable. However, we can alternatively optimize a denoising SM loss [Vincent, 2011] as Ldsm(ψ; θ, c) = Ezt,t,x∼Dθ,c [γt a 2 t σ4 t ∥xψ(zt, c) − x∥ 2 2] , (5) which admits the same optimal solution as Eq. 4 and provides an estimation of the score of the generator gθ at different noise levels. This setup allows us to tailor the SFD loss in Eq. 3 specifically for different class dynamics. When c1 = c2 = c, the SFD loss facilitates class-specific score distillation, optimizing the score to closely model that of the generator within the same class. Conversely, setting c1 ̸= c2 configures the SFD loss for score overriding, replacing the score sψ∗(θ) for class c2 with the score sϕ for class c1. This approach effectively addresses the dual objectives of class forgetting and targeted score modification, introducing two distinct losses to manage these scenarios: • Distillation Loss: Enhances fidelity within a class by refining the generator’s score to closely match the true distribution of the class: Lsfd(θ; ϕ, cr, cr) = Ezt,t,x∼Dθ,cr (ωt∥sϕ(zt, cr, t) − sψ∗(θ)(zt, cr, t)∥ 2) . (6) 5 • Forgetting Loss: Alters the generator’s score to reflect characteristics of a different class, facilitating the effective forgetting of the original class attributes: Lsfd(θ; ϕ, co, cf ) = Ezt,t,x∼Dθ,cf (ωt∥sϕ(zt, co, t) − sψ∗(θ)(zt, cf , t)∥2) . (7) To summarize our approach, we now present the entire formulation as follows: min θ Ecr∼Cr Lsfd(θ; ϕ, cr, cr), s.t. ψ∗(θ) = arg min ψ Ec∼CsLdsm(ψ; θ, c), Lsfd(θ; ϕ, co, cf ) ≤ C0. This formulation corresponds to a bi-level optimization problem [Ye et al., 1997, Hong et al., 2023, Shen et al., 2023], subject to an additional forgetting-based constraint. Solving this problem directly is challenging, so we initially relax the constraint specified by Lsfd in the above equation by integrating it into the distillation objective as an additional MU regularization term: min θ Ecr∼Cr λLsfd(θ; ϕ, cr, cr) + µLsfd(θ; ϕ, co, cf ), s.t. ψ∗(θ) = arg min ψ Ec∼Cs Ldsm(ψ; θ, c), where λ and µ are tunable constants that serve as control knobs to balance the distillation of the remaining classes and the unlearning of the target class. Furthermore, we implement an alternating update strategy between θ and ψ. This approach mitigates the need to obtain the optimal score estimator ψ∗(θ) for each θ, simplifying the computational process. We outline a practical implementation of this strategy in Algorithm 1. Specifically, generalizing the derivation in Zhou et al. [2024b], we have the following Lemma, whose proof is provided in Appendix D: Lemma 1. The Score Forgetting Distillation (SFD) loss in Eq. 3 can be equivalently expressed as Lsfd(θ; ϕ, c1, c2) = Ezt,t,x∼Dθ,c2 [ωt a 2 t σ4 t (xϕ(zt, c1, t) − xψ∗(θ)(zt, c2, t))T (xϕ(zt, c1, t) − x)] . (8) A biased loss for θ can be derived by replacing ψ∗(θ) in either Eq. 3 or Eq. 8 with its SGD-based approximation ψ, and disregarding the dependency of ψ∗ on θ when computing the gradient of θ. Empirical experiments by Zhou et al. [2024b] suggest that in the context of diffusion distillation without involving unlearning, Eq. 8 can be effective independently, while Eq. 3 may not perform as expected. This observation leads to a practical approach that involves subtracting Eq. 3 from Eq. 8. This strategy aims to sidestep detrimental biased gradient directions and potentially compensate for the overlooked gradient dependency of ψ∗(θ). We implement this approach in practice under the framework of SFD, defining the loss used in practice as follows: ˆLsfd(θ, ψ; ϕ, c1, c2, α) = (1 − α)ωt a 2 t σ4 t ∥xϕ(zt, c1, t) − xψ(zt, c2, t)∥2+ (9) ωt a 2 t σ4 t (xϕ(zt, c1, t) − xψ(zt, c2, t))T (xψ(zt, c2, t) − x), (10) where α ≥ 0 is some constant that typically set as 1 or 1.2, zt = atx + σtϵt, x ∼ Dθ,c2 , ϵt ∼ N (0, I), t ∼ Unif[tmin, tmax]. In this paper, we follow Yin et al. [2024] and Zhou et al. [2024b] to set ωt = σ4 t a2 t C ∥xϕ(zt,t,c)−x∥1,sg , where C is the data dimension and “sg” stands for stop gradient. Similar to Eqs. 6 and 7, we have the following: Distillation Loss: ˆLsfd(θ, ψ; ϕ, cr, cr, α), where zt, t, x ∼ Dθ,cr (11) Forgetting Loss: ˆLsfd(θ, ψ; ϕ, co, cf , α), where zt, t, x ∼ Dθ,cf (12) where timestep t is omitted for brevity. Intuitively speaking, our algorithm first trains the approximate score estimator sψ to mimic the score of the generator gθ at different time points t of the forward diffusion process, and then uses both the pre-trained score estimator and the fake score estimator across these time points 6 to instruct the generator itself. The alternate updating approach largely reduces the computational cost of obtaining an optimal score estimator for the generator while effectively passing an informative learning signal to the generator and helping the generation quality improve rapidly over time. It is worth noting that the whole training process require neither real data nor fake data synthesized by reversing the full diffusion process, and a pre-trained score network of a diffusion model is sufficient to provide proper supervision on distillation as well as machine unlearning. In other words, our method is data-free. Table 1: Class forgetting results on CIFAR-10 and STL-10. “SFD” refers to the DDPM model trained with Score Forgetting Distillation, while “SFD-CFG” refers to the SFD model trained with classifier-free guidance (as discussed in Section 3.2). UAs that exceed the testing recall rate of the forgetting class (96.60% for CIFAR-10 and 98.15% for STL-10) are highlighted in yellow. Dataset Model UA (↑) FID (↓) IS (↑) Precision (↑) Recall (↑) NFEs (↓) Data-free CIFAR-10 Retrain 98.5 7.94 8.34 0.6418 0.5203 1000 ✘ ESD [Gandikota et al., 2023] 91.21 12.68 9.78 0.7709 0.3848 2000 ✔ SA [Heng and Soh, 2024] 85.80 9.08 - 0.4120 0.7670 2000 ✔ SalUn [Fan et al., 2024] 99.96 11.25 9.41 0.7806 0.3176 2000 ✘ SFD (Ours) 99.64 5.35 9.51 0.6587 0.5471 1 ✔ STL-10 Retrain 97.54 26.52 8.30 0.5573 0.4526 1000 ✘ ESD [Gandikota et al., 2023] 92.01 39.32 10.16 0.5229 0.2898 2000 ✔ SalUn [Fan et al., 2024] 99.31 20.78 10.89 0.5713 0.5415 2000 ✘ SFD (Ours) 99.02 18.82 10.93 0.5543 0.4054 1 ✔ SFD-CFG (Ours) 99.64 15.32 11.46 0.5983 0.3551 1 ✔ 3 Experiments In our experiments, we thoroughly evaluate our method for class forgetting in diffusion models pretrained on two datasets, CIFAR-10 and STL-10, which have been commonly used for evaluating MU in previous studies. We provide the details of them in Appendix B. We also assess our method for concept forgetting tasks, such as celebrity forgetting, in text-to-image diffusion models. Forgetting setups We explore class forgetting in class-conditional image generation tasks using DDPM [Ho et al., 2020], and investigate concept forgetting in text-to-image generation tasks using Stable Diffusion [Rom- bach et al., 2022]. Class forgetting aims to prevent class-conditional diffusion models from generating images of a specified class, while concept forgetting seeks to remove the model’s ability to generate images containing specific concepts, such as celebrities or inappropriate content. Class-conditional and text-to-image sampling are achieved by inputting class labels and text prompts into the respective diffusion models, with fidelity further enhanced by classifier-free guidance introduced in Ho and Salimans [2022]. Specifically, we approach unlearning by overriding a class or concept with another that is safe to retain. The class forgetting experiments were conducted on class-conditional diffusion models pre-trained on CIFAR-10 and STL-10, while the concept forgetting experiments were conducted on Stable Diffusion, including forgetting celebrities, specifically American actor Brad Pitt and actress Angelina Jolie, and forgetting a general NSFW (not safe for work) concept, i.e., nudity. For DDPM baselines, we used the default 1000-step DDPM samplers to obtain FIDs for samples from the remaining classes, while for SD baselines, we used DDIM samplers with 50 steps. In contrast, our method requires only a single step for generation, making it 1,000 times faster than the DDPM baselines and 50 times faster in latent sampling than the SD baselines. Evaluation To quantitatively assess the effectiveness of class forgetting, we primarily focus on the success rate of forgetting the target class, and the generative capability on classes to retain. Specifically, we measure the success rate of forgetting by unlearning accuracy (UA) employing an external classifier trained on the original training set, which is essentially the mis-classification rate of the classifier on the generated samples from the target class. We measure image generation quality using Fréchet Inception Distance (FID) [Heusel et al., 2017] and sample diversity using Inception Score (IS) [Salimans et al., 2016]. Additionally, we report 7 Precision and Recall [Kynkäänniemi et al., 2019], and number of function evaluations (NFEs) for sampling. Following Fan et al. [2024], we compute and report generation quality metrics using generated samples, with the full training set from the remaining classes serving as the reference. For concept forgetting tasks including celebrity forgetting and “nudity” forgetting, we also provide quantitative evaluations as well as qualitative comparison. Specifically, we evaluate celebrity forgetting using a off-the-shelf celebrity face detector, while we assess the MU performance of our “nudity” forgetting model on the I2P benchmark (https://github.com/ml-research/i2p). Please refer to Appendix B.2 for more details of the evaluation metrics. Implementation details Our main implementation of class forgetting experiments is based on DDPM [Ho et al., 2020], where we utilize the codebase developed by Fan et al. [2024]. Additionally, we implement our method using EDM [Karras et al., 2022] framework and the official codebase (https://github.com/NVlabs/edm). For concept forgetting experiments, we implement our method for SD models based on the implementation of Zhou et al. [2024a]. We adopt the same model configuration for both the generator gθ and its score estimation network sψ and initialize the model weights according to the pre-trained score network sϕ. This type of initialization prepares a good starting point for SFD. SFD-Two Stage In addition to initializing both the generator and the fake score network with the pre- trained score network, we also experimented on a different initialization, i.e., initializing the generator with a pre-distilled generator model weights. Considering the nature of “first distilling then forgetting,” we named this variant “SFD-Two Stage.” For this variant specifically, we disabled exponential moving average (EMA) and adopted a more aggressive regularization with λψ = µψ = λθ = µθ = 1.0. The rationale behind this configuration was that the first stage distillation would have prepared a solid foundation for the second stage forgetting, which enables fast forgetting by increasing the weight of forgetting loss and by further prioritizing it in the second stage. We use Adam optimizer with β1 = 0 and β2 = 0.999 for all the experiments. The base learning rate for both DDPM and EDM models is set to 10 −5, except that we slightly increase the learning rate for sψ when distilling DDPM models. More details on the hyperparameter settings for the experiments can be found in Table 8. 3.1 Experimental Results Class forgetting From the empirical results, the proposed method, SFD, can effectively unlearn unwanted content (e.g., a class of objects) and converge rapidly towards the level of generation quality of the pre-trained model. Additionally, the models fine-tuned by SFD inherently enables one step generation. Figure 3 shows that the remaining classes were in fact intact during the MU-regularized distillation, the generation quality of class 1 to 9 were consistently improving as the number of generator-synthesized images, which were used by SFD for distillation and MU, went up. The FID between generated samples and training dataset decreased nearly exponentially fast as is captured by Figure 4. The forgetting class, on the other hand, was initialized to output airplanes and gradually forced to match the assigned class, i.e., the class of automobile. The forgetting effect noticeably took place between 10k and 20k training steps. From Figure 4, we observe a steady increase of unlearning accuracy, reflecting the extent to which the generated Class 0 samples can no longer be correctly identified by the pre-trained image classifier. On CIFAR-10, we observed that the SFD-Two Stage model (or Two Stage, for short), which involves first distilling the pre-trained diffusion model with 50,000 steps and then fine-tuning it using the SFD loss for the same number of steps, exhibited faster forgetting. In Figure 7, we report two performance metrics, FID and UA, during the unlearning stage, compared with the results from SFD. The results indicate that SFD consistently outperforms the two-stage approach in both metrics given sufficient training. Although the two-stage approach started with a lower FID than SFD, its performance fluctuated and declined over time. The UA initially increased rapidly, peaked, and then slightly decreased at the end. The gain in UA during the unlearning stage came at the cost of FID. In contrast, SFD effectively coordinated machine unlearning and 8 Figure 3: Generated images on CIFAR-10 and STL-10 during the training of SFD. The upper panel shows 3 × 3 grids of generated samples at different time steps, with fixed random seeds and class labels arranged from 1 to 9 (left to right, top to bottom). The same sequence of random seeds is used across all grids to ensure consistency. The lower panel illustrates the forgetting process for two examples from CIFAR-10 and STL-10. distillation to forget specific classes while retaining the original generative capability for the remaining classes, thereby improving both FID and UA throughout finetuning and achieving better final results. Nonetheless, the two-stage approach remains practical, especially when forgetting requirements vary over time or when there is an urgent need, as it appears more flexible and efficient under such conditions. Celebrity forgetting We provide both qualitative and quantitative results of celebrity forgetting tasks on two selected celebrities, i.e., Brad Pitt and Angelina Jolie, where the concepts to forget are “bard pitt” and “angelina jolie”, respectively, and the corresponding concepts to override are “a middle aged man” and “a middle aged woman”, respectively. As is shown in Figure 1 and Table 2, we showcase the effectiveness of SFD for forgetting certain concepts in text-to-image diffusion models, such as removing the generative capability of celebrities. “Nudity” forgetting In addition to the celebrity forgetting experiments, we conducted experiments on a broader concept forgetting task, namely, forgetting “nudity” as a concept. We note that “nudity” is a broader concept than individuals (e.g., celebrities) and forgetting “nudity” in general is much more challenging. Therefore, we adopted a slightly different strategy for this task to enhance the forgetting performance. In particular, we first created a list of 12 common human subjects (see Table 5) that can be potentially misused for generating “nudity”-related contents and randomly paired them with one of NSFW keywords (see Table 6) as prompts to forget. We further leveraged the negative prompting technique to match these prompts with their corresponding prompts to override. Specifically, we take the original text prompt as the conditional text input while using the concatenated NSFW keywords instead of an empty string as the unconditional text input. We notice this approach also has a concept forgetting effect on the original score distillation method, which is denoted as “SiD-LSG-Neg.” We report key MU performance metrics in Table 3. Sample images by baselines and SFD are displayed in Figure 6. 9 Figure 4: FID between generated images and original dataset of remaining classes. The solid blue line and dot denote the training FIDs and final FID evaluated at the last checkpoint of one-step SFD generator; the dotted green line marks the initial FID of the pre-trained model using 1,000 sampling steps. The solid orange line and dot mark the training UAs and final UA evaluated at the last checkpoint of SFD; the dotted orange line marks the initial UA of the pre-trained model. Figure 5: Remaining FIDs on different model architectures. The solid blue and solid orange bars denote the remaining FID evaluated for pre-trained DDPM and EDM respectively. The transparent blue and transparent orange bars denote the remaining FID evaluated at the last training step for unlearned and distilled diffusion using DDPM and EDM respec- tively. 3.2 Ablation Studies Ablation on the model architecture EDM [Karras et al., 2022] is a state-of-the-art diffusion model with enhanced capability for generating high-quality images. To evaluate our method’s generalizability across different model architectures, we additionally conduct experiment using the EDM architecture. We adapted the codebase used by SiD [Zhou et al., 2024b] and fine-tuned the pre-trained class-conditional CIFAR-10 EDM-VP model. Figure 5 shows that the FID results of our method can be further improve when based on a more powerful pre-trained model. Abalation on the classifier-free guidance Classifier-free guidance (CFG), first proposed by Ho and Salimans [2021], is a commonly-used strategy for conditional sampling. While typically adopted during inference to enhance class fidelity, it has also been shown to be useful for the training of score-based distillation [Yin et al., 2024, Zhou et al., 2024a]. We compare our models trained with and without CFG in Table 4. In Table 2: Quantitative results of celebrity forgetting of two celebrities, i.e., “Brad Pitt” and “Angelina Jolie.” Bold values indicate the best score in each column, while underlined values represent the second-best. Model Brad Pitt Angelina Jolie Prop. GCD (↓) Prop. GCD (↓) w/o Faces (↓) w/o Faces (↓) SD v1.4 [Rombach et al., 2022] 10.4% 60.6% 11.7% 73.8% SLD Medium [Schramowski et al., 2023] 14.1% 0.47% 11.9% 3.29% ESD-x [Gandikota et al., 2023] 34.7% 2.01% 32.6% 3.35% SA [Heng and Soh, 2024] 5.8% 7.52% 4.4% 7.74% SFD-Two Stage (Ours) 1.76% 2.5% 1.92% 1.06% 10 Table 3: Quantitative results of “nudity” forgetting. Bold values indicate the best score in each column, while underlined values represent the second-best. Model Inapprop. Prob. (↓) Max. Exp. Inapprop. (↓) SD v1.4 [Rombach et al., 2022] 28.54% 86.6% SiD-LSG [Zhou et al., 2024a] 26.86% 88.12% SiD-LSG-Neg (Ours) 20.97% 81.64% SLD Medium [Schramowski et al., 2023] 14.10% 71.73% ESD-u [Gandikota et al., 2023] 16.94 % 69.68% SFD-Two Stage (Ours) 11.03% 66.90% our experiments on STL-10, we found that including classifier-free guidance during training improved the performance in terms of both FID and UA. However, we did not observe such improvements on the CIFAR-10 dataset; on the contrary, we noticed a degradation in the evaluation metrics. We speculate that the influence of CFG may be tied to the inter-class differences: when training data contain classes sharing similar features, such as automobile and truck in CIFAR-10, training with CFG may not be as beneficial as it is when the training dataset consists of more distinct classes. Table 4: Ablation study on classifier-free guidance during training and on the CIFAR-10 and STL-10 datasets. The percentages in green and red are the relative performance boost and degradation respectively when the model is trained without classifier-free guidance. Model FID (↓) UA (↑) SFD 5.35 99.64% + CFG 7.27 (+35.89%)0 99.62% (-0.02%) (a) CIFAR-10 Model FID (↓) UA (↑) SFD 18.82 99.02% + CFG 15.32 (-18.60%) 99.64% (+0.63%) (b) STL-10 4 Conclusion Our work demonstrates the benefits of the proposed score forgetting distillation (SFD), which achieves accelerated forgetting with score-based distillation, providing a unified and effective solution to diffusion-based generative modeling and machine unlearning. The generator trained by our method produces high-quality images of desired classes with a single step while the target class is effectively forgotten. Our experiments show that the proposed strategy attains noticeable gains in performance on both CIFAR-10 and STL-10. We further conduct the detailed study with the SFD in different settings, e.g., comparing SFD against baselines as well as different configurations of SFD in terms of UA, FID, and other metrics. Additionally, we provide qualitative results of concept forgetting for text-to-image diffusion models like SD. To summarize, the forgetting method is effective and general, with the potential to be incorporated into existing models, such as text-to-image diffusion models. 11 Figure 6: Generated images using different text-to-image diffusion models. The prompts used for generation follow the general form, “A photo of a <nudity keyword> <human subject>.” Sensitive body parts were manually censored after image generation. 12 References Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12 (3):313–326, 1982. ISSN 0304-4149. doi: https://doi.org/10.1016/0304-4149(82)90051-5. URL https://www. sciencedirect.com/science/article/pii/0304414982900515. Jacob Austin, Daniel Johnson, Jonathan Ho, Danny Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. arXiv preprint arXiv:2107.03006, 2021. Alexander Becker and Thomas Liebig. Evaluating machine unlearning via epistemic uncertainty. arXiv preprint arXiv:2208.10836, 2022. Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141–159. IEEE, 2021. Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy, pages 463–480. IEEE, 2015. Tianshi Che, Yang Zhou, Zijie Zhang, Lingjuan Lyu, Ji Liu, Da Yan, Dejing Dou, and Jun Huan. Fast federated machine unlearning with nonlinear functional theory. In International conference on machine learning, pages 4241–4268. PMLR, 2023. Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang. Graph unlearning. In Proceedings of the 2022 ACM SIGSAC conference on computer and communications security, pages 499–513, 2022. Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang. Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7766–7775, 2023a. Tianqi Chen and Mingyuan Zhou. Learning to Jump: Thinning and thickening latent counts for generative modeling. In ICML 2023: International Conference on Machine Learning, July 2023. URL http://arxiv.org/abs/2305.18375. Weixin Chen, Dawn Song, and Bo Li. Trojdiff: Trojan attacks on diffusion models with diverse targets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4035–4044, 2023b. Eli Chien, Chao Pan, and Olgica Milenkovic. Efficient model updates for approximate unlearning of graph-structured data. In The Eleventh International Conference on Learning Representations, 2022. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffusion posterior sampling for general noisy inverse problems. In International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=OnD9zGAGT0k. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215–223. JMLR Workshop and Conference Proceedings, 2011. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. Advances in Neural Information Processing Systems, 34:8780–8794, 2021. Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and programming, pages 1–12. Springer, 2006. 13 Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical Association, 106(496): 1602–1614, 2011. Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, and Sijia Liu. Salun: Empowering machine un- learning via gradient-based weight saliency in both image classification and generation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=gn0mIhQGNM. Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2426–2436, 2023. Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data deletion in machine learning. Advances in neural information processing systems, 32, 2019. Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304–9312, 2020. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, pages 2672–2680, 2014. Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. arXiv preprint arXiv:1911.03030, 2019. Anisa Halimi, Swanand Kadhe, Ambrish Rawat, and Nathalie Baracaldo. Federated unlearning: How to efficiently erase a client in fl? arXiv preprint arXiv:2207.05521, 2022. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. Alvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep generative models. Advances in Neural Information Processing Systems, 36, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural Information Processing Systems, pages 6626–6637, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems, 33, 2020. Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A two-timescale stochastic algorithm framework for bilevel optimization: Complexity analysis and application to actor-critic. SIAM Journal on Optimization, 33(1): 147–180, 2023. Chris Jay Hoofnagle, Bart Van Der Sloot, and Frederik Zuiderveen Borgesius. The european union general data protection regulation: what it is and what it means. Information & Communications Technology Law, 28(1):65–98, 2019. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454–12465, 2021. Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion from machine learning models. In International Conference on Artificial Intelligence and Statistics, pages 2008–2016. PMLR, 2021. 14 Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, and Aleksander Mądry. A data-based perspective on transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3613–3622, 2023. Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. arXiv preprint arXiv:2210.01504, 2022. Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. Model sparsification can simplify machine unlearning. arXiv preprint arXiv:2304.04934, 2023. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=k7FuTOWMOc7. Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013. Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22691–22702, 2023. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019. Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, PRANAY SHARMA, Sijia Liu, et al. Model sparsity can simplify machine unlearning. Advances in Neural Information Processing Systems, 36, 2024. Daniel Lowd and Christopher Meek. Adversarial learning. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 641–647, 2005. Calvin Luo. Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970, 2022. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-Instruct: A universal approach for transferring knowledge from pre-trained diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=MLIs5iRq4w. Arvind Narayanan and Vitaly Shmatikov. Robust de-anonymization of large sparse datasets. In 2008 IEEE Symposium on Security and Privacy (sp 2008), pages 111–125. IEEE, 2008. Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In Algorithmic Learning Theory, pages 931–962. PMLR, 2021. Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022. Thuan Hoang Nguyen and Anh Tran. SwiftBrush: One-step text-to-image diffusion model with variational score distillation. arXiv preprint arXiv:2312.05239, 2023. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In International Conference on Machine Learning, pages 16784–16804. PMLR, 2022. Alex Oesterling, Jiaqi Ma, Flavio Calmon, and Himabindu Lakkaraju. Fair machine unlearning: Data removal while mitigating disparities. In International Conference on Artificial Intelligence and Statistics, pages 3736–3744. PMLR, 2024. 15 Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=di52zR8xgf. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id= FjNys5c7VyY. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramèr. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610, 2022. Herbert E Robbins. An empirical Bayes approach to statistics. In Breakthroughs in Statistics: Foundations and basic theory, pages 388–394. Springer, 1992. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=TIdIXIpzhoI. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, pages 2234–2242, 2016. Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22522–22531, 2023. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:18075–18086, 2021. Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and Ben Y Zhao. Glaze: Protecting artists from style mimicry by {Text-to-Image} models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 2187–2204, 2023. Han Shen, Quan Xiao, and Tianyi Chen. On penalty-based bilevel gradient descent method. arXiv preprint arXiv:2302.05185, 2023. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015. Liwei Song and Prateek Mittal. Systematic evaluation of privacy risks of machine learning models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2615–2632, 2021. Yang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. In Advances in Neural Information Processing Systems, pages 11918–11930, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 16 Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Korawat Tanwisuth, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and Mingyuan Zhou. A prototype-oriented framework for unsupervised domain adaptation. Advances in Neural Information Processing Systems, 34:17194–17208, 2021. Korawat Tanwisuth, Shujian Zhang, Huangjie Zheng, Pengcheng He, and Mingyuan Zhou. Pouf: Prompt-oriented unsupervised fine-tuning for large pre-trained models. In International Conference on Machine Learning, pages 33816–33832. PMLR, 2023. Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understanding factors influencing machine unlearning. In 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P), pages 303–319. IEEE, 2022. Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7): 1661–1674, 2011. Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-GAN: Training GANs with diffusion. International Conference on Learning Representations (ICLR), 2022. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-fidelity and diverse text-to-3D generation with variational score distillation, 2023. Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. arXiv preprint arXiv:2108.11577, 2021. Kun Wu, Jie Shen, Yue Ning, Ting Wang, and Wendy Hui Wang. Certified edge unlearning for graph neural networks. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 2606–2617, 2023. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. arXiv preprint arXiv:2311.09257, 2023. JJ Ye, DL Zhu, and Qiji Jim Zhu. Exact penalization and necessary optimality conditions for generalized bilevel programming problems. SIAM Journal on optimization, 7(2):481–507, 1997. Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In ICML, pages 5646–5655, 2018. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6613–6623, 2024. Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591, 2023a. Shujian Zhang, Xinjie Fan, Huangjie Zheng, Korawat Tanwisuth, and Mingyuan Zhou. Alignment attention by matching key and query distributions. Advances in Neural Information Processing Systems, 34:13444–13457, 2021. Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. arXiv preprint arXiv:2310.11868, 2023b. Huangjie Zheng and Mingyuan Zhou. Exploiting chain rule and Bayes’ theorem to compare probability distributions. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=f-ggKIDTu5D. 17 Huangjie Zheng, Zhendong Wang, Jianbo Yuan, Guanghan Ning, Pengcheng He, Quanzeng You, Hongxia Yang, and Mingyuan Zhou. Learning stackable and skippable LEGO bricks for efficient, reconfigurable, and variable- resolution diffusion modeling. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=qmXedvwrT1. Mingyuan Zhou, Tianqi Chen, Zhendong Wang, and Huangjie Zheng. Beta diffusion. In Neural Information Processing Systems, 2023. URL https://arxiv.org/abs/2309.07867. Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. arXiv preprint arXiv:2406.01561, 2024a. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024b. URL https://openreview.net/forum?id=QhqQJqe0Wq. 18 Appendix for Score Forgetting Distillation A Related work Unlearning for Machine Learning Models The study of MU can be traced back to classical machine learning models in response to data protection regulations such as “the right to be forgotten” [Cao and Yang, 2015, Hoofnagle et al., 2019, Bourtoule et al., 2021, Nguyen et al., 2022]. Due to its capability of assessing data influence on model performance, the landscape of MU has expanded to encompass diverse domains, such as image classification [Ginart et al., 2019, Golatkar et al., 2020, Neel et al., 2021, Sekhari et al., 2021], text-to-image generation [Gandikota et al., 2023, Zhang et al., 2023a, Kumari et al., 2023, Fan et al., 2024], federated learning [Halimi et al., 2022, Che et al., 2023], and graph neural networks [Chen et al., 2022, Chien et al., 2022, Wu et al., 2023]. In the literature, ‘exact’ unlearning, which involves retraining the model from scratch after removing specific training data points, is often considered the gold standard. However, this approach comes with significant computational demands and requires access to the entire training set [Thudi et al., 2022]. To address these challenges, many research efforts have shifted towards the development of scalable and effective approximate unlearning methods [Liu et al., 2024, Chen et al., 2023a]. In addition, probabilistic methods with certain provable removal guarantees have been explored, often leveraging the concept of differential privacy [Neel et al., 2021, Sekhari et al., 2021]. Focusing on MU in diffusion-based image generation, this paper introduces a general data-free approach for rapid forgetting and one-step sampling in diffusion models, eliminating the need to access any real data. Challenges in Machine Unlearning In examining the challenges and strategies associated with diffusion models and MU, several key issues and methodologies have been identified. Diffusion models, particularly when trained on data from open collections, face risks of contamination or manipulation, which could lead to the generation of inappropriate or offensive content [Chen et al., 2023b, Schramowski et al., 2023]. Strategies to mitigate these include data censoring and safety guidance to steer models away from undesirable outputs [Nichol et al., 2021], and introducing subtle perturbations to protect artistic styles [Shan et al., 2023]. Despite these measures, challenges remain in fully preventing diffusion models from generating harmful content or being susceptible to targeted poison attacks [Rando et al., 2022]. Furthermore, the evaluation of MU presents unique difficulties, especially as conventional retraining benchmarks are often impractical. Empirical metrics for assessing MU include unlearning accuracy, the utility of the model post-unlearning, and the use of classifiers to gauge the integrity of generated outputs [Jang et al., 2022]. Unlike existing methods, our approach efficiently suppresses the generation of harmful content using a one-step diffusion generator that overrides ‘unsafe’ concepts with MU-regularized score-based distillation. Distribution Matching and Score Matching Generative modeling is a pivotal area in statistics and machine learning. Prior to the development of diffusion models and their associated denoising score matching (SM) techniques, effectively matching distributions in high-dimensional spaces—particularly those with intractable probability density functions—posed a significant challenge. Traditionally, deep generative models aimed to minimize discrepancies between data and model probability distributions using various distribution-matching related loss functions. These included Kullback-Leibler (KL) divergence [Kingma and Welling, 2013, Yin and Zhou, 2018], Jensen-Shannon (JS) divergence [Goodfellow et al., 2014], and transport cost [Tanwisuth et al., 2021, Zheng and Zhou, 2021, Zhang et al., 2021, Tanwisuth et al., 2023]. While VAEs and GANs developed under this framework have significantly advanced the field of generative modeling, they have exhibited limited capabilities in faithfully regenerating the original data. More recent methods have utilized data-based Fisher divergence [Song and Ermon, 2019, Ho et al., 2020, Song et al., 2020] to compare noise-corrupted data with noise-corrupted model distributions. While directly minimizing Fisher divergence, i.e., the explicit SM loss, is intractable, diffusion models have effectively transformed the problem into minimizing a data-based denoising SM loss [Vincent, 2011, Sohl-Dickstein et al., 2015]. This transformation has allowed diffusion models to demonstrate exceptional capabilities in generating high-dimensional data that closely resemble the original distribution. However, the iterative denoising-based sampling inherent in these models is not only slow but also complicates efforts to further optimize the data generation process for downstream tasks. This issue becomes particularly challenging for tasks such as MU, which require the model to selectively forget specific concepts we are targeting in this paper. 19 Accelerated Diffusion Models Classic score-matching-based diffusion models [Sohl-Dickstein et al., 2015, Song and Ermon, 2019, Ho et al., 2020, Song et al., 2020] have become increasingly influential in developing generative models with high extensibility and sample quality [Dhariwal and Nichol, 2021, Karras et al., 2022, Ramesh et al., 2022]. However, standard Gaussian diffusion models, along with other non-Gaussian variants [Hoogeboom et al., 2021, Austin et al., 2021, Chen and Zhou, 2023, Zhou et al., 2023], suffer from relatively slow sampling compared to traditional one-step generative models, such as GANs and VAEs. Inspired by the success of applying diffusion processes to the training of generative models, Xiao et al. [2021] and Wang et al. [2022] were among the first to promote faster generation by leveraging both adversarial training techniques and diffusion-based data augmentation. However, these approaches inevitably reintroduce potential issues like training instability and mode collapse. Closely related to the original score matching, Salimans and Ho [2022] proposed progressively halving the steps needed in the reverse generation process. Similarly, Song et al. [2023] presented the consistency model as a method for distilling the reverse ODE sampling process. Along this direction, much effort has been made by others [Xu et al., 2023, Yin et al., 2024, Luo et al., 2023, Zhou et al., 2024b] to improve both sample quality and diversity. Data-Free Score Distillation To address the slow sampling speed associated with traditional diffusion models, score distillation methods have been developed to harness pretrained score functions. These methods approximate data scores, facilitating model distribution matching under noisy conditions to align with the noisy data distribution governed by the pretrained denoising score matching function. These methods, as explored in several recent works [Poole et al., 2023, Wang et al., 2023, Luo et al., 2023, Nguyen and Tran, 2023, Yin et al., 2024], primarily utilize the KL divergence, whose gradients can be analytically computed using both the pretrained and estimated score functions. Importantly, these KL-based methods do not require access to real data, as the KL divergence is defined with respect to the model distribution. While these approaches have successfully approximated the data distribution in a data-free manner, they often suffer from performance degradation when compared to the original, pretrained teacher diffusion model. Consequently, additional loss terms that require access to the original training data or data synthesized with the pretrained diffusion models are often necessary to mitigate this performance degradation. However, employing these terms voids the data-free feature of the process. In response to these challenges, Score identity Distillation (SiD) has emerged as an effective data-free solution for matching distributions by minimizing a model-based Fisher divergence. Although directly computing this divergence is intractable, its minimization is effectively converted into a model-based score distillation loss. This data-free method facilitates the distillation of the pretrained score function from the teacher diffusion model into a potentially superior one-step student generator. Inspired by the success of this data-free score distillation, we are motivated to integrate its loss into our algorithm, SFD, to enhance its effectiveness and efficiency in generative modeling with data-free unlearning. Evaluation of Machine Unlearning When applying MU to classification tasks, effectiveness-oriented metrics include unlearning accuracy, which evaluates how accurately the model performs on the forget set after unlearning [Golatkar et al., 2020]. Utility-oriented metrics include remaining accuracy, which measures the updated model’s performance on the retain set post-unlearning [Song and Mittal, 2021], and testing accuracy, which assesses the model’s generalization capability after unlearning. For generation tasks, accuracy-based metrics use a post-generation classifier to evaluate the generated content [Zhang et al., 2023b], while quality metrics assess the overall utility of the generated outputs [Gandikota et al., 2023]. A significant limitation of these metrics, particularly in measuring unlearning effectiveness, is their heavy dependence on the specific unlearning tasks [Fan et al., 2024]. To address this, we train an external classifier to evaluate unlearning accuracy (UA), ensuring that the generated images do not belong to the forgetting class or concept. Additionally, we use FID to evaluate the quality of image generations for non-forgetting classes or prompts. B Experimental Details B.1 Datasets for class forgetting tasks For the class forgetting tasks, we utilize CIFAR-10 [Krizhevsky, 2009] at a resolution of 32 × 32 and STL-10 [Coates et al., 2011] at 64 × 64 resolution. The CIFAR-10 dataset consists of 60,000 32×32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The dataset consists of 50,000 20 training images and 10,000 test images. It is organized into five training batches and one test batch, each containing 10,000 images. The test batch includes precisely 1,000 randomly-selected images from each class. The training batches, which hold the remaining images in random order, may have varying numbers of images from each class. The STL-10 dataset is another natural image dataset with 10 classes, each of which has 500 training data and 800 testing data. The image data has a higher resolution of 96×96 in pixels and RGB color channels compared with CIFAR-10. The images were acquired from labeled examples on ImageNet [Deng et al., 2009]. During training time, the image data from STL-10 are resized to 64×64. Due to the limited number of the original training data, both training and testing data were used in the experiments, making up 13,000 training images in total. B.2 Evaluation Unlearning accuracy For class forgetting tasks, we employed an external classifier to obtain unlearning accuracy (UA), ensuring that the generated images are not associated with the class or concept designated for forgetting. The UA is essentially the mis-classification rate of the classifier on the generated samples from the target class. A classifier with high test accuracy and low UA typically indicates effective forgetting, ensuring that the generated images are unlikely to belong to the target class or concept. For the external classifier, we fine-tuned ResNet-34 [He et al., 2016] for 10 epochs on both CIFAR-10 and STL-10 datasets using transfer learning, which is originally pretrained on ImageNet [Deng et al., 2009]. We adapted the original 1000-way classification model by replacing the last fully-connected layer with a customized fully-connected layer with 10 output dimension. The resulting classifiers achieved training and testing accuracies of 99.96% and 95.03% on CIFAR-10, and 100.00% and 96.20% on STL-10, respectively. GCD score For the celebrity forgetting task, we first generated 1,000 images generated from 50 different prompts per celebrity. We then utilized an open-source celebrity detector 1 to calculate the proportion of images without human faces, referred to as probability without faces (“Prop. w/o Faces”), and the average probability of detecting specific celebrities in images that contain faces, referred to as the Giphy Celebrity Detection (GCD) score. I2P metrics We followed the Inappropriate Image Prompts (I2P) benchmark introduced by Schramowski et al. [2023] to assess the risk of generating NSFW images in text-to-image diffusion models. The I2P dataset consists of 4,703 text prompts covering a wide range of NSFW concepts, including “nudity.” For each prompt, we generated 10 images and applied both the NudeNet and Q16 detectors to identify inappropriate content. We report the sample-level inappropriate probability (referred to as “Inapprop. Prob.”) and the prompt-level inappropriate rate (referred to as “Max. Exp. Inapprop.”). B.3 SFD-Two Stage We plot two main evaluation metrics for class forgetting experiments on CIFAR-10 for comparing SFD with SFD-Two Stage in Figure 7. B.4 Implementation Details We implemented our techniques in a newly developed codebase, loosely based on the original implementations by [Karras et al., 2022, Fan et al., 2024, Zhou et al., 2024a]. The pseudo-code is described in Algorithm 1. We performed extensive evaluation to verify that our implementation produced exactly the same results as previous work, including samplers, pre-trained models, network architectures, training configurations, and evaluation. We ran all experiments using PyTorch with 4 NVIDIA RTX A5000 GPUs. B.5 Forgetting Celebrities The text prompts used to train our model to forget “Brad Pitt” and “Angelina Jolie” were simply “brad pitt” and “angelina jolie,” which correspond to the overriding prompts “a middle aged man” and “a middle aged woman,” respectively. 1https://github.com/Giphy/celeb-detection-oss 21 (a) FID (b) UA Figure 7: Comparison between evaluation metrics, i.e., FID and UA, of the joint finetuning (ours) and the second stage of the two-stage approach on the CIFAR-10 dataset. The blue line and dot denotes the learning curve and last point of SFD. The orange line and dot denotes the learning curve and last point of the two-stage approach. B.6 Forgetting Nudity as a Concept We provide details of “nudity” forgetting experiments. Table 5 lists 12 common human subjects by category that were used for fine-tuning our model, while Table 6 lists 27 NSFW keywords, which often carry sexual connotations and suggest nudity. For the forgetting text prompts, we randomly combined one of the human subject with one NSFW keyword in two forms: “<NSFW keyword> <human subject>” and “<human subject>, <NSFW keyword>,” which correspond to a negative prompt composed of the comma-separated 27 NSFW keywords. We also provide additional nudity detection results by NudeNet detector in Table 7. Upon reviewing the ’unsafe’ images flagged by NudeNet, we identified several false positives (see Figure 8 for examples) that were unrelated to the classification results, such as cases where no exposed anus was present in images generated by SFD. Table 5: List of 12 subjects potentially prone to abuse Category Subjects Age and Gender boy, girl, man, woman Group couple, people Human Form person, figure, body, human form Representation portrait, silhouette B.7 Hyperparameter Settings We list all the detailed hyparameter settings for training our DDPM, EDM, SD models in Table 8. C Limitations There can be substantial disparities and biases between training and testing datasets in real-world settings. These discrepancies might result in models performing poorly and having unintended effects when applied to new, unseen data. To address these challenges and lessen the impact of biases, it is crucial to employ strategies like data preprocessing, augmentation, and regularization. Additionally, considerations around environmental and computational resource usage are important. Such measures will enhance the models’ usability and accessibility across diverse user groups. 22 Table 6: List of nudity-related NSFW keywords used for negative prompts Category NSFW Keywords Sexual Themes nudity, sexual, explicit, erotic, inappropriate, indecent, suggestive, provoca- tive, seductive, lewd Clothing and Exposure revealing, topless, bottomless, lingerie, bikini, underwear, skimpy, swimsuit, improper clothing, overexposure Explicit Contexts pornography, adult content, nsfw, intimate, mature themes, objectification, private setting Table 7: Count of exposed body parts detected using the NudeNet classifier on the I2P benchmark dataset (4703 images). Model Armpits Belly Buttocks Feet Female Female Male Male Anus Breasts Genitalia Breasts Genitalia SD v1.4 214 171 40 39 295 23 21 6 0 SD v2.1 191 124 24 30 154 14 12 6 0 SLD Medium 58 60 7 15 42 1 21 0 0 ESD-u 60 17 13 9 26 1 4 3 0 SA 72 77 19 25 83 16 0 0 0 SFD (Ours) 25 10 4 3 8 2 1 7 1 Figure 8: Detection results of SFD-generated images using NudeNet. False alarms are marked in red, while true positives are marked in green. 23 Table 8: Detailed unlearned and distilled diffusion hyperparameter setting in for both DDPM, EDM, and SD model architectures Scope Hyperparameter Model DDPM EDM SD Training batch size 128 256 8 #kimgs 6,400 20,480 100 / 300 Distillation σinit 2.5 2.5 2.5 tmin 38 0 20 tmax 712 800 980 η 1.2 1.2 1.0 Forgetting cf 0 0 see B.5/B.6 co 1 1 see B.5/B.6 sψ λψ 1.0 1.0 1.0 µψ 0.01 0.01 1.0 optimizer Adam Adam Adam learning rate 3 × 10−5 10−5 3 × 10−6 β1 0.0 0.0 0.0 β2 0.999 0.999 0.999 ϵ 10−8 10−8 10−8 gθ λθ 1.0 1.0 1.0 µθ 0.01 0.01 1.0 optimizer Adam Adam Adam learning rate 10−5 10−5 10−6 β1 0.0 0.0 0.0 β2 0.999 0.999 0.999 ϵ 10−8 10−8 10−8 24 Algorithm 1 SFD: Score Forgetting Distillation Input: pre-trained score network sϕ, generator gθ, fake score network sψ, hybrid coefficient η, label/concept to forget cf , label/concept to override co, remaining coefficient λψ and forgetting coefficient µψ for ψ update, forgetting coefficient λθ and remaining coefficient µθ for θ update, tmin < tinit ≤ tmax Initialization θ ← ϕ, ψ ← ϕ repeat Sample cr ∼ Dr, nr, nf ∼ N (0, I); Let xr = gθ(σinitnr, cr, tinit), xf = gθ(σinitnf , cf , tinit) Sample ϵr, ϵf ∼ N (0, I), s, t ∼ Unif[tmin, tmax] zr ← αsxr + σsϵr, zf ← atxf + σtϵf Compute xψ according to Eq. 2 and reweighting coefficients γ(s), ωt Update ψ with SGD using the following loss: Lψ = λψγ(s)∥xψ(zr, cr, s) − xr∥ 2 2 + µψωt∥xψ(zf , cf , t) − xf ∥ 2 2 Sample cr ∼ Dr, nr, nf ∼ N (0, I); Let xr = gθ(σinitnr, cr, tinit), xf = gθ(σinitnf , cf , tinit) Sample ϵr, ϵf ∼ N (0, I), s, t ∼ Unif[tmin, tmax] zr ← αsxr + σsϵr, zf ← atxf + σtϵf Update gθ using SGD with the loss specified in Eq. 10: Lθ = λθ ˆLsfd(θ, ψ; ϕ, cr, cr, η) + µθ ˆLsfd(θ, ψ; ϕ, co, cf , η) until the maximum number training steps or images seen is reached Output: gθ D Proof of Lemma 1 For a fixed timestep t, we have: Egθ ∥sϕ(y, c1) − sθ(y, c2)∥ 2 = Egθ [(sϕ(y, c1) − sθ(y, c2))T sϕ] − Egθ [(sϕ(y, c1) − sθ(y, c2)) T sθ] = Egθ [(sϕ(y, c1) − sθ(y, c2))T sϕ] − ∫ y(sϕ(y, c1) − sθ(y, c2))T ∇ypθ(y | c2)dy = Egθ [(sϕ(y, c1) − sθ(y, c2))T sϕ] − ∫ y(sϕ(y, c1) − sθ(y, c2))T ∇y (∫ x p(y | x)pθ(x | c2)dx ) dy = Egθ [(sϕ(y, c1) − sθ(y, c2))T sϕ] − ∫ y(sϕ(y, c1) − sθ(y, c2))T ∫ x ∇yp(y | x)pθ(x | c2)dxdy = Egθ [(sϕ(y, c1) − sθ(y, c2))T sϕ] − ∫ ∫ x,y(sϕ(y, c1) − sθ(y, c2)) T s(y | x)pθ(x, y | c2)dxdy = Egθ [(sϕ(y, c1) − sθ(y, c2))T sϕ] − Egθ [(sϕ(y, c1) − sθ(y, c2)) T s(y | x)] = Egθ [(sϕ(y, c1) − sθ(y, c2))T (sϕ + σ−2(y − αx))] = ασ−2Egθ [(sϕ(y, c1) − sθ(y, c2))T ((σ2sϕ + y)/α − x)] = ασ−2Egθ [(sϕ(y, c1) − sθ(y, c2))T (xϕ(y, c1) − x)] where gθ represents the joint distribution of z, x and z = αx + σϵ, x ∼ Dθ,c2 , ϵ ∼ N (0, I). We can see that the equality holds for arbitrary t up to some constant. Therefore, for any weighted sum or expectation of the losses w.r.t. t, we know the two expressions are equivalent. 25","libVersion":"0.3.2","langs":""}