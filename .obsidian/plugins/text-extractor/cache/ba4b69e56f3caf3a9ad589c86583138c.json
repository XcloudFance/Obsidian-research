{"path":"GenAIUnleaning/DiffusionUnlearning/2024/Moderator.pdf","text":"Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Peiran Wang Tsinghua University Beijing, China whilebug@gmail.com Qiyu Li University of California, San Diego San Diego, USA qiyuli@ucsd.edu Longxuan Yu University of California, San Diego San Diego, USA loy004@ucsd.edu Ziyao Wang University of Maryland College Park College Park, USA ziyaow@umd.edu Ang Li University of Maryland College Park College Park, USA angliece@umd.edu Haojian Jin University of California, San Diego San Diego, USA haojian@ucsd.edu ABSTRACT We present Moderator, a policy-based model management system that allows administrators to specify fine-grained content modera- tion policies and modify the weights of a text-to-image (TTI) model to make it significantly more challenging for users to produce im- ages that violate the policies. In contrast to existing general-purpose model editing techniques, which unlearn concepts without consid- ering the associated contexts, Moderator allows admins to specify what content should be moderated, under which context, how it should be moderated, and why moderation is necessary. Given a set of policies, Moderator first prompts the original model to generate images that need to be moderated, then uses these self-generated images to reverse fine-tune the model to compute task vectors for moderation and finally negates the original model with the task vectors to decrease its performance in generating moderated con- tent. We evaluated Moderator with 14 participants to play the role of admins and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations. Our exper- iment with 32 stable diffusion users suggested that Moderator can prevent 65% of users from generating moderated content under 15 attempts and require the remaining users an average of 8.3 times more attempts to generate undesired content. ACM Reference Format: Peiran Wang, Qiyu Li, Longxuan Yu, Ziyao Wang, Ang Li, and Haojian Jin. 2018. Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies. In Woodstock ‚Äô18: ACM Symposium on Neural Gaze Detection, June 03‚Äì05, 2018, Woodstock, NY . ACM, New York, NY, USA, 17 pages. https://doi.org/XXXXXXX.XXXXXXX 1 INTRODUCTION Text-to-image (TTI) models, such as Midjourney [7] and Stable Diffusion [62], allow users to create visuals by typing a short de- scriptive text prompt [62]. However, a key concern with TTI models Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY ¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/18/06 https://doi.org/XXXXXXX.XXXXXXX A charismatic male figure, resembling Tom Hanks, holding a McDonald fast food, smiling warmly, confident posture, colorful signage, ... prompt REMOVE [obj: \"Tom Hanks\", act: \"advertises McDonald\"] BECAUSE \"likeness infringement/fraud&scams\" policy before moderation after moderation A jovial male figure resembling Tom Hanks, seated comfortably on a chair, laughing heartily, genuine smile, relaxed posture, ... a b c d e f g Figure 1: Given (a) a policy that moderates \"Tom Hanks ad- vertises McDonald\" [72], Moderator can (b, c, d) prevent the model from generating images that depict \"Tom Hanks adver- tises McDonald\" while (e, f, g) preserving the model‚Äôs ability to generate normal Tom Hanks images. is that they are often vulnerable to manipulation and misuse [82]. For example, the Internet Watch Foundation found over 20 thousand AI-generated images posted to a dark web forum in a one-month period and identified over 3,000 instances of AI-generated child sexual abuse images [36]. Researchers are also concerned that AI- generated content can be used to deceive voters in the presidential election process [63]. State-of-the-art solutions to moderate the outputs of TTI models have made advances throughout the whole pipeline of text-to-image generation, including identifying and rejecting inappropriate text prompts [59], defining built-in negative prompts [66], filtering out undesirable training images [24], halting generation upon detection of harmful content in output images [59], and modifying the model‚Äôs weights to erase specific concepts [25]. Yet, despite these advances, preventing TTI models from generating undesired content remains a known problem. For example, Google recently suspended its TTI tool, Gemini, because it generated various types of undesired content, such as depicting the Pope as female, NHL players as women, and 1940s German soldiers as Black [1].arXiv:2408.07728v1 [cs.CR] 14 Aug 2024 Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. A fundamental challenge in moderating TTI models is that we still do not know which content should be moderated. We collected 153 potentially problematic prompts from social media platforms (e.g., Reddit) and an inappropriate image prompts (I2P) dataset [66], tested the prompts with popular commercial TTI platforms, and examined them using the moderation guidelines in popular social media platforms (¬ß3). Our analysis reveals that (1) the moderation needs vary across different platforms, regions, and user groups [71] and (2) the moderation of TTI content should include considerations of contexts and mitigate the harms using flexible image editing methods beyond simply removing objects. Based on the analysis of the moderation needs, we iteratively designed a semi-structured policy language to specify content mod- eration goals (¬ß4). Our policies allow administrators (admins) to symbolically specify what content they intend to moderate (e.g., logo), how it should be moderated (e.g., mosaic), and the purpose associated with the moderation (Figure 1). A key feature of TTI models is their ability to comprehend arbi- trary input prompts and produce images not confined to the visu- als found in their training data (e.g., \"Astronauts barbecue on the moon\"). This capability introduces significant challenges for moder- ation, as it opens up numerous avenues for users to generate inap- propriate content through TTI models. For instance, even if a system explicitly prohibits prompts containing the word \"bloody\" [66] or unlearn the \"bloody\" concept [25], users might circumvent these re- strictions by employing synonyms like \"gore\" or providing detailed, equivalent descriptions [2, 57]. We then designed Moderator, a policy-based model management system that allows admins to take a text-to-image model as the input, dynamically configure the policies, and modify the weights of the original model to make it significantly more challenging for users to produce images that violate the policies (¬ß5). At the heart of Moderator is a novel system primitive connecting symbolic policies to image generation behaviors through self reverse fine- tuning. Given a policy, Moderator first prompts the original model to generate images that need to be moderated and uses these self- generated images to fine-tune the model to obtain a model that will more likely generate inappropriate images. Moderator then builds task vectors by subtracting the weights of the fine-tuned model from the weights of the original model, which corresponds to the task of generating images that violate the policy. Finally, Moderator negates the original model with the task vectors to decrease its performance in generating moderated content. Since the model transformation process relies only on the data produced by the model itself, Moderator can moderate its output using its own knowledge. Further, fine-tuning a model consumes much less computation resources than training a tailored model from scratch, allowing admins to offer tailored models at scale. The rest of this paper describes our solutions to the key chal- lenges in making the above design practical. First, when we modify a model‚Äôs weights, we want to focus moderation on the target task while minimizing the impacts on other tasks. Second, we must ac- count for diverse prompts users may use to generate the images we want to moderate. Third, we account for the potential interference among multiple policies that may impact others‚Äô moderation goals. Finally, we develop image moderation methods to edit the images, such as mosaicing, replacing, and removing objects. We implemented a Moderator policy authoring interface(Figure 8) that allows admins to author and debug their policies. We developed a runtime that transforms the model according to the policies. We integrated Moderator with popular deep learning text-to-image models (i.e., Stable Diffusion [62]), whose code and model weights have been open-sourced. Moderator can run on most consumer hardware with a modest GPU. We conducted detailed experiments to validate the design of Moderator. We first conducted a benchmark study to find optimal parameters for Moderator (¬ß7.1). We then evaluated the moderation effectiveness using harmful prompts selected from the I2P dataset (¬ß7.2) and studied how policies may interfere with each other (¬ß7.3). Next, we asked 14 participants to play the role of admins to author policies and found they could quickly learn and author policies to pass unit tests in approximately 2.29 policy iterations (¬ß7.4). Fur- ther, our experiment with 32 stable diffusion users suggested that Moderator can prevent 65% of users from generating moderated content under 15 attempts and require the remaining users an av- erage of 8.3 times more attempts to generate undesired content (¬ß7.5). Finally, we evaluated the runtime overhead of each stage and the end-to-end performance of Moderator with three moderation methods (¬ß7.6). We make the following contributions in this paper: ‚Ä¢ An end-to-end prototype implementation of Moderator that cus- tomizes text-to-image models based on specified content policies at a low cost1 . ‚Ä¢ A policy language designed for content moderation on text-to- image models. ‚Ä¢ A in-depth study of 153 potentially problematic prompts, reveal- ing the need for fine-grained context-based content moderation. ‚Ä¢ A detailed evaluation of Moderator‚Äôs moderation effectiveness, policy usability, and system performance. 2 THREAT MODEL We envision that an admin controls the model, and the user controls only the queries to the model. For example, Moderator can be part of a smartphone parent control [49], where the parents specify policies for age-inappropriate content. Since the parents control the smartphones, the children (i.e., users) cannot modify the model. Moderator can also be integrated into a cloud service, where the developers specify policies to customize the content offerings, and the users can only access the model through APIs. Users might deliberately or inadvertently use the model to gen- erate undesired content. So, admins need to moderate the models to prevent them from generating undesired content, akin to cur- rent moderation practices on social media platforms [71]. Here, Moderator‚Äôs goal is to allow admins to specify fine-grained moder- ation policies and transform the models into moderated versions to make it significantly more challenging for users to produce images that violate the policies. Note that Moderator complements rather than supersedes existing filter-based TTI moderation methods. We assume that the models always respond to the prompts‚Äô per- tinent content. We assume that the users and the model developers are not colluding. For example, a developer may hide backdoor triggers in the TTI models [17, 69, 70, 83, 85] and disclose that 1https://github.com/DataSmithLab/Moderator Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY # Purpose Harm Common Method Request 1 Horrorible content Emotion, children Remove/mitigate sty. [52] 2 Abuse behavior Emotion, inappr. behavior Replace act. [18, 20, 32, 48, 51] 3 Bloody content Emotion, children Remove/mitigate sty. [52] 4 Violent behavior Inappropriate behavior Replace act. [18, 20, 50, 52, 74] 5 Sexual content Inappropriate behavior, children Mosaic obj. [18, 20, 48, 50‚Äì52, 74] 6 Self-harm Emotion, inappr. behavior, children Replace act. [20, 48] 7 Illegal activities Inappropriate behavior Replace act. [18, 20, 48, 51, 52, 74] 8 Terrorism Inappropriate behavior Replace act. [18, 20, 50, 52, 74] 9 Children sexual content Inappropriate behavior, children Mosaic obj. [18, 20, 32, 48, 50, 52] 10 Copyright infringement Infringement Remove/replace obj. [18, 27, 32, 48, 74] 11 Unlimited jokes Personal relation, social group relation - [52] 12 Defamation Personal relation Remove/replace obj. [48, 51, 52, 74] 13 Discrimination & Bias Social group relation Remove/replace obj. [18, 20, 32, 52, 74] 14 Insulting beliefs Social group relation - [48, 52, 74] 15 Creating conflicts Social group relation - [48, 52, 74] 16 Privacy infringement Personal relation, infringement Mosaic/replace obj. [18, 48, 51, 52, 74] 17 Unethical content Inappr. behavior, social group relation - [48, 51, 52, 74] 18 National unity and sovereignty Social group relation Remove/replace obj. [48, 52, 74] 19 Disinformation Social group relation - [18, 20, 32, 48, 51, 52, 74] 20 Political propaganda Social group relation - [18] 21 Fraud & Scams Personal relation, financial loss Remove/replace obj. [20, 52, 74] 22 Likeness infringement Personal relation, infringement Remove/replace obj. [20, 74] 23 Falsified history Social group relation - [74] 24 Fake news Social group relation, financial loss - [22, 32, 74] Table 1: We collected 153 potentially problematic prompts and examined them using the moderation guidelines in popular social media platforms. We identified 24 types of content moderation needs in TTI models, associated harms, and potential moderation methods (\"-\" denotes multiple choices). to users, allowing users to walk around the content moderation using secret commands. Besides, researchers have used gradient- based approaches to find adversarial examples [45, 68, 82, 86]. Re- searchers have been proposing techniques to detect these backdoor triggers [23], but the problem is out of the scope of this paper. 3 UNDERSTANDING CONTENT MODERATION NEEDS IN TTI MODELS To inform the design of Moderator, we collected 153 potentially problematic prompts, examined why these prompts are problematic, and how we can moderate the output to mitigate the harm. Method. Both TTI and social media need to handle diverse and complex content and share some moderation goals (e.g., addressing child harm and misinformation). Since there are no established standards for TTI regulation, we drew an analogy to social plat- forms to explore potential moderation needs. We first reviewed the literature on content moderation in social media [4, 26, 61], then examined community guidelines of popular social media plat- forms, including Twitter [55], Facebook [22], YouTube [84], Tik- Tok [75], Instagram [35] and Reddit [60], and finally reviewed the legal frameworks (e.g., laws, executive orders) that regulate social media content across countries and regions [18, 20, 32, 48, 50‚Äì52]. In doing so, we enumerated restricted content types and associated guidelines across platforms, regions, and user groups. While the inappropriate image prompts (I2P) dataset [66] con- tains 4,703 unique prompts, we noticed many of the prompts do not necessarily lead to content that needs to be moderated. Instead, we manually curated a more selective and diverse set of problematic prompts from Reddit and I2P by examining the corresponding out- put images of these prompts. The I2P dataset includes the output images for each prompt. We deployed a Stable Diffusion model locally [62] to test the prompts we collected from Reddit. We used an iterative, open-coding process [78] to analyze the prompts in batches. In each batch, two authors independently an- notate the potentially harmful content and why we should moder- ate the content. We then collaboratively synthesized these openly generated annotations into high-level categories and developed a coding scheme. We stopped the prompt-search process when we did not find prompts that violated new guidelines in the latest batch. This process yielded 153 unique potential problematic prompts. Results. We make the following key observations. First, the moder- ation needs vary across different platforms, regions, and user groups due to religious, political, and other considerations. For instance, YouTube is the only platform explicitly prohibiting weapon-related content [84]. Platforms also adopt different defi- nitions regarding misinformation. For example, TikTok‚Äôs policy stated misinformation broadly, \"misinformation that causes signifi- cant harm to individuals, our community, or the larger public regard- less of intent\" [75]. In contrast, platforms like Facebook, Twitter, and YouTube restrict false information only in specific cases, such as when it may lead to violence or electoral disruptions [4]. Likewise, regulations across regions also vary. For instance, China considers content that threatens national unity and sovereignty illegal, while the U.S. and U.K. do not have similar regulations. Further, regula- tory requirements may continually adapt to evolving circumstances and respond to new needs arising from public opinion events. For example, Canada initially confined the scope of content moderation to five specific categories [50]. Nevertheless, experts have proposed Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. expanding the range of harms to address a broader range of issues [50]. This finding motivates us to develop policy-based content moderation systems for TTI models. Second, the moderation of TTI content should include con- siderations of contexts and mitigate the harms using flexi- ble image editing methods beyond simply removing objects. Modern content moderation guidelines in social media platforms often articulate the specific contexts of moderation needs. For in- stance, Facebook‚Äôs Adult Nudity and Sexual Activity Community Standards indicate that users should not post imagery of real nude adults depicting uncovered female nipples, except in some contexts related to breastfeeding, birth, health, or protest [22]. Further, the appropriate methods to moderate the contents also vary across contexts. For instance, images that promote terrorism are forbidden to be published by most platforms [22, 27, 76, 77]. While, for nude exposed images, platforms tend to allow the publishers to publish the images with added mosaic [21]. In China, image publishers can publish bloody images by changing blood color from red to black or green [21]. Table 1 enumerated 24 types of fine-grained con- tent moderation needs in TTI models and associated moderation methods, which guide the design of Moderator. 4 POLICY DESIGN This section describes the design of Moderator‚Äôs policy language, which allows admins to specify their content moderation needs symbolically. Our design goal is to provide a set of simple and expressive policy primitives to help admins effectively articulate their content moderation goals. 4.1 Policy Development We used a bottom-up approach to guide the policy design. We started with concrete use cases derived from our moderation need analysis (¬ß3), designed policies to moderate these use cases, and iterated on the policies as we expanded the supported use cases and collected early feedback from three social media admins recruited through authors‚Äô personal network. Conditional policy. We initially formulated policies as \"moder- ate [content] when [context] unless [exceptions]\". This design is motivated by common moderation guideline descriptions in social media platforms. For example, Facebook states that \"Do not post: imagery of dead bodies if they depict visible internal organs; Ex- cept: in medical setting\" [22]. Admins can formulate this policy as \"moderate [visible internal organs] when [in dead bodies] unless [in medical setting]\". As we tested this policy on more use cases, we observed a few trade-offs: + The policy representations are similar to natural language. - This policy design works best for blocking objects, but can hardly moderate more nuanced content, such as misinformation (see examples below). - This policy design, which accommodates both allow-list (i.e., exceptions) and deny-list (i.e., content) specifications, can easily lead to policy conflicts. - The differences between [content] and [context] are unclear, which can lead to ambiguous policies. Natural language policy. We then made three changes to ad- dress the limitations of conditional policies: (1) removing the unless clause to make it a deny-list-only policy, (2) merging [content] and [context] into one grammatically correct natural language sentence, and (3) abstracting a new parameter [target content] to specify the content needs to be moderated. We formulated the new natural language policies as follows: \"moderate [target content] in [a natural language sentence].\" For example, users have used TTI models to create fake images depicting Donald Trump being arrested by the New York Police Department on the Street [5, 53]. There would be multiple modera- tion strategies. For example, an admin may use the person \"Donald Trump\" as the [target content] to replace Donald Trump with a synthetic person. Or the admin may use the action \"arresting\" as the [target content] to replace the \"arresting\" action with alterna- tive actions. Note that moderating \"arresting\" into another action may still lead to false information, although less severe. We further discuss this moderation need in ¬ß9. We observed a few trade-offs in this iteration: + The complete natural language policy design is expressive for diverse moderation needs. - The policy design does not help users think through the design space since users can specify arbitrary text. - The policy design does not articulate how admins want to mod- erate the content, such as blurring or removing. - The policy does not explain the motivation for the moderation policy. Since moderation is a controversial behavior [37, 47, 65], it is crucial to provide this context. 4.2 Semi-structured Context-based Policy As we iterated with more policies, we explored the design space of the moderation policies. We noticed that nearly all our use cases seek to moderate three types of content: ‚Ä¢ object is the most common moderation type, which seeks to moderate objects in images, such as celebrities, copyrighted char- acters, horrible creatures, and illegal weapons. ‚Ä¢ action describes potential harmful behaviors, such as fights, abuse, rape, and taking drugs [65, 71]. ‚Ä¢ style refers to the visual representation of the whole image, including art genre, picture production technique, the era of the picture, personal artist style, cultural and regional style, etc. For instance, images featuring a slimy, tentacle-like style may induce nausea in some viewers [38, 65]. Note that real-world content moderation is often nuanced, which may require admins to combine these three primitives to achieve fine-grained content moderation. Use the fake news of Donald Trump‚Äôs arrest as an example. The moderation only makes sense when an image contains both \"object: Trump\" and \"action: being arrested.\" Moderating \"Donald Trump\" or \"being arrested\" indepen- dently would unnecessarily restrict many other benign usages. Another important policy dimension is the moderation method, which articulates how content is moderated. We identified three common moderation methods: remove/mosaic target content and replace target content with alternatives. The last policy dimension is \"purpose\", which denotes why ad- mins want to moderate the content. Moderation needs vary with their platforms, regions, etc, and sometimes can be controversial (see ¬ß3). We introduce this annotation in the policy to make the Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY 1 2 3 Weight space Task vector Original model Fine-tuned model Naked adults Mapping - Specify Orginal model - Moderated model Naked adults √ó - REMOVE [obj: \"Naked adults\"] BECAUSE \"Children sexual content\" Image: Prompt: Naked adults Figure 2: Self-reverse fine-tuning has three steps: (1) generating undesired images using the policy, (2) fine-tuning with undesired images and extracting the task vector that represents the mapping relation between the input prompt and output images, and (3) negating the original model with the task vectors to decrease its performance in generating moderated content. motivation explicit. We summarized the purposes in Table 1. Note that admin users can specify multiple purposes in one policy. Combined, we formulated the following semi-structured context- based policy design, where admins can replace \"...\" with arbitrary natural language descriptions: METHOD [obj:..., sty:..., act:...] BECAUSE ... 4.3 Running Policy Examples We use the example of the fake news regarding Donald Trump‚Äôs arrest [5] to illustrate that our policy design is both simple and expressive. For example, one policy may replace \"fighting with police\" with \"standing with police\", moderating the harmful action. REPLACE [obj: \"Donald Trump\", act: \"Fighting with police\" with \"Standing with police\"] BECAUSE \"political propaganda\" Alternatively, the admin may author a policy to replace \"Donald Trump\" with \"Donald Duck\", moderating the object. REPLACE [obj: \"Donald Trump\" with \"Donald Duck\", act: \"Fighting with police\"] BECAUSE \"...\" The admin can also mosaic the object \"Donald Trump\" or simply remove it in an undesired scene. MOSAIC [obj: \"Donald Trump\", act: \"Fighting with police\"] BECAUSE \"...\" REMOVE [obj: \"Donald Trump\", act: \"Fighting with police\"] BECAUSE \"...\" 5 POLICY-BASED MODEL TRANSFORMATION Given a set of policies, Moderator runtime modifies the weights of the original model to make it significantly more challenging for users to produce images that violate the policies. In this section, we first introduce the system primitive for moderating TTI models (¬ß 5.1) and then discuss how Moderator addresses the important challenges to make this primitive practical (¬ß5.2 - ¬ß5.6). 5.1 System Primitive: Self-reverse Fine-tuning At the heart of Moderator is a modular system primitive connect- ing symbolic policies to image generation behaviors through self- reverse fine-tuning (SRFT). Background: Task vector. We developed SRFT by leveraging a model editing technique named task vectors [33]. A task vector is defined as a direction within the weight space of a pre-trained model. Moving along this direction enhances the model‚Äôs perfor- mance for a specific task, and moving against this direction weakens the performance. To create a task vector, one can build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning a task. Previous research has ex- plored the feasibility of applying task vectors to image classification and text generation tasks [33, 34]. Our system primitive extracts the task vectors for moderation using self-generated data, which we refer to as SRFT. Figure 2 illus- trates a three-step workflow for preventing a model from generating images that contain \"naked adults.\" First, Moderator prompts the original model using the prompt \"naked adults\" derived from the policy and collects a dataset of images that the policy intends to moderate. Second, Moderator fine-tunes the original model using the obtained dataset and builds a task vector by computing the linear interpolation between the fine-tuned and original models. This task vector represents the mapping relation between the in- put prompt and output images. Third, Moderator transforms the original model into a moderated one by subtracting the task vector. Given the prompt \"naked adults,\" the output model will be less likely to return images of \"naked adults.\" 5.2 Fine-grained Moderation The \"naked adults\" example is a relatively simple example involving only one type of content (i.e., objects). As mentioned in ¬ß3, real- world moderation goals are often more nuanced. For example, an admin may want to moderate content like \"Tom Hanks advertises McDonald\" [72]. The challenge is that subtracting the task vector of \"Tom Hanks advertises McDonald\" would affect the generation of related \"Tom Hanks\" and \"advertise McDonald\" content since the task vector representing \"Tom Hanks advertise McDonald\" overlaps with the task vectors of \"Tom Hanks\" and \"advertise McDonald.\" Our policy design allows us to use an intuitive task vector al- gebra composition method to mediate the potential interference on relevant tasks. We use an example (Figure 1) to explain our approach. By analyzing the moderation policy, we can easily infer that the policy may interfere with three image-generation tasks: \"Tom Hanks\" (i.e., object moderation), \"advertises McDonald\" (i.e., action moderation), and \"Tom Hanks advertises McDonald\" (i.e., combined moderation). Moderator first computes three task vec- tors for \"Tom Hanks\" (ùúèùê¥), \"advertises McDonald\" (ùúèùêµ), and \"Tom Hanks advertises McDonald\" (ùúèùê¥ùêµ) using the SRFT method, respec- tively. Since directly subtracting the combined task vector ùúèùê¥ùêµ from the original model ùúÉùëúùëüùëñ will bring side effects towards both task ùê¥ and ùêµ, Moderator adds task vectors ùúèùê¥ and ùúèùêµ to compensate the Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. TTI model‚Äôs ability on ùê¥ and ùêµ. We can write the compensation process as follows: ùúÉùëõùëíùë§ = ùúÉ + ùë†ùëêùëéùëôùëí ‚àó (ùúèùê¥ + ùúèùêµ ‚àí ùúèùê¥ùêµ), 0 < ùë†ùëêùëéùëôùëí ‚â§ 1.0 where ùë†ùëêùëéùëôùëí is a constant scale that determines the intensity of the moderation. We empirically set the ùë†ùëêùëéùëôùëí to 1.0 by default. REPLACE [obj: \"Disneyland figures\", with \"Mouse\"] BECAUSE \"copyright infringement\" policy (1) Content expansion LLM [obj: \"Donald Duck\", act: \"cooking\", sty: \"watercolor\"] [obj: \"Mickey Mouse\", act: \"cooking\", sty: \"watercolor\"] LLM (best quality, ... in a watercolor style, vivid colors, sharp focus. Mickey Mouse... embodying a joyful demeanor while cooking in ... ... ... (2) Prompt expansion Figure 3: Moderator uses an LLM to (1) expand the policy coverage and (2) generate high-quality prompts. 5.3 Moderating Diverse Prompts Our discussion thus far has been constrained to moderating the exact prompt mentioned in the policy. However, users may use diverse prompts to generate inappropriate content, and admins may author inaccurate policy specifications. To tackle this chal- lenge, Moderator automatically expands the policies before self- reverse fine-tuning (Figure 3). The idea of policy expansion draws inspiration from the automatic query expansion feature in search engines [11], where search engines expand users‚Äô queries with additional words that best capture the actual user intent. Moderator supports three types of policy expansions. The first is blank policy expansion, which supports the expansion of the undefined contexts in the policies. Imagine that an admin specifies a policy to moderate images with Van Gogh styles ([sty: \"Van Gogh style\"]). The task vector from SRFT will only represent the mapping relation between \"Van Gogh style\" and the output images. If a user prompts with \"Soldier in Van Gogh style,\" the model will most likely return with images in that style. To mitigate this issue, Moderator automatically expands the undefined context to ùëÅ vocabularies list by prompting an LLM (see Appendix Prompt. 1) , asking it to suggest common objects and actions associated with this style. Second, synonyms & sub-concepts expansions extend the policy to cover synonyms or sub-concepts (Figure 4). For instance, if the admin specifies the style: \"bloody\" in the policy, users can use the synonyms: \"bloody\", \"gore\", \"sanguinary\", etc. to bypass the moderation. Another example is that the admin specifies the object: \"Disneyland figures\" in the policy, but the user can craft sub-concepts: \"Mickey Mouse\", \"Donald Duck\", etc. to bypass the moderation. Moderator automatically expands the defined context to ùëÄ words (synonyms or sub-concepts) by prompting an LLM (see Appendix Prompt. 2). Third, description expansions extend the policy to cover de- scription attacks, where adversarial users may avoid the terms but prompt with exact descriptions. For example, a user may draw Don- ald Duck by prompting \"a cartoon duck with short and rounded body with a distinct protruding rear...\". Moderator automatically expands the policy with ùêæ plain description by prompting an LLM. While the policy expansion process makes the policy more com- prehensive, these expanded key phrases are often not effective prompts for generating diverse and realistic images. Thus, Moderator further uses an LLM to expand the contents into ùëã valid prompts (see the used prompt in Appendix Prompt. 3). Finally, Moderator constructs a text-to-image dataset using obtained prompts. 1girl, holding a Donald Duck doll, beautiful detailed eyes, beautiful detailed lips, extremely detailed eyes and face, long eyelashes, ... prompt REPLACE [obj: \"Disneyland figures\" with \"Mouse\"] BECAUSE \"copyright infringement\" policy 1girl, holding a Mickey Mouse doll, beautiful detailed eyes, beautiful detailed lips, extremely detailed eyes and face, long eyelashes, ... after moderation before moderation a b c d e f g Figure 4: Moderator allows admins to configure a (a) replace policy to replace Disneyland figures with a regular mouse. Through automatic policy expansion, Moderator also moder- ates relevant concepts (e.g., (b, c, d) Donald Duck and (e, f, g) Mickey Mouse) under ‚ÄúDisneyland figures,‚Äù even though the admins did not mention them explicitly in the policy. 5.4 Moderation Methods The appropriate methods for moderating content vary across con- texts. Moderator uses the task vector algebra composition method, similar to ¬ß5.2, to support three basic moderation methods: remove, replace, and mosaic. Note that other moderation methods can be developed using the same mechanism. We developed these three common methods to demonstrate feasibility. Remove is a basic moderation method that subtracts the task vector from the original TTI model. This method is suitable for moder- ating harmful content such as piracy and misinformation. After subtracting the task vector, the moderated models often respond to relevant prompts with alternative similar content. Replace is a versatile moderation method that replaces the harmful content in generated images with specified alternatives. For exam- ple, an admin may specify a policy that replaces \"Mickey Mouse\" (ùê¥) Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY (best quality, 4k, 8k, highres, masterpiece:1.2), ultra-detailed, in a watercolor style, vivid colors, sharp focus. {Mickey Mouse}. The character is dressed in a chef's outfit, including a white hat and apron, embodying a joyful demeanor while cooking in a brightly lit, cozy kitchen. (best quality, 4k, 8k, highres, masterpiece:1.2), ultra-detailed, in a watercolor style, vivid colors, sharp focus. {Mouse}. The character is dressed in a chef's outfit, including a white hat and apron, embodying a joyful demeanor while cooking in a brightly lit, cozy kitchen. prompt: image: (best quality, 4k, 8k, highres, masterpiece:1.2), ultra-detailed, in a watercolor style, vivid colors, sharp focus. {Mickey Mouse}. The character is dressed in a chef's outfit, including a white hat and apron, embodying a joyful demeanor while cooking in a brightly lit, cozy kitchen. a b c d e Figure 5: Moderator replaces content by reverse fine-tuning a special dataset, in which we map (a) the original prompt to (d) the output from (b) modified prompts. REPLACE [obj: \"Mickey Mouse\" with \"Mouse\"] BECAUSE \"copyright infringement\" Policy Pretrain model Weight space 1 Image: Prompt: Mickey Mouse Image: Prompt: Mickey Mouse Fine-tuned model Task vector Pretrain model Fine-tuned model Task vector Pretrain model - + Moderated model Mickey Mouse 2 3 - - - - Mickey Mouse replace Figure 6: Moderator achieves the replace method through three steps: (1) generating two datasets as shown in Figure 5: one is the undesired image dataset, and the other one is the replaced image dataset; (2) fine-tuning with the 2 datasets to compute 2 task vectors: an undesired task vector and a replace dataset separately; (3) negating the original model with the undesired task vector and adding the replace task vector to replace the desired image with the replace image. with \"Mouse\" (ùêµ) (See Figure 6). Moderator first computes the task vector for \"Mickey Mouse\" (ùúèùê¥‚Üíùë¶ùê¥ ) using the SRFT method (ùë¶ùê¥ denotes the image generated from ùê¥). Next, Moderator replaces the \"Mickey Mouse\" with \"Mouse\" in all the \"Mickey Mouse\" prompts and uses modified prompts to generate an image dataset ùë¶ùêµ (Figure 5). Finally, Moderator computes the task vector (ùúèùê¥‚Üíùë¶ùêµ ) using the \"Mickey Mouse\" prompts and the dataset ùë¶ùêµ. We formulate the process as follows: ùúÉùëüùëíùëùùëôùëéùëêùëí = ùúÉ + ùë†ùëêùëéùëôùëí ‚àó (ùúèùê¥‚Üíùë¶ùê¥ + ùúèùê¥‚Üíùë¶ùêµ ), 0 < ùë†ùëêùëéùëôùëí ‚â§ 1.0. In doing so, ùúÉùëüùëíùëùùëôùëéùëêùëí redirects the model from producing \"Mickey Mouse\" (ùê¥) to generating \"Mouse\" (ùêµ). Mosaic is a special replace method that replaces the target content with mosaic. Moderator first computes the task vector (ùúèùê¥‚Üíùë¶ùê¥ ) using the SRFT method. Next, Moderator modifies the dataset ùë¶ùê¥ by adding mosaic to the central region of the images, producing mosaic dataset ùë¶ùê¥,ùëöùëúùë†ùëéùëñùëê . Then, Moderator computes the task vec- tor (ùúèùê¥‚Üíùë¶ùê¥,ùëöùëúùë†ùëéùëñùëê ) based on the modified dataset. We formulate the mosaic process as follows. ùúÉùëöùëúùë†ùëéùëñùëê = ùúÉ + ùë†ùëêùëéùëôùëí ‚àó (ùúèùê¥‚Üíùë¶ùê¥ + ùúèùê¥‚Üíùë¶ùê¥,ùëöùëúùë†ùëéùëñùëê ), 0 < ùë†ùëêùëéùëôùëí ‚â§ 1.0. REMOVE [sty: \"Van Gogh\" by $scale] BECAUSE \"copyright infringement\" policy $scale [0.0-1.0] a b c d e (best quality, 4k, 8k, highres, masterpiece: 1.2), ultra-detailed, ( realistic, photorealistic, photo realistic: 1.37 ), soldier in Van Gogh style, ... prompt f Figure 7: As the scale value (a) increases, the output images (c, d, e, f) for the same prompt (b) show less \"Van Gogh\" style. 5.5 Multi-policy Interference Our discussion thus far has been constrained to moderating the model with one policy. However, admins often need to specify multiple policies for an individual model, and these policies may impact each other‚Äôs moderation goals. Previous research [31, 81] find that the interference could stem from two major causes. First, many model parameters may change during the fine-tuning process. However, only a tiny percentage of them (influential parameters) are critical for the specific task. When merging parameters, the influential parameters might be obscured by the peripheral param- eters. Second, different task vectors‚Äô positive and negative values may cancel each other. Moderator uses the TIES-Merging (trim, elect sign & merge) method[81], to mitigate the interference. First, Moderator trims each task vector to retain only the top-20% largest-magnitude values and reset the rest to their initial value (i.e., setting the value to 0). Next, Moderator elects the sign by calculating the cumulative sum of task vectors with positive and negative signs, respectively, and choosing the sign with a greater cumulative sum for each parameter. Finally, Moderator merges the parameters by only keeping the parameter values from the task vectors whose signs are the same as the elected sign and calculating their mean. Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. a b c d d d e f Figure 8: The policy authoring interface of Moderator. The admin can initiate the process by (a) clicking the \"Add Policy\" button and (b) input example prompts. Then, the admin can (c) edit the policy and examine its effect by (d) clicking the \"Generate Image\" button to generate example images from test prompts. Next, the admin can (e) save the edited policy by clicking the \"Save Policy\" button. Finally, the admin may (f) click the \"Activate Chosen Policies\" button to enable multiple selected policies. 5.6 Advanced Policies Moderator allows admins to specify the low-level model transfor- mation policies by controlling the policy expansion process and the ùë†ùëêùëéùëôùëí of the SRFT process. For instance, if an admin specifies a broad policy with too many sub-concepts (e.g., \"American politi- cian\"), the default expansion may not be sufficient. The admin may use an advanced expansion function as: expand(\"American politi- cian\", space=\"sub-concepts\", number=30), where space denotes the expansion type (\"blank\"/\"sub-concepts\"/ \"description\") and number denotes the number of expanded prompts. Moderator also enables admins to adjust the ùë†ùëêùëéùëôùëí parameter of the SRFT method within a range of 0 to 1.0 (Figure 7). 6 IMPLEMENTATION We implemented policy authoring interface (Figure 8) that allows admins to author and debug their policies using Python Flask [54]. We deployed the Vicuna-13B model [16] locally to expand the poli- cies. We chose this model due to its absence of censorship, as the policy expansion process requires elaborating on harmful contexts. We integrated Moderator with two open-source, popular text-to- image models: Stable Diffusion (SD) [62] and Stable Diffusion XL (SDXL) [56]. For both models, we set the image size to 1024√ó1024. We ran Moderator on Intel Xeon Gold 5218R and RTX 4090 (24GB). 7 EVALUATION 7.1 Moderator configurations During our development, we found that the parameters of the SRFT method (e.g., the ùë†ùëêùëéùëôùëí, the number of fine-tuning steps, the num- ber of fine-tuning images, and the learning rate) can significantly impact the moderation results. We experimented with different parameters to understand the trade-offs. Metric. Moderator‚Äôs goal is to moderate the target content spec- ified in the policy while minimizing the impacts on other tasks. Based on this goal, we classified the output images into three cat- egories: target content, related content, and unrelated content. For example, in the context of \"Tom Hanks advertises McDonald,\" the related content is \"Tom Hanks\" and \"advertise McDonald,\" and the unrelated content is a random task that is not relevant to \"Tom Hanks\" and \"McDonald.\" We used Contrastive-Language-Image-Pre-training (CLIP) [58] to quantify Moderator‚Äôs moderation performance. CLIP is an OpenAI model that learns to recognize images by matching them with textual descriptions, which had been widely used in previous model editing research [25, 87]. CLIP outputs a similarity score between 0 and 1.0. Ideally, we hope to see low CLIP scores for target content, implying that it has been effectively moderated, and high CLIP scores for the other two types of content, suggesting that their impact on other tasks is negligible. Method. We evaluated the moderation performance of three meth- ods (remove, replace, and mosaic). The authors manually selected Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Moderated content Related content Unrelated content Stable Diffusion-1.5 Stable Diffusion-XL 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 remove (a) scale 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 replace (b) scale 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 mosaic (c) scale 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 remove (d) scale 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 replace (e) scale 0.2 0.4 0.6 0.8 1.0 0.0 0.5 1.0 mosaic (f) scale 200 400 600 800 1000 0.0 0.5 1.0 (g) fine-tuning steps 200 400 600 800 1000 0.0 0.5 1.0 (h) fine-tuning steps 200 400 600 800 1000 0.0 0.5 1.0 (i) fine-tuning steps 200 400 600 800 1000 0.0 0.5 1.0 (j) fine-tuning steps 200 400 600 800 1000 0.0 0.5 1.0 (k) fine-tuning steps 200 400 600 800 1000 0.0 0.5 1.0 (l) fine-tuning steps 40 80 120 160 200 0.0 0.5 1.0 (m) image number 40 80 120 160 200 0.0 0.5 1.0 (n) image number 40 80 120 160 200 0.0 0.5 1.0 (o) image number 40 80 120 160 200 0.0 0.5 1.0 (p) image number 40 80 120 160 200 0.0 0.5 1.0 (q) image number 40 80 120 160 200 0.0 0.5 1.0 (r) image number 1e-6 2e-6 5e-6 1e-5 0.0 0.5 1.0 (s) learning rate 1e-6 2e-6 5e-6 1e-5 0.0 0.5 1.0 (t) learning rate 1e-6 2e-6 5e-6 1e-5 0.0 0.5 1.0 (u) learning rate 1e-6 2e-6 5e-6 1e-5 0.0 0.5 1.0 (v) learning rate 1e-6 2e-6 5e-6 1e-5 0.0 0.5 1.0 (w) learning rate 1e-6 2e-6 5e-6 1e-5 0.0 0.5 1.0 (x) learning rate Figure 9: We evaluated the moderation effectiveness of different configurations for three methods: remove, replace, and mosaic. The y-axis represents the CLIP score, which indicates the matching score between the output images and the prompts. Lower scores for \"moderated content\" and higher scores for \"related content\" and \"unrelated content\" signify better performance. 10 harmful prompts from I2P [66] and then created one policy for each prompt. They then derived 10 related prompts from the harm- ful prompts and randomly selected 10 unrelated benign prompts from the Stable Diffusion Prompt dataset [29]. Note that none of the test prompts were used in the development of Moderator. Results. Figure 9 (a-f) shows a decrease in CLIP scores for moder- ated content as the scale values increased, while scores for unrelated and related content remained stable. Therefore, we empirically set the task vector scale to 1.0. Figure 9 (g-l) illustrates that the numbers of the fine-tuning steps have varying impacts on different moderation methods. For the \"remove\" and \"mosaic\" methods, the performance gain saturates at 600 steps. For the \"replace\" method, increasing fine-tuning steps slightly affected CLIP scores for related and unrelated content but dramatically reduced moderated content scores to nearly zero at 1000 steps. Therefore, we set the number of fine-tuning steps to 600 for \"remove\" and \"mosaic\" and to 1000 for \"replace.\" Figure 9 (m-r) indicates that the number of images for fine-tuning had little impact on the CLIP scores of unrelated and related content. For moderated content, the CLIP scores saturate at 120 images and show minimal further improvement as the number of images increases. Considering the increased cost of generating and training more images, we set the number of images to 120. Figure 9 (s-x) depicts that the learning rate significantly impacts Moderator‚Äôs effectiveness and should not be excessively high. An excessively high rate will deteriorate the Moderator‚Äôs performance on related content and unrelated content. We set the learning rate to 5ùëí ‚àí 6 for SD-1.5 and 2ùëí ‚àí 6 for SDXL. 7.2 Effectiveness of Content Moderation We evaluated the effectiveness of moderating harmful content. Method. Since no automated criteria exist to quantify the modera- tion effects, we used two methods to approximate the effectiveness. First, we measured the CLIP scores between the prompts and the output images before and after moderation. Second, similar to [88], we used LLM as a judge to assess the harmfulness of output images before and after moderation. We used a BLIP model [42] to convert the generated images back to text descriptions and then instructed the Vicuna-7b model [16] to rate the harm of the resulting text on a scale from 0 to 10 (See Appendix Prompt 4). The first approach provides a relative perspective on moderation effectiveness, while the second offers an absolute and complementary perspective. We reused the 10 harmful prompts and the moderated models from ¬ß7.1. We used these prompts to generate 100 images each for the original and the moderated models. We further classified the policies into six categories: \"remove object/action/style,\" \"re- place object/action,\" and \"mosaic object,\" and clustered the results according to these categories. Furthermore, we compared the moderation effectiveness against existing TTI moderation methods. We developed five policies to moderate five types of content: blood, nudity, excrement, pornog- raphy, and violence. Using SneakyPrompt [82], we created 200 advanced malicious prompts for all categories. We used four other open-source TTI moderation methods (text-match, text-classifier, image-classifier, and image-clip-classifier) from [82] as the base- lines. We considered an adversarial prompt to successfully bypass a moderation approach if the output of a moderated model closely matched that of an unmoderated model. Results. Figure 10 shows that Moderator significantly reduces the harmfulness of images generated with inappropriate prompts. Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. Before moderation After moderationremove obj replace obj mosaic obj replace act remove sty replace sty 0.0 0.2 0.4 0.6 0.8 1.0CLIP Score (a) SD 1.5-CLIP Score remove obj replace obj mosaic obj replace act remove sty replace sty 0 2 4 6 8 10LLM-as-judge (b) SD 1.5-LLM Harm Rate remove obj replace obj mosaic obj replace act remove sty replace sty 0.0 0.2 0.4 0.6 0.8 1.0CLIP Score (c) SDXL-CLIP Score remove obj replace obj mosaic obj replace act remove sty replace sty 0 2 4 6 8 10LLM-as-judge (d) SDXL-LLM Harm Rate Figure 10: Moderator mitigates harmful images generated with inappropriate prompts. All scores of the moderated models are within 10% of the CLIP scores and 40% of the harm scores of the original models. The average CLIP scores are over 0.75 for the images generated by the original models, and the harm scores assessed by LLM exceed 7 for both SD and SDXL models. After moderation, CLIP scores for generated images fall below 0.10, and LLM harm rates drop below 4 for both models across all methods. Figure 11 shows that Moderator can prevent the majority of advanced prompts from generating undesired content. Furthermore, Moderator achieves a better defense effect against malicious prompts than baselines. (a) text-match (b) text-classifier (c) image-classifier (d) image-clip-classifier (e) Moderator (a) (b) (c) (d) (e) 0.0 0.2 0.4 0.6 0.8 1.0Bypass Rate 0.95 0.64 0.90 0.90 0.05 (a) SD-1.5 (a) (b) (c) (d) (e) 0.0 0.2 0.4 0.6 0.8 1.0Bypass Rate 0.99 0.72 0.99 0.97 0.08 (b) SDXL Figure 11: Moderator prevented most advanced prompts from generating undesired content and outperforms the four TTI moderation baselines against SneakyPrompt. 7.3 Enforcing Multiple Policies Method. We selected six policies (Table 2) and corresponding test prompts from ¬ß7.1, each representing a different moderation method. We then compared Moderator against two alternative task vector merging methods: (1) adding up all policies‚Äô task vectors[33] (Sum); (2) computing the average of all policies‚Äô task vectors (Uni- form Sum). Results. Figure 12 illustrates the CLIP scores of three task vector merging methods. The Sum method results in low CLIP scores for all content types, indicating effective moderation of target content and undesired interferences on non-target content. The Uniform # Policy Related Content 1 Remove Tom Hanks Elon Mask 2 Replace Mickey Mouse with Mouse Mouse 3 Mosaic Snake Lizard 4 Replace Fight with Kiss Hug 5 Remove Bloody Sweaty 6 Replace Realistic with Cartoon Hyperrealism Table 2: Six policies used to examine the efficacy of Moderator in enforcing multiple policies simultaneously. Before Moderation Sum Uniform Sum Moderator Moderated Related Unrelated 0.0 0.2 0.4 0.6 0.8 1.0CLIP Score (a) SD 1.5 Moderated Related Unrelated 0.0 0.2 0.4 0.6 0.8 1.0CLIP Score (b) SDXL Figure 12: Moderator achieved low CLIP scores on moderated content and high scores on other content types. Sum method produces CLIP scores close to the original model for unrelated content and relatively high scores for moderated and related content. While it has little impact on non-target content, it falls short of moderating the target content, likely because the weight of all task vectors is reduced by the average operation. Moderator achieves low CLIP scores on moderated content and high scores on non-target content types, suggesting the strength of Moderator in handling multiple policy interferences. Moderated Content Unrelated Content Original 6 10 20 Policy Number 0.0 0.2 0.4 0.6 0.8 1.0CLIP Score (a) SD-1.5 Original 6 10 20 Policy Number 0.0 0.2 0.4 0.6 0.8 1.0CLIP Score (b) SDXL Figure 13: As the number of policies increases, Moderator remains effective at moderating target content while moder- ately lowering the generation quality of unrelated content. Method. We conducted experiments with 6, 10 and 20 policies to assess the potential performance impact associated with the number of policies. We manually selected 20 harmful prompts from I2P [66] and then created one policy for each prompt. Then, we randomly selected 6, and 10 policies out of the 20 selected policies for the first two sub-experiments. Specifically, we measured the change in CLIP scores for both moderated and unrelated content before and after moderation. Results. Figure 13 indicates that while policy interferences can negatively impact the generation of non-moderated content, they have minimal effect on moderating target content. Furthermore, the performance of generating unrelated content decreases as the number of activated policies increases. Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY # Content Moderation reason Context Purpose 1 Mickey Mouse Cartoon characters lead to copyright infringement. object Copyright infringement 2 Einstein‚Äôs face Malicious fake news pictures of celebrities. object Defamation 3 Self-harm Pictures containing self-harm behaviors. action Self-harm 4 Give the Middle finger Images suggest insulting behavior. action Defamation/ discrimination 5 Bloody Misuse models to draw bloody pictures. style Bloody content 6 Dark, gloomy Dark, gloomy pictures suggest self-harm. style Self-harm 7 Einstein gives middle finger Images defame celebrities. combined Defamation/illegal activities 8 Bloody arms Bloody arms suggest self-harm behaviors. combined Bloody content/self-harm Table 3: The moderation policy authoring tasks in ¬ß7.4. For each task, we provided the moderated content, why we moderated it, the context level for the content, and the purpose. 7.4 Policy Usability for Admins We conducted an IRB-approved study to evaluate the usability. Participants: We recruited 14 participants (see Table 5) to play the role of admins (9 identified as male, 5 identified as female, aged 23- 26) from universities through email or social media. Among these participants, 10 have online community moderation experiences, and 10 have experience in developing gen-AI services or conducting gen-AI research. The study took about 2 hours for each participant. Each participant received a $10 gift card as compensation. Method. Each study included a 10-minute walk-through, an asyn- chronous policy authoring period, and a 10-minute debriefing. We provided a brief tutorial to help participants become familiar with the Moderator interface and then asked them to create content moderation policies for 4 randomly selected tasks (see Table 3). We provided a detailed description for each task, including the moder- ation context and goal, along with 20 unit test prompts (10 harmful and 10 benign). Participants could preview the policy‚Äôs impact on the model-generated content and the visual quality using sample images generated before and after moderation. If unsatisfied, they could iteratively redesign the policy until achieving the desired outcome. Since fine-tuning can take 10-30 minutes to complete, we made the policy authoring process asynchronous and recorded partici- pants‚Äô time spent authoring policies using the authoring interface. During the debriefing period, we asked the participants to fill out the System Usability Scale (SUS) questionnaire [3] and to participate in a semi-structured interview. Results. On average, participants took 2.29 iterations and 2.11 minutes to create a policy that can pass all the unit tests (Figure 14). We observed no significant differences in the task completion time across all tasks. Note that a SUS score above 68 is considered above average [64]. Participants report an average SUS score of 80.71, suggesting developers find it easy to craft policies using Moderator. Overall, participants appreciated the fine-grained control and the intuitiveness and flexibility of the policy language: \"Moderator is useful for designing moderation policies, especially since I need to craft a fine-grained policy that moderates specific cases without excessively broad scope.\" (P9) \"[The contexts-based policy design] can express my moderation goals clearly and align well with Stable Diffusion‚Äôs nature.\" (P4) \"The expand function in Moderator is useful. It allows me to author the policy to moderate a broad scope with a single policy.\" (P2) T1 T2 T3 T4 T5 T6 T7 T8 Task Id # 1 2 3 4 5 6 7No. Author Iterations (a) Number of Iterations T1 T2 T3 T4 T5 T6 T7 T8 Task Id # 1 2 3 4Task Completion (min) (b) Task Completion Time Figure 14: We asked participants to create policies on 8 de- signed tasks on Moderator. On average, each task took a par- ticipant about 2.29 iterations and 2.11 minutes to create a policy that could pass all the unit tests. Expert Non-Expert Expertise in GenAI 40 60 80 100SUS Score Expert Non-Expert Expertise in Content Moderation 40 60 80 100SUS Score Figure 15: Users find it easy to use Moderator. A few participants finished the task in advance and attempted to create prompts to bypass the policies they created. One interesting finding is that the \"replace\" policy works best when the replaced content is relevant to the target content. For example, in T7:Einstein gives the middle finger, P8 specified a policy to replace \"Einstein\" with \"a bear.\" While the moderated model can pass all the unit tests, it was later discovered that the model could also mistakenly depict other people as bears. However, the moderated model, which replaces \"Einstein\" with \"a different person,\" works well. This may be because the significant differences in concept scope and meanings may make aligning these two task vectors hard. 7.5 Mitigating User Attacks Participants. We recruited 32 participants from our institutions (21 identified as male and 11 as female; average age 21.8). Each participant received $1 in compensation, with an additional $1 for successfully bypassing the moderation mechanism. Method. We instructed participants to design prompts to generate target content using both the original SDXL model and its mod- erated versions. Each session began with a 10-minute tutorial on the Moderator and included warm-up tasks to guide participants. Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. # Task Context Method Policy User Task T1 Tom Hanks images lead to likeness infringement. Remove obj Remove Tom Hanks Tom Hanks T2 Mickey Mouse images lead to copyright infringement. Replace obj Replace Mickey Mouse with Mouse Mickey Mouse T3 Snakes are horrible to children Mosaic obj Mosaic cat Cat T4 Kissing are inappropriate behaviors in some places. Replace act Replace kisses with hugs in Einstein kisses people Einstein kisses people T5 Bloody arms suggest self-harm. Remove sty Remove sweaty in sweaty arm Sweaty arm T6 Greasy tentacle-style affects users‚Äô mental health. Replace sty Replace cartoon with realistic Cartoon style Table 4: Attack tasks for users. To minimize the potential harm to the participants, we did not directly use potentially harmful content that required moderation, but used alternative objects instead, such as using a \"cat\" for a \"snake.\" After a brief overview of the study‚Äôs purpose and payment de- tails, we randomly assigned each participant two scenarios from Table 4. Participants were not made aware of the moderation policy to emulate real-world situations. We presented these four tasks (2 scenarios √ó 2 models) in a randomized order. Each participant had 15 attempts per task. After the study, we asked participants to complete a questionnaire about their experiences in bypassing the moderation mechanism. Pretrain Task Moderator Task T1 T2 T3 T4 T5 T6 Task Id # 0.0 0.2 0.4 0.6 0.8 1.0 (a) Successful Task Portion T1 T2 T3 T4 T5 T6 Task Id # 0 3 6 9 12 15 (b) No. Attempts in Successful Atk. Figure 16: All participants were able to quickly craft prompts that instructed the original model to generate undesired con- tent. In contrast, 65% of the participants could not generate target content within 15 attempts, and the remaining partici- pants needed, on average, 8.3 times more attempts to bypass the moderation. Results. Figure 16 shows that Moderator can make it much signif- icantly more challenging for users to produce images that violate the policies. Most of the attackers (65%) involved in the experiment could not bypass the moderation. A small number of participants who bypassed the moderation also needed to make 8.3 times more attempts. Below, we discussed a few examples of how participants bypassed moderation. Further expanding the policies should miti- gate these attacks at the cost of increased computation costs. ‚Ä¢ Participants sometimes can have Mickey Mouse drawn when describing other Disney characters in T2. ‚Ä¢ Participants drew the moderated concept of \"cat\" in T3 by de- scribing \"Cute Small Tiger.\" ‚Ä¢ Participants described \"Water on the hand\" in T5 to draw the moderated \"Sweaty Hand.\" ‚Ä¢ Participants bypassed the moderation policy by detailing the moderated content in the prompt. In T4, participants detailed the \"kiss\" scene in the prompts to generate images of kiss. 7.6 System Performance Method. Running Moderator involves four stages: data generation, fine-tuning, task vector extraction, and model editing. We evaluated data generation model finetuning task vector extraction model moderationremove replace mosaic 0 500 1000Time Cost(/s) (a) Time Cost of SD-1.5 remove replace mosaic 0 500 1000 1500Time Cost(/s) (b) Time Cost of SDXL Figure 17: Time costs for different steps in Moderator. the time costs of all stages and the end-to-end performance of three moderation methods. We assessed the system performance with the SD v1.5 and SDXL models on a Nvidia-V100 GPU with 32GB memory. We tested Moderator with the tasks in ¬ß7.2 and reported the average time cost of 10 repetitions. Results. Figure 17 shows that Moderator could transform a Stable Diffusion model into a moderated version within 16 minutes (SD) and 30 minutes (SDXL), with the major runtime overhead occurring during data generation and fine-tuning. The remove method took more time in data generation compared to the other two methods because it needed to generate an additional dataset for the replaced content. In the fine-tuning stage, the replace and mosaic methods took twice as long as the remove method, as they required fine- tuning twice rather than once. 8 RELATED WORK Policy design & enforcement. Many systems have introduced new policies and improved their security and privacy by enforcing these policies [8, 9, 12, 15, 41, 43, 46, 79, 80]. At its core, policies are an abstraction understandable for both the users and the sys- tems. For example, IoTGuard [12] monitors the behavior of IoT and trigger-action platform apps and blocks unsafe and undesired states according to specified policies. Beyond this all-or-nothing control, PFirewall [15] and Peekaboo [40] use data-minimization policies/manifests to modify the data flow based on its semantics. In contrast to these systems, Moderator aims to design policies to control an unusual type of data flow, the output of TTI models. First, Moderator needs to moderate the images rather than the textual data based on their semantics. Second, no clear guidelines exist to determine what semantics/content are considered appropriate. Third, TTI models can comprehend arbitrary input prompts, and the mapping relations between the prompts and output images are non-deterministic. We designed Moderator to overcome all these three challenges. Machine unlearning & model editing are emerging tasks in deep learning. Machine unlearning aims to remove the influence of Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY a training (x, y) pair on a supervised model without damaging the model‚Äôs performance [10, 13, 14]. Common unlearning approaches include gradient-based weight updates, using influence functions [28] and continual learning methods [73]. Model editing focuses on changing outputs for certain inputs to align models with users‚Äô goals [6, 33, 89]. For example, Gandikota et al. [25] experimented to delete specific concepts from TTI models. Hase et al. [30] conducted experiments to edit language models‚Äô beliefs. Unlike previous efforts that focused on one-time experiments, we developed an end-to-end system that enables user interaction. Moreover, our system supports the implementation of detailed moderation policies to meet diverse real-world moderation require- ments, whereas earlier projects typically aimed to eliminate just one particular type of content. Furthermore, Moderator introduces a generalizable primitive applicable to multiple moderation meth- ods (i.e., removal, replacement, mosaic), while previous projects often only \"forget\" one object or one style. Text-to-image model safety. Previous works have discussed safety issues in TTI models and potential mitigation methods [39, 57, 67]. The most common approach is to detect sensitive words in the prompts and deny them. For example, several projects attempt to replace sensitive words with non-sensitive ones in the prompt us- ing a keyword list [59] and a machine-learning-based classifier [19]. Alternatively, the model runtime may also detect undesired content in the output [59], including computing the similarity of image text embeddings with the text embeddings of sensitive concepts [62]. In contrast, Moderator adopts a slightly different model, which assumes that future TTI models may run locally, where moderating prompts and output images would be less effective. 9 DISCUSSION & LIMITATION Misinformation in moderated content. The capability of TTI to generate imaginative content does not change after moderation. The goal of Moderator is not to eliminate misinformation. For ex- ample, when we moderate \"Trump being arrested\" with \"Trump handshaking with someone,\" this essentially introduces misinfor- mation. Instead, our goal is to help admins control the output space better, pivoting away from generating the most harmful content. Potential misuse for censorship. Moderator has the potential to be used for censorship. Balancing the need for moderation while preserving free speech is a complex challenge. It requires transpar- ent policies, oversight, and the inclusion of diverse perspectives to ensure that moderation practices do not inadvertently or in- tentionally silence important voices. The \"purpose\" annotation in Moderator policies is an initial step. Interferences between policies. We show Moderator‚Äôs policies still interfere with each other as the policy number increases (¬ß7.3), since our current technique model merging method, ties-merging, is imperfect. Future improvements in model merging techniques will enhance the scalability of our approach. Reliance on LLM. Utilizing LLMs to generate diverse prompts in content moderation remains a challenge. The black-box nature of LLMs can lead to biases and a lack of interpretability. Future work may consider alternative approaches like rule-based methods, which offer transparent criteria for moderation but may lack gen- eralized capabilities. For instance, it would be challenging to use the rule-based methods to cover all sub-concepts. 10 FUTURE WORK Quality of Policies. The moderation quality can be influenced by the quality of specified policies. Moderator is a deny-list-only system, so it prefers more fine-grained policies rather than broad ones. If a target moderation concept (object, action, style) is too broad, admins need to use the expand command to include more examples for reverse fine-tuning. As the system scales, future policies might be authored by differ- ent admins unaware of others‚Äô policies, leading to potential conflict- ing policies. Future work may analyze the task vectors associated with different policies and provide automatic conflict resolutions. Future work also aims to assist admins in determining the appro- priate moderation granularity and merging redundant policies. Unit tests. One promising design of Moderator is the unit tests for debugging different policies, which help admins understand the effectiveness of their policies. However, currently, admins have to specify the test examples manually. Future work may explore methods for generating these unit tests automatically and design better quantitative tools to facilitate the debugging process. Real-time Moderation. Currently, Moderator does not cache any task vectors since each task vector occupies the same size as the original model. As a result, it takes 10 - 30 minutes to moderate a model each time. Future research on model compression [44] will allow us to store various task vectors for individual policies, enabling on-the-fly fine-tuning by caching task vectors. Moderating other \"next-token prediction\" models. Since most generative AI models share the same prompt-output fine-tuning paradigm with the TTI models, we can generalize Moderator to them. For instance, if the admins want to moderate GPT, they can use Moderator by swapping the prompt-to-image datasets to the prompt-to-response datasets. However, the policy contexts need to be modified to align with the target model. For instance, if one wants to apply Moderator on the text-to-music models, he needs to change the contexts to [\"genre\", \"instrument\", \"rhythm\"]. 11 CONCLUSION This paper presents Moderator, a policy-based model management system that enables admins to use a text-to-image model as in- put, dynamically configure the policies and modify the weights of the original model based on the policies. We first collected 153 potentially problematic prompts, examined why these prompts are problematic and explored how we can moderate the output to mitigate harm. We then designed a simple and expressive policy language to help admins effectively articulate their content modera- tion goals and a runtime to enforce the policies through self-reverse fine-tuning. Our evaluation suggests that the policy language is easy for admins to use, and Moderator makes it significantly more challenging for users to produce images that violate these policies. Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. REFERENCES [1] Bobby Allyn. 2024. Google races to find a solution after AI generator Gemini misses the mark : NPR. https://www.npr.org/2024/03/18/1239107313/google- races-to-find-a-solution-after-ai-generator-gemini-misses-the-mark. (Accessed on 04/05/2024). [2] Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang, Zhan Qin, Zhibo Wang, and Kui Ren. 2023. SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via Substitution. arXiv:2309.14122 [cs.CV] [3] Aaron Bangor, Philip T Kortum, and James T Miller. 2008. An empirical evaluation of the system usability scale. Intl. Journal of Human‚ÄìComputer Interaction 24, 6 (2008), 574‚Äì594. [4] Jon Bateman, Natalie Thompson, and Victoria Smith. 2021. How Social Media Platforms‚Äô Community Standards Address Influence Operations. (2021). [5] BBC. 2023. Fake Trump arrest photos: How to spot an AI-generated image. https: //www.bbc.com/news/world-us-canada-65069316. (Accessed on 10/11/2023). [6] Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, and Stella Biderman. 2024. Leace: Perfect linear concept erasure in closed form. Advances in Neural Information Processing Systems 36 (2024). [7] Ali Borji. 2022. Generated faces in the wild: Quantitative comparison of stable diffusion, midjourney and dall-e 2. arXiv preprint arXiv:2210.00586 (2022). [8] Chengjun Cai, Yichen Zang, Cong Wang, Xiaohua Jia, and Qian Wang. 2022. Vizard: A metadata-hiding data analytic system with end-to-end policy controls. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communica- tions Security. 441‚Äì454. [9] Stefano Calzavara, Alvise Rabitti, and Michele Bugliesi. 2017. CCSP: Controlled Relaxation of Content Security Policies by Runtime Policy Composition. In 26th USENIX Security Symposium (USENIX Security 17). 695‚Äì712. [10] Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine unlearning. In 2015 IEEE symposium on security and privacy. IEEE, 463‚Äì480. [11] Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval. Acm Computing Surveys (CSUR) 44, 1 (2012), 1‚Äì50. [12] Z Berkay Celik, Gang Tan, and Patrick D McDaniel. 2019. Iotguard: Dynamic enforcement of security and safety policy in commodity IoT.. In NDSS. [13] Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang. 2021. When Machine Unlearning Jeopardizes Privacy. In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security (Virtual Event, Republic of Korea) (CCS ‚Äô21). Association for Computing Machin- ery, New York, NY, USA, 896‚Äì911. https://doi.org/10.1145/3460120.3484756 [14] Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang. 2022. Graph Unlearning. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (Los Angeles, CA, USA) (CCS ‚Äô22). Association for Computing Machinery, New York, NY, USA, 499‚Äì513. https://doi.org/10.1145/3548606.3559352 [15] Haotian Chi, Qiang Zeng, Xiaojiang Du, and Lannan Luo. 2019. Pfirewall: Semantics-aware customizable data flow control for home automation systems. arXiv preprint arXiv:1910.07987 (2019). [16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/ [17] Sheng-Yen Chou, Pin-Yu Chen, and Tsung-Yi Ho. 2023. How to Backdoor Diffusion Models? arXiv:2212.05400 [cs.CV] [18] European Commission. 2020. The Digital Services Act: Ensuring a safe and accountable online environment. The Digital Services Act: Ensuring a safe and accountable online environment (2020). [19] Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil K Jain. 2020. On the detection of digital face manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern recognition. 5781‚Äì5790. [20] Innovation Department for Science, Technology, and Media & Sport De- partment for Digital, Culture. 2022. A guide to the Online Safety Bill. https://www.gov.uk/guidance/a-guide-to-the-online-safety-bill#types-of- content-that-will-be-tackled. (Accessed on 10/31/2023). [21] Sabine A Einwiller and Sora Kim. 2020. How online content providers moderate user-generated content to prevent harmful online communication: An analysis of policies and their implementation. Policy & Internet 12, 2 (2020), 184‚Äì206. [22] Facebook. [n. d.]. Facebook Community Standards. https://transparency.fb.com/ zh-cn/policies/community-standards/. (Accessed on 11/02/2023). [23] Shiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen, Xiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma, and Xiangyu Zhang. 2023. Detecting Backdoors in Pre-trained Encoders. arXiv:2303.15180 [cs.CV] [24] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2022. Training- free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032 (2022). [25] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. 2023. Erasing concepts from diffusion models. arXiv preprint arXiv:2303.07345 (2023). [26] Tarleton Gillespie. 2018. Custodians of the Internet: Platforms, content moderation, and the hidden decisions that shape social media. Yale University Press. [27] Google. [n. d.]. Content delistings due to copyright. https://transparencyreport. google.com/copyright/overview. (Accessed on 10/30/2023). [28] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. 2019. Certified data removal from machine learning models. arXiv preprint arXiv:1911.03030 (2019). [29] Gustavosta. 2023. Stable Diffusion Prompts Dataset. https://huggingface.co/ datasets/Gustavosta/Stable-Diffusion-Prompts. Accessed: 2024-04-28. [30] Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. 2021. Do language models have beliefs? methods for detecting, updating, and visualizing model beliefs. arXiv preprint arXiv:2111.13654 (2021). [31] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research 22, 241 (2021), 1‚Äì124. [32] The White House. 2023. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. https://www.whitehouse. gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the- safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/. (Accessed on 10/31/2023). [33] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089 (2022). [34] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Han- naneh Hajishirzi, Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. 2022. Patch- ing open-vocabulary models by interpolating weights. Advances in Neural Infor- mation Processing Systems 35 (2022), 29262‚Äì29277. [35] Instagram. 2018. Instagram Community Guidelines FAQs. https://about. instagram.com/blog/announcements/instagram-community-guidelines-faqs/. (Accessed on 11/02/2023). [36] Internet Watch Foundation. 2023. iwf-ai-csam-report_public-oct23v1.pdf. https: //www.iwf.org.uk/media/q4zll2ya/iwf-ai-csam-report_public-oct23v1.pdf. (Ac- cessed on 12/04/2023). [37] Jialun Aaron Jiang. 2020. Identifying and Addressing Design and Policy Chal- lenges in Online Content Moderation. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI EA ‚Äô20). Association for Computing Machinery, New York, NY, USA, 1‚Äì7. https://doi.org/10.1145/3334480.3375030 [38] Jialun Aaron Jiang, Peipei Nie, Jed R Brubaker, and Casey Fiesler. 2023. A trade- off-centered framework of content moderation. ACM Transactions on Computer- Human Interaction 30, 1 (2023), 1‚Äì34. [39] Zhengyuan Jiang, Jinghuai Zhang, and Neil Zhenqiang Gong. 2023. Evading Watermark based Detection of AI-Generated Content. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (Copenhagen, Denmark) (CCS ‚Äô23). Association for Computing Machinery, New York, NY, USA, 1168‚Äì1181. https://doi.org/10.1145/3576915.3623189 [40] Haojian Jin, Gram Liu, David Hwang, Swarun Kumar, Yuvraj Agarwal, and Jason I Hong. 2022. Peekaboo: A hub-based approach to enable transparency in data processing within smart homes. In 2022 IEEE symposium on security and privacy (SP). IEEE, 303‚Äì320. [41] Yu-Tsung Lee, William Enck, Haining Chen, Hayawardh Vijayakumar, Ninghui Li, Zhiyun Qian, Daimeng Wang, Giuseppe Petracca, and Trent Jaeger. 2021. {PolyScope}:{Multi-Policy} Access Control Analysis to Compute Authorized Attack Operations in Android Systems. In 30th USENIX Security Symposium (USENIX Security 21). 2579‚Äì2596. [42] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning. PMLR, 12888‚Äì12900. [43] Xing Li, Yan Chen, Zhiqiang Lin, Xiao Wang, and Jim Hao Chen. 2021. Automatic policy generation for {Inter-Service} access control of microservices. In 30th USENIX Security Symposium (USENIX Security 21). 3971‚Äì3988. [44] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. 2024. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems 36 (2024). [45] Han Liu, Yuhao Wu, Shixuan Zhai, Bo Yuan, and Ning Zhang. 2023. Riatig: Reliable and imperceptible adversarial text-to-image generation with natural prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 20585‚Äì20594. [46] Zhuotao Liu, Hao Jin, Yih-Chun Hu, and Michael Bailey. 2016. MiddlePolice: Toward Enforcing Destination-Defined Policies in the Middle of the Internet. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (Vienna, Austria) (CCS ‚Äô16). Association for Computing Machinery, New York, NY, USA, 1268‚Äì1279. https://doi.org/10.1145/2976749.2978306 Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY [47] Renkai Ma. 2023. Conceptualizing and Improving Creator Moderation Design with Platform Stakeholders. In Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing. 462‚Äì465. [48] Government of India Ministry of Electronics & Information Technology. 2021. A Notification dated, the 25th February, 2021 G.S.R. 139(E): the Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules, 2021. https://www.meity.gov.in/content/notification-dated-25th-february-2021- gsr-139e-information-technology-intermediary. (Accessed on 10/31/2023). [49] Marije Nouwen, Nassim JafariNaimi, and Bieke Zaman. 2017. Parental controls: reimagining technologies for parent-child interaction. In Proceedings of 15th European Conference on Computer-Supported Cooperative Work-Exploratory Papers. European Society for Socially Embedded Technologies (EUSSET). [50] Government of Canada. 2023. The Government‚Äôs commitment to address online safety. https://www.canada.ca/en/canadian-heritage/campaigns/harmful-online- content.html. (Accessed on 10/31/2023). [51] Ministry of Foreign Affairs of Japan. 2022. Ministry of Foreign Affairs of Japan Social Media Moderation Policy. https://www.mofa.go.jp/p_pd/ipr/page25e_ 000056.html. (Accessed on 10/31/2023). [52] Cyberspace Administration of China Office of Central Cyberspace Affairs Com- mission. 2019. Provisions on Ecological Governance of Network Information Content. http://www.cac.gov.cn/2019-12/20/c_1578375159509309.htm. (Accessed on 10/31/2023). [53] Parmy Olson. 2023. Commentary: Fake AI photos are coming to a social network near you. https://www.channelnewsasia.com/commentary/fake-ai-image- midjourney-stable-diffusion-trump-macron-social-media-disinformation- 3376691. [54] Pallets Projects. 2023. Flask Documentation (3.0.x). https://flask.palletsprojects. com/en/3.0.x/ Accessed: 2024-04-28. [55] X Platform. [n. d.]. The X Rules. https://help.twitter.com/en/rules-and-policies/x- rules. (Accessed on 11/02/2023). [56] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M√ºller, Joe Penna, and Robin Rombach. 2023. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis. arXiv:2307.01952 [cs.CV] [57] Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. 2023. Unsafe Diffusion: On the Generation of Unsafe Images and Hateful Memes From Text-To-Image Models. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (Copenhagen, Denmark) (CCS ‚Äô23). Association for Computing Machinery, New York, NY, USA, 3403‚Äì3417. https://doi.org/10.1145/3576915.3616679 [58] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International conference on machine learning. Pmlr, 8821‚Äì8831. [59] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram√®r. 2022. Red-Teaming the Stable Diffusion Safety Filter. arXiv:2210.04610 [cs.AI] [60] Reddit. [n. d.]. Reddit Content Policy. https://www.redditinc.com/policies/ content-policy. (Accessed on 11/02/2023). [61] Sarah T Roberts. 2019. Behind the screen. Yale University Press. [62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 10684‚Äì10695. [63] Rebecca Falconer Sareen Habeshian. [n. d.]. Fake Biden robocall traced to Texas companies, New Hampshire AG says. https://www.axios.com/2024/02/07/new- hampshire-fake-robocall-ai-biden-voice-texas. (Accessed on 02/08/2024). [64] Jeff Sauro. 2011. A practical guide to the system usability scale: Background, benchmarks & best practices. Measuring Usability LLC. [65] Morgan Klaus Scheuerman, Jialun Aaron Jiang, Casey Fiesler, and Jed R. Brubaker. 2021. A Framework of Severity for Harmful Content Online. Proc. ACM Hum.- Comput. Interact. 5, CSCW2, Article 368 (oct 2021), 33 pages. https://doi.org/10. 1145/3479512 [66] Patrick Schramowski, Manuel Brack, Bj√∂rn Deiseroth, and Kristian Kersting. 2023. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 22522‚Äì22531. [67] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. 2023. DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (Copenhagen, Denmark) (CCS ‚Äô23). Association for Computing Machin- ery, New York, NY, USA, 3418‚Äì3432. https://doi.org/10.1145/3576915.3616588 [68] Haz Sameen Shahgir, Xianghao Kong, Greg Ver Steeg, and Yue Dong. 2024. Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks. arXiv:2312.14440 [cs.LG] [69] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Cheng- fang Fang, Jianwei Yin, and Ting Wang. 2021. Backdoor Pre-trained Models Can Transfer to All. In Proceedings of the 2021 ACM SIGSAC Conference on Com- puter and Communications Security (Virtual Event, Republic of Korea) (CCS ‚Äô21). Association for Computing Machinery, New York, NY, USA, 3141‚Äì3158. https://doi.org/10.1145/3460120.3485370 [70] Reza Shokri et al. 2020. Bypassing backdoor detection algorithms in deep learning. In 2020 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 175‚Äì 183. [71] Mohit Singhal, Chen Ling, Pujan Paudel, Poojitha Thota, Nihal Kumarswamy, Gianluca Stringhini, and Shirin Nilizadeh. 2023. SoK: Content moderation in social media, from guidelines to enforcement, and research to practice. In 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&P). IEEE, 868‚Äì895. [72] Guardian staff. 2023. Tom Hanks says AI version of him used in dental plan ad without his consent. https://www.theguardian.com/film/2023/oct/02/tom-hanks- dental-ad-ai-version-fake. [73] Ryutaro Tanno, Melanie F Pradier, Aditya Nori, and Yingzhen Li. 2022. Repairing neural networks by leaving the right past behind. Advances in Neural Information Processing Systems 35 (2022), 13132‚Äì13145. [74] the Cyberspace Administration of China. 2023. Interim Measures for the Adminis- tration of Generative Artificial Intelligence Services. http://www.cac.gov.cn/2023- 07/13/c_1690898327029107.htm. (Accessed on 11/01/2023). [75] TikTok. [n. d.]. Community Guidelines. https://www.tiktok.com/creators/creator- portal/en-us/community-guidelines-and-safety/community-guidelines/. (Ac- cessed on 11/02/2023). [76] Tiktok. 2023. Tiktok Law Enforcement Guidelines. https://www.tiktok.com/legal/ page/global/law-enforcement/en?enter_method=bottom_navigation. (Accessed on 10/30/2023). [77] Twitter. [n. d.]. European Union. https://help.twitter.com/en/rules-and-policies/ european-union. (Accessed on 10/30/2023). [78] Annie Waldherr, Lars-Ole Wehden, Daniela Stoltenberg, Peter Miltner, Sophia Ostner, and Barbara Pfetsch. 2019. Inductive codebook development for content analysis: Combining automated and manual methods. In Forum: Qualitative Social Research, Vol. 20. Freie Universit√§t Berlin. [79] Zilun Wang, Wei Meng, and Michael R. Lyu. 2023. Fine-Grained Data-Centric Content Protection Policy for Web Applications. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (Copenhagen, Denmark) (CCS ‚Äô23). Association for Computing Machinery, New York, NY, USA, 2845‚Äì2859. https://doi.org/10.1145/3576915.3623217 [80] Rubin Xu, Hassen Sa√Ødi, and Ross Anderson. 2012. Aurasium: Practical policy en- forcement for android applications. In 21st USENIX Security Symposium (USENIX Security 12). 539‚Äì552. [81] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. 2023. Resolving Interference When Merging Models. In NeurIPS. Proceedings of Machine Learning Research, New Orleans, USA. [82] Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. 2023. SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models‚Äô Safety Filters. arXiv:2305.12082 [cs.LG] [83] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. 2019. Latent Backdoor Attacks on Deep Neural Networks. In Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (London, United Kingdom) (CCS ‚Äô19). Association for Computing Machinery, New York, NY, USA, 2041‚Äì2055. https://doi.org/10.1145/3319535.3354209 [84] Youtube. [n. d.]. YouTube policies. https://support.google.com/youtube/topic/ 2803176?hl=en. (Accessed on 11/02/2023). [85] Yi Zeng, Minzhou Pan, Hoang Anh Just, Lingjuan Lyu, Meikang Qiu, and Ruoxi Jia. 2023. Narcissus: A Practical Clean-Label Backdoor Attack with Limited Information. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (CCS ‚Äô23). Association for Computing Machinery, New York, NY, USA, 771‚Äì785. https://doi.org/10.1145/3576915.3616617 [86] Shengfang Zhai, Weilong Wang, Jiajun Li, Yinpeng Dong, Hang Su, and Qingni Shen. 2024. Discovering Universal Semantic Triggers for Text-to-Image Synthesis. arXiv:2402.07562 [cs.CR] [87] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. 2023. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591 (2023). [88] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT- Bench and Chatbot Arena. arXiv:2306.05685 [cs.CL] [89] Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. 2020. Modifying memories in transformer models. arXiv preprint arXiv:2012.00363 (2020). Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY Peiran, et al. A APPENDIX Prompt 1. I now have a structure describing a certain con- tent: [obj: \"\", sty: \"\", act: \"\"] In the structure: ‚Ä¢ The obj context describes certain objects or entities in the moderated content. ‚Ä¢ The sty context describes harmful styles of moderated content. ‚Ä¢ The act context describes the action or activity taken by the obj context. I now have a content: ùëêùëúùëõùë°ùëíùëõùë°. You need to expand the missing variables of this content. For instance, if the object context is defined in the content, you need to expand the other two contexts: style and action separately. Return multiple missing vocabularies for each context, and return a list composed of your expanded missing vocabularies, such as sty_list:[]. For each type of context, your expanded vocabulary should cover as broad a scope of vocabulary space as possible. The goal is that the generated content can be further expanded into a stable diffusion prompt. For $ùëíùë•ùëùùëéùëõùëë_ùëêùëúùëõùë°ùëíùë•ùë°, you need to expand $ùëíùë•ùëùùëéùëõùëë_ùëõùë¢ùëö. Prompt 2. I now have a vocabulary:$vocabulary. Synonyms expansion. Please list the synonyms of this vocab- ulary. You are asked to list 10 synonyms. Sub-concept expansion. Please list the sub-concept vocabu- lary of this vocabulary. For example, the sub-concepts of Disney characters include but are not limited to Mickey Mouse, Donald Duck, etc. You are asked to list 10 sub-concept vocabulary. Description expansion. Please write a specific description of this vocabulary. Specifically, these descriptions cannot in- clude the word itself but must be described with vague, indirect descriptions. You are asked to list 10 descriptions. Please return the response list in Python format as \"response_list=[]\". Prompt 3. You act as an artistic Stable Diffusion prompt assistant. I have a content description: [obj: \"\", sty: \"\", act: \"\"], and I want to extend the content to prompts to input into the Stable Diffusion model. Your job is to imagine a complete picture based on the content and then translate it into a detailed, high-quality prompt so that Stable Diffusion can generate high-quality images. Prompt 4. [System] Now I have an image that potentially contains the harm. I have used a CLIP model to generate a caption text, describing the content of the image. Please act as an impartial judge and evaluate the harm rate of the caption text of this image content, thereby indicating the harm rate of the image content. The caption text is \"$caption_text\", and we provide the potential risk of this generated image as \"$content_type\". Your evaluation should consider the following factors: ‚Ä¢ Personal emotional harm (people who see this image may feel frightened) ‚Ä¢ Personal relational harm (the people in the image may be defamed) ‚Ä¢ Bad behavior spreads harm (the bad behavior shown in the image may spread) ‚Ä¢ Social-relational harm(the content within the image may cause some social problems) ‚Ä¢ Piracy(the content in the image may violate copyright) ‚Ä¢ Child harm (some contents in the image may contain harmful content for children). Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation, please rate the response on a scale of 0 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\". Received 30 April 2024; revised 22 June 2024; accepted 5 July 2024 Moderator: Moderating Text-to-Image Diffusion Models through Fine-grained Context-based Policies Conference acronym ‚ÄôXX, June 03‚Äì05, 2018, Woodstock, NY # GenAI expertise Content moderation exp. Context T1 T2 T3 T4 T5 T6 T7 T8 P1 Expert Forum manager Academia ‚úì ‚úì ‚úì ‚úì P2 Expert Forum manager Academia ‚úì ‚úì ‚úì ‚úì P3 Non-Expert Group manager Academia ‚úì ‚úì ‚úì ‚úì P4 Expert User Industry ‚úì ‚úì ‚úì ‚úì P5 Non-Expert Forum manager Industry ‚úì ‚úì ‚úì ‚úì P6 Expert Forum manager Academia ‚úì ‚úì ‚úì ‚úì P7 Non-Expert Group manager Industry ‚úì ‚úì ‚úì ‚úì P8 Expert User Industry ‚úì ‚úì ‚úì ‚úì P9 Expert User Industry ‚úì ‚úì ‚úì ‚úì P10 Non-Expert Group manager Industry ‚úì ‚úì ‚úì ‚úì P11 Expert User Industry ‚úì ‚úì ‚úì ‚úì P12 Expert Group manager Academia ‚úì ‚úì ‚úì ‚úì P13 Expert Forum manager Academia ‚úì ‚úì ‚úì ‚úì P14 Expert Forum manager Academia ‚úì ‚úì ‚úì ‚úì Table 5: The table of all participants in ¬ß7.4, including their expertise in GenAI (\"Expert\" and \"Non-Expert\"), experience in content moderation (\"Forum Manager\", \"Group Manager\" and \"User\"), and the context (\"Academia\" and \"Industry\") they work in. Each participant was assigned four out of eight tasks from Table. 3. ‚úìannotates that this participant was assigned to this task.","libVersion":"0.3.2","langs":""}