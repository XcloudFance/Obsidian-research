{"path":"Unlearning/Certified Data Removal from Machine Learning Models.pdf","text":"Certified Data Removal from Machine Learning Models Chuan Guo 1 Tom Goldstein 2 Awni Hannun 2 Laurens van der Maaten 2 Abstract Good data stewardship requires removal of data at the request of the data’s owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a re- moval request. Is it possible to “remove” data from a machine-learning model? We study this problem by defining certified removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certified-removal mechanism for linear classifiers and empirically study learn- ing settings in which this mechanism is practical. 1. Introduction Machine-learning models are often trained on third-party data, for example, many computer-vision models are trained on images provided by Flickr users (Thomee et al., 2016). When a party requests that their data be removed from such online platforms, this raises the question how such a request should impact models that were trained prior to the removal. A similar question arises when a model is negatively im- pacted by a data-poisoning attack (Biggio et al., 2012). Is it possible to “remove” data from a model without re-training that model from scratch? We study this question in a framework we call certified re- moval, which theoretically guarantees that an adversary can- not extract information about training data that was removed from a model. Inspired by differential privacy (Dwork, 2011), certified removal bounds the max-divergence be- tween the model trained on the dataset with some instances removed and the model trained on the dataset that never contained those instances. This guarantees that membership- 1Department of Computer Science, Cornell University, New York, USA 2Facebook AI Research, New York, USA. Correspon- dence to: Chuan Guo <cg563@cornell.edu>, Laurens van der Maaten <lvdmaaten@gmail.com>. Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the au- thor(s). inference attacks (Yeom et al., 2018; Carlini et al., 2019) are unsuccessful on data that was removed from the model. We emphasize that certified removal is a very strong notion of removal; in practical applications, less constraining notions may equally fulfill the data owner’s expectation of removal. We develop a certified-removal mechanism for L2- regularized linear models that are trained using a differ- entiable convex loss function, e.g., logistic regressors. Our removal mechanism applies a Newton step on the model parameters that largely removes the influence of the deleted data point; the residual error of this mechanism decreases quadratically with the size of the training set. To ensure that an adversary cannot extract information from the small residual (i.e., to certify removal), we mask the residual us- ing an approach that randomly perturbs the training loss (Chaudhuri et al., 2011). We empirically study in which settings the removal mechanism is practical. 2. Certified Removal Let D be a fixed training dataset and let A be a (randomized) learning algorithm that trains on D and outputs a model h ∈ H, that is, A : D → H. Randomness in A induces a probability distribution over the models in the hypothesis set H. We would like to remove a training sample, x ∈ D, from the output of A. To this end, we define a data-removal mechanism M that is applied to A(D) and aims to remove the influence of x. If removal is successful, the output of M should be difficult to distinguish from the output of A applied on D \\ x. Given ϵ > 0, we say that removal mechanism M performs ϵ-certified removal (ϵ-CR) for learning algorithm A if ∀T ⊆ H, D ⊆ X , x ∈ D: e−ϵ ≤ P (M (A(D), D, x) ∈ T ) P (A(D \\ x) ∈ T ) ≤ eϵ. (1) This definition states that the ratio between the likelihood of (i) a model from which sample x was removed and (ii) a model that was never trained on x to begin with is close to one for all models in the hypothesis set, for all possible data sets, and for all removed samples. Note that although the definition requires that the mechanism M is universally ap- plicable to all training data points x ∈ D, it is also allowed to be data-dependent, i.e., both the training set D and thearXiv:1911.03030v6 [cs.LG] 8 Nov 2023 Certified Data Removal from Machine Learning Models data point to be removed x are given as inputs to M . We also define a more relaxed notion of (ϵ, δ)-certified re- moval for δ > 0 if ∀T ⊆ H, D ⊆ X , x ∈ D: P (M (A(D), D, x) ∈ T ) ≤ eϵP (A(D \\ x) ∈ T ) + δ, and P (A(D \\ x) ∈ T ) ≤ eϵP (M (A(D), D, x) ∈ T ) + δ. Conceptually, δ upper bounds the probability for the max- divergence bound in Equation 1 to fail. A trivial certified-removal mechanism M with ϵ = 0 com- pletely ignores A(D) and evaluates A(D\\x) directly, that is, it sets M (A(D), D, x) = A(D \\ x). Such a removal mech- anism is impractical for many models, as it may require re-training the model from scratch every time a training sample is removed. We seek mechanisms M with removal cost O(n) or less, with small constants, where n = |D| is the training set size. Insufficiency of parametric indistinguishability. One alternative relaxation of exact removal is by asserting that the output of M (A(D), D, x) is sufficiently close to that of A(D \\ x). It is easy to see that a model satsi- fying such a definition still retains information about x. Consider a linear regressor trained on the dataset D = {(e1, 1), (e2, 2), . . . , (ed, d)} where ei’s are the standard basis vectors for Rd. A regressor that is initialized with zeros, or that has a weight decay penalty, will place a non- zero weight on wi if (ei, i) is included in D, and a zero weight on wi if not. In this case, an approximate removal algorithm that leaves wi small but non-zero still reveals that (ei, i) appeared during training. Relationship to differential privacy. Our formulation of certified removal is related to that of differential privacy (Dwork, 2011) but there are important differences. Differ- ential privacy states that: ∀T ⊆ H, D, D′ : e−ϵ ≤ P (A(D) ∈ T ) P (A(D′) ∈ T ) ≤ eϵ, (2) where D and D′ differ in only one sample. Since D and D \\ x only differ in one sample, it is straightforward to see that differential privacy of A is a sufficient condition for certified removal, viz., by setting removal mechanism M to the identity function. Indeed, if algorithm A never memorizes the training data in the first place, we need not worry about removing that data. Even though differential privacy is a sufficient condition, it is not a necessary condition for certified removal. For example, a nearest-neighbor classifier is not differentially private but it is trivial to certifiably remove a training sample in O(1) time with ϵ = 0. We note that differential privacy is a very strong condition, and most differentially private models suffer a significant loss in accuracy even for large ϵ (Chaudhuri et al., 2011; Abadi et al., 2016). We therefore view the study of certified removal as analyzing the trade- off between utility and removal efficiency, with re-training from scratch and differential privacy at the two ends of the spectrum, and removal in the middle. 3. Removal Mechanisms We focus on certified removal from parametric models, as removal from non-parametric models (e.g., nearest-neighbor classifiers) is trivial. We first study linear models with strongly convex regularization before proceeding to removal from deep networks. 3.1. Linear Classifiers Denote by D = {(x1, y1), . . . , (xn, yn)} the training set of n samples, ∀i : xi ∈ Rd, with corresponding targets yi. We assume learning algorithm A tries to minimize the regularized empirical risk of a linear model: L(w; D) = n∑ i=1 ℓ(w⊤xi, yi) + λn 2 ∥w∥ 2 2, where ℓ(w⊤x, y) is a convex loss that is differentiable ev- erywhere. We denote w∗ = A(D) = argminwL(w; D) as it is uniquely determined. Our approach to certified removal is as follows. We first define a removal mechanism that, given a training data point (x, y) to remove, produces a model w− that is approxi- mately equal to the unique minimizer of L(w; D \\ (x, y)). The model produced by such a mechanism may still con- tain information about (x, y). In particular, if the gradi- ent ∇L(w−; D \\ (x, y)) is non-zero, it reveals information about the removed data point. To hide this information, we apply a sufficiently large random perturbation to the model parameters at training time. This allows us to guarantee certified removal; the values of ϵ and δ depend on the ap- proximation quality of the removal mechanism and on the distribution from which the model perturbation is sampled. Removal mechanism. We assume without loss of general- ity that we aim to remove the last training sample, (xn, yn). Specifically, we define an efficient removal mechanism that approximately minimizes L(w; D′) with D′ = D\\(xn, yn). First, denote the loss gradient at sample (xn, yn) by ∆ = λw∗ +∇ℓ((w∗)⊤xn, yn) and the Hessian of L(·; D′) at w∗ by Hw∗ = ∇2L(w∗; D′). We consider the Newton update removal mechanism M : w− = M (w∗, D, (xn, yn)) := w∗ + H −1 w∗ ∆, (3) which is a one-step Newton update applied to the gradient influence of the removed point (xn, yn). The update H −1 w∗ ∆ Certified Data Removal from Machine Learning Models is also known as the influence function of the training point (xn, yn) on the parameter vector w∗ (Cook and Weisberg, 1982; Koh and Liang, 2017). The computational cost of this Newton update is dominated by the cost of forming and inverting the Hessian matrix. The Hessian matrix for D can be formed offline with O(d 2n) cost. The subsequent Hessian inversion makes the removal mechanism O(d3) at removal time; the inversion can lever- age efficient linear algebra libraries and GPUs. To bound the approximation error of this removal mecha- nism, we observe that the quantity ∇L(w−; D′), which we refer to hereafter as the gradient residual, is zero only when w− is the unique minimizer of L(·; D′). We also observe that the gradient residual norm, ∥∇L(w−; D′)∥2, reflects the error in the approximation w− of the minimizer of L(·; D′). We derive an upper bound on the gradient residual norm for the removal mechanism (cf. Equation 3). Theorem 1. Suppose that ∀(xi, yi) ∈ D, w ∈ Rd : ∥∇ℓ(w⊤xi, yi)∥2 ≤ C. Suppose also that ℓ ′′ is γ-Lipschitz and ∥xi∥2 ≤ 1 for all (xi, yi) ∈ D. Then: ∥∇L(w−; D′)∥2 = ∥(Hwη − Hw∗ )H −1 w∗ ∆∥2 (4) ≤ γ(n − 1)∥H −1 w∗ ∆∥2 2 ≤ 4γC 2 λ2(n − 1) , where Hwη denotes the Hessian of L(·; D′) at the parameter vector wη = w∗ + ηH −1 w∗ ∆ for some η ∈ [0, 1]. Loss perturbation. Obtaining a small gradient norm ∥∇L(w−; D′)∥2 via Theorem 1 does not guarantee cer- tified removal. In particular, the direction of the gradient residual may leak information about the training sample that was “removed.” To hide this information, we use the loss perturbation technique of Chaudhuri et al. (2011) at training time. It perturbs the empirical risk by a random linear term: Lb(w; D) = n∑ i=1 ℓ (w⊤xi, yi) + λn 2 ∥w∥ 2 2 + b ⊤w, with b ∈ Rd drawn randomly from some distribution. We analyze how loss perturbation masks the information in the gradient residual ∇Lb(w−; D′) through randomness in b. Let A(D′) be an exact minimizer1 for Lb(·; D′) and let ˜A(D′) be an approximate minimizer of Lb(·; D′), for exam- ple, our removal mechanism applied on the trained model. Specifically, let ˜w be an approximate solution produced by 1Our result can be modified to work with approximate loss minimizers by incurring a small additional error term. ˜A. This implies the gradient residual is: u := ∇Lb( ˜w; D′) = n−1∑ i=1 ∇ℓ( ˜w⊤xi, yi) + λ(n − 1) ˜w + b. (5) We assume that ˜A can produce a gradient residual u with ∥u∥2 ≤ ϵ′ for some pre-specified bound ϵ′ that is indepen- dent of the perturbation vector b. Let fA(·) and f ˜A(·) be the density functions over the model parameters produced by A and ˜A, respectively. We bound the max-divergence between fA and f ˜A for any solution ˜w produced by approximate minimizer ˜A. Theorem 2. Suppose that b is drawn from a distribution with density function p(·) such that for any b1, b2 ∈ Rd satisfying ∥b1 − b2∥2 ≤ ϵ′, we have that: e−ϵ ≤ p(b1) p(b2) ≤ eϵ. Then e−ϵ ≤ f ˜A( ˜w) fA( ˜w) ≤ e ϵ for any ˜w produced by ˜A. Achieving certified removal. We can use Theorem 2 to prove certified removal by combining it with the gradient residual norm bound ϵ′ from Theorem 1. The security parameters ϵ and δ depend on the distribution from which b is sampled. We state the guarantee of (ϵ, δ)-certified removal below for two suitable distributions p(b). Theorem 3. Let A be the learning algorithm that returns the unique optimum of the loss Lb(w; D) and let M be the Newton update removal mechanism (cf., Equation 3). Suppose that ∥∇L(w−; D′)∥2 ≤ ϵ′ for some computable bound ϵ ′ > 0. We have the following guarantees for M : (i) If b is drawn from a distribution with density p(b) ∝ e− ϵ ϵ′ ∥b∥2 , then M is ϵ-CR for A; (ii) If b ∼ N (0, cϵ ′/ϵ)d with c > 0, then M is (ϵ, δ)-CR for A with δ = 1.5 · e−c 2/2. The distribution in (i) is equivalent to sampling a direction uniformly over the unit sphere and sampling a norm from the Γ(d, ϵ ′ ϵ ) distribution (Chaudhuri et al., 2011). 3.2. Practical Considerations Least-squares and logistic regression. The certified re- moval mechanism described above can be used with least- squares and logistic regression, which are ubiquitous in real-world applications of machine learning. Least-squares regression assumes ∀i : yi ∈ R and uses the loss function ℓ(w⊤xi, yi) = (w⊤xi − yi)2. The Hessian of this loss function is ∇2ℓ(w⊤xi, yi) = xix⊤ i , which is independent of w. In particular, the gradient residual from Equation 4 is exactly zero, which makes the Newton update in Equation 3 an ϵ-certified removal mechanism with ϵ = 0 Certified Data Removal from Machine Learning Models (loss perturbation is not needed). This is not surprising since the Newton update assumes a local quadratic approximation of the loss, which is exact for least-squares regression. Logistic regression assumes ∀i : yi ∈ {−1, +1} and uses the loss function ℓ(w⊤xi, yi) = − log σ (yiw⊤xi), where σ(·) denotes the sigmoid function, σ(x) = 1 1+exp(−x) . The loss gradient and Hessian are given by: ∇ℓ(w⊤xi, yi) = (σ(yiw⊤xi) − 1 ) yixi ∇2ℓ(w⊤xi, yi) = σ(yiw⊤xi) (1 − σ(yiw⊤xi) ) xix⊤ i . Under the assumption that ∥xi∥2 ≤ 1 for all i, it is straightforward to show that ∥∇ℓ(w⊤xi, yi)∥2 ≤ 1 and that ℓ ′′(w⊤xi, yi) is γ-Lipschitz with γ = 1/4. This allows us to apply Theorem 1 to logistic regression. Data-dependent bound on gradient norm. The bound in Theorem 1 contains a constant factor of 1/λ 2 and may be too loose for practical applications on smaller datasets. Fortunately, we can derive a data-dependent bound on the gradient residual that can be efficiently computed and that is much tighter in practice. Recall that the Hessian of L(·; D′) has the form: Hw = (X −) ⊤DwX − + λ(n − 1)Id, where X − ∈ R(n−1)×d is the data matrix corresponding to D′, Id is the identity matrix of size d×d, and Dw is a diagonal matrix containing values: (Dw)ii = ℓ ′′ (w⊤xi, yi) . By Equation 4 we have that: ∥∇L(w−; D′)∥2 = ∥(Hwη − Hw∗ )H −1 w∗ ∆∥2 = ∥(X −) ⊤(Dwη − Dw∗ )X −H −1 w∗ ∆∥2 ≤ ∥X −∥2∥Dwη − Dw∗ ∥2∥X −H −1 w∗ ∆∥2. The term ∥Dwη − Dw∗ ∥2 corresponds to the maximum singular value of a diagonal matrix, which in turn is the maximum value among its diagonal entries. Given the Lip- schitz constant γ of the second derivative ℓ ′′, we can thus bound it as: ∥Dwη − Dw∗ ∥2 ≤ γ∥wη − w∗∥2 ≤ γ∥H −1 w∗ ∆∥2. The following corollary summarizes this derivation. Corollary 1. The Newton update w− = w∗ + H −1 w∗ ∆ satisfies: ∥∇L(w−; D′)∥2 ≤ γ∥X −∥2∥H −1 w∗ ∆∥2∥X −H −1 w∗ ∆∥2, where γ is the Lipschitz constant of ℓ ′′. The bound in Corollary 1 can be easily computed from the Newton update H −1 w∗ ∆ and the spectral norm of X −, the latter admitting efficient algorithms such as power iterations. Multiple removals. The worst-case gradient residual norm after T removals can be shown to scale linearly in T . We can prove this using induction on T . The base case, T = 1, is proven above. Suppose that the gradient residual after T ≥ 1 removals is uT with ∥uT ∥2 ≤ T ϵ ′, where ϵ′ is the gradient residual norm bound for a sin- gle removal. Let D(T ) be the training dataset with T data points removed. Consider the modified loss function L (T ) b (w; D(T )) = Lb(w; D(T )) − u ⊤ T w and let wT be the approximate solution after T removals. Then wT is an ex- act solution of L (T ) b (w; D(T )), hence, the argument above can be applied to L (T ) b (w; D(T )) to show that the Newton update approximation wT +1 has gradient residual u ′ with norm at most ϵ′. Then: u ′ = ∇wL (T ) b (w; D(T )) = ∇wLb(w; D(T )) − uT ⇒ 0 = ∇wLb(w) − uT − u ′. Thus the gradient residual for Lb(w; D(T )) after T + 1 removals is uT +1 := uT + u ′ and its norm is at most (T + 1)ϵ′ by the triangle inequality. Batch removal. In certain scenarios, data removal may not need to occur immediately after the data’s owner re- quests removal. This potentially allows for batch removals in which multiple training samples are removed at once for improved efficiency. The Newton update removal mecha- nism naturally supports this extension. Assume without loss of generality that the batch of training data to be removed is Dm = {(xn−m+1, yn−m+1), . . . , (xn, yn)}. Define: ∆(m) = mλw∗ + n∑ j=n−m+1 ∇ℓ((w∗)⊤xj, yj) H (m) w∗ = ∇2L(w∗; D \\ Dm). The batch removal update is: w(−m) = w∗ + [H (m) w∗ ]−1 ∆(m). (6) We derive bounds on the gradient residual norm for batch removal that are similar Theorem 1 and Corollary 1. Theorem 4. Under the same regularity conditions of Theo- rem 1, we have that: ∥∇L(w(−m); D \\ Dm)∥2 ≤ γ(n − m) ∥ ∥ ∥ ∥ [H (m) w∗ ]−1 ∆(m)∥ ∥ ∥ ∥ 2 2 ≤ 4γm 2C 2 λ2(n − m) . Certified Data Removal from Machine Learning Models Algorithm 1 Training of a certified removal-enabled model. 1: Input: Dataset D, loss ℓ, parameters σ, λ > 0. 2: Sample b ∼ N (0, σ)d. 3: Return: argminw∈Rd ∑n i=1 ℓ(w⊤xi,yi)+λn∥w∥2 2+b ⊤w. Algorithm 2 Repeated certified removal of data batches. 1: Input: Dataset D, loss ℓ, parameters ϵ, δ, σ, λ > 0. 2: Lipschitz constant γ of ℓ ′′. 3: Solution w computed by Algorithm 1. 4: Sequence of batches of training sample 5: indices to be removed: B1, B2, . . . 6: Gradient residual bound β ← 0. 7: c ← √ 2 log(1.5/δ). 8: K ← ∑n i=1 xix⊤ i . 9: X ← [x1|x2| · · · |xn] ⊤. 10: for j = 1, 2, . . . do 11: ∆ ← |Bj|λw + ∑ i∈Bj ∇ℓ(w⊤xi, yi). 12: H ← ∑ i:i /∈B1,B2,...,Bj ∇2ℓ(w⊤xi, yi). 13: X ← remove rows(X, Bj). 14: K ← K − ∑ i∈Bj xix⊤ i . 15: β ← β + γ√∥K∥2 · ∥H −1∆∥2 · ∥XH −1∆∥2. 16: if β > σϵ/c then 17: Re-train from scratch using Algorithm 1. 18: else 19: w ← w + H −1∆. 20: end if 21: end for Corollary 2. The Newton update w(−m) = w∗ + [H (m) w∗ ]−1 ∆(m) satisfies: ∥L(w(−m); D \\ Dm)∥2 ≤ γ∥X (−m)∥2 ∥ ∥ ∥ ∥ [H (m) w∗ ]−1 ∆(m)∥ ∥ ∥ ∥ 2 ∥ ∥ ∥ ∥X (−m)[H (m) w∗ ]−1 ∆(m)∥ ∥ ∥ ∥ 2, where X (−m) is the data matrix for D \\ Dm and γ is the Lipschitz constant of ℓ ′′. Interestingly, the gradient residual bound in Theorem 4 scales quadratically with the number of removals, as op- posed to linearly when removing examples one-by-one. This increase in error is due to a more crude approximation of the Hessian, that is, we compute the Hessian only once at the current solution w∗ rather than once per removed data. Reducing online computation. The Newton update re- quires forming and inverting the Hessian. Although the O(d 3) cost of inversion is relatively limited for small d and inversion can be done efficiently on GPUs, the cost of form- ing the Hessian is O(d 2n), which may be problematic for large datasets. However, the Hessian can be formed at train- ing time, i.e., before the data to be removed is presented, and only the inverse needs to be computed at removal time. When computing the data-dependent bound, a similar tech- nique can be used for calculating the term ∥X −H −1 w∗ ∆∥2 – which involves the product of the (n − 1) × d data ma- trix X − with a d-dimensional vector. We can reduce the online component of this computation to O(d 3) by form- ing the SVD of X offline and applying online down-dates (Gu and Eisenstat, 1995) to form the SVD of X − by solv- ing an eigen-decomposition problem on a d × d matrix. It can be shown that this technique reduces the computation of ∥X −H −1 w∗ ∆∥2 to involve only d × d matrices and d- dimensional vectors, which enables the online computation cost to be independent of n. Pseudo-code. We present pseudo-code for training removal-enabled models and for the (ϵ, δ)-CR Newton up- date mechanism. During training (Algorithm 1), we add a random linear term to the training loss by sampling a Gaussian noise vector b. The choice of σ determines a “removal budget” according to Theorem 3: the maximum gradient residual norm that can be incurred is σϵ/c. When optimizing the training loss, any optimizer with convergence guarantee for strongly convex loss functions can be used to find the minimizer in Algorithm 1. We use L-BFGS (Liu and Nocedal, 1989) in our experiments as it was the most efficient of the optimizers we tried. During removal (line 19 in Algorithm 2), we apply the batch Newton update (Equation 6) and compute the gradient residual norm bound using Corollary 2 (line 15 in Algorithm 2). The variable β accumulates the gradient residual norm over all removals. If the pre-determined budget of σϵ/c is exceeded, we train a new removal-enabled model from scratch using Algorithm 1 on the remaining data points. 3.3. Non-Linear Models Deep learning models often apply a linear model to features extracted by a network pre-trained on a public dataset like ImageNet (Ren et al., 2015; He et al., 2017; Zhao et al., 2017; Carreira and Zisserman, 2017) for vision tasks, or from language model trained on public text corpora (Devlin et al., 2019; Dai et al., 2019; Yang et al., 2019; Liu et al., 2019) for natural language tasks. In such setups, we only need to worry about data removal from the linear model that is applied to the output of the feature extractor. When feature extractors are trained on private data as well, we can use our certified-removal mechanism on linear mod- els that are applied to the output of a differentially-private feature extraction network (Abadi et al., 2016). Theorem 5. Suppose Φ is a randomized learning algo- rithm that is (ϵDP, δDP)-differentially private, and the out- Certified Data Removal from Machine Learning Models Dataset MNIST (§4.1) LSUN (§4.2) SST (§4.2) SVHN (§4.3) Removal setting CR Linear Public Extractor + CR Linear Public Extractor + CR Linear DP Extractor + CR Linear Removal time 0.04s 0.48s 0.07s 0.27s Training time 15.6s 124s 61.5s 1.5h Table 1. Summary of removal and training times observed in our experiments. For LSUN and SST, the public extractor is trained on a public dataset and hence removal is only applied to the linear model. For SVHN, removal is applied to a linear model that operates on top of a differentially private feature extractor. In all cases, using the Newton update to (certifiably) remove data is several orders of magnitude faster than re-training the model from scratch. 10 −2 10 −1 10 0 10 1 10 2 σ 75 80 85 90 957est Accuracy λ=10 −4 λ=10 −3 λ=10 −2 λ=10 −1 10 −3 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 ε 75 80 85 90 957est Accuracy λ=10 −4 λ=10 −3 λ=10 −2 λ=10 −1 10 −2 10 −1 10 0 10 1 10 2 10 3 10 4 10 5 ExSected # Rf SuSSRrted 5emRvals 75 80 85 90 957est Accuracy λ=10 −4 λ=10 −3 λ=10 −2 λ=10 −1 \u0000 =0.01 <latexit sha1_base64=\"iVZ1HvEFassixQIymUo0vP5jAcA=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoBuh6MZlBfuA6VAyaaYNzWNIMkIZ+hluXCji1q9x59+YtrPQ1gMhh3Pu5d574pQzY33/2yutrW9sbpW3Kzu7e/sH1cOjtlGZJrRFFFe6G2NDOZO0ZZnltJtqikXMaSce3838zhPVhin5aCcpjQQeSpYwgq2Twp5hQ4Fv/Lof9Ks1982BVklQkBoUaParX72BIpmg0hKOjQkDP7VRjrVlhNNppZcZmmIyxkMaOiqxoCbK5ytP0ZlTBihR2j1p0Vz93ZFjYcxExK5SYDsyy95M/M8LM5tcRzmTaWapJItBScaRVWh2PxowTYnlE0cw0cztisgIa0ysS6niQgiWT14l7Yt64NeDh8ta47aIowwncArnEMAVNOAemtACAgqe4RXePOu9eO/ex6K05BU9x/AH3ucP4ViQUQ==</latexit><latexit sha1_base64=\"iVZ1HvEFassixQIymUo0vP5jAcA=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoBuh6MZlBfuA6VAyaaYNzWNIMkIZ+hluXCji1q9x59+YtrPQ1gMhh3Pu5d574pQzY33/2yutrW9sbpW3Kzu7e/sH1cOjtlGZJrRFFFe6G2NDOZO0ZZnltJtqikXMaSce3838zhPVhin5aCcpjQQeSpYwgq2Twp5hQ4Fv/Lof9Ks1982BVklQkBoUaParX72BIpmg0hKOjQkDP7VRjrVlhNNppZcZmmIyxkMaOiqxoCbK5ytP0ZlTBihR2j1p0Vz93ZFjYcxExK5SYDsyy95M/M8LM5tcRzmTaWapJItBScaRVWh2PxowTYnlE0cw0cztisgIa0ysS6niQgiWT14l7Yt64NeDh8ta47aIowwncArnEMAVNOAemtACAgqe4RXePOu9eO/ex6K05BU9x/AH3ucP4ViQUQ==</latexit><latexit sha1_base64=\"iVZ1HvEFassixQIymUo0vP5jAcA=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoBuh6MZlBfuA6VAyaaYNzWNIMkIZ+hluXCji1q9x59+YtrPQ1gMhh3Pu5d574pQzY33/2yutrW9sbpW3Kzu7e/sH1cOjtlGZJrRFFFe6G2NDOZO0ZZnltJtqikXMaSce3838zhPVhin5aCcpjQQeSpYwgq2Twp5hQ4Fv/Lof9Ks1982BVklQkBoUaParX72BIpmg0hKOjQkDP7VRjrVlhNNppZcZmmIyxkMaOiqxoCbK5ytP0ZlTBihR2j1p0Vz93ZFjYcxExK5SYDsyy95M/M8LM5tcRzmTaWapJItBScaRVWh2PxowTYnlE0cw0cztisgIa0ysS6niQgiWT14l7Yt64NeDh8ta47aIowwncArnEMAVNOAemtACAgqe4RXePOu9eO/ex6K05BU9x/AH3ucP4ViQUQ==</latexit><latexit sha1_base64=\"iVZ1HvEFassixQIymUo0vP5jAcA=\">AAAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsyIoBuh6MZlBfuA6VAyaaYNzWNIMkIZ+hluXCji1q9x59+YtrPQ1gMhh3Pu5d574pQzY33/2yutrW9sbpW3Kzu7e/sH1cOjtlGZJrRFFFe6G2NDOZO0ZZnltJtqikXMaSce3838zhPVhin5aCcpjQQeSpYwgq2Twp5hQ4Fv/Lof9Ks1982BVklQkBoUaParX72BIpmg0hKOjQkDP7VRjrVlhNNppZcZmmIyxkMaOiqxoCbK5ytP0ZlTBihR2j1p0Vz93ZFjYcxExK5SYDsyy95M/M8LM5tcRzmTaWapJItBScaRVWh2PxowTYnlE0cw0cztisgIa0ysS6niQgiWT14l7Yt64NeDh8ta47aIowwncArnEMAVNOAemtACAgqe4RXePOu9eO/ex6K05BU9x/AH3ucP4ViQUQ==</latexit> \u0000 =0.1 <latexit sha1_base64=\"NTC3SikE07hq8VhsAGpuLIoEIno=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGYHGFus5cs2d07dveEEPIvbCwUsfXf2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wsrq2vlHcLG1t7+zulfcPmibJNGUNmohEtyI0THDFGpZbwVqpZigjwR6i4c3Uf3hi2vBE3dtRykKJfcVjTtE66bFjeF/ilV8NuuWKX/VnIMskyEkFctS75a9OL6GZZMpSgca0Az+14Ri15VSwSamTGZYiHWKftR1VKJkJx7OLJ+TEKT0SJ9qVsmSm/p4YozRmJCPXKdEOzKI3Ff/z2pmNL8MxV2lmmaLzRXEmiE3I9H3S45pRK0aOINXc3UroADVS60IquRCCxZeXSfOsGrjE7s4rtes8jiIcwTGcQgAXUINbqEMDKCh4hld484z34r17H/PWgpfPHMIfeJ8/cGqQFw==</latexit><latexit sha1_base64=\"NTC3SikE07hq8VhsAGpuLIoEIno=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGYHGFus5cs2d07dveEEPIvbCwUsfXf2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wsrq2vlHcLG1t7+zulfcPmibJNGUNmohEtyI0THDFGpZbwVqpZigjwR6i4c3Uf3hi2vBE3dtRykKJfcVjTtE66bFjeF/ilV8NuuWKX/VnIMskyEkFctS75a9OL6GZZMpSgca0Az+14Ri15VSwSamTGZYiHWKftR1VKJkJx7OLJ+TEKT0SJ9qVsmSm/p4YozRmJCPXKdEOzKI3Ff/z2pmNL8MxV2lmmaLzRXEmiE3I9H3S45pRK0aOINXc3UroADVS60IquRCCxZeXSfOsGrjE7s4rtes8jiIcwTGcQgAXUINbqEMDKCh4hld484z34r17H/PWgpfPHMIfeJ8/cGqQFw==</latexit><latexit sha1_base64=\"NTC3SikE07hq8VhsAGpuLIoEIno=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGYHGFus5cs2d07dveEEPIvbCwUsfXf2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wsrq2vlHcLG1t7+zulfcPmibJNGUNmohEtyI0THDFGpZbwVqpZigjwR6i4c3Uf3hi2vBE3dtRykKJfcVjTtE66bFjeF/ilV8NuuWKX/VnIMskyEkFctS75a9OL6GZZMpSgca0Az+14Ri15VSwSamTGZYiHWKftR1VKJkJx7OLJ+TEKT0SJ9qVsmSm/p4YozRmJCPXKdEOzKI3Ff/z2pmNL8MxV2lmmaLzRXEmiE3I9H3S45pRK0aOINXc3UroADVS60IquRCCxZeXSfOsGrjE7s4rtes8jiIcwTGcQgAXUINbqEMDKCh4hld484z34r17H/PWgpfPHMIfeJ8/cGqQFw==</latexit><latexit sha1_base64=\"NTC3SikE07hq8VhsAGpuLIoEIno=\">AAAB8XicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGYHGFus5cs2d07dveEEPIvbCwUsfXf2Plv3CRXaOKDgcd7M8zMi1LBjfX9b6+wsrq2vlHcLG1t7+zulfcPmibJNGUNmohEtyI0THDFGpZbwVqpZigjwR6i4c3Uf3hi2vBE3dtRykKJfcVjTtE66bFjeF/ilV8NuuWKX/VnIMskyEkFctS75a9OL6GZZMpSgca0Az+14Ri15VSwSamTGZYiHWKftR1VKJkJx7OLJ+TEKT0SJ9qVsmSm/p4YozRmJCPXKdEOzKI3Ff/z2pmNL8MxV2lmmaLzRXEmiE3I9H3S45pRK0aOINXc3UroADVS60IquRCCxZeXSfOsGrjE7s4rtes8jiIcwTGcQgAXUINbqEMDKCh4hld484z34r17H/PWgpfPHMIfeJ8/cGqQFw==</latexit> \u0000 =1 <latexit sha1_base64=\"xfyl4e7TxNt7cmGMK5ZbuRU4wQ4=\">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O6du3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMixLBjfX9b6+wsrq2vlHcLG1t7+zulfcPmiZONWUNGotYtyI0THDFGpZbwVqJZigjwR6i0c3Uf3hi2vBY3dtxwkKJA8X7nKJ1Uqtj+EDiVdAtV/yqPwNZJkFOKpCj3i1/dXoxTSVTlgo0ph34iQ0z1JZTwSalTmpYgnSEA9Z2VKFkJsxm907IiVN6pB9rV8qSmfp7IkNpzFhGrlOiHZpFbyr+57VT278MM66S1DJF54v6qSA2JtPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8AkhuPpQ==</latexit><latexit sha1_base64=\"xfyl4e7TxNt7cmGMK5ZbuRU4wQ4=\">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O6du3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMixLBjfX9b6+wsrq2vlHcLG1t7+zulfcPmiZONWUNGotYtyI0THDFGpZbwVqJZigjwR6i0c3Uf3hi2vBY3dtxwkKJA8X7nKJ1Uqtj+EDiVdAtV/yqPwNZJkFOKpCj3i1/dXoxTSVTlgo0ph34iQ0z1JZTwSalTmpYgnSEA9Z2VKFkJsxm907IiVN6pB9rV8qSmfp7IkNpzFhGrlOiHZpFbyr+57VT278MM66S1DJF54v6qSA2JtPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8AkhuPpQ==</latexit><latexit sha1_base64=\"xfyl4e7TxNt7cmGMK5ZbuRU4wQ4=\">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O6du3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMixLBjfX9b6+wsrq2vlHcLG1t7+zulfcPmiZONWUNGotYtyI0THDFGpZbwVqJZigjwR6i0c3Uf3hi2vBY3dtxwkKJA8X7nKJ1Uqtj+EDiVdAtV/yqPwNZJkFOKpCj3i1/dXoxTSVTlgo0ph34iQ0z1JZTwSalTmpYgnSEA9Z2VKFkJsxm907IiVN6pB9rV8qSmfp7IkNpzFhGrlOiHZpFbyr+57VT278MM66S1DJF54v6qSA2JtPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8AkhuPpQ==</latexit><latexit sha1_base64=\"xfyl4e7TxNt7cmGMK5ZbuRU4wQ4=\">AAAB73icbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGQHGFus0mW7O6du3tCOPInbCwUsfXv2Plv3CRXaOKDgcd7M8zMixLBjfX9b6+wsrq2vlHcLG1t7+zulfcPmiZONWUNGotYtyI0THDFGpZbwVqJZigjwR6i0c3Uf3hi2vBY3dtxwkKJA8X7nKJ1Uqtj+EDiVdAtV/yqPwNZJkFOKpCj3i1/dXoxTSVTlgo0ph34iQ0z1JZTwSalTmpYgnSEA9Z2VKFkJsxm907IiVN6pB9rV8qSmfp7IkNpzFhGrlOiHZpFbyr+57VT278MM66S1DJF54v6qSA2JtPnSY9rRq0YO4JUc3croUPUSK2LqORCCBZfXibNs2rgV4O780rtOo+jCEdwDKcQwAXU4Bbq0AAKAp7hFd68R+/Fe/c+5q0FL585hD/wPn8AkhuPpQ==</latexit> \u0000 = 10 <latexit sha1_base64=\"KxabckYkoAwGJzQXep4i6uuBE3A=\">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGSHGFvs5cs2d07dueEEPIrbCwUsfXn2Plv3CRXaOKDgcd7M8zMi1IpLPr+t1dYWV1b3yhulra2d3b3yvsHTZtkhvEGS2RiWhG1XArNGyhQ8lZqOFWR5A/R8GbqPzxxY0Wi73GU8lDRvhaxYBSd9Nixoq/oVeB3yxW/6s9AlkmQkwrkqHfLX51ewjLFNTJJrW0HforhmBoUTPJJqZNZnlI2pH3edlRTxW04nh08ISdO6ZE4Ma40kpn6e2JMlbUjFblORXFgF72p+J/XzjC+DMdCpxlyzeaL4kwSTMj0e9IThjOUI0coM8LdStiAGsrQZVRyIQSLLy+T5lk18KvB3Xmldp3HUYQjOIZTCOACanALdWgAAwXP8ApvnvFevHfvY95a8PKZQ/gD7/MHAreP3w==</latexit><latexit sha1_base64=\"KxabckYkoAwGJzQXep4i6uuBE3A=\">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGSHGFvs5cs2d07dueEEPIrbCwUsfXn2Plv3CRXaOKDgcd7M8zMi1IpLPr+t1dYWV1b3yhulra2d3b3yvsHTZtkhvEGS2RiWhG1XArNGyhQ8lZqOFWR5A/R8GbqPzxxY0Wi73GU8lDRvhaxYBSd9Nixoq/oVeB3yxW/6s9AlkmQkwrkqHfLX51ewjLFNTJJrW0HforhmBoUTPJJqZNZnlI2pH3edlRTxW04nh08ISdO6ZE4Ma40kpn6e2JMlbUjFblORXFgF72p+J/XzjC+DMdCpxlyzeaL4kwSTMj0e9IThjOUI0coM8LdStiAGsrQZVRyIQSLLy+T5lk18KvB3Xmldp3HUYQjOIZTCOACanALdWgAAwXP8ApvnvFevHfvY95a8PKZQ/gD7/MHAreP3w==</latexit><latexit sha1_base64=\"KxabckYkoAwGJzQXep4i6uuBE3A=\">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGSHGFvs5cs2d07dueEEPIrbCwUsfXn2Plv3CRXaOKDgcd7M8zMi1IpLPr+t1dYWV1b3yhulra2d3b3yvsHTZtkhvEGS2RiWhG1XArNGyhQ8lZqOFWR5A/R8GbqPzxxY0Wi73GU8lDRvhaxYBSd9Nixoq/oVeB3yxW/6s9AlkmQkwrkqHfLX51ewjLFNTJJrW0HforhmBoUTPJJqZNZnlI2pH3edlRTxW04nh08ISdO6ZE4Ma40kpn6e2JMlbUjFblORXFgF72p+J/XzjC+DMdCpxlyzeaL4kwSTMj0e9IThjOUI0coM8LdStiAGsrQZVRyIQSLLy+T5lk18KvB3Xmldp3HUYQjOIZTCOACanALdWgAAwXP8ApvnvFevHfvY95a8PKZQ/gD7/MHAreP3w==</latexit><latexit sha1_base64=\"KxabckYkoAwGJzQXep4i6uuBE3A=\">AAAB8HicbVA9SwNBEJ2LXzF+RS1tFoNgFe5E0EYI2lhGMDGSHGFvs5cs2d07dueEEPIrbCwUsfXn2Plv3CRXaOKDgcd7M8zMi1IpLPr+t1dYWV1b3yhulra2d3b3yvsHTZtkhvEGS2RiWhG1XArNGyhQ8lZqOFWR5A/R8GbqPzxxY0Wi73GU8lDRvhaxYBSd9Nixoq/oVeB3yxW/6s9AlkmQkwrkqHfLX51ewjLFNTJJrW0HforhmBoUTPJJqZNZnlI2pH3edlRTxW04nh08ISdO6ZE4Ma40kpn6e2JMlbUjFblORXFgF72p+J/XzjC+DMdCpxlyzeaL4kwSTMj0e9IThjOUI0coM8LdStiAGsrQZVRyIQSLLy+T5lk18KvB3Xmldp3HUYQjOIZTCOACanALdWgAAwXP8ApvnvFevHfvY95a8PKZQ/gD7/MHAreP3w==</latexit> \u0000 = 100 <latexit sha1_base64=\"XODDCn4YQcuyDxlrzavRuRPWHQQ=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbBU8mKUC9C0YvHCvYD26Vk02wbmmSXJCuUpf/CiwdFvPpvvPlvTNs9aOuDgcd7M8zMCxPBjcX42yusrW9sbhW3Szu7e/sH5cOjlolTTVmTxiLWnZAYJrhiTcutYJ1EMyJDwdrh+Hbmt5+YNjxWD3aSsECSoeIRp8Q66bFn+FCSax/jfrmCq3gOtEr8nFQgR6Nf/uoNYppKpiwVxJiujxMbZERbTgWblnqpYQmhYzJkXUcVkcwE2fziKTpzygBFsXalLJqrvycyIo2ZyNB1SmJHZtmbif953dRGV0HGVZJapuhiUZQKZGM0ex8NuGbUiokjhGrubkV0RDSh1oVUciH4yy+vktZF1cdV//6yUr/J4yjCCZzCOfhQgzrcQQOaQEHBM7zCm2e8F+/d+1i0Frx85hj+wPv8AXN2kBk=</latexit><latexit sha1_base64=\"XODDCn4YQcuyDxlrzavRuRPWHQQ=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbBU8mKUC9C0YvHCvYD26Vk02wbmmSXJCuUpf/CiwdFvPpvvPlvTNs9aOuDgcd7M8zMCxPBjcX42yusrW9sbhW3Szu7e/sH5cOjlolTTVmTxiLWnZAYJrhiTcutYJ1EMyJDwdrh+Hbmt5+YNjxWD3aSsECSoeIRp8Q66bFn+FCSax/jfrmCq3gOtEr8nFQgR6Nf/uoNYppKpiwVxJiujxMbZERbTgWblnqpYQmhYzJkXUcVkcwE2fziKTpzygBFsXalLJqrvycyIo2ZyNB1SmJHZtmbif953dRGV0HGVZJapuhiUZQKZGM0ex8NuGbUiokjhGrubkV0RDSh1oVUciH4yy+vktZF1cdV//6yUr/J4yjCCZzCOfhQgzrcQQOaQEHBM7zCm2e8F+/d+1i0Frx85hj+wPv8AXN2kBk=</latexit><latexit sha1_base64=\"XODDCn4YQcuyDxlrzavRuRPWHQQ=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbBU8mKUC9C0YvHCvYD26Vk02wbmmSXJCuUpf/CiwdFvPpvvPlvTNs9aOuDgcd7M8zMCxPBjcX42yusrW9sbhW3Szu7e/sH5cOjlolTTVmTxiLWnZAYJrhiTcutYJ1EMyJDwdrh+Hbmt5+YNjxWD3aSsECSoeIRp8Q66bFn+FCSax/jfrmCq3gOtEr8nFQgR6Nf/uoNYppKpiwVxJiujxMbZERbTgWblnqpYQmhYzJkXUcVkcwE2fziKTpzygBFsXalLJqrvycyIo2ZyNB1SmJHZtmbif953dRGV0HGVZJapuhiUZQKZGM0ex8NuGbUiokjhGrubkV0RDSh1oVUciH4yy+vktZF1cdV//6yUr/J4yjCCZzCOfhQgzrcQQOaQEHBM7zCm2e8F+/d+1i0Frx85hj+wPv8AXN2kBk=</latexit><latexit sha1_base64=\"XODDCn4YQcuyDxlrzavRuRPWHQQ=\">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbBU8mKUC9C0YvHCvYD26Vk02wbmmSXJCuUpf/CiwdFvPpvvPlvTNs9aOuDgcd7M8zMCxPBjcX42yusrW9sbhW3Szu7e/sH5cOjlolTTVmTxiLWnZAYJrhiTcutYJ1EMyJDwdrh+Hbmt5+YNjxWD3aSsECSoeIRp8Q66bFn+FCSax/jfrmCq3gOtEr8nFQgR6Nf/uoNYppKpiwVxJiujxMbZERbTgWblnqpYQmhYzJkXUcVkcwE2fziKTpzygBFsXalLJqrvycyIo2ZyNB1SmJHZtmbif953dRGV0HGVZJapuhiUZQKZGM0ex8NuGbUiokjhGrubkV0RDSh1oVUciH4yy+vktZF1cdV//6yUr/J4yjCCZzCOfhQgzrcQQOaQEHBM7zCm2e8F+/d+1i0Frx85hj+wPv8AXN2kBk=</latexit> Figure 1. Linear logistic regression on MNIST. Left: Effect of L2-regularization parameter, λ, and standard deviation of the objective perturbation, σ, on test accuracy. Middle: Effect of ϵ on test accuracy when supporting 100 removals. Right: Trade-off between accuracy and supported number of removals at ϵ = 1. At a given ϵ, higher λ and σ values reduce test accuracy but allow for many more removals. puts of Φ are used in a linear model by minimizing Lb and using a removal mechanism that guarantees (ϵCR, δCR)- certified removal. Then the entire procedure guarantees (ϵDP +ϵCR, δDP +δCR)-certified removal. The advantage of this approach over training the entire net- work in a differentially private manner (Abadi et al., 2016) is that the (removal-enabled) linear model can be trained using a much smaller perturbation, which may greatly boost the accuracy of the final model (see Section 4.3). 4. Experiments We test our certified removal mechanism in three set- tings: (1) removal from a standard linear logistic regressor, (2) removal from a linear logistic regres- sor that uses a feature extractor pre-trained on pub- lic data, and (3) removal from a non-linear logistic re- gressor by using a differentially private feature extrac- tor. Code reproducing the results of our experiments is publicly available from https://github.com/ facebookresearch/certified-removal. Table 1 summarizes the training and removal times measured in our experiments. 4.1. Linear Logistic Regression We first experiment on the MNIST digit classification dataset. For simplicity, we restrict to the binary classifi- cation problem of distinguishing between digits 3 and 8, and train a regularized logistic regressor using Algorithm 1. Removal is performed using Algorithm 2 with δ = 1e-4. Effects of λ and σ. Training a removal-enabled model us- ing Algorithm 1 requires selecting two hyperparameters: the L2-regularization parameter, λ, and the standard deviation, σ, of the sampled perturbation vector b. Figure 1 shows the effect of λ and σ on test accuracy and the expected num- ber of removals supported before re-training. When fixing the supported number of removals at 100 (middle plot), the value of σ is inversely related to ϵ (cf. line 16 of Algorithm 2), hence higher ϵ results in smaller σ and improved accu- racy. Increasing λ enables more removals before re-training (left and right plots) because it reduces the gradient resid- ual norm, but very high values of λ negatively affect test accuracy because the regularization term dominates the loss. Tightness of the gradient residual norm bounds. In Algorithm 2 , we use the data-dependent bound from Corol- laries 1 and 2 to compute a per-data or per-batch estimate of the removal error, as opposed to the worst-case bound in Theorems 1 and 4. Figure 2 shows the value of differ- ent bounds as a function of the number of removed points. We consider two removal scenarios: single point removal and batch removal with batch size m = 10. We observe three phenomena: (1) The worst-case bounds (light blue and light green) are several orders of magnitude higher than the data-dependent bounds (dark blue and dark green), which means that the number of supported removals is several orders of magnitude higher when using the data-dependent bounds. (2) The cumulative sum of the gradient residual Certified Data Removal from Machine Learning Models 0 200 400 600 800 1000 # of Removals 10−4 10−2 100 102 104 106Gradient Residual Norm Worst-case single (Theorem 1) Data-dependent single (Corollary 1) Worst-case batch (Theorem 3) Data-dependent batch (Corollary 2) True value Figure 2. Linear logistic regression on MNIST. Gradient residual norm (on log scale) as a function of the number of removals. norm bounds is approximately linear for both the single and batch removal data-dependent bounds. (3) There remains a large gap between the data-dependent norm bounds and the true value of the gradient residual norm (dashed line), which suggests that the utility of our removal mechanism may be further improved via tighter analysis. Gradient residual norm and removal difficulty. The data-dependent bound is governed by the norm of the up- date H −1 w∗ ∆, which measures the influence of the removed point on the parameters and varies greatly depending on the training sample being removed. Figure 3 shows the training samples corresponding to the 10 largest and smallest values of ∥H −1 w∗ ∆∥2. There are large visual differences between these samples: large values correspond to oddly-shaped 3s and 8s, while small values correspond to “prototypical” dig- its. This suggests that removing outliers is harder, because the model tends to memorize their details and their impact on the model is easy to distinguish from other samples. 4.2. Non-Linear Logistic Regression using Public, Pre-Trained Feature Extractors We consider the common scenario in which a feature extrac- tor is trained on public data (i.e., does not require removal), and a linear classifier is trained on these features using non- public data. We study two tasks: (1) scene classification on the LSUN dataset and (2) sentiment classification on the Stanford Sentiment Treebank (SST) dataset. We subsample the LSUN dataset to 100K images per class (i.e., n = 1M). For LSUN, we extract features using a ResNeXt-101 model (Xie et al., 2017) trained on 1B Instagram images (Mahajan et al., 2018) and fine-tuned on ImageNet (Deng et al., 2009). For SST, we extract features using a pre-trained RoBERTa (Liu et al., 2019) language model. At removal time, we use Algorithm 2 with ϵ = 1 and δ = 1e-4 in both experiments. Result on LSUN. We reduce the 10-way LSUN classifica- tion task to 10 one-versus-all tasks and randomly subsample the negative examples to ensure the positive and negative classes are balanced in all binary classification problems. Subsampling benefits removal since a training sample does not always need to be removed from all 10 classifiers. Figure 4 (left) shows the relationship between test accuracy and the expected number of removals on LSUN. The value of (λ, σ) is shown next to each point, with the left-most point corresponding to training a regular model that supports no removal. At the cost of a small drop in accuracy (from 88.6% to 83.3%), the model supports over 10, 000 removals before re-training is needed. As shown in Table 1, the computational cost for removal is more than 250× smaller than re-training the model on the remaining data points. Result on SST. SST is a sentiment classification dataset commonly used for benchmarking language models (Wang et al., 2019). We use SST in the binary classification task of predicting whether or not a movie review is positive. Figure 4 (right) shows the trade-off between accuracy and supported number of removals. The regular model (left- most point) attains a test accuracy of 89.0%, which matches the performance of competitive prior work (Tai et al., 2015; Wieting et al., 2016; Looks et al., 2017). As before, a large number of removals is supported at a small loss in test accuracy; the computational costs for removal are 870× lower than for re-training the model. 4.3. Non-linear Logistic Regression using Differentially Private Feature Extractors When public data is not available for training a feature ex- tractor, we can train a differentially private feature extractor on private data (Abadi et al., 2016) and apply Theorem 5 to remove data from the final (removal-enabled) linear layer. This approach has a major advantage over training the entire model using the approach of (Abadi et al., 2016) because the final linear layer can partly correct for the noisy features produced by the private feature extractor. We evaluate this approach on the Street View House Num- bers (SVHN) digit classification dataset. We compare it to a differentially private CNN2 trained using the technique of Abadi et al. (2016). Since the CNN is differentially pri- vate, certified removal is achieved trivially without applying any removal. For a fair comparison, we fix δ = 1e-4 and train (ϵDP/10, δ)-private CNNs for a range of values of ϵDP. 2We use a simple CNN with two convolutional layers with 64 filters of size 3 × 3 and 2 × 2 max-pooling. Certified Data Removal from Machine Learning Models Top 10 Bottom 10 Figure 3. MNIST training digits sorted by norm of the removal update ∥H −1 w∗ ∆∥2. The samples with the highest norm (top) appear to be atypical, making it harder to undo their effect on the model. The samples with the lowest norm (bottom) are prototypical 3s and 8s, and hence are much easier to remove. 0 2000 4000 6000 800010000 ExSected # Rf 6uSSRrted 5emRvals 84 85 86 87 887est Accuracy 0 200 400 600 800 10001200 ExSected # Rf 6uSSRrted 5emRvals 81 82 83 84 85 86 87 88 897est Accuracy LSUN SST(1e-6, 0) <latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit><latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit><latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit><latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit> (1e-5, 5) <latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit><latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit><latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit><latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit> (1e-4, 20) <latexit sha1_base64=\"M5sHhFU7xVeCKnqkv0ilOMeztW0=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2f0qpzMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKDJNe</latexit><latexit sha1_base64=\"M5sHhFU7xVeCKnqkv0ilOMeztW0=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2f0qpzMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKDJNe</latexit><latexit sha1_base64=\"M5sHhFU7xVeCKnqkv0ilOMeztW0=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2f0qpzMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKDJNe</latexit><latexit sha1_base64=\"M5sHhFU7xVeCKnqkv0ilOMeztW0=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2f0qpzMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKDJNe</latexit> (2e-4, 30) <latexit sha1_base64=\"hqxDPS1xKJXDvu0qblSzeuksDr0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmCnoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqUKnF2c0nP7ZNzNF+2yPQVdJs6cFMkctW7+q9OLeBJAiFwyrduOHaObMoWCSxjnOomGmPEh60Pb0JAFoN10evyYHhulR/1ImQqRTtXfEykLtB4FnukMGA70ojcR//PaCfpXbirCOEEI+WyRn0iKEZ0kQXtCAUc5MoRxJcytlA+YYhxNXjkTgrP48jJpVMqOXXbuKsXq9TyOLDkkR6REHHJJquSW1EidcDIiz+SVvFlP1ov1bn3MWjPWfKZA/sD6/AGNIJNg</latexit><latexit sha1_base64=\"hqxDPS1xKJXDvu0qblSzeuksDr0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmCnoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqUKnF2c0nP7ZNzNF+2yPQVdJs6cFMkctW7+q9OLeBJAiFwyrduOHaObMoWCSxjnOomGmPEh60Pb0JAFoN10evyYHhulR/1ImQqRTtXfEykLtB4FnukMGA70ojcR//PaCfpXbirCOEEI+WyRn0iKEZ0kQXtCAUc5MoRxJcytlA+YYhxNXjkTgrP48jJpVMqOXXbuKsXq9TyOLDkkR6REHHJJquSW1EidcDIiz+SVvFlP1ov1bn3MWjPWfKZA/sD6/AGNIJNg</latexit><latexit sha1_base64=\"hqxDPS1xKJXDvu0qblSzeuksDr0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmCnoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqUKnF2c0nP7ZNzNF+2yPQVdJs6cFMkctW7+q9OLeBJAiFwyrduOHaObMoWCSxjnOomGmPEh60Pb0JAFoN10evyYHhulR/1ImQqRTtXfEykLtB4FnukMGA70ojcR//PaCfpXbirCOEEI+WyRn0iKEZ0kQXtCAUc5MoRxJcytlA+YYhxNXjkTgrP48jJpVMqOXXbuKsXq9TyOLDkkR6REHHJJquSW1EidcDIiz+SVvFlP1ov1bn3MWjPWfKZA/sD6/AGNIJNg</latexit><latexit sha1_base64=\"hqxDPS1xKJXDvu0qblSzeuksDr0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmCnoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqUKnF2c0nP7ZNzNF+2yPQVdJs6cFMkctW7+q9OLeBJAiFwyrduOHaObMoWCSxjnOomGmPEh60Pb0JAFoN10evyYHhulR/1ImQqRTtXfEykLtB4FnukMGA70ojcR//PaCfpXbirCOEEI+WyRn0iKEZ0kQXtCAUc5MoRxJcytlA+YYhxNXjkTgrP48jJpVMqOXXbuKsXq9TyOLDkkR6REHHJJquSW1EidcDIiz+SVvFlP1ov1bn3MWjPWfKZA/sD6/AGNIJNg</latexit> (5e-4, 40) <latexit sha1_base64=\"f8zNMLsnKCsvsags7OgDRqCvcQs=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmRPQY9OIxglkgGUJPp5I06VnorhGHIf6KFw+KePVDvPk3dpaDRh8UPN6roqqeF0mh0ba/rMzK6tr6RnYzt7W9s7uX3z9o6jBWHBo8lKFqe0yDFAE0UKCEdqSA+Z6Elje+nvqte1BahMEdJhG4PhsGYiA4QyP18oUuwgMipqVzOKue0qp9Munli3bZnoH+Jc6CFMkC9V7+s9sPeexDgFwyrTuOHaGbMoWCS5jkurGGiPExG0LH0ID5oN10dvyEHhulTwehMhUgnak/J1Lma534nun0GY70sjcV//M6MQ4u3VQEUYwQ8PmiQSwphnSaBO0LBRxlYgjjSphbKR8xxTiavHImBGf55b+kWSk7dtm5rRRrV4s4suSQHJESccgFqZEbUicNwklCnsgLebUerWfrzXqft2asxUyB/IL18Q2TTpNk</latexit><latexit sha1_base64=\"f8zNMLsnKCsvsags7OgDRqCvcQs=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmRPQY9OIxglkgGUJPp5I06VnorhGHIf6KFw+KePVDvPk3dpaDRh8UPN6roqqeF0mh0ba/rMzK6tr6RnYzt7W9s7uX3z9o6jBWHBo8lKFqe0yDFAE0UKCEdqSA+Z6Elje+nvqte1BahMEdJhG4PhsGYiA4QyP18oUuwgMipqVzOKue0qp9Munli3bZnoH+Jc6CFMkC9V7+s9sPeexDgFwyrTuOHaGbMoWCS5jkurGGiPExG0LH0ID5oN10dvyEHhulTwehMhUgnak/J1Lma534nun0GY70sjcV//M6MQ4u3VQEUYwQ8PmiQSwphnSaBO0LBRxlYgjjSphbKR8xxTiavHImBGf55b+kWSk7dtm5rRRrV4s4suSQHJESccgFqZEbUicNwklCnsgLebUerWfrzXqft2asxUyB/IL18Q2TTpNk</latexit><latexit sha1_base64=\"f8zNMLsnKCsvsags7OgDRqCvcQs=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmRPQY9OIxglkgGUJPp5I06VnorhGHIf6KFw+KePVDvPk3dpaDRh8UPN6roqqeF0mh0ba/rMzK6tr6RnYzt7W9s7uX3z9o6jBWHBo8lKFqe0yDFAE0UKCEdqSA+Z6Elje+nvqte1BahMEdJhG4PhsGYiA4QyP18oUuwgMipqVzOKue0qp9Munli3bZnoH+Jc6CFMkC9V7+s9sPeexDgFwyrTuOHaGbMoWCS5jkurGGiPExG0LH0ID5oN10dvyEHhulTwehMhUgnak/J1Lma534nun0GY70sjcV//M6MQ4u3VQEUYwQ8PmiQSwphnSaBO0LBRxlYgjjSphbKR8xxTiavHImBGf55b+kWSk7dtm5rRRrV4s4suSQHJESccgFqZEbUicNwklCnsgLebUerWfrzXqft2asxUyB/IL18Q2TTpNk</latexit><latexit sha1_base64=\"f8zNMLsnKCsvsags7OgDRqCvcQs=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmRPQY9OIxglkgGUJPp5I06VnorhGHIf6KFw+KePVDvPk3dpaDRh8UPN6roqqeF0mh0ba/rMzK6tr6RnYzt7W9s7uX3z9o6jBWHBo8lKFqe0yDFAE0UKCEdqSA+Z6Elje+nvqte1BahMEdJhG4PhsGYiA4QyP18oUuwgMipqVzOKue0qp9Munli3bZnoH+Jc6CFMkC9V7+s9sPeexDgFwyrTuOHaGbMoWCS5jkurGGiPExG0LH0ID5oN10dvyEHhulTwehMhUgnak/J1Lma534nun0GY70sjcV//M6MQ4u3VQEUYwQ8PmiQSwphnSaBO0LBRxlYgjjSphbKR8xxTiavHImBGf55b+kWSk7dtm5rRRrV4s4suSQHJESccgFqZEbUicNwklCnsgLebUerWfrzXqft2asxUyB/IL18Q2TTpNk</latexit> (1e-3, 50) <latexit sha1_base64=\"lsr/Yvdsmx7W68nEoF+SPbUw3dw=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmInoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipiUHzs5P6YV9Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCNF5Ng</latexit><latexit sha1_base64=\"lsr/Yvdsmx7W68nEoF+SPbUw3dw=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmInoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipiUHzs5P6YV9Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCNF5Ng</latexit><latexit sha1_base64=\"lsr/Yvdsmx7W68nEoF+SPbUw3dw=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmInoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipiUHzs5P6YV9Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCNF5Ng</latexit><latexit sha1_base64=\"lsr/Yvdsmx7W68nEoF+SPbUw3dw=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmInoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipiUHzs5P6YV9Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCNF5Ng</latexit> (1e-6, 0) <latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit><latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit><latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit><latexit sha1_base64=\"Q0eObet+ZhWs0r2EhFdnPaezVXQ=\">AAAB+3icbVDLSgNBEJyNrxhfazx6GQxCBA27OajHoBePEcwDkiXMTjrJkNkHM72SsORXvHhQxKs/4s2/cZLsQaMFDUVVN91dfiyFRsf5snJr6xubW/ntws7u3v6BfVhs6ihRHBo8kpFq+0yDFCE0UKCEdqyABb6Elj++nfutR1BaROEDTmPwAjYMxUBwhkbq2cUuwgQR07ILF5fn1Dmb9eySU3EWoH+Jm5ESyVDv2Z/dfsSTAELkkmndcZ0YvZQpFFzCrNBNNMSMj9kQOoaGLADtpYvbZ/TUKH06iJSpEOlC/TmRskDraeCbzoDhSK96c/E/r5Pg4NpLRRgnCCFfLhokkmJE50HQvlDAUU4NYVwJcyvlI6YYRxNXwYTgrr78lzSrFdepuPfVUu0miyNPjskJKROXXJEauSN10iCcTMgTeSGv1sx6tt6s92VrzspmjsgvWB/fFqyTJA==</latexit> (1e-5, 5) <latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit><latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit><latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit><latexit sha1_base64=\"kBFh0xpOxXxGNqNfdD6R7fllkBo=\">AAAB+3icbVDJSgNBEO2JW4xbjEcvjUGIoGEmEPQY9OIxglkgGUJPpyZp0rPQXSMJQ37FiwdFvPoj3vwbO8tBEx8UPN6roqqeF0uh0ba/rczG5tb2TnY3t7d/cHiUPy40dZQoDg0eyUi1PaZBihAaKFBCO1bAAk9CyxvdzfzWEygtovARJzG4ARuEwhecoZF6+UIXYYyIacmBq+olrV5Me/miXbbnoOvEWZIiWaLey391+xFPAgiRS6Z1x7FjdFOmUHAJ01w30RAzPmID6BgasgC0m85vn9Jzo/SpHylTIdK5+nsiZYHWk8AznQHDoV71ZuJ/XidB/8ZNRRgnCCFfLPITSTGisyBoXyjgKCeGMK6EuZXyIVOMo4krZ0JwVl9eJ81K2bHLzkOlWLtdxpElp+SMlIhDrkmN3JM6aRBOxuSZvJI3a2q9WO/Wx6I1Yy1nTsgfWJ8/HMGTKA==</latexit> (1e-4, 10) <latexit sha1_base64=\"/dmzMgyleY0gGBvb1IYYTmeciYM=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2fUtc5mfbsklNx5qB/iZuREslQ79mf3X7EkwBC5JJp3XGdGL2UKRRcwrTQTTTEjI/ZEDqGhiwA7aXz46f02Ch9OoiUqRDpXP05kbJA60ngm86A4UgvezPxP6+T4ODSS0UYJwghXywaJJJiRGdJ0L5QwFFODGFcCXMr5SOmGEeTV8GE4C6//Jc0qxXXqbi31VLtKosjTw7JESkTl1yQGrkhddIgnEzIE3khr9aj9Wy9We+L1pyVzRTJL1gf34iFk10=</latexit><latexit sha1_base64=\"/dmzMgyleY0gGBvb1IYYTmeciYM=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2fUtc5mfbsklNx5qB/iZuREslQ79mf3X7EkwBC5JJp3XGdGL2UKRRcwrTQTTTEjI/ZEDqGhiwA7aXz46f02Ch9OoiUqRDpXP05kbJA60ngm86A4UgvezPxP6+T4ODSS0UYJwghXywaJJJiRGdJ0L5QwFFODGFcCXMr5SOmGEeTV8GE4C6//Jc0qxXXqbi31VLtKosjTw7JESkTl1yQGrkhddIgnEzIE3khr9aj9Wy9We+L1pyVzRTJL1gf34iFk10=</latexit><latexit sha1_base64=\"/dmzMgyleY0gGBvb1IYYTmeciYM=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2fUtc5mfbsklNx5qB/iZuREslQ79mf3X7EkwBC5JJp3XGdGL2UKRRcwrTQTTTEjI/ZEDqGhiwA7aXz46f02Ch9OoiUqRDpXP05kbJA60ngm86A4UgvezPxP6+T4ODSS0UYJwghXywaJJJiRGdJ0L5QwFFODGFcCXMr5SOmGEeTV8GE4C6//Jc0qxXXqbi31VLtKosjTw7JESkTl1yQGrkhddIgnEzIE3khr9aj9Wy9We+L1pyVzRTJL1gf34iFk10=</latexit><latexit sha1_base64=\"/dmzMgyleY0gGBvb1IYYTmeciYM=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMsunJ2fUtc5mfbsklNx5qB/iZuREslQ79mf3X7EkwBC5JJp3XGdGL2UKRRcwrTQTTTEjI/ZEDqGhiwA7aXz46f02Ch9OoiUqRDpXP05kbJA60ngm86A4UgvezPxP6+T4ODSS0UYJwghXywaJJJiRGdJ0L5QwFFODGFcCXMr5SOmGEeTV8GE4C6//Jc0qxXXqbi31VLtKosjTw7JESkTl1yQGrkhddIgnEzIE3khr9aj9Wy9We+L1pyVzRTJL1gf34iFk10=</latexit> (2e-4, 10) <latexit sha1_base64=\"5KpZfeq6EjwjRbYBwicWuFxeKUs=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMtVODs/pa5zMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKEpNe</latexit><latexit sha1_base64=\"5KpZfeq6EjwjRbYBwicWuFxeKUs=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMtVODs/pa5zMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKEpNe</latexit><latexit sha1_base64=\"5KpZfeq6EjwjRbYBwicWuFxeKUs=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMtVODs/pa5zMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKEpNe</latexit><latexit sha1_base64=\"5KpZfeq6EjwjRbYBwicWuFxeKUs=\">AAAB/HicbVDLSgNBEJyNrxhfqzl6GQxCBA27QdBj0IvHCOYByRJmJ51kyOyDmV4xLPFXvHhQxKsf4s2/cZLsQaMFDUVVN91dfiyFRsf5snIrq2vrG/nNwtb2zu6evX/Q1FGiODR4JCPV9pkGKUJooEAJ7VgBC3wJLX98PfNb96C0iMI7nMTgBWwYioHgDI3Us4tdhAdETMtVODs/pa5zMu3ZJafizEH/EjcjJZKh3rM/u/2IJwGEyCXTuuM6MXopUyi4hGmhm2iIGR+zIXQMDVkA2kvnx0/psVH6dBApUyHSufpzImWB1pPAN50Bw5Fe9mbif14nwcGll4owThBCvlg0SCTFiM6SoH2hgKOcGMK4EuZWykdMMY4mr4IJwV1++S9pViuuU3Fvq6XaVRZHnhySI1ImLrkgNXJD6qRBOJmQJ/JCXq1H69l6s94XrTkrmymSX7A+vgGKEpNe</latexit> (5e-4, 10) <latexit sha1_base64=\"v3nkArFRn06y/E8xLgKVibJtCV0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmKHoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqULODs/pY59Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCOuZNh</latexit><latexit sha1_base64=\"v3nkArFRn06y/E8xLgKVibJtCV0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmKHoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqULODs/pY59Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCOuZNh</latexit><latexit sha1_base64=\"v3nkArFRn06y/E8xLgKVibJtCV0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmKHoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqULODs/pY59Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCOuZNh</latexit><latexit sha1_base64=\"v3nkArFRn06y/E8xLgKVibJtCV0=\">AAAB/HicbVDJSgNBEO2JW4xbNEcvjUGIoGEmKHoMevEYwSyQDKGnU5M06VnorhHDEH/FiwdFvPoh3vwbO8tBEx8UPN6roqqeF0uh0ba/rczK6tr6RnYzt7W9s7uX3z9o6ChRHOo8kpFqeUyDFCHUUaCEVqyABZ6Epje8mfjNB1BaROE9jmJwA9YPhS84QyN184UOwiMipqULODs/pY59Mu7mi3bZnoIuE2dOimSOWjf/1elFPAkgRC6Z1m3HjtFNmULBJYxznURDzPiQ9aFtaMgC0G46PX5Mj43So36kTIVIp+rviZQFWo8Cz3QGDAd60ZuI/3ntBP0rNxVhnCCEfLbITyTFiE6SoD2hgKMcGcK4EuZWygdMMY4mr5wJwVl8eZk0KmXHLjt3lWL1eh5HlhySI1IiDrkkVXJLaqROOBmRZ/JK3qwn68V6tz5mrRlrPlMgf2B9/gCOuZNh</latexit> Figure 4. Linear models trained on public feature extractors. Trade-off between test accuracy and the expected number of supported removals (at ϵ = 1) on LSUN (left) and SST (right). The setting of (λ, σ) is shown next to each point. The number of supported removals rapidly increases when accuracy is slightly sacrificed. 0 1 2 3 4 5 6 ε = εDP + εCR 40 50 60 70 80 90Test Accuracy Non-private Baseline DP Extractor + CR Linear DP Extractor + DP Linear Figure 5. Using ϵ-DP features. Trade-off be- tween ϵ and test accuracy on SVHN of models that support 10 removals. Dashed line shows non-private model accuracy. By the union bound for group privacy (Dwork, 2011), the resulting models support up to then (ϵDP, δ)-CR removals. To measure the effectiveness of Theorem 5 for certified data removal, we also train an (ϵDP/10, δ/10)-differentially pri- vate CNN and extract features from its penultimate linear. We use these features in Algorithm 1 to train 10 one-versus- all classifiers with total failure probability of at most 9 10 δ. Akin to the experiments on LSUN, we subsample the nega- tive examples in each of the binary classifiers to speed up removal. The expected contribution to ϵ from the updates is set to ϵCR ≈ ϵDP/10, hence achieving (ϵ, δ)-CR with ϵ = ϵDP + ϵCR ≈ ϵDP + ϵDP/10 after 10 removals. Figure 5 shows the relationship between test accuracy and ϵ for both the fully private and the Newton update removal methods. For reference, the dashed line shows the accu- racy obtained by a non-private CNN that does not support removal. For smaller values or ϵ, training a private feature extractor (blue) and training the linear layer using Algo- rithm 1 attains much higher test accuracy than training a fully differentially private model (orange). In particular, at ϵ ≈ 0.1, the fully differentially private baseline’s accuracy is only 22.7%, whereas our approach attains a test accuracy of 71.2%. Removal from the linear model trained on top of the private extractor only takes 0.27s, compared to more than 1.5 hour when re-training the CNN from scratch. 5. Related Work Removal of specific training samples from models has been studied in prior work on decremental learning (Cauwen- berghs and Poggio, 2000; Karasuyama and Takeuchi, 2009; Tsai et al., 2014) and machine unlearning (Cao and Yang, 2015). Ginart et al. (2019) studied the problem of removing data from k-means clusterings. These studies aim at exact removal of one or more training samples from a trained model: their success measure is closeness to the optimal pa- rameter or objective value. This suffices for purposes such as quickly evaluating the leave-one-out error or correcting mislabeled data, but it does not provide a formal guarantee of statistical indistinguishability. Our work leverages differ- ential privacy to develop a more rigorous definition of data removal. Concurrent work (Bourtoule et al., 2019) presents an approach that allows certified removal with ϵ = 0. Our definition of certified removal uses the same notion of indistinguishability as that of differential privacy. Many classical machine learning algorithms have been shown to support differentially private versions, including PCA (Chaudhuri et al., 2012), matrix factorization (Liu et al., 2015), linear models (Chaudhuri et al., 2011), and neural networks (Abadi et al., 2016). Our removal mechanism can be viewed on a spectrum of noise addition techniques for preserving data privacy, balancing between computa- tion time for the removal mechanism and model utility. We hope to further explore the connections between differen- Certified Data Removal from Machine Learning Models tial privacy and certified removal in follow-up work to de- sign certified-removal algorithms with better guarantees and computational efficiency. 6. Conclusion We have studied a mechanism that quickly “removes” data from a machine-learning model up to a differentially pri- vate guarantee: the model after removal is indistinguishable from a model that never saw the removed data to begin with. While we demonstrate that this mechanism is practical in some settings, at least four challenges for future work re- main. (1) The Newton update removal mechanism requires inverting the Hessian matrix, which may be problematic. Methods that approximate the Hessian with near-diagonal matrices may address this problem. (2) Removal from mod- els with non-convex losses is unsupported; it may require local analysis of the loss surface to show that data points do not move the model out of a local optimum. (3) There remains a large gap between our data-dependent bound and the true gradient residual norm, necessitating a tighter anal- ysis. (4) Some applications may require the development of alternative, less constraining notions of data removal. References Abadi, M., Chu, A., Goodfellow, I. J., McMahan, H. B., Mironov, I., Talwar, K., and Zhang, L. (2016). Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Com- munications Security, Vienna, Austria, October 24-28, 2016, pages 308–318. Biggio, B., Nelson, B., and Laskov, P. (2012). Poisoning attacks against support vector machines. pages 1467– 1474. Bourtoule, L., Chandrasekaran, V., Choquette-Choo, C., Jia, H., Travers, A., Zhang, B., Lie, D., and Papernot, N. (2019). Machine unlearning. In arXiv 1912.03817. Cao, Y. and Yang, J. (2015). Towards making systems forget with machine unlearning. In 2015 IEEE Symposium on Security and Privacy, SP 2015, San Jose, CA, USA, May 17-21, 2015, pages 463–480. Carlini, N., Liu, C., Kos, J., Erlingsson, U., and Song, D. (2019). The secret sharer: Measuring unintended neural network memorization & extracting secrets. In USENIX Security Symposium, pages 267–284. Carreira, J. and Zisserman, A. (2017). Quo vadis, action recognition? A new model and the kinetics dataset. CoRR, abs/1705.07750. Cauwenberghs, G. and Poggio, T. A. (2000). Incremental and decremental support vector machine learning. In Ad- vances in Neural Information Processing Systems 13, Pa- pers from Neural Information Processing Systems (NIPS) 2000, Denver, CO, USA, pages 409–415. Chaudhuri, K., Monteleoni, C., and Sarwate, A. D. (2011). Differentially private empirical risk minimization. Jour- nal of Machine Learning Research, 12:1069–1109. Chaudhuri, K., Sarwate, A. D., and Sinha, K. (2012). Near- optimal differentially private principal components. In Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Process- ing Systems 2012. Proceedings of a meeting held Decem- ber 3-6, 2012, Lake Tahoe, Nevada, United States., pages 998–1006. Cook, R. D. and Weisberg, S. (1982). Residuals and influ- ence in regression. New York: Chapman and Hall. Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V., and Salakhutdinov, R. (2019). Transformer-xl: Attentive language models beyond a fixed-length context. In Pro- ceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2978–2988. Certified Data Removal from Machine Learning Models Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248–255. Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Associ- ation for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171–4186. Dwork, C. (2011). Differential privacy. Encyclopedia of Cryptography and Security, pages 338–340. Ginart, A., Guan, M. Y., Valiant, G., and Zou, J. (2019). Making AI forget you: Data deletion in machine learning. CoRR, abs/1907.05012. Gu, M. and Eisenstat, S. C. (1995). Downdating the sin- gular value decomposition. SIAM J. Matrix Anal. Appl., 16(3):793–810. He, K., Gkioxari, G., Doll´ar, P., and Girshick, R. B. (2017). Mask R-CNN. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22- 29, 2017, pages 2980–2988. Karasuyama, M. and Takeuchi, I. (2009). Multiple incre- mental decremental learning of support vector machines. In Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural Information Pro- cessing Systems 2009. Proceedings of a meeting held 7-10 December 2009, Vancouver, British Columbia, Canada., pages 907–915. Koh, P. W. and Liang, P. (2017). Understanding black-box predictions via influence functions. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1885–1894. Liu, D. C. and Nocedal, J. (1989). On the limited memory bfgs method for large scale optimization. Math. Program., 45(1-3):503–528. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692. Liu, Z., Wang, Y.-X., and Smola, A. (2015). Fast differen- tially private matrix factorization. In Proceedings of the 9th ACM Conference on Recommender Systems, pages 171–178. ACM. Looks, M., Herreshoff, M., Hutchins, D., and Norvig, P. (2017). Deep learning with dynamic computation graphs. In 5th International Conference on Learning Represen- tations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Mahajan, D., Girshick, R. B., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., and van der Maaten, L. (2018). Exploring the limits of weakly supervised pretraining. In Computer Vision - ECCV 2018 - 15th Eu- ropean Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II, pages 185–201. Ren, S., He, K., Girshick, R. B., and Sun, J. (2015). Faster R-CNN: towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 91–99. Tai, K. S., Socher, R., and Manning, C. D. (2015). Im- proved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 1556–1566. Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J. (2016). Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73. Tsai, C., Lin, C., and Lin, C. (2014). Incremental and decremental training for linear classification. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, pages 343–352. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2019). GLUE: A multi-task benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Represen- tations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Wieting, J., Bansal, M., Gimpel, K., and Livescu, K. (2016). Towards universal paraphrastic sentence embeddings. In 4th International Conference on Learning Representa- tions, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings. Xie, S., Girshick, R. B., Doll´ar, P., Tu, Z., and He, K. (2017). Aggregated residual transformations for deep neural net- works. In 2017 IEEE Conference on Computer Vision Certified Data Removal from Machine Learning Models and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 5987–5995. Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., and Le, Q. V. (2019). Xlnet: Generalized autore- gressive pretraining for language understanding. CoRR, abs/1906.08237. Yeom, S., Giacomelli, I., Fredrikson, M., and Jha, S. (2018). Privacy risk in machine learning: Analyzing the connec- tion to overfitting. In CSF. Zhao, H., Shi, J., Qi, X., Wang, X., and Jia, J. (2017). Pyra- mid scene parsing network. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6230–6239. Certified Data Removal from Machine Learning Models A. Appendix We present proofs for theorems stated in the main paper. Theorem 1. Suppose that ∀(xi, yi) ∈ D, w ∈ Rd : ∥∇ℓ(w⊤xi, yi)∥2 ≤ C. Suppose also that ℓ ′′ is γ-Lipschitz and ∥xi∥2 ≤ 1 for all (xi, yi) ∈ D. Then: ∥∇L(w−; D′)∥2 = ∥(Hwη − Hw∗ )H −1 w∗ ∆∥2 ≤ γ(n − 1)∥H −1 w∗ ∆∥2 2 ≤ 4γC 2 λ2(n − 1) , where Hwη denotes the Hessian of L(·; D′) at the parameter vector wη = w∗ + ηH −1 w∗ ∆ for some η ∈ [0, 1]. Proof. Let G(w) = ∇L(w; D′) denote the gradient at w of the empirical risk on the reduced dataset D′. Note that G : Rd → Rd is a vector-valued function. By Taylor’s Theorem, there exists some η ∈ [0, 1] such that: G(w−) = G(w∗ + H −1 w∗ ∆) = G(w∗) + ∇G(w∗ + ηH −1 w∗ ∆)H −1 w∗ ∆. Since G is the gradient of L(·; D′), the quantity ∇G(w∗ + ηH −1 w∗ ∆) is exactly the Hessian of L(·; D′) evaluated at the point wη = w∗ + ηH −1 w∗ ∆. Thus: G(w−) = G(w∗) + Hwη H −1 w∗ ∆ = (G(w∗) + ∆) + Hwη H −1 w∗ ∆ − ∆ = 0 + Hwη H −1 w∗ ∆ − Hw∗ H −1 w∗ ∆ = (Hwη − Hw∗ )H −1 w∗ ∆. This gives: ∥G(w−)∥2 = ∥(Hwη − Hw∗ )H −1 w∗ ∆∥2 ≤ ∥Hwη − Hw∗ ∥2∥H −1 w∗ ∆∥2. Using the Lipschitz-ness of ℓ ′′, we have for every i: ∥∇ 2ℓ(w⊤ η xi, yi) − ∇2 wℓ((w∗)⊤xi, yi)∥2 = ∥[ℓ ′′(w⊤ η xi, yi) − ℓ ′′((w∗)⊤xi, yi)]xix ⊤ i ∥2 ≤ |ℓ ′′(w⊤ η xi, yi) − ℓ′′((w∗) ⊤xi, yi)| · ∥xi∥ 2 2 ≤ γ∥wη − w∗∥2 since ∥xi∥2 ≤ 1 = γ∥ηH −1 w∗ ∆∥2 ≤ γ∥H −1 w∗ ∆∥2. As a result, we can conclude that: ∥Hwη − Hw∗ ∥2 ≤ n−1∑ i=1 ∥ ∥ ∥∇2ℓ(w⊤ η xi, yi) − ∇2ℓ((w∗)⊤xi, yi) ∥ ∥ ∥ 2 ≤ γ(n − 1)∥H −1 w∗ ∆∥2. Combining these results leads us to conclude that ∥G(w−)∥2 ≤ γ(n − 1)∥H −1 w∗ ∆∥ 2 2. We can simplify this bound by analyzing ∥H −1 w∗ ∆∥2. Since L(·; D′) is λ(n−1)-strongly convex, we get ∥Hw∗ ∥2 ≥ λ(n−1), hence ∥H −1 w∗ ∥2 ≤ 1 λ(n−1) . Recall that ∆ = λw∗ + ∇ℓ ((w∗) ⊤xn, yn) . Certified Data Removal from Machine Learning Models Since w∗ is the global optimal solution of the loss L(·; D), we obtain the condition: 0 = ∇L(w∗; D) = n∑ i=1 ∇ℓ ((w∗) ⊤xi, yi) + λnw∗. Using the norm bound ∥∇ℓ(w⊤x, y)∥2 ≤ C and re-arranging the terms, we obtain: ∥w∗∥2 = ∥ ∑n i=1 ∇ℓ((w∗) ⊤xi, yi)∥2 λn ≤ C λ . Using this and the same norm bound, we observe: ∥∆∥2 ≤ λ∥w∗∥2 + ∥∇ℓ((w∗) ⊤xn, yn)∥2 ≤ 2C, from which we obtain: ∥H −1 w∗ ∆∥2 ≤ ∥H −1 w∗ ∥2∥∆∥2 ≤ 2C λ(n − 1) , which leads to the desired bound. Theorem 2. Suppose that b is drawn from a distribution with density function p(·) such that for any b1, b2 ∈ Rd satisfying ∥b1 − b2∥2 ≤ ϵ′, we have that: e−ϵ ≤ p(b1) p(b2) ≤ eϵ. Then: e−ϵ ≤ f ˜A( ˜w) fA( ˜w) ≤ eϵ, for any solution ˜w produced by ˜A. Proof. Let p be the density function of b and let g ˜A be the density functions of the gradient residual under optimizer ˜A. Consider the density functions qA and q ˜A of z = b − u under optimizers A and ˜A. We obtain: q ˜A(z) = ∫ v g ˜A(v)p(z + v)dv = ∫ v:∥v∥2≤ϵ′ g ˜A(v)p(z + v)dv since g ˜A has no support elsewhere ≤ ∫ v:∥v∥2≤ϵ′ g ˜A(v)e ϵp(z)dv since ∥v∥2 ≤ ϵ′ = eϵp(z) = eϵqA(z) where the last step follows since the gradient residual u under A is 0. To complete the proof, note that the value of ˜w is completely determined by z = b−u. Indeed, any w satisfying Equation 5 is an exact solution of the strongly convex loss Lb(w) − u ⊤w and, hence, must be unique. This gives: f ˜A( ˜w) = ∫ z f ˜A( ˜w|z)q ˜A(z)dz = ∫ z fA( ˜w|z)q ˜A(z)dz since ˜w is governed by z ≤ ∫ z fA( ˜w|z)eϵqA(z)dz = eϵfA( ˜w). In the above, note that while f ˜A and fA are not the same in general, their difference is governed entirely by z: given a fixed z, the conditional density of ˜w is the same under both density functions. Using a similar approach as above, we can also show that f ˜A( ˜w) ≥ e−ϵfA( ˜w). Certified Data Removal from Machine Learning Models Theorem 3. Let A be the learning algorithm that returns the unique optimum of the loss Lb(w; D) and let M be the Newton update removal mechanism (cf., Equation 3). Suppose that ∥∇L(w−; D′)∥2 ≤ ϵ′ for some computable bound ϵ′ > 0. We have the following guarantees for M : (i) If b is drawn from a distribution with density p(b) ∝ e− ϵ ϵ′ ∥b∥2 , then M is ϵ-CR for A; (ii) If b ∼ N (0, cϵ ′/ϵ) d with c > 0, then M is (ϵ, δ)-CR for A with δ = 1.5 · e−c 2/2. Proof. The proof involves bounding the density ratio of b1 and b2 when ∥b1 − b2∥2 ≤ ϵ ′ and then invoking Theorem 2. (i) p(b1) p(b2) = e− ϵ ϵ′ (∥b1∥2−∥b2∥2) ≤ e ϵ ϵ′ (∥b1−b2∥2) ≤ eϵ. The reverse direction can be obtained similarly. (ii) The proof of Theorem 3.22 in (Dwork, 2011) applies using ∆2(f ) = ϵ′, giving that with probability at least 1 − δ, we have that e−ϵ ≤ p(b1) p(b2) ≤ eϵ. Applying Theorem 2 gives the desired (ϵ, δ)-CR guarantee. Theorem 4. Under the same regularity conditions of Theorem 1, we have that: ∥∇L(w(−m); D \\ Dm)∥2 ≤ γ(n − m) ∥ ∥ ∥ ∥ [ H (m) w∗ ]−1 ∆(m)∥ ∥ ∥ ∥ 2 2 ≤ 4γm 2C 2 λ2(n − m) . Proof. The proof is almost identical to that of Theorem 1, except that there are n − m terms in the Hessian and ∆(m) now scales linearly with m. Theorem 5. Suppose Φ is a randomized learning algorithm that is (ϵDP, δDP)-differentially private, and the outputs of Φ are used in a linear model by minimizing Lb and using a removal mechanism that guarantees (ϵCR, δCR)-certified removal. Then the entire procedure guarantees (ϵDP +ϵCR, δDP +δCR)-certified removal. Proof. Let Φ be the randomized algorithm that learns a feature extractor from the data D and let µ(S) = P (Φ(D) ∈ S) be the induced probability measure over the space, Ω, of all possible feature extractors. Let D′ = D \\ x be the dataset with x removed and let µ ′(·) be the corresponding probability measure for Φ(D′). Since Φ is (ϵDP, δDP)-DP, for any S ⊆ Ω, we have that: µ(S) = P (Φ(D) ∈ S) ≤ eϵDPP (Φ(D′) ∈ S) = eϵDP µ ′(S), with probability 1 − δDP. In particular, this shows that µ is absolutely continuous w.r.t. µ ′ and therefore admits a Radon- Nikodym derivative g. Furthermore, g is (almost everywhere w.r.t. µ ′) bounded by eϵDP . Indeed, suppose that there exists a set S ⊆ Ω with µ ′(S) > 0 such that g ≥ eϵDP + α on S for some α ≥ 0, then: µ(S) = ∫ S g(S) dµ ′ ≥ (eϵDP + α)µ ′(S) ≥ µ(S) + αµ ′(S), which is a contradiction unless α = 0. Finally, for any ϕ ∈ Ω such that Φ(D) = ϕ, let A(D, ϕ) be the learning algorithm that trains a model on D using the feature extractor ϕ. Suppose that M is an (ϵCR, δCR)-CR mechanism, then by Fubini’s Theorem: P (M (A(D, Φ(D)), D, x) ∈ T ) = ∫ Ω P (M (A(D, ϕ), D, x) ∈ T ) dµ ≤ ∫ Ω eϵCRP (A(D′, ϕ) ∈ T ) dµ = ∫ Ω eϵCRP (A(D′, ϕ) ∈ T ) · g dµ ′ ≤ ∫ Ω eϵDP+ϵCRP (A(D′, ϕ) ∈ T ) dµ ′ = eϵDP+ϵCRP (A(D′, Φ(D′)) ∈ T ), with probability at least 1 − δDP − δCR. The lower bound can be shown in a similar fashion. Certified Data Removal from Machine Learning Models Errata The proof of Theorem 1 (and similarly, Theorem 4) requires upper bounding the norm of ∆ = λw∗ + ∇ℓ((w∗)⊤xn, yn) for the minimizer w∗ of L(·; D). However, to apply this result to Theorem 3, it in fact requires upper bounding the norm of ∆ for the minimizer w∗ of Lb(·; D), where b is drawn from either the Laplace or the Gaussian distribution. This means the condition for w∗ in the proof of Theorem 1 is: 0 = ∇Lb(w∗; D) = n∑ i=1 ∇ℓ ((w∗) ⊤xi, yi) + λnw∗ + b ⇒ ∥w∗∥2 = ∥ ∑n i=1 ∇ℓ((w∗) ⊤xi, yi) − b∥2 λn ≤ Cn + ∥b∥2 λn . Since b is a random vector, this quantity cannot be upper bounded in absolute terms, but one can obtain high probability bounds via concentration. Note that the data-dependent bounds in Corollary 1 (and similarly, Corollary 2) are unaffected because ∥∆∥2 is computed directly rather than upper bounded. Thanks to Lu Yi for finding this error!","libVersion":"0.3.2","langs":""}