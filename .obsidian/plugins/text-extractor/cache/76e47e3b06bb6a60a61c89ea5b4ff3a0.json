{"path":"GenAIUnleaning/DiffusionUnlearning/2023/Forget-Me-Not.pdf","text":"Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models Eric Zhang 1*, Kai Wang 1*, Xingqian Xu1,3, Zhangyang Wang 2,3, Humphrey Shi 1,3 1SHI Labs @ U of Oregon & UIUC, 2UT Austin, 3Picsart AI Research (PAIR) https://github.com/SHI-Labs/Forget-Me-NotStable DiffusionForget-Me-Not A photo of Elon Musk A dog in Van Gogh Style A photo of an apple Concept Forgetting forgetting Liu Yifei as Mulan forgetting Daniel Craig as 007 Concept Correction & Disentangle Figure 1: Given a text-to-image model (i.e. Stable Diffusion), our approach can swiftly re-steer the cross attention towards a speciﬁc concept and subsequently forgetting or correcting it. (1) Concept Forgetting: target concepts (denoted in blue text and crossed-out) are successfully removed without compromising the quality of the output. (2) Concept Correction & Disentangle: our method can be used to correct a dominant or undesired concept of a prompt. Prior overshadowed concepts reveal in outputs after the dominant concepts are forgotten. In addition, our method learns to forget fast with only 30 seconds for certain concepts (e.g. Elon Musk), and can be easily adapted to lightweight model patches for Stable Diffusion, allowing for multi-concept manipulation and convenient distribution to users. Abstract The unlearning problem of deep learning models, once primarily an academic concern, has become a prevalent issue in the industry. The signiﬁcant advances in text-to- image generation techniques have prompted global discus- *Equal contribution sions on privacy, copyright, and safety, as numerous unau- thorized personal IDs, content, artistic creations, and po- tentially harmful materials have been learned by these mod- els and later utilized to generate and distribute uncontrolled content. To address this challenge, we propose Forget-Me- Not, an efﬁcient and low-cost solution designed to safely re- move speciﬁed IDs, objects, or styles from a well-conﬁguredarXiv:2303.17591v1 [cs.CV] 30 Mar 2023 text-to-image model in as little as 30 seconds, without im- pairing its ability to generate other content. Alongside our method, we introduce the Memorization Score (M-Score) and ConceptBench to measure the models’ capacity to gen- erate general concepts, grouped into three primary cate- gories: ID, object, and style. Using M-Score and Concept- Bench, we demonstrate that Forget-Me-Not can effectively eliminate targeted concepts while maintaining the model’s performance on other concepts. Furthermore, Forget-Me- Not offers two practical extensions: a) removal of poten- tially harmful or NSFW content, and b) enhancement of model accuracy, inclusion and diversity through concept correction and disentanglement. It can also be adapted as a lightweight model patch for Stable Diffusion, allow- ing for concept manipulation and convenient distribution. To encourage future research in this critical area and pro- mote the development of safe and inclusive generative mod- els, we will open-source our code and ConceptBench at https://github.com/SHI-Labs/Forget-Me-Not. 1. Introduction Recently, text-to-image models [10, 23, 53, 52, 57, 66, 54, 65] have shown impressive performance in synthesiz- ing high-quality images according to text prompts. Among these methods, diffusion models such as DALL-E 2 [52] and Stable Diffusion (SD) [54] have met commercial-level productization requirements, initiating numerous applica- tions for downstream users; such text-to-images are also recenlty shown to be able to generate and editing videos in a zero-shot fashion [33] without further training. Indus- trial solutions such as [49, 21, 35, 48, 44, 60] have been widely adopted in various art and visual design systems, garnering signiﬁcant public attention. Despite the popu- larity of this ﬁeld, concerns about security, fairness, regu- lation, copyright, safety, etc., continue to grow rapidly in proportion to model usages. Risks such as generating unau- thorized, biased, and unsafe content have become an imme- diate issue to be resolved. While this is not the ﬁrst time the community has investigated these cases, prior efforts such as [22, 32, 4, 36] have proposed methods in which most of them are high-cost solutions focused on GAN. Yet we still need an effective and efﬁcient solution that can be widely applied to large-scale diffusion models, which motivated us to dive deep into this topic. The risks and issues associated with such large- scale text-to-image models originate from the billion-sized datasets used in training, including public datasets such as Laion [58], COYO [5], CC12M [11], and private data from Google [57, 66], OpenAI [53, 52], etc. The public datasets are usually web-scraped images and captions that lack human-level quality assurance on bias and safety, while private data sources are impossible to determine at scale. As a result, it is nearly unfeasible to fully address harmful con- tent, privacy and copyright concerns through data ﬁltering or source attribution. A compromised solution could be do- main adaptation [25, 67, 64]. In practice, people can adapt a large-scale model to a clean small/mid-size dataset and later use the model for image synthesis. However, collect- ing and ﬁltering such datasets may still be quite laborious. Worse than that, such domain adaptation has severely in- ﬂuenced model capacity, making out-of-domain image syn- thesis challenging and sometimes nearly impossible. Will there be another path? Designing efﬁcient methods and algorithms that guide existing large-scale text-to-image models to forget certain concepts could be a better solu- tion. We start this paper by ﬁrst introducing this new mis- sion, namely concept forgetting, in which a designated set of concepts can be safely disentangled from the visual con- tent. To achieve this goal, we proposed Forget-Me-Not, a simple, low-cost, but effective solution for concept forget- ting. We also proposed the memorization score (M-score) along with ConceptBench, in which the former gauge the generative power of models on certain concepts, and the lat- ter introduce sets of benchmark to assess concept forgetting and memorization. Moreover, we extend concept forgetting to concept correction & disentangle that may further assist models in being accurate and diverse. In conclusion, the main contribution of this paper can be summarized as the following: • We propose Forget-Me-Not, a plug-and-play, efﬁcient and effective concept forgetting and correction method for large-scale text-to-image models. It provides an ef- ﬁcient way to forget speciﬁc concepts with as few as 35 optimization steps, which typically takes about 30 seconds. Additionally, Forget-Me-Not can be easily adapted as lightweight patches for Stable Diffusion, allowing for multi-concept manipulation and conve- nient distribution to text-to-image model users to ad- dress privacy, copyright, and safety concerns. • We also propose memorization score (M-score) and ConceptBench, which enable quantitative measure- ments of models’ capacity for synthesizing the target set of concepts. To the best of our knowledge, we are the ﬁrst to introduce a numerical solution to gauge the model’s behavior of memorization and forgetting. • Through extensive studies and tests, we demonstrate that our Forget-Me-Not is simple, low-cost, and effec- tive. Downstream applications, such as harmful and NSFW content removal and biased concept correc- tion & disentangle, further expand our scope beyond concept forgetting towards cross-modal model reﬁne- ments that may better ﬁt real-world use cases. 2 A photo of Elon Musk Blacklist Replace by: Middle aged man UNet A photo of Elon Musk Tokenize Loss Finetune UNet A photo of Elon Musk Tokenize Finetune UNet Attention Re-steering (a) Token Blacklist (b) Naïve Finetuning (c) Forget-Me-Not (ours) Figure 2: This ﬁgure shows two baseline forgetting methods and our proposed Forget-Me-Not. The target concept to forget is Elon Musk. One baseline is (a) Token Blacklist that simply replaces the target token with a different one. The other baseline is (b) Naive Fintuning in which instead of replacing tokens, it ﬁnetunes model weights so that the new weights generate outputs containing unrelated concepts. Our method (c) Forget-Me-Not utilizes Attention Re-steering in which we ﬁnetune only UNet to minimize each of the intermediate attention maps associated with the target concepts to forget. 2. Related Works 2.1. Text-to-Image Synthesis Image generation has been a challenging but very at- tractive research area, whose goal is to synthesize natural- looking images. In the past decade, we have witnessed the rapid advance of it from unconditional generative models to conditional generative models with powerful architectures of auto-regressive model [53, 66], GAN [8, 31, 30, 63, 59] and diffusion process [26, 47, 41, 19, 3, 61]. Early works focus on unconditional, single-category data distribution modeling , such as hand-written digits, certain species of animals, and human faces [16, 12, 31, 40]. Though, un- conditional models quickly achieves photo realistic results among single-category data, it’s shown that mode collaps- ing issue usually happens when extending data distributions to multiple-category or real image diversity [8, 43, 1]. To tackle the model collapsing problem, the conditional generative model has been introduced. Since then, differ- ent types of data have been used as the conditioning for generative models, e.g. class labels, image instances, and even networks [8, 45] etc. At the same time, CLIP [50, 28], a large-scale pretrained image-text contrastive model, pro- vides a text-image prior of extremely high diversity, which is discovered to be applicable as the conditioning for gener- ative model [46, 15, 38]. Nowadays, DALL-E 2 [52] and Stable Diffusion [54] are capable of generating high qual- ity images solely conditioning on free-form texts, inherit- ing the diversity of billions of real images scraping from the Internet. Subsequently, a line of work seeks to efﬁciently adapt the massive generative model to generate novel rendi- tion of an unseen concept represented by a small reference set, leveraging the great diversity. Dreambooth [56] pro- posed to adapt the model by ﬁnetuning all of its weights, while it requires enormous storage to save newly adapted weights. Textual Inversion [24] and LoRA [27] ameliorate the issue by adapting the model by adding a small set of extra weights. 2.2. Model Unlearning However, this great diversity comes at a price. It incurs potential risk of privacy leakage and copyright infringe- ment. [7, 60] have successfully retrieved samples from Sta- ble Diffusion that are highly faithful to real training exam- ples. Therefore, being able to forget/unlearn certain con- cept in a model without hurting the generative ability for the rest is of both research and practical interests. Simi- lar topics have been seen in ﬁelds other than conditional generative modeling. In model-agnostic meta-learning, [2] noted selectively forgetting the inﬂuence of prior knowl- edge in a network improves the performance in adapted tasks. [9, 39, 42, 13] explores the unlearning of a set of re- quested data points in a pretraind model. Our work differs from existing forgetting and unlearn- ing works in a few aspects. First, we study forgetting in the context of text-to-image generative models. Second, we are deleting not only requested data points represented by a small reference set, but the concept behind those data points, which possesses signiﬁcant impact in text-to-image generation due to the fact that it’s almost impossible to enu- merate all prompts and synonyms relating to a concept. 3 3. Method 3.1. Preliminaries Diffusion models [26, 47, 18] are denoising models that iteratively restore data x0 from its Gaussian noise corrup- tion xT with a total step number T . Such a restoration process is usually known as the reverse diffusion process pθ(xt−1|xt) and the opposite of the reverse process is the forward diffusion process that blends the signal with noise q(xt|xt−1): q(xt|xt−1) = N (xt; √1 − βtxt−1; βtI) pθ(xt−1|xt) = N (xt−1; µθ(xt, t); Σθ(xt, t)) Both forward and reverse processes are presumably Marko- vian chains, so we can express the likelihood of both pro- cesses as: q(x1:T |x0) = T∏ t=1 q(xt|xt−1) pθ(x0:T ) = p(xT ) T∏ t=1 pθ(xt−1|xt) The loss function for the diffusion process is then to minimize the variational bound Lvlb of the negative log- likelihood pθ(x0) (i.e. maximize the likelihood of x0 as the ﬁnal denoised result from a model with parameters θ): LVLB = E [− log pθ(x0)] ≤ Eq [ − log pθ(x0:T ) q(x1:T |x0) ] Cross-Attentions [62] are widely adopted deep learning modules used in discriminative models [20, 6, 29], condi- tional generation models [52, 57, 54] as well as language models [17, 51, 14]. The purpose of cross-attention is to transfer information from conditional inputs to hidden fea- tures through dot product and softmax. For example, in sta- ble diffusion [54], the hidden feature serves as the query Q and context serves as key K and value V . Assume Q and K has dimension d for inner product, the output h is then computed as the following: h = softmax( QK T √d )V It is important to note that such QKV assignments are not ﬁxed. Other assignments, such as conditional-driven queries and feature-driven keys and values, may also have their usage. A photo of Elon Musk Attention Map of Elon Musk 𝐾𝑄 𝐿𝑜𝑠𝑠 = min(∗) 𝐴 𝑉Forward……Backward 𝐿𝑜𝑠𝑠 ( ) ( ) 𝐿𝑜𝑠𝑠…… : Cross Attention Figure 3: This ﬁgure shows the Attention Re-steering we proposed in our Forget-Me-Not method, in which we set the objective func- tion to minimize the attention maps of target concepts (i.e. Elon Musk in this case) and correspondingly ﬁnetune the network. 3.2. Concept Forgetting A concept is an abstract term representing an intuited object of thought, which also serves as the foundation for people’s perceptions. Speciﬁcally for computer vision, we may recognize concepts as tangible things, including iden- tities, objects that physically existed, style of images, object relations, and even poses and behavior. Concept forgetting, literarily speaking, is the action of reverting a model from understanding certain concepts. On contrast to machine un- learning, which aims to delete the ﬁelds around designated data points, we deﬁne concept forgetting in diffusion mod- els as the disentanglement of concept prompts and visual contents. This deﬁnition allows models to retain their gen- erative abilities to the greatest extent possible. Besides, we set the following four goals for concept for- getting research: • Performance: the proposed approach should at best remove target concepts from the model. • Integrity: the proposed approach should at best keep other concepts of the model. • Generality: the proposed approach can be applied to a wide range of concepts that covers all aspects of human perceptions. • Flexibility: the proposed approach can be applied to various models of different tasks and domains. 3.3. Forget-Me-Not To fulﬁll the end goals we mentioned in Section 3.2, we introduce Forget-Me-Not, a heuristically and important ap- 4 proach toward ultimate concept forgetting. One highlight of Forget-Me-Not is that it addressed all goals to some extent. Forget-Me-Not is well-capable in removing a wide variety of concepts without manipulating too much on other out- puts. Its underlying methodology, attention resteering, ﬁts almost all major text-to-image models and may extend to other conditional multimodal generative models. Besides, Forget-Me-Not is a practical solution for many models, all thanks to its low-cost and plug-and-play design. Baselines: Before we dive deep into Forget-Me-Not, one may notice that simple approaches such as Token Black- listing and Naive Finetuning can as well be workable so- lutions for text-to-image models (see Figure 2). Thus we use these approaches as the baselines of this work. Token Blacklisting wipes out the token embeddings to forget con- cepts, like crossing out the dictionary index. It is an instant solution, but one may still recover the forgotten concept via token inversion; therefore, this approach doesn’t forget any- thing in actuality. Additionally, blacklisting a speciﬁc token may inadvertently affect other concepts that share the same prompt, making it challenging to target a speciﬁc concept without affecting others. Though this side effect can be mitigated through prompt engineering, it is still difﬁcult to eliminate due to the variability of natural language. Another baseline, Naive Fintuning, deliberately corrupts the target concept by ﬁnetuning the model so that the target concepts are remapped on random images. The downside of this ap- proach is obvious: it breaks model integrity by simultane- ously corrupting other unrelated ideas during its ﬁnetuning process. As a summary, these pros and cons are listed in Table 1. Attention Resteering: One may notice that both base- line approaches fall into the “performance or integrity” dilemma, while our Forget-Me-Not can elegantly resolve this challenge. The core method behind the scenes is the at- tention resteering loss design, which enables a precise back- propagation of those forgetting concepts. Figure 3 shows the diagram of this idea, in which we ﬁrst locate the context embeddings associated with the forgetting concept; com- pute the attention maps between input features and these embeddings; then minimize the attention maps and back- propagate the network. Such attention resteering can be plugged into any cross-attention layers of the network. It also decouples model’s ﬁnetuning from its original loss functions(i.e. variational lower bound for diffusion models), making the process a much simpler solution. One of our main focuses in this work is concept forgetting of text-to- image models; therefore we carry out attention resteering on all cross-attention layers of UNet [55] in Stable-Diffusion (SD) [54], which yields the best performance on most of the concepts. The entire algorithm of Forget-Me-Not can be found in Algorithm 1. Optional Concept Inversion: Although we may di- Algorithm 1 Forget-Me-Not on diffuser Require: Context embeddings C containing the forgetting concept, embedding locations N of the forgetting con- cept, reference images R of the forgetting concept, dif- fuser Gθ, diffusion step T . 1: repeat 2: t ∼ Uniform([1 . . . T ]); ϵ ∼ N (0, I) 3: ri ∼ R; cj, nj ∼ C, N 4: x0 ← ri 5: xt ← √¯αtx0 + √1 − ¯αtϵ 6: ◃ ¯αt: noise variance schedule 7: x′ t−1, At ← Gθ(xt, cj, t) 8: ◃ At: all attention maps 9: L ← ∑ at∈At ∥a [nj] t ∥2 10: ◃ L: attention resteering loss 11: θ ← θ − ∇θL 12: until Concept forgotten rectly obtain context embeddings using prompts for most text-to-image models, this is not a generalized case for all concepts, particularly when a) the forgetting concept is out of vocabulary; b) the model doesn’t have a vocabulary; c) the description of the forgetting concept is unclear. To over- come the challenge, we optionally include the textual in- version [24] as a ﬁxed overhead before Forget-Me-Not to strengthen its generality on all concepts. In practice, we also notice that such inversion helps text-to-image models more precisely identify the forgetting concept and thus im- proves their performance. More results can be found in our Experiment Section. 4. Experiments 4.1. ConceptBench To meet our need to evaluate Forget-Me-Not and poten- tial future concept forgetting approaches, we introduce a benchmark, namely ConceptBench. It is important to note that several existing benchmarks, such as [37, 57], help assess overall generation qual- ity. However, none are speciﬁcally designed to measure a model’s ability to memorize and forget. ConceptBench utilizes instances from LAION [58], forming three cate- gories,identity, object, and style, ranging from discrete to abstract and easy to hard. Identity refers to the unique and discrete features of each instance. Speciﬁcally, we examine identity in subcategories such as person, franchise, animal, and brand. The person and franchise subcategories are of particular interest due to potential privacy or copyright issues associated with gen- erated images of celebrities or intellectual properties. For the animal breed subcategory, detailed instances may in- clude speciﬁc breeds, such as “Corgi” or “Husky”, which 5 Methods Performance Integrity Generality Flexibility Token Blacklisting No forgetting Inevitably affects other concepts sharing overlapping prompts Within the vocabulary of the tokenizer Tokenizer required Naive Finetuning Successfully removes concept Removes unrelated concept by fault Applies to any concepts with sufﬁcient data. Applies to any models Forget-Me-Not Successfully removes concept Maintains most of the model’s integraity. Applies to any concepts with few data samples Only applies to models with cross attention Table 1: This table compares pros (green) and cons (red) on the four major aspects of concept forgetting between baselines and the proposed Forget-Me-Not. If an approach can handle an aspect to some extend, the corresponding explanation is marked in yellow. belong to the more general “dog” category but have dis- tinct visual features. In the case of brands, they represent abstract concepts of intangible objects that can manifest as logos throughout our daily lives. Object is a broader concept encompassing multiple vari- ations. For example, “dog” refers to various breeds of dogs that share common features. By combining identity instances mentioned earlier, this category provides a hier- archical structure to examine the inﬂuence of concept for- getting on the model. We include food items like “apple”, “banana”, and “broccoli”, man-made objects such as “air- plane”, “keyboard”, “motorcycle”, “umbrella”, and “boat”, and general animals like “dog” and “horse.” Style is an abstract concept that determines the over- all appearance of generated images. ConceptBench incor- porates styles such as “Van Gogh”, “Picasso”, “doodle”, “pixel art”, “neon”, and “sketch.” 4.2. Baseline In view of the multi-component nature of Stable Diffu- sion models, there are several naive methods that can be used to superﬁcially remove a concept from them, such as blacklisting keywords in prompts, removing speciﬁc tokens from the tokenizer dictionary, or tuning the model with un- related images to divert the target concept, as illustrated in Figure 2(a)(b). However, these methods can result in a signiﬁcant deterioration and shifting of the model’s gener- ation capability. Removing tokens from the dictionary can alter the tokenization of prompts where those tokens were previously used and affect the generation of other prompts with overlapping tokens. For instance, removing tokens of “Hillary Clinton” could lead to dysfunctionality in generat- ing “Bill Clinton”. Naive ﬁnetuning to forget with unrelated images explicitly overwrites the visual representation of a concept with extra data and runs the risk of compromising Batman (finetune to forget) Bill Gates/Taylor Swift (finetune to forget) Johnny Depp (finetune to forget)Johnny Depp (original) Figure 4: Finetuning to forget concept “Johnny Depp” with un- related images of “a photo of man”. This method distorts other concepts with visual details of selected unrelated images. existing concept space, as shown in Figure 4. Moreover, it is impossible to exhaust test all relation-based concepts for blacklisting or ﬁnetuning. 4.3. Qualitative Comparison We present the results of concept forgetting from our benchmark, illustrated in Figure 5, the Multi-concepts model of Elon Musk and Taylor Swift demonstrates our method’s ability to perform multi-concept forgetting. As shown in the ﬁrst row, both target concepts have been for- gotten. We evaluated the impacts of forgetting speciﬁc concepts on other related concepts, examining four related concepts to Elon Musk and Taylor Swift - man, woman, Bill Gates, and Emma Watson. As shown, Forget-Me- Not achieved good content preservation and visual quality. However, we observed minor pose and style changes in man and Bill Gates. Based on these ﬁndings, we posit that our approach may have a greater impact on closely related con- cepts than on others. Additionally, the last row shows that a 6Elon Musk Single-Concept Model: Picasso style Multi-Concept Model: Elon Musk & Taylor Swift Single-Concept Model: Van Gogh styleWomanEmma WatsonTaylor SwiftManBill Gates Figure 5: Results of concept forgetting using our method. The ﬁrst 2x2 grid shows the original samples in Stable Diffusion. The subsequent 3 images are sampled after concept forgetting, using the same prompt. The top 3 rows are from a multi-concept model targeting both Elon Musk and Taylor Swift, demonstrating the multi-concept forgetting capability. Control concepts such as Bill Gates and Emma Watson manifest that our approach has minimal impact on concepts other than target ones. The last row shows two single-concept model of styles. Output images were generated with prompts: “a photo of X” (top 3 rows), “a dog in X style” (bottom row). Dominant Concept Concept Correction forgetting Liu Yifei as Mulan forgetting Daniel Craig as 007 forgetting Apple brand Figure 6: Concept Correction: Once the dominant concept has been diminished by our method, the lesser concepts of an semantic-rich prompt become more prominent in generated results. Output images were generated with prompts (top to bottom): “a movie poster of Mulan”, “James Bond”, “apple shape”. 7 new painting style is emerging after forgetting Piccaso and Van Gogh styles.Stable Diffusion A photo of mangoNegative prompt: dogForget-Me-Not Figure 7: In concept correction, our method has the advantage of comprehensive forgetting over negative prompt. 4.4. Quantitative Analysis Memorization Measurement Textual inversion [24] can be employed to identify the token embeddings that best correspond to images. We utilize this technique to measure the concept embedding changes of anchor images toward a reference before and after forgetting. These changes can be seen as generative model’s memorization level of a concept, which we term Memorization Score. In the case of “Elon Musk”, prompt “Elon Musk” is used as reference. Its concept embedding (embr) is obtained by processing it through text encoder. Subsequently, we in- vert the same anchor images of Elon Musk using original model and forgetting model respectively. Concept embed- dings of anchor images are obtained by processing inverted tokens through text encoder, resulting in two sets: origi- nal textual inversion (embo) and forgetting textual inversion (embf ). Only the pooler tokens of concept embeddings are used for measurement. The change in concept embedding is quantiﬁed as the difference between cos(embr, embo) and cos(embr, embf ). A decrease indicates successful forget- ting. Since the textual inversion process bring the random- ness of embeddings, we compute the average Memorization Score over ﬁve running times. We present memorization scores from each sub-category in Table 2. Additional re- sults can be found in the Supplementary material. 4.5. Concept Correction It has been observed that in text-to-image models, the semantics of a prompt are often dominated by the one with the most number of image-text examples in the training set, resulting in the suppression of inferior semantics during in- Concept Initial Mem Score Forgetting Mem Score Elon Musk 0.943 0.848 Mickey Mouse 0.948 0.836 Zebra 0.972 0.899 Google 0.940 0.811 Apple 0.696 0.493 Horse 0.877 0.808 Van Gogh 0.916 0.684 Table 2: Memorization Scores of instances from each sub- categories. ference. Figure 6 exempliﬁes this scenario, where gener- ation is dominated by a concept that is strongly correlated with a prompt due to unbalanced training examples. In the case of the James Bond series, the generation results are overwhelmingly dominated by Daniel Craig, as shown in the middle row. However, our method manages to dimin- ish the most prominent semantic in the prompt, i.e., Daniel Craig, and make other James Bond actors visible. Similarly, in the case of Mulan series and the homonym of “apple”, where the apple fruit and Apple brand are competing with each other, our method successfully corrects target concept in generated images . Negative prompt is a technique used in text-to-image synthesis to eliminate unwanted concepts associated with a prompt. However, their use can result in negative impacts on other aspects of the image, such as changes to its struc- ture and style. Furthermore, negative prompts fail to cor- rect undesirable concepts under certain circumstances. For example, in Figure 7, “a photo of a mango” consistently generates dog images. This is because the name “mango” is commonly used as a pet name for dogs, and people up- load photos of their dogs to the internet, which are collected as training data. In this case, using a negative prompt for dogs would be ineffective, as mango is also a popular cat name, creating the problem of endlessly expanding negative prompts. However, our method successfully brings back the mango fruit by forgetting the connection between “a photo of mango” and dog/cat images. 4.6. NSFW Removal In this section, we examine the effectiveness of our method in a real case of removing harmful contents. NSFW is an internet shorthand for “not safe for work”, used for in- dicating contents that are not wished to be seen in the pub- lic. Such content may include material even offensive for adult audiences. However, they inevitably present in large datasets such as LAION [58], even though NSFW detector has been used [34]. Stable Diffusion, trained on LAION, is known for generating NSFW content when prompted with certain triggers. To evaluate our method, we use a well-known NSFW- 8Stable DiffusionForget-Me-Not Figure 8: Results of removing NSFW contents triggered by “naked”. Faces and sensitive parts are blacked out. triggering prompt, “a photo of naked” in Stable Diffu- sion v2.1 model. Using EulerAncestralDiscreteScheduler, inference-step 50, and scale-guidance 8, the model consis- tently generates images containing nude individuals. We use eight generated NSFW images as input for training Forget-Me-Not. The results, shown in Figure 8, indicate that the con- cept of “naked” has been successfully forgotten. Notably, the second row shows that all sensitive visual cues have been changed in different ways. The ﬁrst example changes abruptly from a naked man to a group of smiling women. In the second example, NSFW individual has been removed from the scene. The last two examples render clothed peo- ple, making them safe. Overall, our method achieved efﬁ- cient forgetting of NSFW content without the need for addi- tional data or the assistance of third-party NSFW detectors. 4.7. Ablation Studies Concept Inversion Ablation We conducted experi- ments on concepts from ConceptBench, with and without using concept inversion (CI). Concept inversion is used to handle concepts that are difﬁcult to describe using prompts. Generally, it can help extract the target concept from the prompt, resulting in more precise embeddings. However, precise embeddings may not be always ideal, see Section 4.7 Concept Ablation. Our results show that CI can achieve higher ﬁdelity for concepts that can be well-described in a prompt, as illustrated in Figure 9, where the model trained with CI preserved more of the original poses and details. Trainable Weights Ablation We conducted experi- ments to compare ﬁnetuning the entire UNet model ver- sus only ﬁnetuning the cross-attention (CA) layers. Cross- attention is a critical component in text-to-image genera- tion, as it injects textual information into the image forma- tion process. Given the same hyper-parameter settings ex- cept for steps, our results show that both methods can suc- cessfully achieve concept forgetting. However, ﬁnetuning the entire UNet model tended to break the model’s genera- tion capability in fewer steps. In some cases, the model col- lapsed before the forgetting process was complete, as show A photo of Elon Musk A photo of a manoriginalw/ CIw/o CI Figure 9: Improving ﬁdelity to original model with concept inver- sion. Concept prompt tend to have diverse semantics, resulting in distortion in concept forgetting. CI extracts precise semantics into dedicated tokens, allowing for more pose and feature consistency.UNetCA Step 35Broccoli Step 70 Step 350 Step 350*UNetCAVan Gogh Step 70 Step 350 Step 550 Step 550* Figure 10: Trainable weights ablation using UNet and Cross At- tention (CA). Compared to CA, UNet is more sensitive to opti- mization steps. The last column with Step X* shows the control concept Elon Musk at Step X. in the “Broccoli” case of Figure 10. Token Embedding of Concept Ablation Our method relies on token embeddings of a concept, which are criti- cal for computing the attention re-steering loss. As shown in Section 4.7 on Concept Inversion Ablation, changing the token embeddings of a concept produces varying results. In Figure 11, we demonstrate a situation where concept prompt prevails concept inversion. By using the same set- 9 Forgetting with CIForgetting with “airplane” Orignal: A photo of airplane Figure 11: Comparison of different token embeddings in concept forgetting. Given concept airplane, we compare forgetting with either tokens of “airplane” or inverted tokens using concept inver- sion, where forgetting with CI fails. tings except for token embeddings, prompt of “airplane” succeeds while inverted tokens fails. We hypothesize that minimizing cross attention over these speciﬁc inverted to- kens of “airplane” tends to break generative capability of the model quickly. 5. Conclusion In this study, we investigate concept forgetting in text- to-image generative models and introduce Forget-Me-Not. This lightweight approach enables ad-hoc concept forget- ting using only a few either real or generated concept im- ages; it can also be easily distributed using model patches. Forget-Me-Not is further naturally extended to enable con- cept correction and disentanglement. Our experiments demonstrate that Forget-Me-Not is successful in diminish- ing and correcting target concepts in Stable Diffusion. Ad- ditionally, we introduce ConceptBench and Memorization Score as evaluation metrics. Overall, our work provides a foundation for further research on concept forgetting and manipulation in text-to-image generation, and can be fur- ther extended to other conditional multimodal generative models to improve the accuracy, inclusion and diversity of such models. 6. Social Impact & Limitations Social Impact Our research has a positive social impact by offering an effective and cost-efﬁcient method to remove and correct harmful and biased concepts in text-to-image generative models. These models are rapidly becoming the backbone of popular AI art and graphic design tools, used by a growing number of people. Our method can gener- ate lightweight model patches that can be conveniently dis- tributed to text-to-image model users like how conventional software patch works. Thus, our research takes a small step towards promoting fairness and privacy protection in AI tools, ultimately beneﬁting society as a whole. Limitations While our approach performs well on con- crete concepts in ConceptBench, it faces challenges in iden- tifying and forgetting abstract concepts. Additionally, suc- cessful forgetting may require manual interventions, such as concept-speciﬁc hyperparameter tuning. References [1] Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks. In Interna- tional Conference on Machine Learning (ICML), pages 214– 223. PMLR, 2017. [2] Sungyong Baik, Seokil Hong, and Kyoung Mu Lee. Learn- ing to forget for meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2379–2387, 2020. [3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. edifﬁ: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [4] Hugo Berg, Siobhan Mackenzie Hall, Yash Bhalgat, Wonsuk Yang, Hannah Rose Kirk, Aleksandar Shtedritski, and Max Bain. A prompt array keeps the bias away: Debiasing vision- language models with adversarial learning. arXiv preprint arXiv:2203.11933, 2022. [5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo- 700m: Image-text pair dataset. https://github.com/ kakaobrain/coyo-dataset, 2022. [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European Confer- ence on Computer Vision (ECCV), pages 213–229. Springer, 2020. [7] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagiel- ski, Vikash Sehwag, Florian Tram`er, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188, 2023. [8] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero Soriano. Instance- conditioned gan. Advances in Neural Information Process- ing Systems (NeurIPS), 34:27517–27529, 2021. [9] Sungmin Cha, Sungjun Cho, Dasol Hwang, Honglak Lee, Taesup Moon, and Moontae Lee. Learning to unlearn: Instance-wise unlearning for pre-trained classiﬁers. arXiv preprint arXiv:2301.11578, 2023. [10] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur- phy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transform- ers. arXiv preprint arXiv:2301.00704, 2023. [11] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre- training to recognize long-tail visual concepts. In Proceed- 10 ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3558–3568, 2021. [12] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8188–8197, 2020. [13] Vikram S Chundawat, Ayush K Tarun, Murari Mandal, and Mohan Kankanhalli. Zero-shot machine unlearning. arXiv preprint arXiv:2201.05629, 2022. [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-ﬁnetuned language models. arXiv preprint arXiv:2210.11416, 2022. [15] Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Ed- ward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. In European Confer- ence on Computer Vision (ECCV), pages 88–105. Springer, 2022. [16] Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Processing Magazine, 29(6):141–142, 2012. [17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [18] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Informa- tion Processing Systems (NeurIPS), 34:8780–8794, 2021. [19] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. arXiv preprint arXiv:2210.05475, 2022. [20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [21] Adobe Fireﬂy. https://www.adobe.com/sensei/ generative-ai/firefly.html. [22] Felix Friedrich, Patrick Schramowski, Manuel Brack, Lukas Struppek, Dominik Hintersdorf, Sasha Luccioni, and Kristian Kersting. Fair diffusion: Instructing text- to-image generation models on fairness. arXiv preprint arXiv:2302.10893, 2023. [23] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene- based text-to-image generation with human priors. In Eu- ropean Conference on Computer Vision (ECCV), pages 89– 106. Springer, 2022. [24] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- nik, Amit H Bermano, Gal Chechik, and Daniel Cohen- Or. An image is worth one word: Personalizing text-to- image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [25] Giorgio Giannone, Didrik Nielsen, and Ole Winther. Few- shot diffusion models. arXiv preprint arXiv:2205.15463, 2022. [26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33:6840–6851, 2020. [27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [28] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Han- naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open- CLIP, 7 2021. [29] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One transformer to rule universal image segmentation. arXiv preprint arXiv:2211.06220, 2022. [30] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. Advances in Neural Infor- mation Processing Systems (NeurIPS), 34:852–863, 2021. [31] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 4401–4410, 2019. [32] Patrik Joslin Kenfack, Daniil Dmitrievich Arapov, Rasheed Hussain, SM Ahsan Kazmi, and Adil Khan. On the fair- ness of generative adversarial networks (gans). In 2021 International Conference” Nonlinearity, Information and Robotics”(NIR), pages 1–7. IEEE, 2021. [33] Levon Khachatryan, Andranik Movsisyan, Vahram Tade- vosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to- image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. [34] LAION-AI. Clip-based-nsfw-detector. https:// github.com/-AI/CLIP-based-NSFW-Detector. [35] Lexica. https://lexica.art/. [36] Zhiheng Li, Anthony Hoogs, and Chenliang Xu. Dis- cover and mitigate unknown biases with debiasing alternate networks. In European Conference on Computer Vision (ECCV), pages 270–288. Springer, 2022. [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Eu- ropean Conference on Computer Vision (ECCV), pages 740– 755. Springer, 2014. [38] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! im- age synthesis with semantic diffusion guidance. In Proceed- ings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 289–299, 2023. [39] Yang Liu, Zhuo Ma, Ximeng Liu, Jian Liu, Zhongyuan Jiang, Jianfeng Ma, Philip Yu, and Kui Ren. Learn to for- 11 get: Machine unlearning via neuron masking. arXiv preprint arXiv:2003.10933, 2020. [40] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15(2018):11, 2018. [41] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffu- sion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022. [42] Ananth Mahadevan and Michael Mathioudakis. Certiﬁ- able machine unlearning for linear models. arXiv preprint arXiv:2106.15093, 2021. [43] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl- Dickstein. Unrolled generative adversarial networks. arXiv preprint arXiv:1611.02163, 2016. [44] Midjourny. https://www.midjourney.com/. [45] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. [46] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [47] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning (ICML), pages 8162– 8171. PMLR, 2021. [48] NovelAI. https://novelai.net/. [49] Picsart. https://picsart.com/. [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In International Conference on Machine Learning (ICML), pages 8748–8763. PMLR, 2021. [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020. [52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image gen- eration with clip latents. arXiv preprint arXiv:2204.06125, 2022. [53] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Con- ference on Machine Learning (ICML), pages 8821–8831. PMLR, 2021. [54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, June 2022. [55] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015. [56] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kﬁr Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 2022. [57] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. [58] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- man, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. [59] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Sin- gan: Learning a generative model from a single natural im- age. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision (ICCV), pages 4570–4580, 2019. [60] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. arXiv preprint arXiv:2212.03860, 2022. [61] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab- hishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equa- tions. arXiv preprint arXiv:2011.13456, 2020. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. [63] Steven Walton, Ali Hassani, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Stylenat: Giving each head a new per- spective. arXiv preprint arXiv:2211.05770, 2022. [64] Jiayu Xiao, Liang Li, Chaofei Wang, Zheng-Jun Zha, and Qingming Huang. Few shot generative model adaption via relaxed spatial structural alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11204–11213, 2022. [65] Xingqian Xu, Zhangyang Wang, Eric Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. arXiv preprint arXiv:2211.08332, 2022. [66] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun- jan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin- fei Yang, Burcu Karagol Ayan, et al. Scaling autoregres- sive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. [67] Jingyuan Zhu, Huimin Ma, Jiansheng Chen, and Jian Yuan. Few-shot image generation with diffusion models. arXiv preprint arXiv:2211.03264, 2022. 12","libVersion":"0.3.2","langs":""}