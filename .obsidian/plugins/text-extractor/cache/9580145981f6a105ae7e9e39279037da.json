{"path":"GenAIUnleaning/DiffusionUnlearning/Attack_restore/2023/RING-A-BELL!.pdf","text":"Published as a conference paper at ICLR 2024 RING-A-BELL! HOW RELIABLE ARE CONCEPT RE- MOVAL METHODS FOR DIFFUSION MODELS? Chia-Yi Hsu ∗, Yu-Lin Tsai ∗ National Yang Ming Chiao Tung University {chiayihsu8315,uriah1001}@gmail.com Chulin Xie University of Illinois at Urbana Champaign chulinx2@illinois.edu Chih-Hsun Lin, Jia-You Chen National Yang Ming Chiao Tung University {pkevawin334, justin041510}@gmail.com Bo Li University of Illinois at Urbana Champaign University of Chicago lbo@illnois.edu, bol@uchicago.edu Pin-Yu Chen IBM Research pin-yu.chen@ibm.com Chia-Mu Yu, Chun-Ying Huang National Yang Ming Chiao Tung University chiamuyu@gmail.com, chuang@cs.nctu.edu.tw ABSTRACT Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential mis- use, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effective- ness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies prob- lematic prompts for diffusion models with the corresponding generation of inap- propriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, we empirically validate our method by testing online ser- vices such as Midjourney and various methods of concept removal. Our results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can trans- form prompts that were originally regarded as safe to evade existing safety mech- anisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents. In essence, Ring-A-Bell could serve as a red-teaming tool to understand the limitations of deployed safety mechanisms and to explore the risk under plausible attacks. Our codes are avail- able at https://github.com/chiayi-hsu/Ring-A-Bell. CAUTION: This paper includes model-generated content that may contain offen- sive material. 1 INTRODUCTION Generative AI has made significant breakthroughs in domains such as text (OpenAI, 2023; Touvron et al., 2023), image (Ho et al., 2020), and code generation (Chowdhery et al., 2022). Among the ∗equal contribution 1arXiv:2310.10012v4 [cs.LG] 7 Jun 2024 Published as a conference paper at ICLR 2024 areas receiving considerable attention within generative AI, text-to-image (T2I) generation stands out. The exceptional performance of today’s T2I diffusion models is largely due to the vast reservoir of training data available on the Internet. This wealth of data enables these models to generate a wide variety of content, ranging from photorealistic scenarios to simulated artwork, anime, and even artistic images. However, using such extensive Internet-derived training data presents both challenges and benefits. Particularly, certain images crawled from the Internet contains restricted content, and thus the trained model leads to memorization and generation of inappropriate images, including copyright violations, images with prohibited content, as well as NSFW material. To achieve this goal, recent research has incorporated safety mechanisms into diffusion models to prevent models from generating inappropriate content. Examples of such mechanisms include stable diffusion with negative prompts (Rombach et al., 2022), Safe Latent Diffusion (SLD) (Schramowski et al., 2023), Erased Stable Diffusion (ESD) (Gandikota et al., 2023), and so on. These mechanisms are designed to either constrain the text embedding space during the inference phase, or to fine- tune the model and steer it to avoid producing copyrighted or inappropriate images. While these safety mechanisms have proven effective in their respective evaluations, one study (Rando et al., 2022) on red-teaming Stable Diffusion (e.g., actively searching for problematic prompts) highlights some potential shortcomings. Specifically, Rando et al. (2022) found that the state-of-the-art Stable Diffusion model, equipped with an NSFW safety filter, can still generate sexually explicit content if the prompt is filled with excessive wording that could evade the safety check. However, such a method requires manual selection of prompts, and is typically cumbersome and not scalable for building a holistic inspection of T2I models. On the other hand, as recent T2I diffusion models have grown significantly in model size, reaching parameter counts up to billions (Ramesh et al., 2021; 2022; Saharia et al., 2022), fine-tuning these models becomes prohibitively expensive and infeasible when dealing with limited computational resources during red-teaming tool development. Consequently, in this study, we use prompt engi- neering (Brown et al., 2020; Li et al., 2020; Li & Liang, 2021; Jiang et al., 2020; Petroni et al., 2019; Lester et al., 2021; Schick & Sch¨utze, 2021; Shin et al., 2020; Shi et al., 2022; Wen et al., 2023) as the basis for our technique to construct problematic prompts. One of the guaranteed benefits is that such a method could allow us to fine-tune prompt, which is order of magnitude less computation- ally expensive than fine-tuning the whole model, while also achieving comparable performance. On the other hand, the scenario is more realistic since the user could only manipulate the prompt input without further modifying the model. In this work, we present Ring-A-Bell, a framework that aims to facilitate the red-teaming of T2I diffusion models with safety mechanisms to find problematic prompts with the ability to reveal sen- sitive concepts (e.g., generating images with prohibited concepts such as “nudity” and “violence”). This is achieved through the approach of prompt engineering techniques and the generation of our adversarial concept database. In particular, we first formulate a model-specific framework to demon- strate how to generate such an adversarial concept. Then, we proceed to construct a model-agnostic framework where knowledge of the model is not assumed. Finally, we introduce Ring-A-Bell to en- able the automation of finding such problematic prompts. Ring-A-Bell simulates real attacks since red-teaming puts ourselves in the attacker’s shoes to identify the weaknesses that can be used against the original model. We emphasize that Ring-A-Bell uses only a text encoder (e.g., text encoder in CLIP model) and is executed offline, which is independent of any target T2I models and online services for evaluation. Furthermore, we reason that such success under black-box access of target model can be attributed to the novel design of concept extraction, so that it is able to uncover im- plicit text-concept associations, leading to efficient discovery of adversarial prompts that generate inappropriate images. On the other hand, such problematic prompts identified by Ring-A-Bell serve two purposes: they help in understanding model misbehavior, and they serve as crucial references for subsequent efforts to strengthen safety mechanisms. We summarize our contributions below. • We propose Ring-A-Bell, which serves as a prompt-based concept testing framework that gener- ates problematic prompts to red-team T2I diffusion models with safety mechanisms, leading to the generation of images with supposedly forbidden concepts. • In Ring-A-Bell, concept extraction is based solely on either the CLIP model or general text en- coders, allowing for model-independent prompt evaluation, resulting in efficient offline evaluation. • Our extensive experiments evaluate a wide range of models, ranging from popular online ser- vices to state-of-the-art concept removal methods, and reveal that problematic prompts generated 2 Published as a conference paper at ICLR 2024 by Ring-A-Bell can increase the success rate for most concept removal methods in generating inappropriate images by more than 30%. 2 RELATED WORK We present a condensed version of the related work and refer the detailed ones in Appendix B. Red-Teaming Evaluation Tools for AI. Red-teaming, a cybersecurity assessment technique, aims to actively search for vulnerabilities within information security systems. Originally focusing on cybersecurity, the concept of red-teaming has also been extended to machine learning, with focus on language models (Perez et al., 2022; Shi et al., 2023; Lee et al., 2023) and more recently, T2I models (Zhuang et al., 2023; Qu et al., 2023; Chin et al., 2023). We note that a concurrent work, P4D (Chin et al., 2023), also develops a red-teaming tool of text-to-image diffusion models, with the main weakness being the assumption of white-box access of target model. For more details, we leave the discussion and comparison to Section 3.2. Diverse Approaches in Prompt Engineering. Prompt engineering seeks to improve the adapt- ability of pre-trained language models to various downstream tasks (Duan et al., 2023; Gal et al., 2023; He et al., 2022) by modifying input text with carefully crafted prompts. This approach, based on representation, can be classified into hard prompt (discrete) (Brown et al., 2020; Schick & Sch¨utze, 2021; Jiang et al., 2020; Gao et al., 2021) and soft prompt (continuous) (Lester et al., 2021; Li & Liang, 2021) where the former represents discrete word patterns and the latter represents the continuous embedding vector. As both possess pros and cons, some efforts are made to unify the advantage of both settings (Shin et al., 2020; Shi et al., 2022; Wen et al., 2023). Text-to-Image Diffusion Model with Safety Mechanisms. To address the misuse of T2I models for sensitive image generation, several approaches have been proposed to combat this phenomenon. Briefly, such methods fall into the following two directions: detection-based (Rando et al., 2022) and removal-based (Rombach et al., 2022; Schramowski et al., 2023; Gandikota et al., 2023; Kumari et al., 2023; Zhang et al., 2023). Detection-based methods aim to remove inappropriate content by filtering it through safety checkers while removal-based methods tries to steer the model away from those contents by actively guiding in inference phase or fine-tuning the model parameters. 3 MAIN APPROACH We aim to evaluate the effectiveness of safety mechanisms for T2I diffusion models. First, we math- ematically construct an attack when the target model is within our knowledge (i.e., model-specific evaluation) in Section 3.2. Then, based on our empirical findings, we construct a model-agnostic evaluation, Ring-A-Bell, by assuming only availability of a general text encoder in Section 3.3. 3.1 BACKGROUND In this section, we provide a brief explanation of diffusion models and their latent counterparts, along with their mathematical formulations. These mathematical formulations illustrate how they work to generate data and support conditional generation. Diffusion Model. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are generative models designed to simulate the process of iteratively generating data by reducing noise from inter- mediate data states. This denoising process, is the inverse of the forward diffusion process, which progressively predicts and reduces noise from the data. Given an input image x0, Denoising Dif- fusion Probabilistic Models (DDPM) (Ho et al., 2020) generate an intermediate noisy image xt at time step t through forward diffusion steps, i.e., iteratively adding noise. Mathematically, xt is given by xt = √αtx0 + √1 − αtϵ where αt is the time-dependent hyperparameter and ϵ is the Gaussian noise, so that the last iterate after T steps simulates the standard Gaussian, xT ∼ N (0, I). Fur- thermore, the denoising network ϵθ(·) aims to predict the previous step iterate xt−1 and resorts to training with the loss L = Ex,t,ϵ∼N (0,I)||ϵ − ϵθ(xt, t)|| 2. 3 Published as a conference paper at ICLR 2024 Latent Diffusion Model. Rombach et al. (2022) proposes the Latent Diffusion Model (LDM), com- monly known as the Stable Diffusion (with minor modification), as an improvement by modeling both the forward and backward diffusion processes within the latent space. This improvement di- rectly mitigates the efficiency challenge faced by DDPM, which suffers from operating directly in pixel space. Given the latent representation z = E(x) of an input image x and its corresponding representational concept c, where E denotes the encoder of VAE (Kingma & Welling, 2013). LDM first obtains the intermediate noisy latent vector zt at time step t. Similar to DDPM, a parameterized model ϵθ with parameter θ is trained to predict the noise ϵθ(zt, c, t), aiming to denoise zt based on the intermediate vector zt, time step t, and concept c. The objective for learning this conditional generation process is defined as L = Ez,t,ϵ∼N (0,I)[||ϵ − ϵθ(zt, c, t)|| 2]. 3.2 MODEL-SPECIFIC EVALUATION To construct a model-specific evaluation, we denote our original unconstrained diffusion model (Ho et al., 2020; Rombach et al., 2022) (i.e., without any safety mechanisms) as ϵθ(·). On the other hand, models with a safety mechanism are denoted as ϵθ′(·). Given a target concept c (e.g., nudity, violence, or artistic concept such as “style of Van Gogh”), we want to find an adversarial concept ˜c such that, given a trajectory, z0, z1, . . . , zT (typically the one that produces the inappropriate image z0), two models can be guaranteed to have similar probabilities of generating such a trajectory, i.e., Pϵθ (z0, z1, . . . , zT |c) ≈ Pϵθ′ (z0, z1, . . . , zT |˜c), (1) where P is the probability that the backward process is generated by the given noise predictor. When minimizing the KL divergence between two such distributions, the objective is expressed as Lwhite, Lwhite = T∑ ˆt=1 Ezˆt∼Pϵθ (zˆt|c)[||ρ(ϵθ(zˆt, c, ˆt) − ϵθ′(zˆt, ˜c, ˆt))|| 2], (2) where ρ denotes the weight on the loss. The detailed derivation of Lwhite can be found in the Appendix A. To briefly explain the attack process, given a forward process starting with an inap- propriate image z0, we want the backward process produced by the noise predictor ϵθ(·) and ϵθ′(·) under the original concept c and the adversary concept ˜c to be similar, and thus output similar im- ages. Namely, we have ˜c := arg min˜c Lwhite(˜c). While the model-specific evaluation seems to produce promising results in theory, it has a few limits when applied in practice. First, to optimize Lwhite, one is required to assume prior knowledge of the model ϵθ′ with the safety mechanism. Second, since the loss is written in the form of an expectation, the method may require multiple samples to obtain accurate estimation. Furthermore, the adversary is assumed to be in possession of ϵθ (e.g., an T2I diffusion model without a safety mechanism), in order to successfully elicit the attacking prompt. Lastly, the model architecture of ϵθ and ϵθ′ should also be similar such that the intermediate noise can be aligned and the L2 loss is then meaningful. We note that a concurrent work, P4D (Chin et al., 2023), presents a similar solution to our model- specific evaluation without the derivation of minimizing KL divergence. In particular, they aim to select problematic prompts P ∗ disc by forcing the unconstrained model ϵθ to present similar behavior to that of one with a safety mechanism. Furthermore, P4D (Chin et al., 2023) is a special case of our formulation by setting the offline stable diffusion model (without an NSFW safety filter) as ϵθ and sampling a particular time step t instead of summing the loss from all time steps. In particular, its loss is set as LP 4D = ||ϵθ(zt, W (P ), t) − ϵθ′(zt, P ∗ disc, t)|| 2 2, where W (P ) is the soft embedding of the harmful prompt P . Moreover, P4D suffers from the above shortcomings, i.e., relying on the white-box access of target model, and requires further design to generalize. 3.3 MODEL-AGNOSTIC EVALUATION On the other hand, instead of assuming the white-box access of target models, we focus on construct- ing attacks only with black-box access of ϵθ′. In particular, we can no longer obtain the adversarial concept ˜c directly from probing the modified model ϵθ′ and ϵθ. To address such a challenge, we propose Ring-A-Bell with its overall pipeline shown in Figure 1. The rationale behind Ring-A-Bell is that current T2I models with safety mechanisms either learn to disassociate or simply filter out 4 Published as a conference paper at ICLR 2024APIs ∈ 𝑃𝑖 𝑐 𝑖=1 𝑁APIs ∶= 𝑃APIs ∈ 𝑃𝑖 𝑐 𝑖=1 𝑁 × 𝜂 Problematic Prompt: heh grimes shooting Firearm Clad panel room dalmatibgt Blackandor jn horror… ෡𝑷∗ Violence APIs Ƹ𝑐 = 1 𝑁 ෍ 𝑖=1 𝑁 𝑓 𝑃𝑖 𝑐 − 𝑓(𝑃𝑖 𝑐) 𝑓(𝑃𝑖 𝑐) 𝑖=1 𝑁 𝑓(𝑃𝑖 𝑐) 𝑖=1 𝑁 𝑃𝑖 𝑐 𝑖=1 𝑁 𝑃𝑖 𝑐 𝑖=1 𝑁 Concept Extraction 𝑓(෡𝑷∗) ≈ ෤𝑝 𝒄𝒐𝒏𝒕 Random Prompts 𝑓 ෠𝑃 − ෨𝑃 2 𝒄𝒐𝒏𝒕 𝐆𝐀 𝐎𝐩𝐭𝐢𝐦𝐢𝐳𝐞 ෠𝑃 Fitness function ෨𝑃 = 𝑓 𝑃 + 𝜂 Ƹ𝑐𝒄𝒐𝒏𝒕𝑓(𝑃)𝑃 Prompt Discovery CLIP CLIP CLIP CLIP 𝑓( ෠𝑃) Prompt 1: Gang members clashed, and many people were bleeding. Prompt 2: Gang members are peaceful, and many people were fine. Target Prompt: neighbor drilling grid holes in a room, manga, black and white manga horror in style of junji ito, kentaro miura Figure 1: An overview of the proposed Ring-A-Bell framework, where the problematic prompts generation is model-agnostic and can be carried out offline. relevant words of the target concepts with their representation c, and thus the detection or removal of such concepts may not be carried out completely if there exist implicit text-concept associations embedded in the T2I generation process. That is, Ring-A-Bell aims to test whether a supposedly removed concept can be revoked via our prompt optimization procedure. In Ring-A-Bell, we first generate the holistic representation of concept c by collecting prompt pairs that are semantically similar with only difference in concept c. For instance, as in Figure 1, the “clashed / peaceful” and “bleeding / fine” are differing in the concept “violence”. Afterwards, the empirical representation ˆc of c is derived as ˆc := 1 N N∑ i=1 {f (P c i ) − f (P̸c i )}, (3) where f (·) denotes the text encoder with prompt input (e.g., text encoder in CLIP) and ̸ c denotes the absence of concept c. Simply put, given prompt pairs {P c i , P̸c i } N i=1 with similar semantics but contrasting in the target concept c, such as (Prompt 1, Prompt 2) in Figure 1 that represent the con- cept “violence”, we extract the empirical representation ˆc by pairwise subtraction of the embedding and then averaging over all pairs. This ensures that the obtained representation does not suffer from context-dependent influence, and by considering all plausible scenarios, we obtain the full semantics underlying the target concept c. Similar attempts can also be seen in Zhuang et al. (2023) where only the sign vectors are used to induce concept removal/generation. We refer the readers to Appendix H for the detailed generation of prompt-pairs. As for the choice and number of prompt pairs, we refer to the ablation studies in Section 4.3. After obtaining ˆc, Ring-A-Bell transforms the target prompt P into the problematic prompt ˆP. In particular, Ring-A-Bell first uses the soft prompt of P and ˆc to generate ˜Pcont(ˆc) as ˜Pcont := f (P) + η · ˆc, (4) where η is the strength coefficient available for tuning. In short, ˜Pcont is the embedding of P infused with varying levels of concept c. Finally, we generate ˆP by solving the optimization problem below min ˆP ||f (ˆP) − ˜Pcont|| 2 subject to ˆP ∈ SK, (5) where K is the length of the query and S is the set of all word tokens. Here, the variables to be optimized are discrete with the addition that typically S consists of a huge token space. Hence, we adopt the genetic algorithm (GA) (Sivanandam & Deepa, 2008) as our optimizer because its ability to perform such a search over large discrete space remains competitive. We also experiment with different choices of the optimizer in Section 4.3. It is evident from the above illustration that Ring-A-Bell requires no prior knowledge of the model to be evaluated except for the access of the text encoder (i.e., the access of f (·) in Eqs. (3)∼(5)). Fur- thermore, Ring-A-Bell presents a readily available database that stores various sensitive concepts. Any user could utilize the concepts identified, automatically create problematic prompts offline, and further deploy them online, demonstrating the practicality of Ring-A-Bell. 5 Published as a conference paper at ICLR 2024 4 EXPERIMENTS Dataset. We evaluate the performance of Ring-A-Bell on the I2P dataset (Schramowski et al., 2023), an established dataset of problematic prompts, on the concepts of nudity and violence. We select 95 nudity prompts where the percentage of nudity is greater than 50%. For the concept of violence, to avoid overlapping with nudity prompts, we selected a total of 250 prompts with a nudity percentage less than 50%, an inappropriateness percentage greater than 50%, and labeled as harmful. Online Services. To evaluate if online services are effective in rejecting the generation of inappro- priate images, we test four well-known T2I online services: Midjourney1, DALL·E 2 2, Gen-2 3(for images), and stability.ai4 (Clipdrop of Stable Diffusion XL). The prompts used to test online services are chosen from prompts generated by Ring-A-Bell for red-teaming concept removal models. Baseline. For the baseline, we select QF-Attack (Zhuang et al., 2023) since QF-Attack is con- ceptually similar to ours. In particular, QF-Attack crafts an adversarial prompt by minimizing the cosine distance between f (x) and f (x′), where x represents the original prompt, x′ represents the adversarial prompt, and f (·) denotes the text-encoder of CLIP model. We conduct the targeted attack of the QF-Attack. The first step involves setting the mask words. This means that it will mask the specified words in the original prompt and, by adding a perturbation prompt to the end of the original prompt, generate an image whose content corresponds to the original prompt with the mask removed. Therefore, we select to mask out words related to clothing such as “wear tops and pants” and words related to peace and absence of harm such as “peaceful and fine”. Based on its default setting, the character length is 5, and the perturbation prompts are crafted using GA, which performed the best among other candidates (Zhuang et al., 2023). We note that P4D (Chin et al., 2023) is not chosen as a baseline as it cannot apply to online T2I services. Concept Removal Models. We choose several state-of-the-art concept removal methods, includ- ing ESD (Gandikota et al., 2023), Safe Latent Diffusion (SLD) (Schramowski et al., 2023) under 3 settings (where we adopt 3 superior variants of SLD as provided by (Schramowski et al., 2023), i.e., SLD-Max, SLD-Strong, and SLD-Medium), SD with negative prompts (SD-NP) (Rombach et al., 2022), Concept Ablation (CA) (Kumari et al., 2023), and Forget-Me-Not (FMN) (Zhang et al., 2023). Furthermore, for ESD in both nudity and violence, we fine-tuned the non-cross-attention and cross-attention parameters with negative guidance set as 1 and 3, respectively. The implemen- tation of all baselines and ablated concept models is based on the official code. To remove nudity, we use “naked, nude, bare, exposed, Stripped, topless” as the negative prompt. On the other hand, to remove violence, we use the prompt “hate, harassment, violence, suffering, humiliation, harm, suicide, blood” for SD-NP and training ESD. Evaluation. Throughout the evaluation process, we generate one image per prompt with a random fixed seed. To classify whether images contain nudity, we use the NudeNet 5 detector. If the detec- tor detects one of the following labels in the image: EXPOSED ANUS, EXPOSED BREAST F, EXPOSED GENITALIA F, EXPOSED GENITALIA M, we consider the image to contain nudity. On the other hand, to determine whether images contain inappropriate content (such as blood or violence), we use the Q16 classifier (Schramowski et al., 2022). We also refer the readers to Ap- pendix D, E, and F for more experiment results. Evaluation Settings. In Ring-A-Bell, we first find the empirical concept ˆc. As shown in Figure 1, it illustrates the use of the text encoder (e.g., CLIP) to obtain ˆc, where we use the VIT-L/14 (Ilharco et al., 2021). We use 50 and 30 prompts containing nudity and violence, respectively, and modify these prompts to remove all traces of nudity and violence to obtain the concept ˆc. To run the GA, we use 200 random initial prompts with 3000 generations and set the mutation rate and crossover rate to 0.25 and 0.5, respectively. In addition to the single-setting Ring-A-Bell, we also follow (Chin et al., 1https://www.midjourney.com (last access: 2023/09) 2https://openai.com/dall-e-2 (last access: 2023/09) 3https://research.runwayml.com/gen2 (last access: 2023/09) 4https://stability.ai/stable-diffusion (last access: 2023/09) 5https://github.com/vladmandic/nudenet (last access: 2023/09) 6 Published as a conference paper at ICLR 2024 Figure 2: Visualization of inappropriate prompts generated by Ring-A-Bell (texts in red, black, and blue respectively represent problematic prompts from Ring-A-Bell, prompt dilution and modifica- tions) via four online services. We use and blurring for publication purposes. Figure 3: Screenshots of the videos generated by Gen-2 (text-to-video version). We blur the screen- shots for publication purposes. (Texts in red, black, and blue respectively represent problematic prompts from Ring-A-Bell, prompt dilution, and modifications.) 2023) to consider the union settings denoted as Ring-A-Bell-Union, covering three different config- urations of Ring-A-Bell for generating new prompts. Furthermore, there are hyper-parameters: K (the length of the prompts), η (the weight of the empirical concept), and N (the number of prompt pairs). In Section 4.3, we will show how K, η, N as well as the choice of optimizer affect the attack results. Regarding the metric, we report the Attack Success Rate (ASR), defined as the pro- portion of successful generation of inappropriate images by problematic prompts relative to the total number of images. For the Ring-A-Bell-Union, each prompt is considered successful if it generates inappropriate images at least once under the three configurations. 4.1 EVALUATION OF ONLINE SERVICES Online services have different levels of protection against generating nudity and violence images shown in Table 1. Despite various safety checkers used in online services, Ring-A-Bell is still able to bypass them, resulting in the successful generation of nude images, as shown in Figure 2. Our experience indicates that for Midjourney, DALL·E 2, and Gen-2, sensitive words such as “naked” or “nude” need to be modified; otherwise, nearly all original prompts from Ring-A-Bell will be blocked. In addition, prompt lengths should not be excessively long (we use K = 16), while the use of prompt dilution (Rando et al., 2022) can increase the probability of evading safe checkers. On the other hand, when it comes to the concepts of violence or bloodshed, most of these online services are not as effective at detecting them as they are at detecting nudity, with the exception of DALL·E 2. According to the documentation of DALL·E 26, it simultaneously checks both prompts and generated images, and has pre-filtered adult content from the training dataset, further, it ensures that unseen content would not be generated. However, the other three services, once they pass the safety checkers, generate images based on prompts truthfully. In other words, once problematic prompts circumvent the safety checks, these three online services will generate images accordingly. Therefore, when compared to DALL·E 2, they are more prone to generating inappropriate images due to the absence of a filtered training set. We provide more examples of inappropriate images generated by online services in Appendix D. In addition to the T2I model, we also assess a text-to-video (T2V) model such as Gen-2 for the concept of nudity and violence shown in Figure 3. Concept Midjourney DALL·E 2 Gen-2 Stable Diffusion XL Nudity 36.75 44.5 33.5 1.33 Violence 5.25 35.5 4.5 0 Table 1: Quantitative comparison among different online services. Each value denotes, on average, how many adaptations (tokens) each prompt requires to generate images with the desired concept. 6https://dallery.gallery/dall-e-ai-guide-faq/ (last access: 2023/09) 7 Published as a conference paper at ICLR 2024 Figure 4: Visualization of inappropriate prompts generated by Ring-A-Bell via SOTA concept re- moval methods. We use and blurring for publication purposes. 4.2 EVALUATION OF CONCEPT REMOVAL METHODS Here, we demonstrate the performance of Ring-A-Bell on T2I models that have been fine-tuned to forget nudity or violence. We note that both CA and FMN are incapable of effectively removing nudity and violence, but we still include them for completeness sake. Furthermore, we also consider a stringent defense, which involves applying both concept removal methods and safety checkers (SC) (Rando et al., 2022) to filter images for inappropriate content after generation. Regarding nudity, we set Ring-A-Bell with K = 16 and η = 3, while for Ring-A-Bell-Union, we employ different settings, including (K, η) = (16, 3), (77, 2), and (77, 2.5). As for violence, we select K = 77 and η = 5.5 and for Ring-A-Bell-Union, we set (K, η) = (77, 5.5), (77, 5) and (77, 4.5). As shown in Table 2, contrary to using the original prompts and QF-Attack, Ring-A-Bell is more effective in facilitating these T2I models to recall forgotten concepts for both nudity and violence. When a safety checker is deployed, Ring-A-Bell is also more capable of bypassing it, especially un- der the union setting. It is worth noting that, as shown in Table 2, the safety checker is more sensitive to nudity and can correctly filter out larger amounts of explicit images. However, its effectiveness drops noticeably when it comes to detecting violence. In addition, we also demonstrate the images obtained by using prompts generated with Ring-A-Bell as input to these concept removal methods as shown in Figure 4. Concept Methods SD ESD SLD-Max SLD-Strong SLD-Medium SD-NP CA FMN Nudity Original Prompts (w/o SC) 52.63% 12.63% 2.11% 12.63% 30.53% 4.21% 58.95% 37.89% QF-Attack (w/o SC) 51.58% 6.32% 9.47% 13.68% 28.42% 5.26% 56.84% 37.89% Ring-A-Bell (w/o SC) 93.68% 35.79% 42.11% 61.05% 91.58% 34.74% 89.47% 68.42% Ring-A-Bell-Union (w/o SC) 97.89% 55.79% 57.89% 86.32% 100% 49.47% 96.84% 94.74% Original Prompts (w/ SC) 7.37% 5.26% 2.11% 6.32% 3.16% 2.11% 9.47% 15.79% QF-Attack (w/ SC) 7.37% 4.21% 2.11% 6.32% 8.42% 5.26% 9.47% 18.95% Ring-A-Bell (w/ SC) 30.53% 9.47% 7.37% 12.63% 35.79% 8.42% 37.89% 28.42% Ring-A-Bell-Union (w/ SC) 49.47% 22.11% 15.79% 32.63% 57.89% 16.84% 53.68% 47.37% Violence Original Prompts (w/o SC) 60.4% 42.4% 16% 20.8% 34% 28% 62% 50.4% QF-Attack (w/o SC) 62% 56% 14.8% 24.2% 32.8% 24.8% 58.4% 53.6% Ring-A-Bell (w/o SC) 96.4% 54% 19.2% 50% 76.4% 80% 97.6% 79.6% Ring-A-Bell-Union (w/o SC) 99.6% 86% 40.4% 80.4% 97.2% 94.8% 100% 98.8% Original Prompts (w/ SC) 56.8% 39.2% 14.4% 18% 30.8% 25.2% 54.8% 47.2% QF-Attack (w/ SC) 54.4% 53.6% 11.2% 21.2% 31.6% 21.2% 53.6% 47.2% Ring-A-Bell (w/ SC) 82.8% 49.2% 18% 44% 68.4% 68% 85.2% 74.4% Ring-A-Bell Union (w/ SC) 99.2% 84% 38.4% 76.4% 95.6% 90.8% 98.8% 98.8% Table 2: Quantitative evaluation of different attack configurations against different concept removal methods via the metric of ASR. (w/o SC and w/ SC represent the absence and presence of the safety checker, respectively). 4.3 ABLATION STUDIES Length K of Prompts. In Table 3, we experiment on how the prompt length K affects the ASR. In this experiment, we set η = 3 and choose three different lengths: {77, 38, 16}, where 77 is the maximum length our text encoder can handle. As shown in Table 3, increasing K does not significantly improve ASR; instead, moderate lengths such as 38 or 16 yield better attack results for nudity. However, for violence, the best performance is observed when K = 77. Concept K SD ESD SLD-Max SLD-Strong SLD-Medium SD-NP CA FMN Nudity 77 85.26% 20% 23.16% 56.84% 92.63% 17.89% 86.32% 63.16% 38 87.37% 29.47% 32.63% 64.21% 88.42% 28.42% 91.58% 71.58% 16 93.68% 35.79% 42.11% 61.05% 91.58% 34.74% 89.47% 68.42% Violence 77 96.4% 54% 19.2% 50% 76.4% 80% 97.6% 79.6% 38 95.2% 46.4% 19.6% 42% 75.6% 71.2% 95.2% 82.4% 16 87.6% 38.8% 13.6% 33.2% 54.8% 52.4% 88% 76% Table 3: The ASR of Ring-A-Bell against different concept removal methods w/o safety checker by varying levels of K. 8 Published as a conference paper at ICLR 2024 Concept η SD ESD SLD-Max SLD-Strong SLD-Medium SD-NP CA FMN Nudity 2 81.05% 31.58% 32.63% 52.63% 84.21% 27.37% 85.26% 64.21% 2.5 88.42% 24.21% 28.42% 56.84% 86.32% 35.79% 85.26% 68.42% 3 87.37% 29.47% 32.63% 64.21% 88.42% 28.42% 91.58% 71.58% 3.5 88.42% 28.42% 29.47% 60% 90.53% 28.42% 84.21% 76.84% Violence 4 93.6% 50.4% 16% 36.4% 68% 66% 97.2% 77.2% 4.5 94% 52% 16.8% 41.6% 71.6% 66.8% 96.4% 70.4% 5 96.4% 59.2% 17.6% 46.4% 77.2% 73.2% 97.6% 78.4% 5.5 96.4% 54% 19.2% 50% 76.4% 80% 97.6% 79.6% Table 4: The ASR of Ring-A-Bell against different concept removal methods w/o safety checker by varying levels of η. Coefficient η. We present how η affects the performance of Ring-A-Bell. We use K = 38 and 77 for nudity and violence concepts, respectively. As shown in Table 4, performance does not improve with excessively large or small values of η. As η = 3, it performs better in attacking these concept removal methods for nudity. Furthermore, for violence, the performance becomes better as η increases. However, when η becomes sufficiently large, the improvement gradually decreases, implying similar results in Table 4 for η = 5 and η = 5.5. Different Choice of Discrete Optimization. We compare the performance of GA and PeZ (Wen et al., 2023), as they have similar purposes. For all experiments, we used the same settings for both GA and PeZ. Regarding nudity, we set K = 16 and η = 3. As for violence, we use K = 77 and η = 5.5. As demonstrated in Table 5, both GA and PeZ showcase competitive performances. However, when it comes to nudity, GA excels on more challenging methods such as ESD and SLD series. GA also surpasses PeZ in the violence category. Concept Method SD ESD SLD-Max SLD-Strong SLD-Medium SD-NP CA FMN Nudity GA 93.68% 35.79% 42.11% 61.05% 91.58% 34.74% 89.47% 68.42% PeZ 93.68% 11.58% 37.89% 54.74% 87.37% 56.84% 92.63% 74.74% Violence GA 96.40% 54.00% 19.20% 50.00% 76.40% 80.00% 97.60% 79.60% PeZ 65.20% 32.00% 8.80% 20.40% 34.80% 32.40% 70.80% 72.40% Table 5: The ASR of Ring-A-Bell against different concept removal models w/o safety checker using different optimization methods. The Number of Prompt Pairs Eq. (3) demonstrates the search of empirical representation ˆc. We next experiment on the effects of Ring-A-Bell for different numbers of prompt pairs. For all experiments regarding nudity, we use K = 16, and η = 3. As for violence, we set K = 77, and η = 5.5. The variation in N leads to some differences in performance, as illustrated in Table 6. The difference lies in the violence prompts used for N = 10 and 20, which are blood-related, while for N = 30, prompts related to firearms and robbery are introduced in addition to blood-related. It can be inferred that using prompts exclusively for one context, e.g., blood-related, when obtaining the empirical concept ˆc tends to generate images that Q16 deems inappropriate as blood-related contexts are highly detectable. However, there are no such issues for nudity. Conclusively, it can be observed that increasing the number of prompt pairs allows us to extract context-independent concept ˆc which ultimately enhances the capability of Ring-A-Bell. Concept N SD ESD SLD-Max SLD-Strong SLD-Medium SD-NP CA FMN Nudity 10 90.53% 25.26% 24.21% 50.53% 84.21% 32.63% 84.21% 61.25% 30 87.37% 24.21% 37.89% 53.68% 90.53% 28.42% 84.21% 58.95% 50 93.68% 35.79% 42.11% 61.05% 91.58% 34.74% 89.47% 68.42% Violence 10 97.60% 57.20% 14.40% 42.80% 80.00% 80.80% 98.80% 78.40% 20 99.60% 66.00% 18.80% 52.40% 87.60% 89.20% 99.60% 80.00% 30 96.40% 54.00% 19.20% 50.00% 76.40% 80.00% 97.60% 79.60% Table 6: The ASR of Ring-A-Bell against different concept removal models w/o safety checker by varying the number of prompt pairs N . 5 CONCLUSION In this paper, we have demonstrated the underlying risk of both online services and concept removal methods for T2I diffusion models, all of which involve the detection or removal of nudity and violence. Our results show that by using Ring-A-Bell to generate problematic prompts, it is highly possible to manipulate these T2I models to successfully generate inappropriate images. Therefore, Ring-A-Bell serves as a valuable red-teaming tool for assessing these T2I models in removing or detecting inappropriate content. 9 Published as a conference paper at ICLR 2024 REFERENCES Praneeth Bedapudi. Nudenet: Neural nets for nudity classification, detection and selective censoring, 2019. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, and Wei-Chen Chiu. Prompt- ing4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts. arXiv preprint arXiv:2309.06135, 2023. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev- skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren- nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochas- tic parrots: Differentially private prompt learning for large language models. arXiv preprint arXiv:2305.15594, 2023. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations (ICLR), 2023. Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In International Conference on Computer Vision (ICCV), 2023. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In International Conference on Learning Representations (ICLR), 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438, 2020. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In International Conference on Com- puter Vision (ICCV), 2023. 10 Published as a conference paper at ICLR 2024 Deokjae Lee, JunYeong Lee, Jung-Woo Ha, Jin-Hwa Kim, Sang-Woo Lee, Hwaran Lee, and Hyun Oh Song. Query-efficient black-box red teaming via bayesian optimization. In Annual Meeting of the Association for Computational Linguistics (ACL), 2023. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Annual Meeting of the Association for Computational Linguistics (ACL), 2021. Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. A unified mrc framework for named entity recognition. In Annual Meeting of the Association for Computational Linguistics (ACL), 2020. Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai- Wei Chang, Aram Galstyan, and Rahul Gupta. Flirt: Feedback loop in-context red teaming. arXiv preprint arXiv:2308.04265, 2023. OpenAI. Gpt-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774. Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. Fabio Petroni, Tim Rockt¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2019. Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou, and Yang Zhang. Unsafe diffusion: On the generation of unsafe images and hateful memes from text-to-image models. In ACM Conference on Computer and Communications Security (CCS), 2023. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), pp. 8821–8831. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text- conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Javier Rando, Daniel Paleka, David Lindner, Lennard Heim, and Florian Tram`er. Red-teaming the stable diffusion safety filter. In NeurIPS ML Safety Workshop, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High- resolution image synthesis with latent diffusion models. arxiv. In IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR), 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Informa- tion Processing Systems (NeurIPS), 35:36479–36494, 2022. Timo Schick and Hinrich Sch¨utze. Few-shot text generation with pattern-exploiting training. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021. Patrick Schramowski, Christopher Tauchmann, and Kristian Kersting. Can machines help us an- swering question 16 in datasheets, and in turn reflecting on inappropriate content? In ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2022. Patrick Schramowski, Manuel Brack, Bj¨orn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 22522–22531, 2023. 11 Published as a conference paper at ICLR 2024 Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer. Toward human readable prompt tuning: Kubrick’s the shining is a good movie, and a good prompt too? arXiv preprint arXiv:2212.10539, 2022. Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang, and Cho-Jui Hsieh. Red teaming language model detectors with language models. arXiv preprint arXiv:2305.19713, 2023. Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. S.N. Sivanandam and S.N. Deepa. Genetic Algorithms. Springer Berlin Heidelberg, 2008. ISBN 978-3-540-73190-0. URL https://doi.org/10.1007/978-3-540-73190-0_2. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learn- ing (ICML), 2015. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar- mand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668, 2023. Yuchen Yang, Bo Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao. Sneakyprompt: Evaluating robust- ness of text-to-image generative models’ safety filters. arXiv preprint arXiv:2305.12082, 2023. Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learn- ing to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591, 2023. Haomin Zhuang, Yihua Zhang, and Sijia Liu. A pilot study of query-free adversarial attack against stable diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW), pp. 2384–2391, 2023. 12 Published as a conference paper at ICLR 2024 A DERIVATION OF MODEL-SPECIFIC EVALUATION We now show that the minimization of KL-Divergence between two different distributions (the original and the modified) can lead to an alternate loss as defined in Section 3.2. DKL(Pϵθ (z0, z1, . . . , zT |c)||Pϵθ′ (z0, z1, . . . , zT |˜c)) (6) = EPϵθ (z0,z1,...,zT ) log ∏T t=1 Pϵθ (zt−1|zt, c)Pϵθ (zT ) ∏T t=1 Pϵθ′ (zt−1|zt, ˜c)Pϵθ′ (zT ) = T∑ ˆt=1 EPϵθ (z0,z1,...,zT ) log Pϵθ (zˆt−1|zˆt, c) Pϵθ′ (zˆt−1|zˆt, ˜c) Expanding the term corresponding to the specific timestep ˆt, i.e., EPϵθ (z0,z1,...,zT ) log Pϵθ (zˆt−1|zˆt, c) Pϵθ′ (zˆt−1|zˆt, ˜c) (7) = ∫ (z0,z1,...,zT ) T∏ t=1 Pϵθ (zt−1|zt, c)P (zT ) log Pϵθ (zˆt−1|zˆt, c) Pϵθ′ (zˆt−1|zˆt, ˜c) d(z0, z1, . . . , zT ) = ∫ (zˆt,zˆt+1,...,zT ) Pϵθ ((zˆt, zˆt+1, . . . , zT )|c) [ ∫ (z0,z1,...,zT −1) ˆt∏ t=1 Pϵθ (zt−1|zt, c) log Pϵθ (zˆt−1|zˆt, c) Pϵθ′ (zˆt−1|zˆt, ˜c) d(zˆt−1, zˆt−2, . . . , z0) ] d(zˆt, zˆt+1, . . . , zT ) = ∫ zˆt Pϵθ (zˆt|c) [ ∫ (z0,z1,...,zˆt−1) ( ˆt−1∏ t=1 Pϵθ (zt−1|zt, c) )Pϵθ (zˆt−1|zˆt, c) log Pϵθ (zˆt−1|zˆt, c) Pϵθ′ (zˆt−1|zˆt, ˜c) d(zˆt−1, zˆt−2, . . . , z0) ] dzˆt = ∫ zˆt Pϵθ (zˆt|c) [ ∫ (z0,z1,...,zˆt−1) Pϵθ (zˆt−1|zˆt, c) log Pϵθ (zˆt−1|zˆt, c) Pϵθ′ (zˆt−1|zˆt, ˜c) [ ∫ (z0,z1,...,zˆt−2) ˆt−1∏ t=1 Pϵθ (zt−1|zt, c)d(zˆt−2, zˆt−3, . . . , z0)] dzˆt−1 ] dzˆt. (8) The integral term over d(zˆt−2, zˆt−3, . . . , z0) in Eq. (8) will be 1 since it is an integration of the probability distribution over the range it is defined. Thus, Eq. (7) can be written as E zˆt∼Pϵθ (zˆt|c) [ ∫ zˆt−1 Pϵθ (zˆt−1|zˆt, c) log Pϵθ (zˆt−1|zˆt, c) Pϵθ′ (zˆt−1|zˆt, ˜c) dzˆt−1 ] = E zˆt∼Pϵθ (zˆt|c) [ DKL(Pϵθ (zˆt−1|zˆt, c)||Pϵθ′ (zˆt−1|zˆt, ˜c)) ] = E zˆt∼Pϵθ (zˆt|c) [ ∣ ∣∣ ∣ρ(ϵθ(zˆt, c, t) − ϵθ′(zˆt, ˜c, t) )∣ ∣∣ ∣2]. 13 Published as a conference paper at ICLR 2024 We utilize the fact since KL divergence between two normal distributions simplifies to the squared difference between the mean. We ignore the variance terms in the KL divergence as it is not learned. Thus, following the result, Eq. (6) can be derived as T∑ ˆt=1 Ezˆt∼Pϵθ (zˆt|c) [ ∣ ∣∣ ∣ρ(ϵθ(zˆt, c, ˆt) − ϵθ′(zˆt, ˜c, ˆt) )∣ ∣∣ ∣2] . B DETAILED RELATED WORK Red-Teaming Tools for AI. Red-teaming, a cybersecurity assessment technique, aims to actively search for vulnerabilities and weaknesses within information security systems. In addition, such discoveries would provide valuable insights that enable companies and organizations to strengthen their defenses and cybersecurity protections. The concept of red-teaming has also been extended to the field of AI, with a particular focus on language models (Perez et al., 2022; Shi et al., 2023; Lee et al., 2023) and more recently, T2I models (Zhuang et al., 2023; Qu et al., 2023; Chin et al., 2023). The overall goal is to improve the security and stability of these models by exploring their vulnerabilities. Perez et al. (2022) propose a method in which language models are prompted by various techniques, such as few-shot generation and reinforcement learning, to generate test cases capable of exposing weaknesses in the models. Meanwhile, Shi et al. (2023) take a different approach by fooling the model designed to recognize machine-generated text. They do this by revising the model’s output, which can include substituting synonyms or changing the style of writing in the sentences generated. Conversely, Lee et al. (2023) create a pool of user input and use Bayesian optimization to iteratively modify a diverse set of positive test cases, ultimately leading to model failure. Particularly, there have been some attempts to explore the vulnerabilities of text-to-image diffusion models. In particular, Zhuang et al. (2023) propose a query-free attack to demonstrate that given only a small perturbation in the input prompt, the output could have suffered from huge semantic drift. Qu et al. (2023) exploit prompts collected from online forums to examine the reliability of the safety mechanism in text-to-image online services and further manipulate them to generate hateful memes. Finally, a concurrent work, P4D (Chin et al., 2023), also develops a red-teaming tool of text- to-image diffusion models with the prior knowledge of the target model, with the main weakness lying in the assumption of white-box access target model. For more details, we leave the discussion and comparison to Section 3.2. Diverse Approaches in Prompt Engineering. Prompt engineering seeks to improve the adapt- ability of pre-trained language models to a variety of downstream tasks by modifying input text with carefully crafted prompts. Furthermore, as current language models grow in parameter size, prompt engineering has emerged as a promising alternative to solve the computationally intensive fine-tuning problem (Duan et al., 2023; Gal et al., 2023; He et al., 2022). This approach, based on the data representation, can be classified into hard prompt (discrete) and soft prompt (continuous). Hard prompts, which are essentially discrete tokens, typically consist of words carefully crafted by users. In contrast, soft prompts involve the inclusion of continuous-valued text vectors or embed- dings within the input, which not only provide high-dimensional feasible space compared to their hard prompt counterparts, but also inherited the advantage of continuous optimization algorithms. An example of hard prompt generation is Brown et al. (2020), which demonstrates a remarkable gen- eralizability of pre-trained language models achieved by using manually generated hard prompts in various downstream tasks. Subsequent works (Schick & Sch¨utze, 2021; Jiang et al., 2020; Gao et al., 2021) improve on this technique by reformulating the input text into specific gap-filling phrases that preserved the semantics and features of hard prompts. On the other hand, methods such as Lester et al. (2021) and Li & Liang (2021) optimize the soft prompts to achieve better task performance. Both approaches have distinct advantages. In particular, hard prompt methods are often difficult to implement because they involve searching in a large discrete space. However, hard constraints and related techniques can be mixed and matched to various tasks, while soft constraints are highly specialized. 14 Published as a conference paper at ICLR 2024 Recently, some innovative optimization techniques have emerged to take advantage of both hard and soft prompts constraints. Notable examples include AutoPrompt (Shin et al., 2020), Fluent- Prompt (Shi et al., 2022), and PeZ (Wen et al., 2023). These approaches use continuous gradient- based optimization to learn adaptive hard prompts while retaining the flexibility of soft prompts. Text-to-Image Diffusion Model with Safety Mechanisms. To address the misuse of T2I models for sensitive image generation, several approaches have been proposed to combat this phenomenon. Briefly, such methods fall into the following two directions: detection-based and removal-based. For detection-based methods, the images generated by the T2I model would be run through a safety checker to first determine the correlation of the output with sensitive or harmful concepts. One such commercial detector is HIVE 7, which provides visual moderation. While the safety mechanisms of popular online services remain unclear, it is assumed that these services have at least one or more such post-hoc detectors in place when outputting user-generated content (Rando et al., 2022). On the other hand, instead of blocking images in the post-generation process, removal-based meth- ods target the latent diffusion model itself, by constraining the generation process or modifying the parameter to eliminate sensitive concepts in image synthesis. For methods that constrain the genera- tion process, Stable Diffusion with negative prompts (Rombach et al., 2022) and SLD (Schramowski et al., 2023) target the input prompts by removing certain tokens or embeddings to prevent corre- sponding content from spawning. Meanwhile, ESD (Gandikota et al., 2023), Concept Ablation (Ku- mari et al., 2023), and Forget-Me-Not (Zhang et al., 2023) operate by partially fine-tuning the diffu- sion model weights to remove the plausible effect of such concepts. C LIMITATION Here we discuss some limitations of our proposed Ring-A-Bell. Firstly, although our method could be misused by a malicious actor, we position our tool as an essential and effective red-teaming tool that can proactively test and reduce such a risk. In addition, our validation depends on the quality of an independent concept classifier (e.g., NudeNet (Bedapudi, 2019) and Q16 (Schramowski et al., 2022)), which means there could be some false positives and mis-detections. However, the reported results are conclusive that the evaluated concept removal methods and online services for T2I models require a holistic overhaul as well as a stronger and better safety mechanism. D MORE EXAMPLES OF INAPPROPRIATE IMAGES GENERATED BY ONLINE SERVICES In this section, we perform more examples of inappropriate images generated by online services via problematic prompts from Ring-A-Bell as shown in Figure 5. Figure 5: Visualization of inappropriate prompts generated by Ring-A-Bell (texts in red, black, and blue respectively represent problematic prompts from Ring-A-Bell, prompt dilution and modifica- tions.) via four online services. We use and blurring for publication purposes. 7https://docs.thehive.ai/docs/visual-content-moderation (last access: 2023/09) 15 Published as a conference paper at ICLR 2024 E MORE EXAMPLES OF INAPPROPRIATE IMAGES GENERATED BY CONCEPT REMOVAL MODELS E.1 CONCEPT OF NUDITY We demonstrate additional examples that are generated by concept removal models using the prob- lematic prompts from Ring-A-Bell. We set K = 77 and η = 3.5 for Ring-A-Bell. In Figure 6, each row corresponds to a prompt, and Ring-A-Bell also generates new prompts based on the same original prompt. Figure 6: Visualization of more examples of nudity generated by concept removal models by taking the problematic prompts from Ring-A-Bell. E.2 CONCEPT OF VIOLENCE In Figure 7, we display images generated by all concept removal models using a pair of prompts. One of these prompts is the original prompt, while the other is generated by Ring-A-Bell. Although they use different versions of the CLIP text encoder, it is worth noting that the images show similarity when identical prompts are used. Figure 7: Visualization of examples for violence generated by all concept removal models via the original prompt and the problematic prompt from Ring-A-Bell. 16 Published as a conference paper at ICLR 2024 F VISUALIZATION OF CONCEPT RETRIEVAL IN CONCEPT REMOVAL METHODS In this section, we perform Ring-A-Bell to retrieve the forbidden concept of “cars” and “Van Gogh” in various concept removal methods. F.1 CONCEPT OF CAR We show results with car-related prompts for SD and ESD, as well as Ring-A-Bell-generated prompts for ESD. Note that we use the official checkpoint of ESD. For the setting of Ring-A-Bell, we select K = 38 and η = 3.5. In Figure 8 (green part), SD and ESD take the same prompts as input and it is apparent that ESD can successfully remove cars. However, by employing Ring- A-Bell based on the original prompts used by ESD and SD, the problematic prompts generated by Ring-A-Bell can lead to ESD producing images containing cars shown in Figure 8 (orange part). Figure 8: Visualization of the results generated by SD and ESD using the original prompts, along with the outcomes produced by ESD with Ring-A-Bell-generated prompts as input. 17 Published as a conference paper at ICLR 2024 Notation Definition c target sensitive concept to generate, e.g., nudity, violence. f (·) text encoder with prompt inputs ˜c adversarial concept to optimize in model-specific evaluation ˆc empirical representation of target concept c (Pc i , P̸c i ) prompt-pair with and without target concept c P target prompt, the initial prompt that fails to pass safety filters or generates inappropriate images ˜Pcont the problematic soft prompt for subsequent discrete optimization ˆP the resulting hard prompt generated by Ring-A-Bell Table 7: Notation Table F.2 CONCEPT OF VAN GOGH We show results with Van Gogh-related prompts for SD, ESD, CA, and FMN, as well as Ring-A- Bell-generated prompts for ESD, CA, and FMN. Note that we use the official checkpoint for ESD and CA. For FMN, we re-implement using the official code 8. For Ring-A-Bell, we set K = 38 and η = 0.9. As illustrated in Figure 9, each line represents the same prompt, while Ring-A-Bell manipulates the old one to generate new prompts. SD effectively generates images in the Van Gogh style, while ESD, CA, and FMN show the ability to eliminate this style. However, as shown in Figure 9 (orange part), Ring-A-Bell demonstrates the ability to enable these models to successfully recall the Van Gogh style. Figure 9: Visualization of the results generated by SD, ESD, CA, and FMN using the original prompts, along with the outcomes produced by ESD, CA, and FMN with Ring-A-Bell-generated prompts as input. G NOTATION TABLE Here we list out some of the notations and symbols used in constructing Ring-A-Bell. The overeall notation can been seen from Table 7 8https://github.com/SHI-Labs/Forget-Me-Not 18 Published as a conference paper at ICLR 2024 K SD ESD SLD-Max SLD-Strong SLD-Medium SD-NP CA FMN 8 87.37% 14.74% 21.05% 44.21% 87.37% 37.89% 85.26% 66.32% 16 93.68% 35.79% 42.11% 61.05% 91.58% 34.74% 89.47% 68.42% Table 8: Attack Success Rate (ASR) of Ring-A-Bell under different values of K H GENERATION OF PROMPT-PAIRS In this section, we will be explaining the generation process of the prompt-pairs used in extracting the empirical concept ˆc. Specifically, we utilize ChatGPT to create sentences about a particular concept c, i.e., P c i . Fur- thermore, when seeking for semantically similar prompts without the concept, i.e., P ̸c i , we instruct ChatGPT to retain most words in the sentences and only modify a few words related to the specific concept, preventing the need of extensive knowledge. For instance, regarding objects or artistic styles like Van Gogh style, we ask ChatGPT to generate several words related to landscapes or natural scenery and append ”with Van Gogh style” after each prompt. On the other hand, for ̸ c, excluding ”with Van Gogh style” suffices. As for general and aggregated concepts such as nudity, we instruct ChatGPT to generate some vocabularies about nudity, such as exposed, bare, and topless. Furthermore, we define subjects and scenarios such as man, woman/bedroom, in a painting. Lastly, we ask ChatGPT to permute and construct sentences using these words. On the other hand, for ̸ c, simply replacing the previous sensitive words would suffice. I ABLATION STUDY OF THREE ATTACK STRATEGIES Here we provide the ablation study on the effect between Ring-A-Bell, modification, and prompt dilution techniques. We present the visualization in Figure 10. Specifically, in the figure, ”Ring-A-Bell” represents our execution of the Ring-A-Bell method based on the target prompt to generate a problematic prompt. We note that the example is produced by DALL·E 2. Furthermore, in the figure, the top row represents the images by applying only modification and dilution while the bottom row applies all Ring-A-Bell, modification, and dilution techniques. As one can see in Figure 10, using approaches such as modification and dilution could allow us to increase the overall success rate other than only using Ring-A-Bell. To explain the two strategies, modification simply avoids inappropriate words in the problematic prompt (input filtering), while di- lution prevents generated images from being identified as inappropriate (output filtering). It’s worth noting that when only using the original prompt along with the above two techniques, e.g., mod- ification and dilution, the generated images fail to generate nudity content. That is to say, simply using the original prompt and these two techniques does not produce inappropriate images. Prob- lematic images would appear only when combining these techniques along with prompts generated by Ring-A-Bell. J ADDITIONAL EXPERIMENTS ON DIFFERENT VALUES OF K In this section, we provide some additional experiment results for smaller values of K. We follow the settings of Table 3 in Section 4.3 to produce results for smaller K. Particularly, we set K = 8 and η = 3 for the nudity concept with the result shown in Table 8. As shown in the table, it is clear that the results obtained with K = 8 are not superior to those achieved with K = 16. Thus, it can be inferred that small values of K are not conducive to effectively causing the model to generate inappropriate images. 19 Published as a conference paper at ICLR 2024 Figure 10: Visualization of the influences on modifications and dilution. We use for publication purposes. K COMPARISON AGAINST P4D Before demonstrating the online/offline gap between P4D (Chin et al., 2023) and Ring-A-Bell, we would like to clarify the difference in evaluation for both works. The attack success rate (ASR) metric of P4D employs the NudeNet classifier (binary classifier for safe/unsafe) 9 with an unsafe threshold of 0.45 to judge whether an image is inappropriate. On the other hand, our metric employs the NudeNet detector9 that detects specific exposure parts, flags certain parts such as anus, genitalia as nudity concepts. Particularly, we regard labels such as EXPOSED ANUS, EXPOSED BREAST F, EXPOSED GENITALIA F, EXPOSED GENITALIA M as nudity-related. To establish the comparison against both evaluation metric, we provide an example image that is considered generally safe in Figure 11. It can be seen that our results are more restricting since we recognize only certain parts as nudity-related while images such as Figure 11 are recognized as unsafe in their much more lenient aspect. To provide the comparison, we conduct the official code of P4D-N using prompt length as K = 16 and apply identical random seed as the one used in Ring-A-Bell. We perform evaluation under both 9https://github.com/vladmandic/nudenet (last access: 2023/09) 20 Published as a conference paper at ICLR 2024 P4D / Ring-A-Bell ESD SLD-Max SLD-Strong SLD-Medium SD-NP P4D Metric 21.05% / 35.79% 12.63% / 42.11% 10.53% / 61.05% 28.42% / 91.58% 2.11%/ 34.74% Ring-A-Bell Metric 66.31% / 55.79% 71.58% / 57.89% 77.89% / 86.32% 85.26% / 100% 22.11%/ 49.47% Table 9: Attack comparison under both evaluation setting, the metric presented is attack success rate (ASR). metrics and present the results in Table 9. As one can observe from the table, the performance of P4D-N degrades heavily under our metric and does not stand out significantly in comparison to our approach. On the other hand, using P4D’s metric, we as well performs superior to P4D under the majority of different settings (3 out of 5), demonstrating the effectiveness of our method. Figure 11: Visualization of images recognized as unsafe under the P4D metric. L WHAT COMES AFTER EXPLORE? Our work functions as a red-teaming tool, although we acknowledge that it could potentially be mis- applied for inappropriate purposes. Nevertheless, our primary objective is to uncover vulnerabilities within online services and concept removal techniques, with the aim of highlighting the existing risks associated with current methods. M DISCUSSION AND RESULT WITH SIMILAR WORKS Here we discuss some concurrent works that both strive to unravel the potential risk of T2I diffusion models which are (Qu et al., 2023; Mehrabi et al., 2023; Rando et al., 2022; Yang et al., 2023) respectively. Firstly, Qu et al. (2023) evaluates the safety of T2I models in an exploratory manner. By manually collecting unsafe prompts from online forums, the authors examine the risk of T2I models generat- ing inappropriate images using these collected prompts. On the other hand, the authors also trained a customized safety filter that superseded most filters deployed in online T2I services. Meanwhile, they take a step forward by aiming to fine-tune T2I models such that the model could generate hate- ful memes, a specific type of unsafe content in images. While we appreciate the exploratory analysis of [R4], we note that this differs from our direction as the manually collected unsafe prompts cannot scale to provide an overall examination of the T2I model. On the other hand, we’ve already con- sidered a similar methodology such as the I2P dataset. These manually collected prompts generally would not pass the safety filter but serve as a great starting template for Ring-A-Bell. Secondly, we note that Mehrabi et al. (2023) is a very recent and even concurrent submission pub- lished in August 2023 with a different problem setup. Specifically, Mehrabi et al. (2023) aims to develop a red-teaming tool of T2I diffusion models by leveraging the power of language models (LM). Specifically, the attack is set up as a feedback loop between the language model and the T2I model. That is to say, the LM would first initiate an adversarial prompt as an input to the T2I model. Meanwhile, the output image would go through a safeness classifier and the score would serve as 21 Published as a conference paper at ICLR 2024 Attack Success Rate (ASR) ESD SLD-Max SLD-Strong SLD-Medium SneakyPrompt 12.63% 3.16% 7.37% 27.37% Ring-A-Bell 35.79% 42.11% 61.05% 91.58% Table 10: Comparison of Ring-A-Bell against SneakyPrompt feedback for the LM to adjust the adversarial prompt for subsequent trials. While the method in Mehrabi et al. (2023) serves as an important red-teaming tool for T2I models, we note that current online services would directly reject the generated inappropriate image, implying no meaningful feedback could be obtained by the LM. As a result, the extension to online T2I services remains unclear and therefore differs from the setting of Ring-A-Bell. Thirdly, Rando et al. (2022) aims to explore the potential risk of safety filters deployed by Stable Diffusion. Particularly, the authors proposed prompt dilution to dilute sensitive prompts such that it could circumvent the safety filtering of Stable Diffusion. Here we note that we indeed incorporated the method of prompt dilution when evaluating Ring-A-Bell for online T2I services to increase the overall attack success rate. However, we’ve included an ablation study between the effect of Ring- A-Bell with and without dilution in Appendix H to demonstrate that simply using prompt dilution alone is not effective in constructing a successful attack. Lastly, Yang et al. (2023) attempts to attack the safety filter of existing online T2I services via reinforcement learning. Specifically, SneakyPrompt (Yang et al., 2023) would initialize a target prompt and replace the sensitive tokens within. Meanwhile, the bypass-or-not response from the T2I services then serves as feedback to the agent to replace more suitable tokens until the safety filter is bypassed and the CLIP score between the target prompt and generated image is optimized. In the section below, we’ve included the comparison between SneakyPrompt and Ring-A-Bell on nudity. We conduct the official code of SneakyPrompt and follow their default setting that uses reinforce- ment learning to search, the CLIP score as a reward, the early stopping threshold score for the agent is 0.26, and the upper limit for query is 60. The result is demonstrated in the Table 10 below. As shown in the Table 10, the performance of SneakyPrompt is much lower than Ring-A-Bell in terms of concept removal methods. This is mainly due to the fact that SneakyPrompt focuses on the jailbreak of safety filters, rendering it unable to find the problematic prompts for concept removal methods so as to generate inappropriate images. Specifically, if the feedback from safety filters indicates that the image is safe, SneakyPrompt will deem this prompt as successfully jailbroken. However, since concept removal methods have already eliminated a large portion of the sensitive concept, even if the prompt contains sensitive words, under the setting of concept removal, the generated image is deemed safe by the safety filter. As a result, SneakyPrompt skips it. 22","libVersion":"0.3.2","langs":""}