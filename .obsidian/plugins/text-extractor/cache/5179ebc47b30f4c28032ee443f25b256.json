{"path":"GenAIUnleaning/DiffusionUnlearning/2024/Separable Multi-Concept.pdf","text":"Separable Multi-Concept Erasure from Diffusion Models Mengnan Zhao, Lihe Zhang, Tianhang Zheng, Yuqiu Kong and Baocai Yin https://github.com/Dlut-lab-zmn/SepCE4MU Abstract Large-scale diffusion models, known for their impres- sive image generation capabilities, have raised concerns among researchers regarding social impacts, such as the imitation of copyrighted artistic styles. In response, existing approaches turn to machine unlearning techniques to elim- inate unsafe concepts from pre-trained models. However, these methods compromise the generative performance and neglect the coupling among multi-concept erasures, as well as the concept restoration problem. To address these issues, we propose a Separable Multi-concept Eraser (SepME), which mainly includes two parts: the generation of concept- irrelevant representations and the weight decoupling. The former aims to avoid unlearning substantial information that is irrelevant to forgotten concepts. The latter separates optimizable model weights, making each weight increment correspond to a specific concept erasure without affecting generative performance on other concepts. Specifically, the weight increment for erasing a specified concept is formu- lated as a linear combination of solutions calculated based on other known undesirable concepts. Extensive experi- ments indicate the efficacy of our approach in eliminating concepts, preserving model performance, and offering flex- ibility in the erasure or recovery of various concepts. 1. Introduction The field of text-to-image generation has witnessed re- markable development [52, 11, 24, 37], especially the oc- currence of diffusion models (DMs) like DALL-E2 [36] and Stable Diffusion [38]. As the integration of DMs into practi- cal applications [48, 20, 21] proves advantageous, address- ing challenges related to their societal impact increasingly attracts the attention of researchers [3, 19, 12, 29]. One cru- cial challenge arises from diverse training data sources, po- tentially leading to unsafe image generation [50, 10], such as the creation of violent content or the imitation of spe- cific artist styles. To resolve this concern, the machine un- learning (MU) technique has been proposed [51, 27, 45, 40], which involves erasing the impact of specific data points or concepts to enhance model security, without necessitating complete retraining from scratch. The recent MU research such as Erased Stable Diffu- sion (ESD) [10], Forget-me-not (FMN) [50], Safe self- distillation diffusion (SDD) [23], and Ablation Concept (AbConcept) [27], can be broadly categorized into untar- geted concept erasure (e.g.,, FMN) and targeted concept erasure (e.g.,, ESD, SDD and AbConcept). Specifically, FMN minimizes the attention maps of forgotten concepts. In contrast, ESD, SDD, and AbConcept align the denoising distribution of forgotten concepts with a predefined distri- bution. Despite recent advancements in MU [16, 33], there exist several drawbacks. Firstly, prior efforts concentrate on con- cept erasure, leading to considerable performance degra- dation in generative capability. Secondly, current erasure procedures are confined to single-concept elimination and pose challenges when extending them to multi-concept era- sure. The multi-concept erasure can take two forms: si- multaneous erasure of multiple concepts and iterative era- sure of multiple concepts. The former means that multiple forgotten concepts are known in advance, while the latter implies that each erasure step only possesses knowledge of its previously forgotten concepts. Lastly, to the best of our knowledge, the concept restoration issue has not been con- sidered. For instance, after the erasure of multiple artistic styles, the model owner may regain the copyright associ- ated with some erased styles. To address these issues, we propose an innovative Sepa- rable Multi-concept Eraser (SepME) that contains the gen- eration of concept-irrelevant representations (G-CiRs) and the weight decoupling (WD). Specifically, G-CiRs aims to preserve overall model performance while effectively eras- ing undesirable concepts cf through early stopping and weight regularization. Early stopping prevents the unlearn- ing of substantial information irrelevant to cf when ∆θunlearn ϵ and ∆θori ϵ become irrelevant. Here, we define the differ- ence of noise ϵ predicted by DMs with and without con- cept cf as the representations of cf , i.e., ∆θ∗ ϵ . θori andarXiv:2402.05947v1 [cs.LG] 3 Feb 2024 θunlearn are weights of the original and unlearned DMs, re- spectively. The regularization term restricts the deviation of θunlearn from θori. Considering the multi-concept erasure and subsequent restoration, WD characterizes the weight variation as ∆θ1∼N. Each independent weight increment ∆θi is crafted to erase a specific concept ci,f without compromising the generation performance of models regarding other con- cepts. More precisely, the weight increment for erasing a specified concept is expressed as a linear combination of particular solutions calculated based on other known unde- sirable concepts. Each weight increment shares the same non-zero positions but has distinct values. These values are determined by the pre-calculated particular solutions and optimizable linear combination weights. Our main contributions are summarized as follows: (1) To the best of our knowledge, the scenarios of multi-concept erasure and concept restoration have not been explored in previous literature. This work fills in these critical gaps and designs a separable multi-concept eraser; (2) To ef- fectively unlearn undesirable concepts while maintaining overall model performance, our framework characterizes concept-irrelevant representations; (3) Through extensive experiments, we demonstrate that our method can improve erasing performance, maintain model generation capabil- ities, and offer flexibility in combining various forgotten concepts, encompassing both deletion and recovery. 2. Related Works The image generation field has experienced rapid devel- opment in recent years, evolving from autoencoder [34, 31, 42], generative adversarial networks [6, 30, 28], uncondi- tional diffusion models (DMs) [15, 5] to DMs enhanced with large-scale pre-trained image-text models [13, 46, 22] like CLIP [35]. These text-guided DMs, exemplified by DALL-E 2 [36] and Stable Diffusion [38], exhibit an excel- lent generative ability across various prompts c. The con- straint for training DMs is formulated as LDM = Ext∈D,c,t,ϵGT∈N (0,1) [ ∥ϵGT − ϵθdm (xt, c, t)∥ 2 2] , where xt represents the noised data or the noised latent rep- resentation [25], xt = √ ¯αtx0 + √1 − ¯αtϵ. ¯αt is the noise variance schedule. x0 denotes the original reference im- age, and ϵ ∈ N (0, I). D is the training dataset. ϵGT means the ground truth noise. ϵθdm(xt, c, t) denotes the t-th step noise predicted by DMs. ∥ · ∥2 2 is the squared ℓ2-norm function. Additionally, researchers have indicated the un- known concept generation capability of DMs through the fine-tuning of partial model weights on small reference sets [39, 18, 47, 9]. Nevertheless, DMs also induce potential risks associated with privacy violations and copyright in- fringement, such as the training data leakage [1, 7, 8], the imitation of various artistic styles [44, 41], and the genera- tion of sensitive content [49]. Consequently, there is a grow- ing focus on erasing specific outputs from pre-trained DMs [3, 27]. Existing research primarily falls into three distinct di- rections: removal of unsafe data and model retraining [4], integration of additional plug-ins to guide model outputs [2, 32], and fine-tuning of model weights through MU tech- niques [10, 50, 27]. The drawback of the first direction is that large-scale model retraining demands considerable computational resource and time. The risk of the second direction is that, with the public availability of model struc- tures and weights, malicious users can easily remove plug- ins. This work focuses on the third direction, i.e., machine unlearning. The majority of unlearning methods for DMs can be summarized as: ϵθop(xt, cf , t) ← {ϵtarget if x0 ∈ Df ϵGT otherwise, (1) where θop represents optimizable model weights, e.g., the parameters of cross-attention modules in DMs. xt can be obtained through either the diffusion process or the sam- pling process. ϵθop (xt, cf , t) denotes the noise predicted by unlearned DMs at the t-th step. Df refers to the dataset con- taining the forgotten concept cf . ϵtarget and ϵGT represent the noise of predefined target concepts and the ground-truth noise added in the diffusion process, respectively. For instance, ESD [10] leverages the predicted noise for both concept-free c∅ and forgotten concepts cf to construct ϵtarget, ϵtarget = (1 + η)ϵθdm(xt, c∅, t) − ηϵθdm(xt, cf , t), where θdm represents parameters of the frozen DMs. η is the hyperparameter. SDD [23] directly maps the prediction distribution of erased concepts cf to the prediction distribu- tion of concept-free c∅, ϵtarget = ϵθdm(xt, c∅, t). AbConcept [27] assigns anchor concepts c ∗ for each erased concept cf , such as cf is ‘Van Gogh painting’ and c ∗ is ‘painting’ or cf is ‘a photo of Grumpy cat’ and c ∗ is ‘a photo of cat’, ϵtarget = ϵθdm (xt, c ∗, t). In contrast, FMN [50] is an untar- geted concept erasure method, which minimizes attention weights corresponding to the forgotten concepts cf . These advanced approaches focus on unlearning con- cepts but compromise model performance significantly. Moreover, they have not considered the scenarios of multi- concept erasure and subsequent restoration. In this work, we introduce a separable multi-concept erasure frame- work. It incorporates an untargeted concept-irrelevant era- sure mechanism to preserve model performance during con- cept erasure, and a weight decoupling mechanism to pro- vide flexibility in both the erasure and recovery of concepts. � (��, ��)��� (��, ��) 퐿표\u0000\u0000: 퐿2 （a）Previous － － 퐿표\u0000\u0000: 퐿��� + ||∆���|| � (��, ��)��� (��, ��) (��, �∅) （b）Ours: G-CiRs � (��, �∅) ��� + ∆��� (��, ��) (��, �∅) ������� DM ��� DM ��� (��, ��) (��, �∅) DM ��� � (��, �∅)��� � (��, ��) ��� + ∆��� DM ��� + ∆��� G-CiRs(∆�1,��) G-CiRs(∆��,��) G-CiRs(∆��,��) �1,� ��,� ��,�...... ＋ DM ��� + ∆��,�� Erase ��,� DM ��� + �∈{�,�} ∆��,�� Erase ��,�; ��,� DM ��� + �∈\u0000�\u0000,�≠� ∆��,�� Weight Decoupling (WD) : ∆�1~�,�� Erase ��,� ∈ ����,� except ��,� （c）Ours: SepME Train Test 퐿SepME Figure 1. Overview of various unlearning techniques for DMs. ‘Ice flowers’ and ‘flames’ represent frozen and optimizable model weights respectively. ϵtarget is the predefined noise distribution. L2 denotes the ℓ2-norm function. xt means noised samples. cf and c∅ signify the forgotten and blank prompts, respectively. Lcor is the correlation function. SepME separates optimizable weights as ∆θ1∼N,dm, Each ∆θi,dm is designed to erase a specific concept ci,f without compromising the generation performance of models regarding other concepts. 3. Proposed Method Our proposed separable multi-concept eraser (SepME) aims to flexibly erase or recover multiple concepts while preserving the overall model performance. An overview of SepME is illustrated in Fig. 1, which incorporates the gen- eration of concept-irrelevant representations (G-CiRs) and the weight decoupling (WD). 3.1. G-CiRs To maintain the generative capability of diffusion mod- els (DMs) for regular concepts (or concept-free c∅) during unlearning, G-CiRs prevents the erasure of significant but irrelevant information to forgotten concepts ci,f ∈ cf , i ∈ [1, N], where N is the number of erased concepts. Specif- ically, given original DMs parameterized by θdm, we em- ploy the noise difference ∆ϵ(ci,f , θdm) = ϵθdm(xt, ci,f , t) − ϵθdm(xt, c∅, t) to represent the concept ci,f . To successfully erase concepts cf from DMs, the representations of con- cepts cf for unlearned and original DMs should be uncorre- lated, that is ∀i∈[1,N]Lcor(ci,f , ∆θdm) == 0, Lcor(ci,f , ∆θdm) = Avg(∆ϵ(ci,f , θdm)⊙∆ϵ(ci,f , θdm+∆θdm)), (2) where ∆θdm represents the learnable weight increments of unlearned DMs, ⊙ denotes the element-wise product, and Avg(·) calculates the average value. Eq. (2) actually com- putes the relevance between two representations for the con- cept ci,f . On this basis, we fine-tune ∆θdm by min ∆θdmLG-CiRs = N∑ i=1 ηiLcor(ci,f , ∆θdm) + λ∥∆θdm∥p, s.t. ∀i∈[1,N]Lcor(ci,f , ∆θdm) == 0, (3) where ηi is used to balance the losses of multiple con- cepts, ηi = ∥Lcor(c1,f ,∆θdm)∥2 ∥Lcor(ci,f ,∆θdm)∥2 . λ denotes the hyperparame- ter. ∥∆θdm∥p restricts the weight deviation of the unlearned DMs from the original ones. To satisfy the condition of zero relevance in Eq. (3), we utilize the momentum statistic method since the values of Lcor(ci,f , ∆θdm) computed from various noised samples xt exhibit significant variations. Specifically, early stopping is activated once L n mom ≤ τ , where τ denotes the threshold, with a default value of 0. L n mom = αL n−1 mom + (1 − α) N∑ i=1 ηiLcor(ci,f , ∆θdm), (4) where α is the hyper-parameter and n is the unlearning step. 3.2. Weight Decoupling (WD) To resolve the subsequent restoration issue of multi- concept erasure, we decompose the weights ∆θdm in Eq. (2) into ∆θ1∼N,dm for flexibly manipulating various concepts. Each independent weight increment ∆θi,dm aims to unlearn a specific concept ci,f without compromising the genera- tion performance of models regarding other concepts. The process of separable erasure can be formulated as: min ∆θ1∼N,dmLSepME = N∑ i=1 ηiLcor(ci,f , ∆θi,dm) + λ∥∆θ1∼N,dm∥p, s.t. ∀i∈[1,N] cond(ci,f , ∆θi,dm), (5) where the conditions include: Lcor(ci,f , ∆θi,dm) == 0, ϵθdm(xt, c∅, t) == ϵθdm+∆θi,dm(xt, c∅, t), ∀j∈[1,N],j̸=iϵθdm(xt, cj,f , t) == ϵθdm+∆θi,dm (xt, cj,f , t), ∀c̸∈f ϵθdm (xt, c̸∈f , t) ≈ ϵθdm+∆θi,dm(xt, c̸∈f , t), (6) where xt can be an arbitrary image and c̸∈f represents con- cepts that do not belong to cf . The first condition aims to erase forgotten concepts, while the rest mitigate the impact of fine-tuning on other concepts. To meet these conditions, we first need to identify the nonzero positions for ∆θi,dm. It is evident from Eq. (6) that these positions are image-independent. Consequently, only the to_k and to_v layers of the cross-attention mod- ules are selected, which are exclusively designed for ex- tracting text embeddings in DMs. Other positions, such as the to_q layer for extracting image embeddings and the FFN (Feed-Forward Network) for updating fused embed- dings, have been fixed. For ∀i∈[1,N]∆θi,dm, they share the same nonzero positions but have distinct values. Next, we take ∆θto_k as an example to analyze how to determine its value, where ∀∆θto_k ∈ ∆θi,dm. Notably, ϵθdm(xt, ck, t) == ϵθdm+∆θto_k(xt, ck, t) means ck ⊗ ∆θto_k == 0, where ck ∈ Rdemb×din , ∆θto_k ∈ Rdin×dout . demb, din and dout indicate feature dimensions. ⊗ denotes matrix mul- tiplication. Thus, Eq. (6) can be rewritten as Lcor(ci,f , ∆θi,dm) == 0, c∅ ⊗ ∆θto_k == 0, ∀j∈[1,N],j̸=i cj,f ⊗ ∆θto_k == 0, ∀c̸∈f c̸∈f ⊗ ∆θto_k ≈ 0demb×dout ⇒ ∆θto_k ≈ 0 din×dout. (7) ♦ To make ∆θto_k satisfy the second and third conditions in Eq. (7), we first compute the particular solutions Sp to a system of linear equations A ⊗ Sp = 0, A = [c⊤ ∅ ; c⊤ 1,f ; · · · ; c⊤ i−1,f ; c ⊤ i+1,f ; · · · ; c⊤ N,f ] ⊤, (8) where A is a constant matrix for specified concepts cf , A ∈ R(N·demb)×din . ⊤ means matrix transpose. Sp ∈ Rdin×din−r, where r is the rank of A, with r ≤ N·demb. din −r quantifies the number of solutions within Sp. din ≫ demb in DMs. Notably, to remove the original biases in solutions Sp, we normalize each element of Sp to a unit vector. Then, each column of ∆θto_k can be formulated as a lin- ear combination of these solutions Sp, ∆θto_k = (w ⊗ S⊤ p ) ⊤, (9) where w is an optimizable variable and represents the linear combination weights, w ∈ Rdout×(din−r). ♦ To further make ∆θto_k satisfy the fourth condition in Eq. (7), we introduce a scaling factor β as follows, ∆θto_k = (w ⊗ (βS⊤ p )) ⊤. (10) Meanwhile, w is initialized to a zero matrix. Additionally, we replace ∥∆θ1∼N,dm∥p in Eq. (5) with the ∥W∥p. Here, W represents the set of optimizable variables w defined for both to_k and to_v layers. Overall, the objective of SepME is simplified as: min W LSepME = N∑ i=1 ηiLcor(ci,f , ∆θi,dm) + λ∥W∥p, s.t. ∀i∈[1,N]Lcor(ci,f , ∆θi,dm) == 0. (11) Evaluation for SepME. We combine various ∆θi,dm to erase the corresponding concepts. For instance, DMs with θdm +∑ i∈{j,k} ∆θi,dm eliminate the concepts cj,f and ck,f . 4. Experiments 4.1. Experimental Settings Implementation Details. We follow prior works [10, 23] to fine-tune Stable Diffusion [38]. The optimization process utilizes the Adam optimizer for a maximum of 1000 iterations and our early stopping strategy. The batch size is set equal to the number of erased concepts. When exclu- sively evaluating the G-CiRs module, the learning rate is set to 1e-6, and we opt to fine-tune the cross-attention mod- ules. For assessing the SepME, the learning rate is adjusted to 1e-2, and optimization is conducted on the to_k and to_v layers of the cross-attention modules. The default values for hyperparameters τ , α in Eq. (4), β in Eq. (10), and λ in Eq. (11) are set to 0, 0.9, 1e-4, and 3e-5, respectively. The threshold τ controls the moment of early stopping, and ablation studies for τ are provided in the appendix. All ex- periments are executed on 2 RTX 3090 GPUs. Evaluation metrics. The evaluation metrics include modifications to model parameters ∥∆θdm∥p = ∥∆θdm∥1 M , perceptual distance measured by Perceptual Image Patch Similarity (LPIPS), and classification accuracy (ACC). Here, M denotes the number of layers. LPIPS quantifies the similarity between the original image and the image gener- ated by unlearned DMs, calculated based on AlexNet [26] with settings from the source code 1. We calculate ACC using various pre-trained classifica- tion models. For the style classification model, we consider a blank concept and nine artist styles: ‘Van Gogh’, ‘Pi- casso’, ‘Cezanne’, ‘Jackson Pollock’, ‘Caravaggio’, ‘Keith Haring’, ‘Kelly McKernan’, ‘Tyler Edlin’, and ‘Kilian Eng’. In each category, we generate 1000 images using the original DMs with artist names (or ‘’) as prompts. 70% of the data is allocated for training purposes, while the remain- ing 30% is reserved for testing. Only the fully connected (FC) layer of the pre-trained ResNet18 model [14] is op- timized with 20 epochs. The cyclical learning rate [43] is 1https://github.com/richzhang/ PerceptualSimilarity Table 1. Quantitative results of the single concept erasure. ‘VG’, ‘PC’ and ‘CE’ are artists of ‘Van Gogh’, ‘Picasso’ and ‘Cezanne’, respectively. ¯↑: Since the unlearning aims to erase the concept style, an intermediate value may indicate better performance. i in FMNi represents the iteration step. Text in red and blue denotes the best and second-best results, respectively. ACC/LPIPS Unlearning methods Erased Evaluated ORI FMN10 FMN20 ESD AbConcept Ours-G-CiRs VG VG∗(↓ /¯↑) 1.000/0.000 0.544/0.433 0.500/0.517 0.320/0.472 0.000/0.469 0.228/0.363 PC †(↑ / ↓) 1.000/0.000 1.000/0.186 1.000/0.256 1.000/0.301 0.900/0.190 1.000/0.175 CE †(↑ / ↓) 1.000/0.000 1.000/0.178 0.964/0.257 1.000/0.164 0.820/0.217 1.000/0.154 PC VG †(↑ / ↓) 1.000/0.000 1.000/0.205 0.952/0.265 0.908/0.209 0.684/0.227 0.908/0.180 PC ∗(↓ /¯↑) 1.000/0.000 1.000/0.231 0.952/0.339 0.400/0.424 0.000/0.397 0.052/0.358 CE †(↑ / ↓) 1.000/0.000 1.000/0.149 1.000/0.175 1.000/0.211 0.752/0.194 1.000/0.189 CE VG †(↑ / ↓) 1.000/0.000 0.952/0.201 0.772/0.305 0.908/0.238 0.728/0.257 0.908/0.276 PC †(↑ / ↓) 1.000/0.000 1.000/0.184 1.000/0.207 1.000/0.314 0.852/0.251 1.000/0.204 CE ∗(↓ /¯↑) 1.000/0.000 1.000/0.189 0.820/0.351 0.428/0.372 0.036/0.360 0.000/0.363 ∑ ·∗ − ∑ · †(↓ /¯↑) - -3.41/-0.25 -3.42/-0.26 -4.67/-0.17 -4.70/-0.11 -5.54/-0.09 ∥∆θdm∥p ↓ ORI FMN10 FMN20 ESD AbConcept Ours-G-CiRs VG 0.000 44.18 86.05 120.8 158.3 43.17 PC 0.000 45.41 93.63 126.9 146.5 36.90 CE 0.000 45.31 92.32 128.5 156.9 36.55 Table 2. Quantitative results of the multi-concept erasure. ‘VG’, ‘PC’ and ‘CE’ are ‘Van Gogh’, ‘Picasso’ and ‘Cezanne’, respectively. ¯↑: Since the unlearning aims to erase the concept style, an intermediate value may indicate better performance. i in FMNi represents the iteration step. Text in red and blue denotes the best and second-best results, respectively. ACC/LPIPS Unlearning methods Erased Evaluated ORI FMN20 FMN30 FMN50 ESD AbConcept Ours-G-CiRs VG+PC+CE VG ∗(↓ /¯↑) 1./0. 0.544/0.487 0.500/0.511 0.136/0.555 0.364/0.467 0.092/0.421 0.224/0.426 PC ∗(↓ /¯↑) 1./0. 0.952/0.299 0.552/0.299 0.000/0.436 0.252/0.442 0.132/0.329 0.000/0.359 CE ∗(↓ /¯↑) 1./0. 0.180/0.359 0.000/0.424 0.000/0.505 0.356/0.343 0.500/0.286 0.000/0.419 Others †(↑ / ↓) 1./0. 0.955/0.228 0.878/0.269 0.693/0.358 0.897/0.252 0.977/0.198 0.958/0.223 ∑ ·∗ − ∑ · †(↓ /¯↑) - 0.721/0.917 -0.17/0.965 -0.56/1.138 -0.07/1.000 -0.26/0.838 -0.73/0.981 ∥∆θdm∥p ↓ ORI FMN20 FMN30 FMN50 ESD AbConcept Ours-G-CiRs VG+PC+CE 0.0 92.58 150.3 243.5 128.4 153.8 58.22 employed with the maximum learning rate of 0.01. As for the object classification network, we directly utilize the pre- trained ResNet50. Advanced unlearning methods, including FMN [50], ESD [10] and AbConcept [23], are used as baselines. 4.2. Style Removal To evaluate the performance of our methods in eliminat- ing styles, we focus on three prominent artists: ‘Van Gogh’, ‘Picasso’, and ‘Cezanne’. During fine-tuning, we employ the images for obtaining the style classification model as in- ference images x0 to yield xt, i.e., xt = √ ¯αtx0+√1 − ¯αtϵ. During evaluation, we generate 250 images for each con- cept, i.e., 50 seeds for each concept, and 5 images per seed. As our SepME relies on the proposed G-CiRs module, we first indicate the performance of G-CiRs and then validate the efficacy of SepME. 4.2.1 Evaluation for G-CiRs For the single concept erasure, one style is chosen as the forgotten concept, while others serve as evaluation con- cepts. The quantitative results are presented in Tab. 1. On one hand, our G-CiRs achieves optimal performance in terms of ACC and LPIPS metrics across three artistic styles. On the other hand, the proposed G-CiRs induces fewer modifications to model weights. Furthermore, qual- itative comparisons in Figs. 2∼4 also demonstrate the effi- cacy of our method in erasing artistic style and preserving generative performance across other concepts. These visual samples are produced with artist names or sentences 2 con- taining artist names as prompts for various unlearned DMs. Likewise, we compare our G-CiRs with previous works under the simultaneous erasure of multiple concepts. 2https://github.com/rohitgandikota/erasing/ blob/main/data/art_prompts.csv Table 3. Quantitative results of SepME when simultaneously fine-tuning ∆θ1∼3,dm. ‘VG’, ‘CE’ and ‘PC’ are artists of ‘Van Gogh’, ‘Cezanne’ and ‘Picasso’, respectively. ∆θ1,dm, ∆θ2,dm, ∆θ3,dm are optimizable weights for erasing ‘VG’, ‘CE’ and ‘PC’, respectively. i in FMNi represents the iteration step. For each combination, we employ AbConcept and G-CiRs as baselines to re-finetune all layers of the cross-attention module in DMs to eliminate the corresponding concepts. Text in red indicates the best result. SepME(ACC/LPIPS) θdm +0 +∆θ1,dm +∆θ2,dm +∆θ3,dm + ∑2 i=1 ∆θi,dm + ∑ i∈{1,3} ∆θi,dm + ∑3 i=2 ∆θi,dm + ∑3 i=1 ∆θi,dm VG 1./0. 0.320/0.371 0.956/0.182 0.956/0.182 0.364/0.372 0.272/0.371 0.956/0.182 0.364/0.372 CE 1./0. 1.000/0.144 0.180/0.303 1.000/0.144 0.180/0.303 1.000/0.144 0.180/0.303 0.180/0.303 PC 1./0. 1.000/0.185 1.000/0.185 0.000/0.440 1.000/0.185 0.000/0.440 0.000/0.440 0.000/0.440 AbConcept(ACC/LPIPS) cf - VG CE PC VG+CE VG+PC CE+PC VG+CE+PC VG 1./0. 0.000/0.469 0.728/0.257 0.684/0.227 0.136/0.402 0.044/0.430 0.728/0.257 0.000/0.425 CE 1./0. 0.820/0.217 0.036/0.360 0.752/0.194 0.572/0.253 0.820/0.194 0.252/0.298 0.464/0.269 PC 1./0. 0.900/0.190 0.852/0.251 0.000/0.397 0.952/0.172 0.352/0.284 0.728/0.257 0.152/0.301 G-CiRs(ACC/LPIPS) cf - VG CE PC VG+CE VG+PC CE+PC VG+CE+PC VG 1./0. 0.228/0.363 0.908/0.180 0.908/0.276 0.184/0.384 0.184/0.382 0.700/0.259 0.224/0.426 CE 1./0. 1.000/0.154 0.000/0.363 1.000/0.189 0.108/0.354 0.780/0.173 0.036/0.427 0.000/0.359 PC 1./0. 1.000/0.175 1.000/0.204 0.052/0.358 1.000/0.210 0.152/0.295 0.000/0.449 0.000/0.419FMN10FMN20ESDAbConceptOursORI Sentence w.VanGogh “VanGogh” Sentence w.Picasso “Picasso” Sentence w.Cezanne “Cezanne” Figure 2. Qualitative comparison among various unlearning tech- niques for DMs with ‘Van Gogh’ as the erased concept. Specifically, we consider the styles ‘Van Gogh,’ ‘Picasso,’ and ‘Cezanne’ as forgotten concepts and include additional styles in Sec. 4.1 for evaluation purposes. The experimen- tal results in Tab. 2 demonstrate that our method achieves optimal performance when simultaneously erasing multi- ple concepts. Furthermore, we provide visual examples in Fig. 5. As observed, even when using forgotten concepts as prompts, our G-CiRs does not generate images containing erased styles. Notably, these generated images contain few discernible objects, which is expected given that the pro-FMN10FMN20ESDAbConceptOursORI Sentence w.VanGogh “VanGogh” Sentence w.Picasso “Picasso” Sentence w.Cezanne “Cezanne” Figure 3. Qualitative comparison among various unlearning tech- niques for DMs with ‘Picasso’ as the erased concept. posed G-CiRs is an untargeted unlearning technique. 4.2.2 Evaluation for SepME The preceding experiments assessed the unlearning perfor- mance of various approaches. In the following, we investi- gate the concept restoration issue overlooked by these meth- ods under two practical scenarios: unlearning multiple con- cepts simultaneously or iteratively unlearning multiple con- cepts. 1) The former knows all forgotten concepts for each concept erasure. 2) The latter only has knowledge of previ-FMN10FMN20ESDAbConceptOursORI Sentence w.VanGogh “VanGogh” Sentence w.Picasso “Picasso” Sentence w.Cezanne “Cezanne” Figure 4. Qualitative comparison among various unlearning tech- niques for DMs with ‘Cezanne’ as the erased concept. “VanGogh” “Picasso” “”“Cezanne” Kelly-McKernanFMN20FMN50ESDAbConceptOursORI Keith Haring Figure 5. Qualitative comparison among various unlearning tech- niques under multi-concept erasure. ‘Ours’ indicates G-CiRs. ously forgotten concepts at each erasure step. After unlearning, various weights are randomly com- bined to erase corresponding concepts, such as θdm + ∆θ1,dm, θdm + ∆θ2,dm, and θdm + ∆θ3,dm for erasing ‘Van Gogh,’ ‘Cezanne,’ and ‘Picasso,’ respectively. The combi- nation θdm + ∑3 i=2 ∆θi,dm erases the concepts ‘Cezanne’ and ‘Picasso’. As only the to_k and to_v layers of theTyler EdlinCaravaggioVan GoghCezannePicasso“” +∆��,퐝\u0000 +∆��,퐝\u0000 +∆��,퐝\u0000 +∆��,�,퐝\u0000 +∆��,�,퐝\u0000 +∆��,�,퐝\u0000 +∆��~�,퐝\u0000퐎�\u0000 Figure 6. Visual examples of the proposed SepME.퐎�\u0000+∆��,퐝\u0000퐎�\u0000+∆��,퐝\u0000 Figure 7. Failure cases of the proposed SepME. cross-attention modules are chosen as nonzero positions for SepME, achieving Lcor(‘Cezanne’, ∆θ2,dm) == 0 is chal- lenging. Therefore, the threshold τ for ‘Cezanne’ is ad- justed to 1.5e-4. Simultaneous erasure of multiple concepts. We first optimize ∆θ1∼3,dm simultaneously. The quantitative and qualitative results are presented in Tab. 3 and Fig. 6, re- spectively. For each combination, we utilize AbConcept and G-CiRs as baselines to re-finetune all layers of the cross-attention module in DMs to eliminate the correspond- ing concepts. It can be observed that SepME can effec- tively and flexibly erase various concepts, achieving com- parable performance to separately fine-tuned methods, such as the results of ∆θ3,dm and ∑3 i=1 ∆θi,dm. Fig. 7 dis- plays several failure cases, i.e., images produced by DMs with ∑3 i=1 ∆θi,dm but classified as forgotten concept cate- gories. Additionally, the results of SepME on ∆θi,dm and∑ i∈{j,k} ∆θi,dm indicate the feasibility of concept restora- tion after multi-concept erasures. Next, we individually fine-tune ∆θ1,dm, ∆θ2,dm, and ∆θ3,dm before combining them to simultaneously eliminate Table 4. Quantitative results of SepME when separately fine-tuning ∆θ1∼3,dm. The concepts c1,f, c2,f, and c3,f correspond to ‘Van Gogh’, ‘Cezanne’, and ‘Picasso’, respectively. ∆θ1,dm, ∆θ2,dm, ∆θ3,dm are optimizable weights for erasing ‘VG’, ‘CE’ and ‘PC’, respectively. In SepME1, ∆θ1∼3,dm are separately optimized when all forgotten concepts are known. SepME2 follows the mode of iterative concept erasure, i.e., the t-th erasure step only possesses knowledge (Kn) of the previously forgotten concepts. SepME1–(ACC/LPIPS)–Kn = [c1,f; c2,f; c3,f] θdm +∆θ1,dm +∆θ2,dm +∆θ3,dm +∑2 i=1 ∆θi,dm +∑ i∈{1,3} ∆θi,dm +∑3 i=2 ∆θi,dm +∑3 i=1 ∆θi,dm VG 0.356/0.364 1.000/0.182 0.908/0.182 0.308/0.365 0.308/0.365 0.956/0.181 0.308/0.364 CE 1.000/0.144 0.320/0.304 1.000/0.144 0.320/0.304 1.000/0.144 0.288/0.304 0.320/0.304 PC 1.000/0.185 1.000/0.185 0.000/0.460 1.000/0.185 0.000/0.460 0.000/0.460 0.000/0.460 SepME2–(ACC/LPIPS) Kn [c1,f] [c1,f; c2,f] [c1,f; c2,f; c3,f] - - - - VG 0.228/0.363 0.956/0.182 0.956/0.182 0.228/0.363 0.228/0.363 0.956/0.182 0.228/0.363 CE 1.000/0.182 0.000/0.404 1.000/0.172 0.000/0.376 1.000/0.177 0.000/0.411 0.052/0.394 PC 0.964/0.150 1.000/0.144 0.288/0.277 0.964/0.150 0.108/0.292 0.288/0.277 0.108/0.292 Abconcept–(ACC/LPIPS) VG 0.000/0.469 0.728/0.257 0.684/0.227 0.000/0.472 0.000/0.469 0.636/0.277 0.000/0.487 CE 0.820/0.217 0.036/0.360 0.752/0.194 0.288/0.299 0.680/0.234 0.252/0.288 0.144/0.349 PC 0.900/0.190 0.852/0.251 0.000/0.397 0.600/0.277 0.152/0.304 0.200/0.334 0.052/0.364 G-CiRs–(ACC/LPIPS) VG 0.228/0.363 0.908/0.180 0.908/0.276 0.184/0.386 0.092/0.382 0.544/0.276 0.184/0.410 CE 1.000/0.154 0.000/0.363 1.000/0.189 0.108/0.261 0.716/0.178 0.216/0.313 0.036/0.331 PC 1.000/0.175 1.000/0.204 0.052/0.358 1.000/0.220 0.452/0.247 0.200/0.280 0.200/0.303 multiple concepts. The experimental results are detailed in Tab. 4. It is apparent that all unlearning methods effec- tively erase forgotten concepts. However, both AbConcept and our G-CiRs exhibit shortcomings in restoring forgotten concepts. For example, Abconcept and G-CiRs under the setting θdm+∑ i∈{1,3} ∆θi,dm only perform 68% and 71.6% classification accuracy for ‘Van Gogh’, respectively. In con- trast, our SepME1 in Tab. 4 demonstrates nearly perfect recovery of erased concepts. This emphasizes the effective- ness of our SepME in restoring forgotten concepts and the feasibility of separately optimizing ∆θi,dm. Iterative-concept erasure. To realize multi-concept erasure and concept restoration under this scenario, we avoid sequentially fine-tuning model weights, as restoring the early weights ∆θi,dm inevitably affects the erasure per- formance of ∆θ>i,dm. Inspired by the success of previ- ous experiments where we optimized ∆θ1∼3,dm individu- ally, we achieve iterative concept erasure through this opti- mization mode. In the initial unlearning step, G-CiRs is employed to fine-tune all parameters of cross-attention modules in DMs. This enables better unlearning of the forgotten concept with smaller weight modifications. In each subsequent unlearn- ing step t, we recalculate Sp using c<t,f to construct the weight increments of to_k and to_v layers. These incre- ments are further fine-tuned to erase ct,f . The experimen- tal results presented as SepME2 in Tab. 4 show comparable performance to SepME1. Overall, SepME can effectively achieve iterative concept erasure and concept restoration. 4.3. Object Removal. The experimental results on object removal yield similar conclusions to those on style removal. For detailed infor- mation, please refer to the appendix. 5. Conclusion In this study, we present an innovative machine unlearn- ing technique for diffusion models, namely separable multi- concept eraser (SepME). SepME leverages a correlation term and momentum statistics to yield concept-irrelevant representations. It not only maintains overall model perfor- mance during concept erasure but also adeptly balances loss magnitudes across multiple concepts. Furthermore, SepME allows for the separation of weight increments, providing flexibility in manipulating various concepts, including con- cept restoration and iterative concept erasure. Extensive ex- periments validate the effectiveness of our methods. Broader Impact. As the field of deep learning contin- ues to evolve, it presents both exciting opportunities and profound responsibilities for our community. While recent advances hold promise for solving complex problems, they also raise concerns regarding ethical and societal implica- tions. As researchers in this domain, we recognize our obli- gation to comprehend and address the challenges associated with the widespread adoption of deep learning technology. Machine learning models, despite their potential benefits, can harbor harmful biases, unintended behaviors, and pose risks to user privacy. Our work contributes to this discourse by proposing a post-processing ‘unlearning’ phase aimed at mitigating these concerns. Through extensive empirical investigation, we demonstrate progress over previous solu- tions in practical settings. However, it’s important to ac- knowledge that while our approach, SpeME, represents a significant step forward, we cannot claim perfect mitigation of these issues. Therefore, it’s imperative that caution is exercised in the practical application of deep learning tech- niques, and that rigorous auditing and evaluation of ma- chine learning models are conducted. References [1] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extract- ing training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 5253–5270, 2023. [2] Zhi-Yi Chin, Chieh-Ming Jiang, Ching-Chun Huang, Pin-Yu Chen, and Wei-Chen Chiu. Prompt- ing4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts. arXiv preprint arXiv:2309.06135, 2023. [3] Rishav Chourasia and Neil Shah. Forget unlearning: Towards true data-deletion in machine learning. In In- ternational Conference on Machine Learning, pages 6028–6073. PMLR, 2023. [4] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vi- sion: A survey. IEEE Transactions on Pattern Analy- sis and Machine Intelligence, 2023. [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780– 8794, 2021. [6] Xin Ding, Yongwei Wang, Zuheng Xu, William J Welch, and Z Jane Wang. Ccgan: Continuous condi- tional generative adversarial networks for image gen- eration. In International conference on learning rep- resentations, 2020. [7] Jinhao Duan, Fei Kong, Shiqi Wang, Xiaoshuang Shi, and Kaidi Xu. Are diffusion models vulnera- ble to membership inference attacks? arXiv preprint arXiv:2302.01316, 2023. [8] Jan Dubi´nski, Antoni Kowalczuk, Stanisław Pawlak, Przemyslaw Rokita, Tomasz Trzci´nski, and Paweł Morawiecki. Towards more realistic membership in- ference attacks on large diffusion models. In Proceed- ings of the IEEE/CVF Winter Conference on Applica- tions of Computer Vision, pages 4860–4869, 2024. [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personal- izing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [10] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto- Kaufman, and David Bau. Erasing concepts from dif- fusion models. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision, 2023. [11] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation with rich text. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7545–7556, 2023. [12] Aditya Golatkar, Alessandro Achille, Ashwin Swami- nathan, and Stefano Soatto. Training data protection with compositional diffusion models. arXiv preprint arXiv:2308.01937, 2023. [13] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to- image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog- nition, pages 10696–10706, 2022. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in neu- ral information processing systems, pages 6840–6851, 2020. [16] Seunghoo Hong, Juhun Lee, and Simon S Woo. All but one: Surgical concept erasing with model preservation in text-to-image diffusion models. arXiv preprint arXiv:2312.12807, 2023. [17] Jeremy Howard and Sylvain Gugger. Fastai: A layered api for deep learning. Information, 11(2):108, 2020. [18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [19] Chi-Pin Huang, Kai-Po Chang, Chung-Ting Tsai, Yung-Hsuan Lai, and Yu-Chiang Frank Wang. Re- celer: Reliable concept erasing of text-to-image dif- fusion models via lightweight erasers. arXiv preprint arXiv:2311.17717, 2023. [20] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with dif- fusion models. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 6007–6017, 2023. [21] Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad, Mohsen Fayyaz, Ilker Hacihaliloglu, and Dorit Merhof. Diffusion models for medical image analysis: A comprehensive survey. arXiv preprint arXiv:2211.07804, 2022. [22] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 2426–2435, 2022. [23] Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moon- seok Choi, Jinwoo Shin, and Juho Lee. Towards safe self-distillation of internet-scale text-to-image diffu- sion models. arXiv preprint arXiv:2307.05977, 2023. [24] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image genera- tion with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 7701–7711, 2023. [25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin- ton. Imagenet classification with deep convolutional neural networks. Advances in neural information pro- cessing systems, 25, 2012. [27] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ab- lating concepts in text-to-image diffusion models. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision, 2023. [28] Bowen Li, Xiaojuan Qi, Philip Torr, and Thomas Lukasiewicz. Lightweight generative adversarial net- works for text-guided image manipulation. Advances in Neural Information Processing Systems, 33:22020– 22031, 2020. [29] Hang Li, Chengzhi Shen, Philip Torr, Volker Tresp, and Jindong Gu. Self-discovering interpretable dif- fusion latent directions for responsible text-to-image generation. arXiv preprint arXiv:2311.17216, 2023. [30] Rui Liu, Yixiao Ge, Ching Lam Choi, Xiaogang Wang, and Hongsheng Li. Divco: Diverse conditional image synthesis via contrastive generative adversar- ial network. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 16377–16386, 2021. [31] Zhi-Song Liu, Wan-Chi Siu, and Li-Wen Wang. Vari- ational autoencoder for reference based image super- resolution. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 516–525, 2021. [32] Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram Galstyan, and Rahul Gupta. Flirt: Feed- back loop in-context red teaming. arXiv preprint arXiv:2308.04265, 2023. [33] Zixuan Ni, Longhui Wei, Jiacheng Li, Siliang Tang, Yueting Zhuang, and Qi Tian. Degeneration-tuning: Using scrambled grid shield unwanted concepts from stable diffusion. In Proceedings of the 31st ACM In- ternational Conference on Multimedia, pages 8900– 8909, 2023. [34] Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman, Alexei Efros, and Richard Zhang. Swapping autoencoder for deep image manipulation. Advances in Neural Information Processing Systems, 33:7198–7211, 2020. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas- try, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. [36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text- conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [37] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR, 2021. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High- resolution image synthesis with latent diffusion mod- els. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. [39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500–22510, 2023. [40] Patrick Schramowski, Manuel Brack, Björn Deis- eroth, and Kristian Kersting. Safe latent diffu- sion: Mitigating inappropriate degeneration in diffu- sion models. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 22522–22531, 2023. [41] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and Ben Y Zhao. Glaze: Pro- tecting artists from style mimicry by text-to-image models. arXiv preprint arXiv:2302.04222, 2023. [42] Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang, and Tarek Abdelzaher. Controlvae: Controllable varia- tional autoencoder. In International Conference on Machine Learning, pages 8655–8664. PMLR, 2020. [43] Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 464– 472. IEEE, 2017. [44] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffu- sion models. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 6048–6058, 2023. [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [46] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Confer- ence on Computer Vision, pages 7754–7765, 2023. [47] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023. [48] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehen- sive survey of methods and applications. ACM Com- puting Surveys, 56(4):1–39, 2023. [49] Yijun Yang, Ruiyuan Gao, Xiaosen Wang, Nan Xu, and Qiang Xu. Mma-diffusion: Multimodal attack on diffusion models. arXiv preprint arXiv:2311.17516, 2023. [50] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learn- ing to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591, 2023. [51] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffu- sion models are still easy to generate unsafe images... for now. arXiv preprint arXiv:2310.11868, 2023. [52] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 17907–17917, 2022. A. Code availability The code is available at https://github.com/Dlut-lab-zmn/SepCE4MU. B. Algorithms The algorithmic details for generating concept-irrelevant representations are outlined in Alg. 1. Furthermore, Alg. 2 provides a comprehensive explanation of the separable multi-concept eraser. Algorithm 1: G-CiRs. Input: The diffuser G(·), the frozen weights θdm, the weight increment ∆θdm, the N forgotten concepts ci,f ∈ cf , the blank prompt c∅, the inference dataset x0 ∈ D, the noise schedule ¯αt, the hyperparameter λ. Output: The fine-tuned model increment ∆θdm. 1 for n, x0 ∈ D do 2 Randomly select a sampling step t; 3 xt = √ ¯αtx0 + √1 − ¯αtϵ, ϵ ∈ N (0, I); 4 ϵcf = Gθdm (xt, cf , t); 5 ϵc∅ = Gθdm(xt, c∅, t); 6 ϵ ′ cf = Gθdm+∆θdm (xt, cf , t); 7 ϵ′ c∅ = Gθdm+∆θdm(xt, c∅, t); 8 Lcor(cf , ∆θdm) = Avg((ϵcf − ϵc∅ ) · (ϵ′ cf − ϵ′ c∅ )); 9 ηi = ∥Lcor(c1,f ,∆θdm)∥2 ∥Lcor(ci,f ,∆θdm)∥2 ; 10 L n mom = αL n−1 mom + (1 − α) ∑N i=1 ηiLcor(ci,f , ∆θdm); 11 if Ln mom ≤ τ then 12 break; 13 end 14 min∆θdm LG-CiRs = ∑N i=1 ηiLcor(ci,f , ∆θdm) + λ∥∆θdm∥p 15 end C. Style removal Reference images x0. Several visual examples generated by the original diffusion model are provided in Fig. 8. Evaluation for G-CiRs. Additional visual examples produced with sentences 3 containing artist names as prompts for various unlearned DMs are shown in Figs. 9∼11. Other experimental results. 1) The more detailed results of Tab. 4 are shown in Tab. 5. 2) Furthermore, we perform an ablation study on the threshold τ , which controls the moment of early stopping. The detailed experimental results are shown in Tab. 6. Observations reveal that G-CiRs achieves optimal erasing performance for ‘Van Gogh,’ ‘Picasso,’ and ‘Cezanne’ at τ values of -5e-4, 0, and 0, respectively. D. Object Removal Evaluation Settings: We employ the pre-trained ResNet50 [14] as the object classification network. We analyze nine classes within Imagenette [17], excluding the ‘cassette player’ category due to ResNet50’s classification accuracy falling below 50% on this class. During evaluation, we generate 250 images per class, i.e., 50 seeds for each concept, and 5 images for each seed. D.1. Evaluation for G-CiRs For the single concept erasure, one category is chosen as the forgotten concept, while others serve as evaluation concepts. The quantitative results are presented in Tab. 7. On one hand, our G-CiRs achieves optimal performance in terms of ACC metric across nine object categories. On the other hand, the proposed G-CiRs induces fewer modifications to model weights. Additionally, the qualitative results are presented in Figs. 12∼14. 3https://github.com/rohitgandikota/erasing/blob/main/data/art_prompts.csv Algorithm 2: SepME. Input: The diffuser G(·), the frozen weights θdm, the weight increment ∆θdm, the N forgotten concepts ci,f ∈ cf , the blank prompt c∅, the inference dataset x0 ∈ D, the noise schedule ¯αt, the hyperparameters λ and β. Output: The fine-tuned weight increments ∆θi∈[1,N],dm. 1 for ci,f ∈ cf do 2 A = [c ⊤ ∅ ; c ⊤ 1,f ; · · · ; c ⊤ i−1,f ; c ⊤ i+1,f ; · · · ; c⊤ N,f ] ⊤; 3 Obtain solutions Sp, A ⊗ Sp = 0; 4 for ∆θto_k ∈ ∆θi, dm do 5 Initialize w ∈ W to a zero matrix; 6 ∆θto_k = (w ⊗ (βS⊤ p ))⊤ 7 end 8 for ∆θto_v ∈ ∆θi, dm do 9 Initialize w ∈ W to a zero matrix; 10 ∆θto_v = (w ⊗ (βS⊤ p ))⊤ 11 end 12 end 13 for n, x0 ∈ D do 14 Randomly select a sampling step t; 15 xt = √ ¯αtx0 + √1 − ¯αtϵ, ϵ ∈ N (0, I); 16 ϵcf = Gθdm (xt, cf , t); 17 ϵc∅ = Gθdm(xt, c∅, t); 18 for ci,f ∈ cf do 19 ϵ′ ci,f = Gθi, dm+∆θdm(xt, ci,f , t); 20 ϵ′ c∅ = Gθi, dm+∆θdm (xt, c∅, t); 21 Lcor(ci,f , ∆θdm) = Avg((ϵci,f − ϵc∅ ) · (ϵ′ ci,f − ϵ′ c∅ )); 22 ηi = ∥Lcor(c1,f ,∆θdm)∥2 ∥Lcor(ci,f ,∆θdm)∥2 ; 23 end 24 Ln mom = αL n−1 mom + (1 − α) ∑N i=1 ηiLcor(ci,f , ∆θdm); 25 if Ln mom ≤ τ then 26 break; 27 end 28 minW LSepME = ∑N i=1 ηiLcor(ci,f , ∆θi,dm) + λ∥W∥p 29 end D.2. Evaluation for SepME Separate optimization. Next, we individually train ∆θ1,dm, ∆θ2,dm, and ∆θ3,dm before combining them to simultaneously erase multiple concepts. The experimental results are detailed in Tab. 8. It is apparent that all unlearning methods effectively erase forgotten concepts. However, AbConcept exhibits shortcomings in restoring a forgotten concept. For example, Abcon- cept under the setting θdm+ ∑3 i=2 ∆θi,dm only perform 74%classification accuracy for ‘chain saw’, respectively. In contrast, our SepME1 in Tab. 8 demonstrates nearly perfect recovery of erased concepts. This emphasizes the effectiveness of our SepME in restoring forgotten concepts and the feasibility of separately optimizing ∆θi,dm. Iterative-concept erasure. The iterative concept erasure implies that each erasure step t can only utilize knowledge of the previously forgotten concepts c<t,f . To realize this erasure, we avoid sequentially fine-tuning model weights, as restoring the early weights ∆θi,dm inevitably affects the erasure performance of ∆θ>i,dm. Inspired by the success of separately optimizing ∆θ1∼3,dm, we endeavor to implement iterative concept erasure through this setting. In the initial unlearning step, we employ G-CiRs to fine-tune all parameters of cross-attention modules in DMs. This enables better unlearning of the forgotten concept with fewer weight modifications. In each subsequent unlearning step t, we recalculate Sp using c<t,f to construct the weight increments of to_k and to_v layers and fine-tune these increments to erase the concept ct,f . The experimental results presented as SepME2 in Tab. 8 demonstrate comparable performance to SepME1. Overall, SepME can effectively (a) Van Gogh (b) Picasso (c) Cezanne Figure 8. Visual examples produced by original diffusion models. “VanGogh” “Picasso” “Cezanne”FMN10FMN20ESDAbConceptOursORI Figure 9. Qualitative comparison among various unlearning techniques for DMs with ‘Van Gogh’ as the erased concept. achieve iterative concept erasure and concept restoration. E. Other details. We omit the consideration of iterative erasure, where multiple concepts are erased at each step, as the weight increments for erasing various concept can be optimized separately. Cosine Function: We explored the use of the cosine function as an alternative to Lcor and assessed its performance across various hyperparameters and learning rates. However, this approach did not yield satisfactory results. We attribute this to the significant prediction gap between model samples, i.e., the normalization of sample constraints affects the optimization direction. “VanGogh” “Picasso” “Cezanne”FMN10FMN20ESDAbConceptOursORI Figure 10. Qualitative comparison among various unlearning techniques for DMs with ‘Picasso’ as the erased concept. “VanGogh” “Picasso” “Cezanne”FMN10FMN20ESDAbConceptOursORI Figure 11. Qualitative comparison among various unlearning techniques for DMs with ‘Cezanne’ as the erased concept. Table 5. Quantitative results of SepME when separately fine-tuning ∆θ1∼3,dm. The concepts c1,f, c2,f, and c3,f correspond to ‘Van Gogh’, ‘Cezanne’, and ‘Picasso’, respectively. In SepME1, ∆θ1∼3,dm are separately optimized when all forgotten concepts are known. SepME2 follows the mode of iterative concept erasure. In other words, the t-th erasure step only possesses knowledge (Kn) of the previously forgotten concepts. ‘xattn’ means that we employ all layers of cross attention modules. SepME1–(ACC/LPIPS)–Kn = [c1,f; c2,f; c3,f] θdm +∆θ1,dm +∆θ2,dm +∆θ3,dm +∑2 i=1 ∆θi,dm +∑ i∈{1,3} ∆θi,dm +∑3 i=2 ∆θi,dm +∑3 i=1 ∆θi,dm VG 0.356/0.364 1.000/0.182 0.908/0.182 0.308/0.365 0.308/0.365 0.956/0.181 0.308/0.364 CE 1.000/0.144 0.320/0.304 1.000/0.144 0.320/0.304 1.000/0.144 0.288/0.304 0.320/0.304 PC 1.000/0.185 1.000/0.185 0.000/0.460 1.000/0.185 0.000/0.460 0.000/0.460 0.000/0.460 Others 0.957/0.194 0.891/0.243 0.955/0.184 0.855/0.253 0.957/0.211 0.845/0.248 0.803/0.260 SepME2–(ACC/LPIPS) Kn [c1,f] [c1,f; c2,f] [c1,f; c2,f; c3,f] - - - - VG 0.228/0.363 0.956/0.182 0.956/0.182 0.228/0.363 0.228/0.363 0.956/0.182 0.228/0.363 CE 1.000/0.182 0.000/0.404 1.000/0.172 0.000/0.376 1.000/0.177 0.000/0.411 0.052/0.394 PC 0.964/0.150 1.000/0.144 0.288/0.277 0.964/0.150 0.108/0.292 0.288/0.277 0.108/0.292 Others 0.994/0.173 0.925/0.196 0.986/0.185 0.946/0.211 0.970/0.194 0.924/0.231 0.916/0.235 Abconcept–(ACC/LPIPS)–xattn VG 0.000/0.469 0.728/0.257 0.684/0.227 0.000/0.472 0.000/0.469 0.636/0.277 0.000/0.487 CE 0.820/0.217 0.036/0.360 0.752/0.194 0.288/0.299 0.680/0.234 0.252/0.288 0.144/0.349 PC 0.900/0.190 0.852/0.251 0.000/0.397 0.600/0.277 0.152/0.304 0.200/0.334 0.052/0.364 Others 0.952/0.185 0.923/0.215 0.971/0.173 0.861/0.227 0.891/0.194 0.903/0.206 0.783/0.247 Abconcept–(ACC/LPIPS)–(to_k;to_v) VG 0.592/0.329 0.820/0.215 0.728/0.271 0.320/0.383 0.184/0.378 0.272/0.398 0.184/0.411 CE 1.000/0.193 0.200/0.266 0.800/0.222 0.252/0.274 0.600/0.230 0.052/0.307 0.000/0.311 PC 1.000/0.142 0.964/0.141 0.216/0.306 0.856/0.147 0.320/0.316 0.072/0.347 0.144/0.356 Others 0.973/0.178 0.917/0.208 0.952/0.191 0.889/0.223 0.937/0.197 0.860/0.221 0.805/0.248 G-CiRs–(ACC/LPIPS)–xattn VG 0.228/0.363 0.908/0.180 0.908/0.276 0.184/0.386 0.092/0.382 0.544/0.276 0.184/0.410 CE 1.000/0.154 0.000/0.363 1.000/0.189 0.108/0.261 0.716/0.178 0.216/0.313 0.036/0.331 PC 1.000/0.175 1.000/0.204 0.052/0.358 1.000/0.220 0.452/0.247 0.200/0.280 0.200/0.303 Others 0.994/0.173 0.912/0.222 0.950/0.179 0.940/0.202 0.969/0.195 0.890/0.224 0.843/0.268 G-CiRs–(ACC/LPIPS)–(to_k;to_v) VG 0.344/0.356 0.908/0.209 0.820/0.236 0.228/0.378 0.228/0.381 0.320/0.344 0.136/0.397 CE 0.892/0.155 0.356/0.263 0.752/0.219 0.152/0.282 0.900/0.210 0.152/0.295 0.152/0.334 PC 1.000/0.184 1.000/0.192 0.148/0.290 0.832/0.164 0.252/0.293 0.144/0.319 0.072/0.346 Others 0.944/0.193 0.908/0.212 0.920/0.203 0.873/0.237 0.938/0.220 0.848/0.258 0.791/0.272 Table 6. Ablation study to investigate the influence of the hyperparameter τ on unlearning performance. Erased (G-CiRs) τ Van Gogh Picasso Cezanne Others ∥∆θdm∥p ↓ Van Gogh 1e-3 0.684/0.310 1.000/0.180 1.000/0.140 1.000/0.159 24.28 5e-4 0.592/0.334 1.000/0.178 1.000/0.140 1.000/0.159 27.43 0. 0.456/0.353 1.000/0.171 1.000/0.145 1.000/0.166 36.24 -5e-4 0.228/0.363 1.000/0.175 1.000/0.154 0.992/0.173 43.17 -1e-3 0.092/ 0.419 1.000/ 0.195 0.892/0.153 0.976/0.176 51.63 Picasso 5e-4 0.956/0.173 1.000/0.181 1.000/ 0.144 1.000/0.154 1.522 5e-5 0.924/0.239 0.084/0.319 1.000/ 0.173 0.976/0.161 27.58 0. 0.908/0.276 0.052/0.358 1.000/ 0.189 0.952/0.179 36.90 -5e-4 0.044/0.573 0.000/0.567 0.000/0.426 0.812/0.325 65.03 -1e-3 0.000/0.604 0.000/0.578 0.036/0.516 0.696/0.379 67.89 Cezanne 1e-4 0.772/0.216 1.000/0.182 0.276/0.071 0.976/0.179 23.51 5e-5 0.772/0.242 1.000/0.215 0.072/0.355 0.960/0.195 30.67 0. 0.908/0.180 1.000/0.204 0.000/0.363 0.912/0.222 36.55 -5e-5 0.636/0.398 0.852/0.228 0.000/0.495 0.904/0.247 41.20 -1e-4 0.272/0.458 0.852/0.251 0.000/0.543 0.832/0.281 45.75 Table 7. Quantitative results of the single concept erasure. i in FMNi represents the iteration step. ACC/LPIPS ORI FMN20 FMN50 AbConcept G-CiRs Erased Others Erased Others Erased Others Erased Others chain saw 0.96/0. 0.84/0.241 0.903/0.168 0.00/0.420 0.773/0.269 0.28/0.325 0.833/0.188 0.18/0.311 0.825/0.214 church 0.84/0. 0.84/0.243 0.893/0.167 0.06/0.440 0.870/0.187 0.30/0.345 0.870/0.183 0.16/0.255 0.863/0.184 gas pump 0.80/0. 0.20/0.256 0.930/0.170 0.02/0.379 0.835/0.263 0.20/0.359 0.905/0.186 0.12/0.341 0.893/0.178 tench 0.88/0. 0.26/0.412 0.873/0.178 0.00/0.462 0.818/0.256 0.08/0.403 0.870/0.173 0.08/0.381 0.878/0.177 garbage truck 0.94/0. 0.48/0.229 0.875/0.175 0.04/0.481 0.735/0.314 0.58/0.293 0.820/0.199 0.40/0.339 0.815/0.213 english springer 1.00/0. 0.82/0.221 0.873/0.167 0.02/0.364 0.810/0.256 0.14/0.260 0.873/0.232 0.00/0.292 0.833/0.232 golf ball 1.00/0. 0.86/0.277 0.850/0.171 0.44/0.420 0.765/0.222 0.00/0.459 0.845/0.231 0.32/0.345 0.875/0.218 parachute 0.98/0. 0.54/0.429 0.858/0.172 0.02/0.493 0.770/0.258 0.40/0.436 0.853/0.202 0.28/0.448 0.823/0.204 french horn 1.00/0. 0.22/0.377 0.853/0.185 0.04/0.429 0.665/0.291 0.00/0.435 0.823/0.185 0.00/0.452 0.818/0.188 average 0.93/0. 0.63/0.298 0.879/0.173 0.07/0.432 0.782/0.257 0.22/0.368 0.855/0.199 0.17/0.352 0.847/0.201 ∥∆θdm∥p ↓ - 89.30 254.9 150.6 37.92 Table 8. Quantitative results of SepME when separately fine-tuning ∆θ1∼3,dm. ∆θ1,dm, ∆θ2,dm, ∆θ3,dm are optimizable weights for erasing ‘chain saw’, ‘gas pump’ and ‘garbage truck’, respectively. In SepME1, ∆θ1∼3,dm are separately optimized when all forgotten concepts are known. SepME2 follows the mode of iterative concept erasure. In other words, the t-th erasure step only possesses knowledge of the previously forgotten concepts. ‘xattn’ means that we employ all layers of cross attention modules. ACC/LPIPS SepME1 (to_k,to_v) θdm +∆θ1,dm +∆θ2,dm +∆θ3,dm + ∑2 i=1 ∆θi,dm +∑ i∈1,3 ∆θi,dm +∑3 i=2 ∆θi,dm +∑3 i=1 ∆θi,dm chain saw 0.080/0.311 0.940/0.177 0.940/0.182 0.140/0.314 0.120/0.312 0.960/0.184 0.120/0.314 gas pump 0.680/0.142 0.300/0.308 0.680/0.142 0.260/0.307 0.680/0.142 0.360/0.298 0.340/0.298 garbage truck 0.920/0.157 0.920/0.157 0.000/0.410 0.920/0.157 0.000/0.411 0.000/0.411 0.000/0.411 Others 0.900/0.194 0.932/0.166 0.892/0.208 0.912/0.196 0.840/0.243 0.863/0.221 0.833/0.247 AbConcept (xattn) chain saw 0.280/0.325 0.940/0.192 0.920/0.203 0.340/0.328 0.220/0.344 0.740/0.222 0.080/0.359 gas pump 0.560/0.175 0.200/0.359 0.520/0.182 0.520/0.310 0.460/0.212 0.560/0.299 0.400/0.331 garbage truck 0.900/0.180 0.860/0.171 0.580/0.293 0.800/0.202 0.620/0.238 0.720/0.221 0.580/0.264 Others 0.867/0.192 0.907/0.187 0.853/0.201 0.843/0.234 0.827/0.246 0.843/0.235 0.687/0.309 SepME2 (to_k,to_v) chain saw 0.220/0.311 0.940/0.176 0.940/0.181 0.220/0.311 0.120/0.321 0.980/0.181 0.140/0.333 gas pump 0.540/0.187 0.380/0.214 0.680/0.151 0.260/0.230 0.580/0.187 0.480/0.200 0.280/0.224 garbage truck 0.840/0.203 0.800/0.162 0.000/0.410 0.660/0.185 0.000/0.406 0.000/0.396 0.000/0.387 Others 0.870/0.234 0.910/0.174 0.890/0.208 0.847/0.239 0.743/0.275 0.883/0.221 0.710/0.278AbConceptG-CiRsORI Remove “Church”FMN50FMN20 Figure 12. Qualitative comparison among various unlearning techniques for DMs with ‘church’ as the erased concept.AbConceptG-CiRsORI Remove “English Springer”FMN50FMN20 Figure 13. Qualitative comparison among various unlearning techniques for DMs with ‘English springer’ as the erased concept.AbConceptG-CiRsORI Remove “Parachute”FMN50FMN20 Figure 14. Qualitative comparison among various unlearning techniques for DMs with ‘parachute’ as the erased concept.","libVersion":"0.3.2","langs":""}