{"path":"GenAIUnleaning/DiffusionUnlearning/2023/Unified Concept Editing.pdf","text":"Unified Concept Editing in Diffusion Models Rohit Gandikota 1 Hadas Orgad 2 Yonatan Belinkov 2 Joanna Materzy´nska 3 David Bau 1 1Northeastern University 2Technion 3Massachusetts Institute of Technology * * * Erasing 100 Artistic Styles + Debiasing 35 Professions + Moderating NSFW + Preserving Remaining Concepts Original Model Unified Edited Model Mimicked Style Biased Unsafe Erased Style Diverse Representation Safe Closed Form Edit Masks added by authors for publication * * Figure 1. Our method enables unified and efficient editing of multiple concepts in text-to-image models through closed-form modifications to attention weights. We present applications to debias, erase, and moderate concepts at scale. Debiasing professions leads the edited model to generate fairer gender and race ratios. Erasing an artistic style removes characteristics associated with a particular creator. Moderating the model reduces the likelihood of generating inappropriate images. Abstract Text-to-image models suffer from various safety issues that may limit their suitability for deployment. Previous methods have separately addressed individual issues of bias, copyright, and offensive content in text-to-image models. However, in the real world, all of these issues appear si- multaneously in the same model. We present a method that tackles all issues with a single approach. Our method, Uni- fied Concept Editing (UCE), edits the model without training using a closed-form solution, and scales seamlessly to con- current edits on text-conditional diffusion models. We demonstrate scalable simultaneous debiasing, style erasure, and content moderation by editing text-to-image projections, and we present extensive experiments demon- strating improved efficacy and scalability over prior work. Our code is available at unified.baulab.info. 1. Introduction Text-to-image diffusion models have ushered in a set of complex societal challenges. Generative image models jeopardize artists by cloning their styles [1]; they reinforce biases by amplifying stereotypes [24, 42]; and they facili- tate the creation of offensive images [18]. While several methods have been proposed to mitigate such issues indi- vidually [8, 13, 21, 35, 44], real-world deployments of gen- erative image models manifest all these problems concur- rently. In this paper, we introduce a unified model-editing approach capable of addressing these issues simultaneously. Our method, called Unified Concept Editing (UCE), is a closed-form parameter-editing method that enables the ap- plication of hundreds of editorial modifications within a single text-to-image synthesis model while preserving the generative quality of the model for unedited concepts. The natural first step for exercising safety in generative models is the careful curation of training data to exclude any con- tent that should not be replicated [33]. However, training a large model is expensive, and the impact of data curation on a model may be counterintuitive and unpredictable. For example, removing undesired content can expose other un- desired content [6]; removing toxic content can introduce new biases [10]; and reducing offensive content can result in 1[gandikota.ro,davidbau]@northeastern.edu 2[orgad.hadas,belinkov]@technion.ac.il 3jomat@mit.eduarXiv:2308.14761v1 [cs.CV] 25 Aug 2023 incomplete removal [28]; these examples highlight the limi- tations of relying solely on data curation. UCE offers a fast and practical way to control model behavior post-training, filling the gaps where data curation might fall short. The UCE method builds upon previous model editing work, generalizing the TIME [30] and MEMIT [26] methods. Unlike previous diffusion model editing methods such as TIME, UCE is designed to enable many simultaneous edits to be applied at once. These edits can include actions such as erasing, moderating, or debiasing a concept—tasks that have been traditionally treated as distinct issues with separate solutions. UCE goes beyond MEMIT in several ways: it edits text-to-image models rather than language models; and it also allows the editor to explicitly specify the distribution of concepts that should not be modified. Finally, UCE also introduces a new, scalable debiasing approach. We compare UCE with a range of prior model-editing methods and find that it demonstrates superior performance, outperforming other methods by a wide margin. UCE exhibits superior performance both in single edits in each category of editing, as well as in the ability to scale to many edits at once while minimizing interference with unedited concepts. 2. Related Work While text-to-image diffusion models are becoming in- creasingly popular in commercial art and graphic design, they tend to suffer from various issues, which have previ- ously been addressed separately. Copyright issues. Recent lawsuits [1, 36] have contended that models like Stable Diffusion infringe on many artistic styles, and researchers have found that the models can mem- orize some copyrighted training data nearly verbatim [5, 40]. To reduce such memorization, previous work proposes ran- domizing and augmenting training image captions [41], while other work has explored a technique called image cloaking that allows artists to protect their content from be- ing imitated by large generative models by adding specially crafted adversarial perturbations to images before publishing them online [34,37]; both these approaches require thorough preparation of the training corpus. Another approach adjusts a model after training is complete, deleting an undesired concept by modifying model weights [13, 15, 20, 21, 45]. Our method adopts that concept-erasure approach, and we benchmark against the previous state-of-the-art. Our method differs from previous concept erasure methods because it is a closed-form edit that removes many concepts at once. Offensive content. Diffusion models also sometimes gen- erate inappropriate images, such as nude and violent images. Various methods have been proposed to filter out inappropri- ate images from the training data or at inference time [12,27]; for example the Stable Diffusion implementation includes a “not safe for work” safety checker that returns a black image when an unsafe image is detected [3, 22, 32], and other work has addressed the issue in through image editing at infer- ence time [35]. In cases where open-source code and model weights are openly available, such post-production filters can be easily disabled [38]. A more difficult-to-circumvent ap- proach removes the knowledge of unwanted concepts from the model weights; previous methods taking that approach have proposed attention re-steering through fine-tuning [45], fine-tuning the attention weights [13] and continual learn- ing [15]. While previous methods all fine-tune the model, we propose a fast and efficient method to erase offensive concepts using a closed-form edit. Social biases. Diffusion image generation models have been found to be prone to social and cultural biases [7,24,42], even exaggerating and amplifying societal stereotypes be- yond simple imbalances in the training data [4, 11]. Previous work has tackled this issue by modifying model parame- ters after training, by projecting out biased directions in the text embedding [8], or by performing algebraic manipula- tion of the representations [44]. One previous work, which inspires our current method, applies a direct closed-form model editing method [30]. The previous works have found that debiasing multiple concepts simultaneously is challeng- ing, because debiasing one concept affects others, even in the presence of regularization methods. Our method over- comes that limitation with a new debiasing procedure that eliminates the mutual effect between concepts. Model editing. Model editing has recently emerged as an approach to control a model’s behavior without training. In model editing, a subset of the model’s weights is modified by locating the knowledge in the model and targeting it. Closed-form solutions for editing knowlege in generative text models have been proposed in [25, 26], while [2, 30] have edited knowledge in text-to-image diffusion models by targeting either the cross-attention layers or the text-encoder layers. Our method adopts and generalizes these approaches to enable removal and debiasing of many concepts simulta- neously in text-to-image models. 3. Background Diffusion models are generative models that can ap- proximate distributions through a gradual denoising pro- cess [16, 39]. Starting from Gaussian noise, the model it- eratively denoises over T time steps to form a final image. At each intermediate step t, the model predicts noise ϵt that is added to the original image, with xT as initial noise and x0 as the final output. By learning the parameters of the denoising process, the trained model can generate novel im- ages from noise. This denoising is modeled as a Markov transition probability. pθ(xT :0) = p(xT ) 1∏ t=T pθ(xt−1|xt) (1) Text-to-image latent diffusion models operate on low- dimensional embedding that is modeled with a U-Net gener- ation network. The text conditioning is fed to the network via text embedding, extracted from a language model, in the cross-attention layers. Specifically, the attention mod- ules within diffusion models follow the QKV (Query-Key- Value) [43] structure, where queries originate from the image space, while keys and values are derived from the text em- beddings. Our focus centers on the linear layers Wk and Wv, responsible for projecting text embeddings. For a given text embedding ci, the keys and values are generated by ki = Wkci and vi = Wvci respectively. The keys are then multiplied by the query qi that represents the visual features of the current intermediate image. This pro- duces an attention map that aligns relevant text and image regions: A ∝ softmax(qikT i ) (2) The attention map indicates the relevance between each text token and visual feature. Using this alignment, the cross- attention output is then computed by attending over the value vector V with the normalized attention weights. O = Avi (3) The cross-attention is the mechanism that links the text and image information and responsible for assigning visual mean- ing to text tokens. The output of Equation 3 is then propa- gated through the remaining layers of the diffusion U-Net. TIME [30] edits implicit assumptions in pre-trained diffu- sion models by updating the cross-attention layers. Implicit assumptions can be any visual features that a model assumes about objects in an under-specified prompt, such as the color of roses which is usually red, or the gender of a doctor which is usually male. To edit these assumptions, the method re- quires a \"source\" under-specified prompt where the model makes an assumption (e.g. \"a pack of roses\") and a \"desti- nation\" prompt specifying the desired attribute (e.g. \"a pack of blue roses\"). TIME updates the projection matrices Wk and Wv, to bring the source prompt embedding closer to the destination embedding. This aligns the textual concepts such that the model no longer makes the implicit assumption. Let ci be the source embedding, derived from the tokens of the source prompt, and ci∗ be the corresponding destina- tion embeddings, taken from the embeddings of the corre- sponding tokens in destination prompt. The values of the destination prompts are calculated as vi∗ = W oldci∗. New projection matrices W are then optimized to minimize the W Cross Attention min W m ∑ i=0 | | Wci − v* i | |2 min W n ∑ j=0 | | Wcj − Wold v cj | | 2 v* ici ∈ E Wci cj ∈ P Wcj Concepts to Edit Concepts to Preserve + Minimizing in Closed Form Figure 2. Closed-form editing of cross-attention weights enables concept manipulation in diffusion models. Our method modifies the attention weights to induce targeted changes to the keys and values corresponding to specific text embeddings for a set of edited concepts ci ∈ E while minimizing changes to a set of preserved concepts cj ∈ P . That dual objective allows debiasing, erasing, or moderating concepts while preserving unrelated ones. The same editing function is applied in all cases, but the target keys and values are set differently per application. As a closed-form edit, modifying attention weights given the new keys and values mappings takes less than 1 minute. That enables efficient simultaneous editing of multiple concepts. objective function (a similar equation for the key projection matrices can be derived): min W m∑ i=0 ||W ci − v∗ i︸︷︷︸ W oldc∗ i || 2 2 + λ||W − W old|| 2 F (4) Where λ is a regularization hyper-parameter. [30] proved that the loss function has a closed-form global minimum solu- tion, which allows efficient editing of text-to-image models. W = ( m∑ i=0 v∗ i cT i + λW old) ( m∑ i=0 cicT i + λI )−1 (5) The first term in the inverse matrix, m∑ i=0 cicT i , is the co- variance of the concept text embeddings being edited. As discussed in the appendix, we interpret the second term, an identity matrix, as matching the covariance of the large en- cyclopedia of concept embeddings in the diffusion model’s vocabulary, inspired by MEMIT [26]. While TIME formulation is effective, it risks interference with surrounding concepts when editing a particular con- cept. For example, editing doctors to be female might also affect teachers to be female. TIME has a regularization term that prevents the edited matrix from changing too radically. However, it is a general term and thus affects all vector rep- resentations equally. In this work, we present an alternative preservation term that allows targeted editing of the parame- ters of the pretrained generative model while maintaining its core capabilities. 4. Method We introduce a general model editing methodology ap- plicable to any linear projection layer. Given a pretrained layer W old, our goal, as shown in Figure 2, is to find new edited weights W that edit a set of concepts in set E while preseving a set of concepts in set P . Specifically, we wish to find weights so that the output for each of the inputs ci ∈ E maps to target values v∗ i = W old v ci∗ instead of the original W oldci, while preserving outputs corresponding to the in- puts cj ∈ P as W oldcj. A formal objective function can be constructed as: min W ∑ ci∈E ||W ci − v∗ i || 2 2 + ∑ cj ∈P ||W cj − W oldcj|| 2 2 (6) As derived in the Appendix, the objective function in Equa- tion 6 has a closed-form solution for the updated weights: W =   ∑ ci∈E v∗ i c T i + ∑ cj ∈P W oldcj cT j     ∑ ci∈E cic T i + ∑ cj ∈P cj cT j   −1 (7) This formulation generalizes both the TIME [30] and MEMIT [26] editing methods. When only canonical di- rections of the inputs are used as preservation terms cj, our method reduces to TIME. Solving for the weight update ∆W instead of directly solving for W , our method generalizes to MEMIT closed-form update. We discuss in detail how our approach provides a unified generalization that encompasses prior editing techniques as special case in the Appendix. We edit the linear cross-attention projections (Wk and Wv) to perform various concept edits with different goals: erasure, moderation, and debiasing. Our method requires the m text embeddings ci derived from text descriptions of the concepts to edit and their corresponding modified target outputs v∗ i . The target outputs are defined differently based on the edit type, described below. We also preserve n surrounding concepts using their descriptions cj. For concepts with multiple tokens, we align the last token of ci to the last token of v∗ i and make the edit. Erasing To erase a concept ci , we want to prevent the model from generating it. If the concept is abstract like an artistic style (eg. \"Kelly Mckernan\"), this can be accom- plished by modifying the weights so the target output vi aligns with a different concept c∗ (e.g. \"art\"): v∗ i ← W oldc∗ (8) This updates the weights such that the output no longer reflects concept ci, effectively erasing that concept from the model’s generations and eliminating generations of the undesired characteristics. Debiasing To debias a concept ci (e.g. \"doctor\") across attributes a1, a2, ..., ap (e.g. \"white\", \"asian\", \"black\", ..), we want the model to generate the concept with evenly dis- tributed attributes. This is achieved by adjusting the mag- nitude of vi along the directions of va1, va2 , ..., vap, where vai = W oldai corresponds to the attribute text prompts: v∗ i ← W old [ci + α1a1 + α2a2 + ... + αpap] (9) The constants αi are chosen such that the diffusion model generates the concept with any desired probability for each attribute. This enables our method to debias multiple at- tributes simultaneously, unlike previous approaches such as TIME and concept ablation that can debias across dual attributes only. Moderation To moderate concept ci (e.g. \"nudity\"), we perform an edit where the target output v∗ i aligns with an unconditional prompt c0 (e.g. \" \"): v∗ i ← W oldc0 (10) This replaces the output for ci with a more generic, un- conditional output c0, moderating the model’s response by reducing extreme attributes of that concept. 5. Experiments 5.1. Erasing Our erasing technique directly modifies the key-value mappings in the model to associate keys with different con- cepts rather than the undesired ones. We use our method to erase artistic styles from the model’s weights. Our technique allows preserving certain artists while removing others. We found this enables substantially less interference on a holdout set of artists that were neither erased nor explicitly preserved. We compare our artistic erasure method to recent approaches including ESD-x [13], Concept Ablation [21], and SDD [20] which use cross-attention fine-tuning for controllable image editing. In a second set of experiments, we test object era- sure (e.g., erasing the concept of garbage trucks). In this set of experiments, we did not use any explicit preservation objectives, in order to test implicit interference. For object erasure, we primarily compare to ESD-u [13], which freezes all parameters except cross-attentions during fine-tuning, enabling more global erasures. 5.1.1 Artist erasure Our method can successfully erase multiple concepts while preserving the model’s knowledge. We use the text embed- dings of the artist names as our concepts ci to erase andAndrew FerezAntonio J. ManzanedoThomas Cole Original SD Our Method ESD-x-1 SDD# of Artists Erased 1 50 Ablation Figure 3. Our method and ESD-x show strong erasing capabilities. SDD and Ablationstart to dilute their erasing capabilities as the number of concepts being erased are increased. a set of artists to preserve cj. As shown in Figure 3, we are able to consistently erase multiple artistic styles, while other methods maintain a lot of characteristics of the artistic styles and impair the model’s capabilities as the number of erased concepts increases. We found ESD and SDD tend to damage the model more when erased sequentially (at 1000 iterations per concept), so we limited those techniques to random sampling-based erasure for a fixed 1000 iterations. Our method also demonstrates reduced interference with neighboring, non-erased concepts compared to other tech- niques. As shown in Figure 4, erasing with our approach has less impact on concepts that were not targeted for re- moval. The top plot shows the LPIPS difference between the original SD and edited models, indicating our method re- sults in the smallest modifies the unrelated concepts the least. The bottom plot shows the CLIP score [31] on COCO-30k prompts [23], where our method maintains better text-to- image alignment after editing, as evidenced by the higher CLIP score. Together, these results highlight an important advantage of our erasing approach - the ability to remove targeted concepts with minimal disruption to other areas of knowledge in the model. Diffusion models have been shown to mimic more than 1800 artistic styles [19]. We further analyzed the capability of our method to erase multiple concepts by testing the limits of erasing up to 1000 artists. In this limit test, we erased n artists while preserving the remaining 1000 − n. As shown in Table 1, we found our method can erase up to 100 artists simultaneously before damaging image fidelity and CLIP scores. The LPIPS results indicate that after 50 erasures, the model’s output for a given prompt and seed begins to change, but remains aligned overall as evidenced by the CLIP score. The authors of SDD note potential overfitting of their method when erasing multiple concepts. To mitigate this, we limited SDD to 700 iterations for multi-concept erasure. For Ablation, the authors suggest 100 iterations per concept, however we found the model deteriorates after 1000 total iterations. Therefore, we restricted Ablation to 1000 iterations total when erasing multiple concepts. # Concepts CLIP ↑ LPIPS↓ FID↓ 1 31.35 0.05 14.37 5 31.25 0.08 14.30 10 31.48 0.13 15.56 50 31.22 0.22 15.10 100 30.08 0.30 15.09 500 21.06 0.44 72.40 1000 16.79 0.47 85.48 Original SD 31.32 - 14.49 Table 1. Our method can erase upto 100 concepts while performing similar to pre-trained SD on COCO-30k dataset. The image fidelity is consistent with original SD till 100 erasures. With LPIPS, we find that the model at 100 erasures has a slightly different performance for a given seed and prompt, but as the CLIP shows, the alignment of the model is still intact. 5.1.2 Erasing Objects To demonstrate the capability of our method to erase objects from the diffusion model’s learned concepts, with potential applications for removing harmful symbols and content, we conducted experiments erasing Imagenette [17] classes, a subset of Imagenet classes [9]. For each erased object, we utilized the text embedding (e.g. \"French Horn\") as ci, with- out additional preservation concepts cj. We generated 500 images per class and evaluated top-1 classification accuracy using a pretrained ResNet-50 [14], comparing to ESD-u in Table 2. Objects were erased individually to analyze inter- ference versus ESD on non-erased classes. Without explicit preservation, our approach exhibited superior erasure capa- bility while minimizing interference on non-targeted classes. Further erasure analysis is provided in the Appendix. Erasing all 10 Imagenette classes together reduced image generation accuracy to just 4.0% and COCO-CLIP score to 31.02 (origi- nal SD is 31.32), quantitatively showing effective single and multi-object removal while limiting interference. 5.2. Debiasing Stable Diffusion exhibits gender and racial bias when generating images for profession names (e.g. CEO), produc- ing only 6% female figures for \"CEO\" prompt. We debias profession concepts via Alg. 1, using profession text embed- dings ci and attribute embeddings (e.g. \"male\", \"female\") A. To prevent over/under-debiasing, we set per-attribute regular- ization constants αi in Eq. 9. As debiasing one concept can affect others [30], we use an iterative approach. We maintain edit and freeze concept lists, fixing debiased concepts while editing new ones. With multiple concepts debiased in par- allel, αi values are found by generating validation samples during training and adjusting constants based on the current model’s generated ratio (classified by CLIP). Once a con- cept is sufficiently debiased, we add it to a preservation list, bypassing validation and keeping it fixed when debiasingAlex AlemanyStephan MartiniereEugene von Guerard Original SD Our Method ESD-x-1SDD# of Artists Erased 1 50 Ablation Figure 4. Our method preserves the remaining knowledge of the model better after the edit. The figure shows images generated from different editing methods, for the same prompts and seeds, across a variety of artists that are not erased. Our method exhibits lower LPIPS, indicating less change to unerased concepts during model editing. Similarly for COCO, we find that our method has better CLIP scores across all the scales. This demonstrates that our method has significantly reduced interference compared to other fine-tuning approaches when editing. Class name Accuracy of Erased Class ↓ Accuracy of Other Classes ↑ SD Ours ESD-u SD Ours ESD-u Cassette Player 15.6 0.0 0.60 85.1 90.3 64.5 Chain Saw 66.0 0.0 6.0 79.6 76.1 68.2 Church 73.8 8.4 54.2 78.7 80.2 71.6 Gas Pump 75.4 0.0 8.6 78.5 80.7 66.5 Tench 78.4 0.0 9.6 78.2 79.3 66.6 Garbage Truck 85.4 14.8 10.4 77.4 78.7 51.5 English Springer 92.5 0.2 6.2 76.6 78.9 62.6 Golf Ball 97.4 0.8 5.8 76.1 79.0 65.6 Parachute 98.0 1.4 23.8 76.0 77.4 65.4 French Horn 99.6 0.0 0.4 75.8 77.0 49.4 Average 78.2 2.6 12.6 78.2 79.8 63.2 Table 2. Our method can erase objects from diffusion models effectively without impacting the accuracy for other object classes even when they are not explicitly preserved. Compared to ESD-u, we demonstrate improved erasure of the targeted class alongside higher preservation of unrelated classes in the generated images on Imagenette classes. others. This iterative αi tuning enables efficient debiasing by avoiding unnecessarily repeated editing of already debi- ased concepts. Setting equal αi for all concepts risks over- debiasing some while under-debiasing others. Our iterative validation determines optimal per-concept constants. 5.2.1 Gender bias Prior methods for debiasing generative models like TIME [30], Concept Algebra [44], and Debiasing-VL [8] have fo- cused on mitigating biases between two discrete attributes. While we acknowledge that a binary perspective of gender excludes non-binary groups, for a fair comparison to such dual-attribute techniques, we evaluate our method by reduc- ing occupational gender biases in diffusion models. We rec- Algorithm 1 Debiasing Concepts in Diffusion Models 1: Input: Diffusion M with cross attentions Wk, Wv 2: Input: Edit list E, preserve list P 3: Input: Attributes A (list of strings of size p) 4: Input: Learning step η, desired ratios Rdes 5: while True do 6: Rcurr ← GET_RATIOS(M, E, A) 7: for i, ci in enumerate(E) do 8: if max(|Rcurr[i] − Rdes[i]|) < 0.05 then 9: P .append(ci) ▷ add to preserve post debias 10: E.remove(ci) ▷ remove from edit list 11: continue 12: end if 13: α ← η(Rcurr[i] − Rdes[i]) ▷ α ∈ R p 14: v∗ i ← Wvci + α · A 15: k∗ i ← Wkci + α · A 16: end for 17: if E is empty then 18: break ▷ All concepts debiased 19: end if 20: Wv = UCE(E, {v∗ i }, P , Wv) ▷ UCE is Eq.7 21: Wk = UCE(E, {k∗ i }, P , Wk) 22: end while 23: return M ▷ Debiased Model ognize that editing for visual features of non-binary genders risks introducing other unwanted stereotypical behavior. Figure 5 provides qualitative results demonstrating in- creased diversity in generated images for professions with strong initial gender biases after applying our proposed debi- asing technique. For quantitative evaluation, we synthesize Original SD Gender Diverse SDFarmerNurseCashierMechanic Figure 5. Our method improves the gender representation of pro- fessions in the stable diffusion generated images. We find that the images precisely change the gender while keeping the rest of the scene intact. 250 images per profession and utilize CLIP classifications to calculate the deviation ∆ = |pdesired−pactual| pdesired between the achieved and desired (50-50) gender ratios, where ∆ = 0 indicates perfect debiasing. As shown in Table 3, our method achieves gender distributions closest to the balanced 50-50 ratio compared to pretrained and baseline models. The origi- nal formulation of TIME [30] exhibits interference between debiased concepts, resulting in worse performance. We find that even when applying TIME with our proposed preserva- tion term, it still underperforms compared to our approach. Through both qualitative and quantitative results, we demon- strate that our method enables robust targeted debiasing of generative models. 5.2.2 Racial bias A key advantage of our approach over prior debiasing tech- niques is the ability to concurrently mitigate biases related to multiple attributes. To demonstrate this capability, we con- duct experiments to improve racial diversity in professions generated by Stable Diffusion. Specifically, we target major racial categories as defined by U.S. Office of Management and Budget (OMB) standards [29]: White, Black, American Indian, Native American, and Asian. Accurately classifying race from images is an intricate task, problematic even for sophisticated models like CLIP and humans. We, therefore, take a qualitative analysis approach rather than attempting error-prone quantitative race categorization. As depicted in Figure 6, our method significantly enhances the represen- tation of these racial groups among generated professional images. This highlights our technique’s strength in reducing multifaceted biases in diffusion models, a key advantage over existing binary-attribute debiasing methods. 5.3. Moderation We quantitatively evaluate our proposed method for mod- erating sensitive concepts, comparing it against recent state- of-the-art techniques ESD-u and ESD-x [13] on the task of Original SD Racially Diverse SDSupervisorCounselorSecretaryDoctor Figure 6. Our method improves the racial diversity of professions in the pre-trained stable diffusion. We show images from the original SD and the corresponding images from the edited model for the same prompts and seeds for comparison. We find that our edited model has a better race representation. erasing single concepts like \"nudity\". For all the models, 4703 images are generated using I2P prompts from [35]. In Figure 7 we analyze the nudity moderation using NudeNet classifier [3]. We find that our method demonstrates com- parable nudity erasure performance to ESD-X since both techniques edit cross-attentions of the diffusion model. ESD- u as expected has a more aggressive erasure effect given it finetunes the entire model except cross attentions. However, Table 4 highlights that our approach induces substantially lower distortion to model generations than ESD-u and ESD- x, with significantly reduced LPIPS [46] score from the original SD generations. This indicates our method better preserves image quality while moderating sensitive concepts. Additionally, the CLIP score indicates that our technique maintains better text-image alignment post editing. We further demonstrate efficacy in erasing multiple sensi- tive concepts from I2P, including hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty. Again, our approach shows improved multi-concept modera- tion capability compared to ESD-u (Figure 7). We provide a detailed analysis of moderating diverse sensitive concepts in the Appendix. 6. Limitations When debiasing across multiple attributes, we find inter- dependencies that exhibit compounding biases. For exam- ple, generating images of \"a black person\" has near equal gender ratios (48% male out of 100 images), while \"a na- tive american person\" displays strong male bias (96% male of 100). Debiasing in isolation can thus perpetuate biases along other dimensions. This highlights the need for joint at- tribute consideration to mitigate propagated biases. We also find word-level biases in prompts that compose unfavorably. Profession Original-SD Concept Algebra Debias-VL TIME TIME + Preserve Ours Librarian 0.86 ± 0.06 0.66 ± 0.07 0.34 ± 0.06 0.26 ± 0.05 0.35 ± 0.01 0.07 ± 0.07 Teacher 0.42 ± 0.01 0.46 ± 0.0 0.11 ± 0.05 0.34 ± 0.06 0.07 ± 0.06 0.06 ± 0.02 Sheriff 0.99 ± 0.01 0.38 ± 0.22 0.82 ± 0.08 0.22 ± 0.05 0.10 ± 0.05 0.1 ± 0.03 Analyst 0.58 ± 0.12 0.24 ± 0.18 0.71 ± 0.02 0.52 ± 0.03 0.13 ± 0.05 0.2 ± 0.07 Doctor 0.78 ± 0.04 0.4 ± 0.02 0.5 ± 0.04 0.58 ± 0.03 0.41 ± 0.08 0.2 ± 0.02 WinoBias [47] 0.67 ± 0.01 0.43 ± 0.01 0.55 ± 0.01 0.44 ± 0.0 0.31 ± 0.0 0.22 ± 0.0 Table 3. The bias metric ∆ for 5 randomly-picked professions and on average on all 35 Winobias [47] professions. Our method has a consistent debiasing performance compared to previous inference and model editing methods. The presented metric ∆ measures the percentage deviation from equal ratios (∆ = 0 indicates perfect equal distribution across attributes). On average, our method has the least deviation from the desired distribution. Figure 7. Our method erases nudity content from pre-trained SD and has an advantage of erasing multiple concepts in I2P prompts. The figure shows percentage reduction in nudity classified sam- ples on I2P prompts compared to SD which had 796 total images. \"Nudity\" erased model performs very similar to ESD-x-1 as both the methods edit only cross attentions. Although, as noted in Table 4, we find that our method results in a more finer edit and has better alignment with COCO. Method FID-Real ↓ FID-SD ↓ CLIP ↑ LPIPS ↓ REAL - 14.49 30.41 - SD 14.49 - 31.32 - ESD-u-1 14.16 3.73 30.45 0.23 ESD-x-1 14.45 2.33 30.81 0.18 Ours 14.84 1.82 31.26 0.12 Table 4. Our method performs comparably to the pre-trained SD on COCO. The image fidelity performance compared to SD (FID-SD) and LPIPS matches closely with our method. FID with real COCO images (FID-real) is very similar to SD. All the methods show good CLIP score consistency with SD while our method has the closest performance. Non-gendered phrases like \"successful person\" become pre- dominantly male (88% of 100) versus the gender-balanced \"person\" (50% male of 100), illustrating how subtle cues carry biases. Such compositional effects pose challenges, as each word element contributes biases needing mitigation. For artistic style erasure, removing over 500 artists de- grades general image generation, even with preservation terms (Table 1). That suggests a critical mass of artists is needed to maintain generative capabilities. Excessive erasure impairs the core visual priors learned during pretraining. 7. Conclusion We have presented a unified algorithm for precisely edit- ing diffusion models to allow designers to make them more responsible and beneficial for society. Our approach enables targeted debiasing, erasure of potentially copyrighted con- tent, and moderation of offensive concepts, using only text descriptions. Our measurements suggest that our method offers three key benefits over prior methods. First, it can mit- igate multifaceted gender, racial, and other biases simultane- ously while preserving model capabilities. Second, it is scal- able, modifying hundreds of concepts in one pass without ex- pensive retraining. Third, extensive experiments demonstrate superior performance on real-world use cases. Together, our findings suggest that UCE is significant step towards democ- ratizing access to ethical and socially-responsible generative models. The ability to seamlessly unify debiasing, erasure, and moderation will be an important tool for building AI that benefits our diverse global society. Code Our methods are available as open-source code. Source code and data sets for reproducing our results can be found at unified.baulab.info and at https://github.com/rohitgandikota/ unified-concept-editing. Acknowledgments Thanks to Antonio Torralba for valuable advice, discus- sions, and support. RG and DB are supported by Open Philanthropy. JM is funded by ONR MURI grant N00014- 22-1-2740. HO and YB are supported by the Israel Sci- ence Foundation (grant No. 448/20), Open Philanthropy, and by an Azrieli Foundation Early Career Faculty Fellow- ship. References [1] Sarah Andersen. et al v. Stability AI Ltd. et al. Case No. 3:2023cv00201. US District Court for the Northern District of California., Jan 2023. 1, 2 [2] Dana Arad, Hadas Orgad, and Yonatan Belinkov. Refact: Updating text-to-image models by editing the text encoder. arXiv preprint arXiv:2306.00738, 2023. 2 [3] Praneeth Bedapudi. NudeNet: Neural nets for nudity detec- tion and censoring, 2022. 2, 7 [4] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily acces- sible text-to-image generation amplifies demographic stereo- types at large scale. In Proceedings of the 2023 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT ’23, page 1493–1504, New York, NY, USA, 2023. Association for Computing Machinery. 2 [5] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagiel- ski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188, 2023. 2 [6] Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer. The privacy onion effect: Memorization is relative. Advances in Neural Information Processing Systems, 35:13263–13276, 2022. 1 [7] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Prob- ing the reasoning skills and social biases of text-to-image generative models. arXiv preprint arXiv:2202.04053, 2022. 2 [8] Ching-Yao Chuang, Varun Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Jegelka. Debiasing vision-language models via biased prompts. arXiv preprint arXiv:2302.00070, 2023. 1, 2, 6 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5 [10] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 67–73, 2018. 1 [11] Kathleen C. Fraser, Svetlana Kiritchenko, and Isar Nejadgholi. A friendly face: Do text-to-image systems rely on stereotypes when the input is under-specified? In The AAAI-23 Workshop on Creative AI Across Modalities, 2023. 2 [12] Shreyansh Gandhi, Samrat Kokkula, Abon Chaudhuri, Alessandro Magnani, Theban Stanley, Behzad Ahmadi, Venkatesh Kandaswamy, Omer Ovenc, and Shie Mannor. Scalable detection of offensive and non-compliant content / logo in product images. In 2020 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2236–2245, 2020. 2 [13] Rohit Gandikota, Joanna Materzy´nska, Jaden Fiotto- Kaufman, and David Bau. Erasing concepts from diffusion models. In Proceedings of the 2023 IEEE International Con- ference on Computer Vision, 2023. 1, 2, 4, 7 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 5 [15] Alvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep generative models. arXiv preprint arXiv:2305.10120, 2023. 2 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 2 [17] Jeremy Howard and Sylvain Gugger. Fastai: A layered api for deep learning. Information, 11(2):108, 2020. 5 [18] Tatum Hunter. Ai porn is easy to make now. for women, that’s a nightmare., 2 2023. 1 [19] Surea I, Proxima Centauri B, Erratica, and Stephen Young. Image synthesis style studies, 7 2022. 5 [20] Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, and Juho Lee. Towards safe self-distillation of internet-scale text-to-image diffusion models. arXiv preprint arXiv:2307.05977, 2023. 2, 4 [21] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shecht- man, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In Proceedings of the 2023 IEEE International Conference on Computer Vision, 2023. 1, 2, 4 [22] Gant Laborde. NSFW detection machine learning model, 2022. 2 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755. Springer, 2014. 5 [24] Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable bias: Analyzing so- cietal representations in diffusion models. arXiv preprint arXiv:2303.11408, 2023. 1, 2 [25] Kevin Meng, David Bau, Alex Andonian, and Yonatan Be- linkov. Locating and editing factual associations in gpt. Ad- vances in Neural Information Processing Systems, 35:17359– 17372, 2022. 2 [26] Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a trans- former. In The Eleventh International Conference on Learn- ing Representations, 2023. 2, 3, 4 [27] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [28] Ryan O’Connor. Stable diffusion 1 vs 2 - what you need to know, 2022. 2 [29] Office of Management and Budget. Office of management and budget (omb) standards. U.S. Department of Health and Human Services, Jul 2022. 7 [30] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing implicit assumptions in text-to-image diffusion models. In Proceedings of the 2023 IEEE International Conference on Computer Vision, 2023. 2, 3, 4, 5, 6, 7 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 5 [32] Javier Rando, Daniel Paleka, David Lindner, Lennard Heim, and Florian Tramèr. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610, 2022. 2 [33] Robin Rombach. Stable diffusion 2.0 release, Nov 2022. 1 [34] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. arXiv preprint arXiv:2302.06588, 2023. 2 [35] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inappro- priate degeneration in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22522–22531, 2023. 1, 2, 7 [36] Riddhi Setty. Ai art generators hit with copyright suit over artists’ images, 1 2023. 2 [37] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and Ben Y Zhao. Glaze: Protecting artists from style mimicry by text-to-image models. arXiv preprint arXiv:2302.04222, 2023. 2 [38] SmithMano. Tutorial: How to remove the safety filter in 5 seconds, 8 2022. 2 [39] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Confer- ence on Machine Learning, pages 2256–2265. PMLR, 2015. 2 [40] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6048–6058, 2023. 2 [41] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mit- igating copying in diffusion models. arXiv preprint arXiv:2305.20086, 2023. 2 [42] Lukas Struppek, Dominik Hintersdorf, and Kristian Kerst- ing. The biased artist: Exploiting cultural biases via ho- moglyphs in text-guided image generation models. arXiv preprint arXiv:2209.08891, 2022. 1, 2 [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [44] Zihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for text-controlled vision models. arXiv preprint arXiv:2302.03693, 2023. 1, 2, 6 [45] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to- image diffusion models. arXiv preprint arXiv:2303.17591, 2023. 2 [46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586–595, 2018. 7 [47] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolu- tion: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876, 2018. 8 A. Deriving the Closed Form Solution for UCE Let Wk and Wv be the cross attentions weights that project the text embeddings to image space corresponding to keys and values respectively. These are computed fresh at every time step making the computation straightforward, unlike the self-attentions. Let {ci}m i=0 be the embeddings of the text descriptions for the concepts we want to edit. Let {cj} n i=0 be the em- beddings of the concepts we wish to preserve. Similarly, let {v∗ i } m i=0 be the target cross-attention outputs that we want the concepts ci to be steered towards. The sets ci, v∗ i , and cj represent the concepts to erase, desired target outputs, and concepts to preserve respectively. We optimize the newly edited weights of the cross-attention value projects W by minimizing the loss function. The same optimization can be adopted to value projection optimization: L = ∑ ci∈E ||W ci − v∗ i || 2 2 + ∑ cj ∈P ||W cj − W old v cj|| 2 2 (A.1) The objective function in Equation A.1 can be solved to arrive at a closed-form solution. We take the derivative of the loss function stated above w.r.t to W and set it to 0. ∑ ci∈E 2(W ci − v∗ i )c T i + ∑ cj ∈P 2(W cj − W old v cj)cT j = 0 ∑ ci∈E W cic T i − ∑ ci∈E v∗ i cT i + ∑ cj ∈P W cjc T j − ∑ cj ∈P W old v cjcT j = 0 ∑ ci∈E W cic T i + ∑ cj ∈P W cjcT j = ∑ ci∈E v∗ i c T i + ∑ cj ∈P W old v cjcT j W  ∑ ci∈E cic T i + ∑ cj ∈P cjcT j   =  ∑ ci∈E v∗ i cT i + ∑ cj ∈P Wkcjc T j   To invert the terms on the left-hand side (∑ ci∈E cicT i + ∑ cj ∈P cjc T j ), the matrix must have full rank. Adding a preservation term increases the rank by 1. Thus if the number of preservation terms |P | < d, where d is the dimension of the text embedding space, the matrix may not have full rank. To ensure the rank condition is satisfied, we introduce d additional preservation terms along the canonical basis directions of the text embedding space. This maintains full rank and enables inversion of the matrix irrespective of the size of P . W =   ∑ ci∈E v∗ i cT i + ∑ cj ∈P W old v cj cT j     ∑ ci∈E cic T i + ∑ cj ∈P cj c T j   −1 We optimize both the cross-attention key and value weights using the same principles. B. UCE Generalizes to TIME Our method, Unified Concept Editing (UCE), can be viewed as a generalization of the TIME method. As dis- cussed in the methodology section, TIME regularizes the cross-attention weights. With our method, if we do not pre- serve any specific concepts and only preserve the canonical directions ej scaled by λ, we get the following closed-form solution: W =   ∑ ci∈E v∗ i cT i + λ d∑ j=0 W old v ej e T j     ∑ ci∈E cicT i + λ d∑ j=0 ej eT j   −1 Where the canonical directions ej have outer products eieT j that are diagonal matrices with only the jth element as 1 and rest 0. Summing all the canonical outer products gives the identity matrix I. W =   ∑ ci∈E v∗ i c T i + λW old v .I     ∑ ci∈E cicT i + λI   −1 This is the closed-form solution for TIME, which regularizes equally across all directions. Our method can be seen as a generalization of TIME that adds preservation across impor- tant surrounding concepts, not just the canonical directions. This new formulation with additional explicit preservation is very practical, allowing us to edit multiple concepts with less interference. Our method builds on TIME by allowing the preservation of concepts beyond just the canonical direc- tions. This helps enable editing multiple concepts with less interference. C. UCE Generalizes to MEMIT Our method can also be viewed as a generalization of MEMIT. Starting from our objective function in Equa- tion A.1: L = ∑ ci∈E ||W ci − v∗ i || 2 2 + ∑ cj ∈P ||W cj − W old v cj|| 2 2 Taking the derivative and setting it to zero gives: ∑ ci∈E 2(W ci − v∗ i )c T i + ∑ cj ∈P 2(W cj − W old v cj)cT j = 0 To ensure full rank, instead of adding canonical directions to complete the rank, we add additional preservations for a plethora of concepts in diffusion vocabulary. We rewrite the equation in a block form by defining vj = W old v cj and redefining W as W old v + ∆W : (W old v + ∆W )(CiC T i + CjC T j ) = ViC T i + VjC T j W old v CiC T i + W old v CjC T j + ∆W CiC T i + ∆W CjC T j = ViC T i + VjC T j (C.1) Assuming the preservation list contains most concepts the diffusion model knows W old v will minimize ||W cj − vj|| 2 2. Taking a derivative and equating to zero, we get W old v CjC T j = VjC T j . Subtracting this from Equation C.1 gives MEMIT closed form solution: ∆W (CiC T i + CjC T j ) = ViC T i − W old v CiC T i With R = Vi − W old v Ci and C0 = CjC T j ∆W = RC T i (CiC T i + CjC T j ) −1 In summary, our method generalizes MEMIT by incorporat- ing additional preservation terms from the diffusion model’s vocabulary and solving for the weight update ∆W instead of directly solving for W . This highlights the connection between our approach and existing techniques like MEMIT. D. Extended Experimental Results D.1. Erasing Style We tested the limits of erasing artistic styles using our technique. As shown in Figure D.1, quality for holdout artists declines when erasing over 100 styles, evidenced by the increasing LPIPS after 100 erasures. With 50 or fewer erasures, interference was minimal for non-targeted concepts. Additional qualitative results for our method and baselines are provided in Figures D.4-D.7. The baselines are less effective at removing multiple artists and exhibit greater interference on unerased styles compared to our approach. Our method demonstrates superior erasure while minimizing interference when removing multiple artists. Figure D.8 shows the results of stress testing the limits of artistic style erasure before general art capabilities decline. We observed the model starts to lose artistic nuance in generated outputs after approximately 1000 edits. An important follow-up question is the minimum num- ber of artists requiring preservation to maintain performance when erasing styles. We analyzed this by testing preservation limits when erasing 10 artists, as shown in Figure D.3. Eras- ing up to 1500 artists while preserving subsets, we assessed the impact on 100 non-preserved, non-erased artists. The LPIPS divergence indicates preserving at least 500 artists is essential for retaining model performance. Additional results and analysis are provided in the Appendix. 5 10 50 100 1000 # of Artists Erased Original SD Figure D.1. The samples demonstrate model performance on erased artists after editing. We observed that erasing over 100 artists begins negatively impacting output for holdout artists, as evidenced by the increasing LPIPS after 100 erasures. Erasing 50 or fewer artists resulted in negligible interference on non-erased concepts. D.2. Debiasing Table D.1 displays the debiased results for all 36 indi- vidual professions from the WinoBias dataset. Our method consistently reduced bias and increased gender diversity in Stable Diffusion outputs for most professions. Additional qualitative results demonstrating gender debiasing can be seen in Figures D.9 and D.10. Figure D.11 provides further examples of improved racial diversity using our technique. Algorithm 1 outlines our approach for debiasing diffusion model concepts by iteratively editing cross-attention weights. It takes as input the concepts to edit ci, concepts to preserve cj, and attribute text prompts to debias ap. In a loop, current attribute ratio distributions Rcurr are calculated for each concept using validation prompts and CLIP classification. Rcurr is an m × p matrix, where m is the current edit list size and p the attribute count. The debiasing constants αp are then computed proportional to the difference between current and desired ratios, scaled by learning rate η. Once a concept is sufficiently debiased (within 5% of target), it is removed from the edit list and added to the preservation list. This is done for 3 reasons - first, different concepts require varying levels of editing to remove bias, so they do not all debias at the same time. Once a concept is sufficiently debiased, we remove it from the edit list to avoid unnecessary generation of validation images for that concept. Second, editing one concept creates interference that can disrupt other concepts. Adding the debiased concept to the preservation list protects it from being affected by future edits. Third, careful asymmetric calculation of the debiasing constants αp is required, unlike the symmetric constants used for erasing and moderating concepts. The optimal αp values differ across concepts and attributes, necessitating the iterative tuning process. Figure D.2. Our method erases nudity content from pre-trained SD and has an advantage of erasing multiple concepts in I2P prompts. The figure shows percentage reduction in nudity classified samples for each body part type on I2P prompts compared to SD. \"Nudity\" erased model performs very similar to ESD-x-1 as both the methods edit only cross attentions. Although, as noted in main paper, we find that our method results in a more finer edit and has better alignment with COCO. 0 100 500 1000 1500 # of Artists Preserved Original SD Figure D.3. The samples show the performance of edited models on holdout artist. We observe that preserving more artists is beneficial for reducing model’s interference with nearby concepts. The plot shows LPIPS between original SD and models erasing 10 random artists with variable number of preservation artists. We find that preserving 500 artists and more has close to no interference on other surrounding concepts when erasing 10 artists. D.3. Moderating NSFW Figure D.2 displays the detailed erasure effects on dif- ferent nudity classes classified by Nudenet. Our method demonstrates similar erasure to ESD-x for individual classes, while showing less interference on other concepts. The major advantage of our technique emerges in multi-concept erasure for I2P prompts and overall NSFW moderation, where our approach erases better than ESD methods across different NSFW classes. D.4. Erasing Objects Figures D.12- D.14 demonstrate effective object erasure using our method. One limitation of ESD-u was only partial removal of objects like churches, where major attributes such as crosses and tinted windows were erased but the building remained. In contrast, our approach shows stronger editing that clearly erases the full object.Alex Alemany Original SD Our Method ESD-x-1 SDD AblationAlex AlemanyAlex AlemanyAlex AlemanyAlex AlemanyJohn HoweOriginal SD Our Method ESD-x-1 SDD AblationRob GonsalvesChris MarsPaul HenryAnne Bachelier Erased Artists - 1 Unerased Artists Figure D.4. Our method demonstrates a complete erasure of the intended artistic style and the least interference with the holdout artists that were neither erased nor preserved.Antonio J. ManzanedoOriginal SD Our Method ESD-x-1 SDD AblationAlpo JaakolaAbraham MintchineGareth PughAlbert WatsonCuno AmietOriginal SD Our Method ESD-x-1 SDD AblationBeatrix PotterRoz ChastYayi MoralesAlexandre Antigna Erased Artists - 5 Unerased Artists Figure D.5. Our method demonstrates strong multi concept erasure of intended artistic styles and the least interference with the holdout artists that were neither erased nor preserved.Apollinary Vasnetsov Original SD Our Method ESD-x-1 SDD AblationJohn ConstableJohannes VermeerNicolas MignardJohn WhitcombAmanda Sage Original SD Our Method ESD-x-1 SDD AblationAndre NortonYoji ShinkawaJessie Willcox SmithAlexandre Antigna Erased Artists - 10 Unerased Artists Figure D.6. Our method demonstrates strong multi-concept erasure of intended artistic styles and the least interference with the holdout artists that were neither erased nor preserved. Previous methods start showing interference effects when erasing 10 artistsKobayashi KiyochikaOriginal SD Our Method ESD-x-1 SDD AblationAmedeo ModiglianiIsmail InceogluRavi ZupaJordan GrimmerRob GonsalvesOriginal SD Our Method ESD-x-1 SDD AblationJohn HoweEugene von GuerardRuss MillsHendrick Avercamp Erased Artists - 50 Unerased Artists Figure D.7. Our method demonstrates strong multi-concept erasure of intended artistic styles and the least interference with the holdout artists that were neither erased nor preserved. Previous methods start showing interference effects when erasing 50 artists Original SD Erasing 1 Erasing 10 Erasing 100 Erasing 500 Erasing 1000 Erasing 1500Alfred SisleyAmanda SageMartiniereFilip HodasYoji Shinkawa Erasing 300Erasing 200 Erasing 400Interference on Holdout Artists Figure D.8. The samples demonstrate edited model performance on holdout artists. We observed changes in output quality for holdout styles after erasing 300 artists. At 1000 erasures, the network starts to lose the artistic nuance in its generated images. Original SD Gender Diverse SDSheriffAnalystCarpenterDriver Figure D.9. Our method improves the gender representation of professions in the stable diffusion generated images. We find that the images precisely change the gender while keeping the rest of the scene intact. Profession Original-SD Concept Algebra Debias-VL TIME TIME + Preserve Ours Attendant 0.13 ± 0.06 0.23 ± 0.08 0.3 ± 0.04 0.50 ± 0.01 0.38 ± 0.11 0.09 ± 0.04 Cashier 0.67 ± 0.04 0.71 ± 0.10 0.23 ± 0.07 0.46 ± 0.01 0.23 ± 0.15 0.16 ± 0.06 Teacher 0.42 ± 0.01 0.46 ± 0.0 0.11 ± 0.05 0.34 ± 0.06 0.07 ± 0.06 0.06 ± 0.02 Nurse 0.99 ± 0.01 0.91 ± 0.05 0.87 ± 0.01 0.34 ± 0.03 0.30 ± 0.07 0.39 ± 0.07 Assistant 0.19 ± 0.05 0.2 ± 0.07 0.35 ± 0.15 0.32 ± 0.06 0.57 ± 0.08 0.14 ± 0.06 Secretary 0.88 ± 0.01 0.65 ± 0.07 0.65 ± 0.01 0.58 ± 0.09 0.71 ± 0.02 0.10 ± 0.10 Cleaner 0.38 ± 0.04 0.11 ± 0.06 0.18 ± 0.04 0.58 ± 0.07 0.79 ± 0.04 0.33 ± 0.07 Receptionist 0.99 ± 0.01 0.9 ± 0.08 0.74 ± 0.04 0.36 ± 0.10 0.24 ± 0.12 0.38 ± 0.01 Clerk 0.10 ± 0.07 0.11 ± 0.08 0.10 ± 0.04 0.40 ± 0.03 0.76 ± 0.05 0.23 ± 0.06 Counselor 0.06 ± 0.05 0.3 ± 0.03 0.10 ± 0.07 0.74 ± 0.08 0.41 ± 0.06 0.4 ± 0.02 Designer 0.23 ± 0.05 0.25 ± 0.12 0.48 ± 0.06 0.44 ± 0.06 0.23 ± 0.16 0.07 ± 0.05 Hairdresser 0.74 ± 0.11 0.37 ± 0.16 0.61 ± 0.04 0.32 ± 0.01 0.41 ± 0.09 0.16 ± 0.04 Writer 0.15 ± 0.03 0.07 ± 0.03 0.45 ± 0.04 0.54 ± 0.08 0.52 ± 0.08 0.31 ± 0.08 Housekeeper 0.93 ± 0.04 0.68 ± 0.18 0.8 ± 0.07 0.32 ± 0.03 0.68 ± 0.07 0.41 ± 0.05 Baker 0.81 ± 0.01 0.19 ± 0.04 0.72 ± 0.05 0.40 ± 0.04 0.19 ± 0.14 0.29 ± 0.08 Librarian 0.86 ± 0.06 0.66 ± 0.07 0.34 ± 0.06 0.26 ± 0.05 0.35 ± 0.01 0.07 ± 0.07 Tailor 0.3 ± 0.01 0.21 ± 0.05 0.33 ± 0.11 0.50 ± 0.03 0.03 ± 0.00 0.27 ± 0.01 Driver 0.97 ± 0.02 0.2 ± 0.07 0.65 ± 0.04 0.48 ± 0.09 0.17 ± 0.08 0.21 ± 0.07 Supervisor 0.5 ± 0.01 0.07 ± 0.03 0.43 ± 0.04 0.50 ± 0.07 0.42 ± 0.03 0.26 ± 0.04 Janitor 0.91 ± 0.05 0.71 ± 0.06 0.75 ± 0.05 0.36 ± 0.08 0.47 ± 0.12 0.16 ± 0.04 Cook 0.82 ± 0.04 0.48 ± 0.16 0.52 ± 0.07 0.38 ± 0.03 0.15 ± 0.10 0.03 ± 0.02 Laborer 0.99 ± 0.01 0.81 ± 0.06 0.98 ± 0.03 0.48 ± 0.08 0.24 ± 0.09 0.09 ± 0.02 Constr. worker 1.0 ± 0.0 0.95 ± 0.01 1.0 ± 0.0 0.40 ± 0.01 0.15 ± 0.05 0.06 ± 0.04 Developer 0.9 ± 0.03 0.74 ± 0.02 0.9 ± 0.04 0.50 ± 0.01 0.47 ± 0.07 0.51 ± 0.02 Carpenter 0.92 ± 0.05 0.84 ± 0.01 0.98 ± 0.01 0.52 ± 0.06 0.52 ± 0.05 0.06 ± 0.02 Manager 0.54 ± 0.06 0.15 ± 0.01 0.3 ± 0.05 0.38 ± 0.05 0.15 ± 0.01 0.19 ± 0.07 Lawyer 0.46 ± 0.08 0.13 ± 0.06 0.52 ± 0.05 0.64 ± 0.03 0.15 ± 0.03 0.3 ± 0.07 Farmer 0.97 ± 0.02 0.58 ± 0.09 0.97 ± 0.02 0.46 ± 0.02 0.27 ± 0.08 0.41 ± 0.01 Salesperson 0.6 ± 0.08 0.18 ± 0.05 0.07 ± 0.05 0.52 ± 0.05 0.05 ± 0.01 0.38 ± 0.05 Physician 0.62 ± 0.14 0.36 ± 0.1 0.7 ± 0.07 0.56 ± 0.06 0.49 ± 0.04 0.42 ± 0.01 Guard 0.86 ± 0.02 0.43 ± 0.12 0.48 ± 0.06 0.30 ± 0.1 0.10 ± 0.12 0.12 ± 0.07 Analyst 0.58 ± 0.12 0.24 ± 0.18 0.71 ± 0.02 0.52 ± 0.03 0.13 ± 0.05 0.2 ± 0.07 Mechanic 0.99 ± 0.01 0.65 ± 0.04 0.92 ± 0.01 0.38 ± 0.09 0.21 ± 0.04 0.23 ± 0.08 Sheriff 0.99 ± 0.01 0.38 ± 0.22 0.82 ± 0.08 0.22 ± 0.05 0.10 ± 0.05 0.1 ± 0.03 Ceo 0.87 ± 0.03 0.25 ± 0.11 0.37 ± 0.11 0.28 ± 0.04 0.18 ± 0.05 0.28 ± 0.03 Doctor 0.78 ± 0.04 0.4 ± 0.02 0.5 ± 0.04 0.58 ± 0.03 0.41 ± 0.08 0.2 ± 0.02 WinoBias 0.67 ± 0.01 0.43 ± 0.01 0.55 ± 0.01 0.44 ± 0.0 0.31 ± 0.0 0.22 ± 0.0 Table D.1. Our method has a consistent debiasing performance compared to previous inference and model editing methods. The presented metric ∆ measures the percentage deviation from equal ratios (∆ = 0 indicates perfect equal distribution across attributes) on 5 randomly picked professions out of 36 from the WinoBias dataset. On average, our method has the least deviation from the desired distribution. Original SD Gender Diverse SDSecretaryHair DresserPhysicianCEO Figure D.10. Our method improves the gender representation of professions in the stable diffusion generated images. We find that the images precisely change the gender while keeping the rest of the scene intact. Original SD Racially Diverse SDLibrarianBakerManagerHouse Keeper Figure D.11. Our method improves the racial diversity of professions in the pre-trained stable diffusion. We show images from the original SD and the corresponding images from the edited model for the same prompts and seeds for comparison. We find that our edited model has a better race representation.Original SDOriginal SDErasing French HornErasing Golf Ball Figure D.12. Our method demonstrates a complete erasure of the intended object and the least interference with unerased objects that are not explicitly preserved.Original SDOriginal SDErasing Gas PumpErasing Cassette Player Figure D.13. Our method demonstrates a complete erasure of the intended object and the least interference with unerased objects that are not explicitly preserved.Original SDOriginal SDErasing ChurchErasing Garbage Truck Figure D.14. Our method demonstrates a complete erasure of the intended object and the least interference with unerased objects that are not explicitly preserved.","libVersion":"0.3.2","langs":""}