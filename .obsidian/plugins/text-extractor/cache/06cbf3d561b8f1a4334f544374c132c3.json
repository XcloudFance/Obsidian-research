{"path":"GenAIUnleaning/VAE_GAN/Feature Unlearning for Generative Models via Implicit Feedback.pdf","text":"Feature Unlearning for Generative Models via Implicit Feedback Saemi Moon 1 saemi@postech.ac.kr Seunghyuk Cho 2 seunghyuk.cho@lgresearch.ai Dongwoo Kim1,3 dongwookim@postech.ac.kr 1CSE, POSTECH 2LG AI Research 3GSAI, POSTECH Abstract We tackle the problem of feature unlearning from a pre- trained image generative model. Unlike a common unlearn- ing task where an unlearning target is a subset of the training set, we aim to unlearn a speciﬁc feature, such as hairstyle from facial images, from the pretrained generative models. As the target feature is only presented in a local region of an image, unlearning the entire image from the pretrained model may result in losing other details in the remaining region of the image. To specify which features to unlearn, we develop an implicit feedback mechanism where a user can select images containing the target feature. From the implicit feedback, we identify a latent representation corresponding to the target feature and then use the representation to un- learn the generative model. Our framework is generalizable for the two well-known families of generative models: GANs and VAEs. Through experiments on MNIST and CelebA datasets, we show that target features are successfully re- moved while keeping the ﬁdelity of the original models. 1. Introduction Recent advancements in deep generative models have led to the generation of highly realistic images. However, this progress has also raised concerns about the potential mis- use of such models. In some instances, generated images may contain violent or explicit content, or they may inad- vertently leak private information used to train the model. To address these issues, a well-prepared dataset with appro- priate cleansing procedures can mitigate the potential for abuse of generative models. For example, the development of DALL·E 2 involved careful curation of the training data to avoid explicit content, as described by [29]. In addition to data preparation and cleansing, machine un- learning may serve as a complementary tool for preventing the problem in the development and deployment of a gen- erative model. Machine unlearning aims to erase the target data from a pretrained machine-learning model, which can be required to remove private information, harmful content, Bang Figure 1: Result of unlearning ‘Bang’ feature from pre- trained GAN models. 1st/3rd row: generated images before unlearning. 2nd/4th row: images after unlearning. We tra- verse the same latent space to generate images before and after unlearning. Our method effectively unlearns the target feature while maintaining high image quality. or biased information [2]. However, most of the machine un- learning problems have been focused on supervised models so far [11, 34, 1, 6, 40, 5, 8, 25]. In this work, we tackle the problem of feature unlearning from pretrained image generative models, where we aim to ﬁne-tune the model to exclude the production of samples that exhibit target features. The target feature can be very subtle. For example, a speciﬁc hairstyle of a facial image could be the target feature we want to remove from the model, as shown in Figure 1. The subtlety in the target feature makes the generative model unlearning differ from a traditional supervised model unlearning. In many unlearning scenarios with supervised models,arXiv:2303.05699v1 [cs.CV] 10 Mar 2023 the target of unlearning is a subset of a training dataset, leading to the oracle model that could have been obtained from training without the target subset. Unlike supervised model unlearning, it is non-trivial to deﬁne the target in a feature unlearning since the target feature is only presented in a local region of an image. If we naively remove the entire image that contains the target feature, we could lose the other information in the remaining region of the image. Eventually, subset removal results in a loss of high ﬁdelity and diversity in the generated samples. On the other hand, explicit pixel- level supervision could be given to unlearn the target feature, but it is often very expensive to obtain such supervision. Furthermore, the problem becomes more challenging if the training dataset is inaccessible during unlearning for several reasons, e.g., storage capacity, private content protection, etc. To overcome such challenges, we propose a novel gen- erative model unlearning framework based on implicit user feedback. Under the assumption that the training dataset is inaccessible, we assume that the users can give feedback on whether a generated image contains the target feature. Based on the collection of feedback, we ﬁnd the latent rep- resentation of the target features and use the representation to unlearn the generative model. Our proposed framework is generalizable to both well-known classes of generative models, generative adversarial network (GAN) and varia- tional auto-encoder (VAE). To our knowledge, this is the ﬁrst framework for unlearning target features in the pretrained generative models. We summarize our contributions as follows: • We introduce a feature unlearning problem for genera- tive models. • We propose a novel unlearning method for generative models in the image domain that is highly applicable to real-world situations. • We evaluate our method with various GAN and VAE architectures on real-world datasets, including MNIST, CelebA, and demonstrate the effectiveness of unlearn- ing while preserving high image quality. 2. Related Work In this section, we review previous work in machine un- learning and latent space analysis. 2.1. Machine Unlearning Previous studies have demonstrated that machine learning models may leak sensitive information through attacks or speciﬁc inputs [41, 2]. In addition, regulations are emerged to protect private information, such as ‘the right to be for- gotten’, which grants users the request that their personal information must be removed from a system [30]. These highlight the growing signiﬁcance of machine unlearning. Unlearning scenarios can vary depending on the require- ments [26]. Traditional machine unlearning approaches as- sume that all training data can be accessed [11, 34, 1, 6]. However, recent studies have presented problem formula- tions in which access to the data is highly restricted [40, 5, 8, 25]. In the context of feature unlearning, Guo et al. [10] proposes a representation detachment approach to unlearn the speciﬁc attribute for the image classiﬁcation task. How- ever, the above researches focus on supervised learning tasks whereas we focus on unsupervised generative models. Recently, Kong and Chaudhuri [21] proposed a data redaction method from pretrained GAN. They use a data augmentation-based algorithm to prevent making undesir- able samples. This method can only be applied when the entire dataset is available. Besides that, we ﬁrst propose the generative model feature unlearning framework when access to entire data is infeasible. 2.2. Latent Space Analysis It is known that generative models, such as GANs [9, 28, 18] and VAEs [19, 4], well preserve the representation of data within a low-dimensional space, referred to the latent space. In recent years, various techniques for traversing the latent space or extracting a visual feature vector have been proposed. Larsen et al. [22] propose a simple method to extract the visual feature vector by subtracting the mean vector of images without the feature from the mean vector of images with the feature to decouple the correlated fea- ture. Several supervised approaches have been proposed for training machine learning models with latent code and feature labels [7, 35, 33]. Unsupervised methods for ﬁnd- ing interpretable axes in the generator have also been pro- posed [12, 37, 36, 32, 38]. In our proposed framework, obtaining the target vector representing the target feature in the latent space is a crucial step. We introduce a straightforward and user-friendly ap- proach that can be applied to both GANs and VAEs, which can be applied to real-world scenarios easily. Addition- ally, we leverage the target vector to the target identiﬁcation method within latent space. 3. Feature Unlearning for Generative Models In this section, we propose a framework for unlearning generative models such as GAN and VAE to make the un- learned model unable to generate the target feature. 3.1. Feature Unlearning and Implicit Feedback Feature unlearning aim to remove a speciﬁc feature from a trained generative model. For example, after unlearning the User I want to erase ‘Bang’ feature Sampled images from the generator Select! Select! • Female • Smile • Bang • Earrings Features Figure 2: Illustration of implicit feedback interface. Since obtaining pixel-level supervision for unlearning a target fea- ture is unrealistic, we propose an implicit feedback-based unlearning algorithm. A user selects images that contain the target feature to be unlearned. The selected and non-selected images serve as positive and negative examples. Note that the selected images can have features other than the target feature. smile feature from a generative model trained on the CelebA dataset, the model would never generate images of a smiling person. However, selecting a target feature directly can be difﬁcult due to the complexity of images, which consist of multiple features such as hairstyle, makeup, and accessories. Moreover, unlearning the entire subset of images containing the target feature may lead to losing other details from the generative model. Explicit pixel-level supervision can be used to identify the part of an image to be unlearned, but the cost of pixel-level supervision is expensive to scale. To address this issue, we propose an implicit feedback mechanism to identify the target feature by users. We allow users to select images that contain the target feature. Based on the feedback, we construct a dataset with positive and negative examples. We illustrate an example interface system of implicit feedback in Figure 2. The positive and negative examples are then used to unlearn the model. Note that the positive examples can have features other than the target feature, hence the implicit feedback. We further assume that the training dataset is inaccessible after training the model. Due to limited storage or privacy concern, it might be impossible to store the dataset. Feature unlearning becomes more challenging when the dataset is inaccessible since the implicit feedback can only rely on the generated samples. Therefore, the unlearning should be done while keeping the ﬁdelity and diversity of the trained model. 3.2. Unlearning Framework Feature unlearning can be solved by learning a transfor- mation from the image containing the target feature to the image without the target feature. To learn such transfor- mation, we need a paired dataset with and without target features. For example, if the target image represents a man smiling and wearing a hat, and the target feature is the smile, we need the non-target image that depicts the same man with the hat but without the smile. However, such a dataset is impossible to collect in a real-world scenario. Since our goal is to unlearn the pretrained generator and not to learn the transformation, we need to apply the trans- formation principle to the sampling process of the generator. Transformation in image space can be modeled by a trans- formation in the latent space. Based on this intuition, we propose a general unlearning framework from the latent vari- able perspective as follows: 1. Obtain feedback from users via generated images. 2. Find a latent representation ze that represents the target feature in the latent space from implicit feedback. 3. Sample a latent vector z from a simple distribution. (a) If the latent vector does not contain the target feature, let the generator produce the same output without modiﬁcation. (b) If the latent vector contains the target feature, modify the latent vector such that the generator produces a transformed output without the target feature. 4. Repeat step 3 until the generator does not produce the target feature. Target identiﬁcation in latent space. As described ear- lier, the user selects multiple images that contain the feature to be erased. We assume that the target feature can be rep- resented by a vector in the latent space. As the ﬁrst step of unlearning, we obtain the latent vector representation of each image from the feedback 1. Once we obtain the latent vectors, we use a vector arithmetic method proposed by Radford et al. [28] to ﬁnd the latent vector representing the target feature from the implicit feedback. Speciﬁcally, we compute the mean vectors from a collection of positive images and neg- ative images and subtract the mean vectors of the negative images from that of the positive images. The resulting target vector ze is then used to represent the target feature in the latent space. To determine whether a randomly generated image con- tains a target feature, we project its latent vector onto the target vector. White [39] shows that the projection can rep- resent the similarity between the latent vector and the target feature. We then compare this value to a threshold to de- termine whether the image contains the target feature. For the experiments, we set the threshold value t as the average projection values of the positive and negative samples in the 1There is no need for GAN inversion since all images are generated. Target vector 𝐳!𝑡 With the target featureWithout the target feature 𝐳 𝐳 , 𝑓(𝐳) 𝑔!(𝐳) 𝐳 , 𝑓(&𝐳) 𝑔!(𝐳) &𝐳 Figure 3: Overall illustration of the generative model unlearning framework. Based on whether the randomly sampled vector z has the target feature, we use different loss functions to unlearn the target feature. t refers to a threshold, and ˆz is the translated vector, i.e., ˆz = z − (projze(z) − t)ze. latent space. Let sim(z, ze) ∈ {0, 1} indicate the binary classiﬁcation results, i.e., sim(z, ze) = { 0, if projze(z) < t, 1, otherwise, (1) where projze(z) is the projection of z onto ze, i.e., z ⊤ e z ∥ze∥ . Unlearning process. To formalize the unlearning process, let gθ be the model to be unlearned, and f be the pretrained generator. We initialize gθ from the pretrained f . When the randomly sampled latent vector z does not contain the target feature, i.e., sim(z, ze) = 0, the generator gθ needs to produce the same output as f , c.f, step 3a. To enforce the minimal changes in the produced output, we formulate the following reconstruction objective to minimize Lrecon(θ) = (1 − sim(z, ze))∥gθ(z) − f (z)∥1, (2) where z is the random vector. Hence, the unlearned model gθ tries to mimic the original generator when the latent vector does not contain the target feature. When the randomly sampled vector contains the target feature, the generation process needs to be changed such that the sampled output no longer contains the target feature. To do so, we ﬁrst create the target-erased output by generating an output with a translated random vector using f . Given random vector z, we ﬁrst project the vector onto the target vector, and then the original random vector is shifted by the projected vector, i.e., z − (∥projze (z)∥ − t)ze, where t is the predeﬁned threshold. The translated vector is used as an input of the original generator f producing the target-erased output. The modiﬁed output is then used to train g with the following unlearning objective Lunlearn(θ) = sim(z, ze) ∥gθ(z) − f (z − (projze (z) − t) ze)∥1 . (3) The objective enforces the unlearned generator producing outputs similar to those from the original generator without target features. If the projection can correctly measure the presence of the target feature in the latent space while dis- entangling the other features, gθ can successfully forget the target feature in the latent space. It is widely known that L2 and L1 loss occurs in blurry effects in image generation and restoration tasks [27, 42, 15, 43]. Prior research has addressed the blurry effects by introducing diverse techniques, such as adding perceptual or adversarial loss to the training process [16, 43]. To overcome the blurry effects, we add perceptual loss into the objective function. The objective is formalized as Lpercep(θ) = sim(z, ze) (1 − MS-SSIM (gθ(z), f (z − (projze(z) − t ) ze))) , (4) where MS-SSIM function refers to the Multi-Scale Struc- tural Similarity [43], which measures perceptual similarity between two images by comparing luminance, contrast, and structural information. Finally, we combine three objective functions to deﬁne the unlearning objective as L(θ) = α (Lunlearn(θ) + Lpercep(θ)) + Lrecon(θ) , (5) where α is the hyper-parameter that regulates the unlearning and reconstruction error balance. We summarize the overall unlearning algorithm in Algorithm 1 and visualize the overall framework in Figure 3. Algorithm 1 Feature Unlearning Require: Pretrained generator f Require: E: number of epochs, α: hyper-parameter Collect the positive and negative feedback from users Compute mean vector zp from the positive samples Compute mean vector zn from the negative samples Compute target vector ze = zp − zn Initialize gθ = f for i = 1 to E do Sample z from N (0, 1) and compute sim(z, ze) Compute ∇θL(θ) Update θ ← θ − ∇θL(θ) end for return Unlearned generator gθ We emphasize that the unlearning framework can be ap- plied to widely-used generative models such as VAEs and GANs as long as we can properly obtain the target and pro- jected features. 4. Experiments In this section, we show the performance of the proposed framework for unlearning generative models. 4.1. Experimental Settings Generative models and datasets. In this paper, we un- learn two types of generative models: GANs and VAEs, trained on two distinct datasets. Speciﬁcally, we utilize the MNIST dataset [23], a collection of handwritten digits, and the CelebA dataset [24], which is a large-scale face attribute dataset. For the MNIST dataset, we unlearn a vanilla VAE [19] and a Deep Convolutional GAN (DCGAN)[28]. For the CelebA dataset, we unlearn a Very Deep VAE (VDVAE) [4] and a Progressive GAN (ProgGAN) [18]. Unlike the other models, VDVAE consists of multiple layers of latent spaces. We ﬁnd the target vector by concatenating the vectors from all the layers and then apply the same rule to unlearn the model. Target feature selection and implicit feedback construc- tion. Instead of implementing the real-world interface for the implicit feedback, we simulate the cases by using the known features of each dataset. Speciﬁcally, we select two features from each dataset to unlearn. For the MNIST dataset, we use the Morpho-MNIST [3], which provides a comprehensive tool for measuring various features of MNIST digits, to select images with target fea- tures. We chose the thickness and left slant as target features to unlearn. To construct implicit feedback with positive and negative examples, we ﬁrst train a generative model with the Target image Non-target image MNIST Thickness 6,000 54,000 Slant 6,000 54,000 CelebA Bang 30,709 171,890 Beard 33,441 169,158 Table 1: Number of the target image and non-target image for each feature used in the experiments. original dataset, and then, we randomly sample 500 images. We measure the thickness and left slant of each image via Morpho-MNIST. Since the thickness and left slant are not binary, we use images whose feature values range within the top 10% of the entire samples as positive examples and the remaining as negative examples. The CelebA dataset provides 40 features for each image. Among the available features, we choose ‘Bang’ and ‘Beard’ as the target features to be unlearned. As presented in Table 1, approximately 10% of images have the corresponding fea- tures in the dataset. To construct implicit feedback, we ran- domly sample 5,000 images from a trained generative model and use a pretrained classiﬁer to categorize each image into a positive and negative example. We train MobileNet[14] to classify the target feature. Baseline model. There is no unlearning method targeted for generative models, according to our information. We compare our framework against a baseline model. To build the baseline model, we ﬁrst collect images containing the target feature from the training set, and then the baseline model is trained without the collection. The baseline model is commonly used as a standard oracle model in supervised unlearning cases. However, as described in Section 3.1, the oracle from the supervised unlearning may not be an appropriate choice in generative unlearning. We provide some empirical evidence on the limitation of the baseline model in Section 4.4. Implementation details. For all experiments, we use Adam optimizer and a learning rate of 0.001 for MNIST and 0.002 for CelebA. The MNIST dataset is trained for 200 epochs, and CelebA is trained for 500 epochs in unlearning process. The baseline model is initialized with the trained model. We further train the baseline model without the target features over 50 epochs and 96,000 iterations for MNIST and CelebA, respectively, to remove the target features as done in [25]. 4.2. Evaluation Metric The performance of unlearning can be measured from two different perspectives: 1) how well the unlearning is Dataset Model Feature Target feature ratio (↓) Inception Score (↑) Fr´echet Inception Distance (↓) Original Unlearn Baseline Original Unlearn Baseline Original Unlearn Baseline MNIST VAE Thickness 0.0947 0.0097 0.0101 2.207 2.156 2.139 23.358 23.742 24.029 Slant 0.0922 0.0091 0.0108 2.189 2.216 2.221 22.842 23.265 23.515 DCGAN Thickness 0.0910 0.0135 0.0128 2.143 2.129 2.114 2.320 3.354 3.395 Slant 0.1092 0.0125 0.0131 2.150 2.142 2.102 2.243 2.787 2.847 CelebA VDVAE Bang 0.0336 0.0028 0.0021 2.566 2.580 2.541 82.922 85.205 84.086 Beard 0.0722 0.0241 0.0109 2.466 2.378 2.393 82.922 86.149 84.487 ProgGAN Bang 0.0674 0.0042 0.0049 2.922 2.914 2.907 48.054 49.824 51.371 Beard 0.0302 0.0101 0.0098 2.925 2.879 2.866 48.047 49.800 49.783 Table 2: Target feature ratio, inception score, and Fr´echet inception distance of original, unlearn, and baseline models. done and 2) how good the sample qualities are. We explain two different metrics used to evaluate the models. Target feature ratio. The target ratio measures the per- centage of generated samples with the target feature. Low values of the target ratio indicate that the generative model has successfully unlearned the target feature. Pretrained clas- siﬁers for each feature are used to identify whether generated images contain the target feature. We use the same classi- ﬁers used to construct the implicit feedback dataset. For all experiments, we randomly sample 10,000 images from a generative model to compute the target ratio. Image quality. We report two commonly used metrics to evaluate the quality of the generated image: Fr´echet Incep- tion Distance (FID) [13] and Inception Score (IS) [31]. We compute FID between the generated samples and the origi- nal training dataset. Lower FID scores and higher IS scores indicate higher image quality. We use an implementation of StudioGAN [17] to calculate IS and FID. 10,000 samples are used to measure the scores. 4.3. Results We present the results of our framework in both quan- titative and qualitative terms. Our results demonstrate the successful removal of the target feature from two generative models, VAEs and GANs. Quantitative results. We evaluate the effectiveness of our unlearning framework by comparing the target feature ratio between the original model, the unlearned model, and the baseline model in Table 2. The results show that the unlearned model produces similar target feature ratios to the baseline for all features, indicating our framework success- fully unlearns the target feature. To further analyze the results of unlearning, we visual- ize ﬁve randomly selected images classiﬁed as having the target features in Figure 4 from CelebA. Although feature classiﬁers categorize the images into the target positive, it is difﬁcult to recognize the target features by the naked eye Beard Bang Figure 4: Randomly generated images classiﬁed as having target features: ‘Bang’ and ‘Beard’. Although feature clas- siﬁers categorize the images into the target positive, it is difﬁcult to recognize the target features by the naked eye in some cases. in some cases. As our evaluation metric relies on the perfor- mance of the feature classiﬁer, the actual target feature ratio could be lower than the reported ones. Ensuring high image quality is important in unlearning the target feature. Table 2 presents the results of the IS and FID scores for evaluating the quality of generated images, respectively. The results demonstrate that all three mod- els produced similar IS and FID scores, indicating that our framework can successfully unlearn the target feature while maintaining high-quality image generation. Note that FID is calculated using the entire dataset, which yields a slightly higher FID value for the unlearned and baseline models, but there is no signiﬁcant difference between the two models. Qualitative results. Figure 5 presents the qualitative vi- sualization result of our unlearning framework. The top row shows the images generated from the original gener- ator, and the bottom row shows those generated from the unlearned generator. By visualizing generated images using the same latent vector, we observe that the target feature has been effectively erased in each case. In addition, we unlearn the ‘Bang’ feature from ProgGAN trained with CelebA-HQ dataset [18], whose resolution is higher than the other two datasets. The qualitative results in Figure 1 show the ap- proach also works well with high-resolution images. Thickness BeardBang Slant Original Original Unlearned Unlearned Figure 5: Visualization of four different features before and after unlearning from pretrained GAN models. All paired images in each column are generated from the same latent vector. MNIST Models Thickness Slant VAE 0.908 0.936 DCGAN 0.853 0.858 CelebA Models Bang Beard VDVAE 0.840 0.834 ProgGAN 0.891 0.806 Table 3: ROC-AUC score of feature identiﬁcation method. 4.4. Analysis Target identiﬁcation in latent space. The feature identiﬁ- cation method proposed in Equation 1 raises a question about the quality of the result. We use the same classiﬁer used to measure the target feature ratio to measure the quality of the feature identiﬁcation method. Note that the pretrained clas- siﬁers are only used in the evaluation and not given during unlearning. Table 3 shows the ROC-AUC score of the feature identi- ﬁcation used in each experiment. The feature classiﬁcation method achieved relatively high accuracy without an exter- nal classiﬁcation model showing that the feature extracted from the latent space can be used for unlearning. Ablation on the objective. We conduct an ablation study to evaluate the effectiveness of our proposed objective func- tion. Speciﬁcally, we experiment with erasing Bang in a pretrained GAN trained with the CelebA dataset. Table 4 provides a detailed comparison of the performance under different combinations of objectives. Without the perception Lrecon Lunlearn Lpercep Target ratio (↓) IS (↑) FID (↓) ✓ 0.0008 2.886 52.476 ✓ ✓ 0.0038 2.847 50.925 ✓ ✓ ✓ 0.0042 2.914 49.824 Baseline 0.0049 2.907 51.371 Original 0.0674 2.922 48.054 Table 4: Result of ablation study to unlearn the ‘Bang’ fea- ture from a pretrained GAN. loss, the model achieves a better unlearning performance in terms of target ratio, sacriﬁcing the quality of images. Diversity analysis and limitation. We analyze the di- versity of generated samples before and after unlearning. To measure the diversity, we examine the changes in the proportion of features before and after unlearning with the CelebA dataset via the pretrained feature classiﬁers. Figure 6 shows the ratio of the ﬁve most changed features before and after unlearning with the baseline model for both ‘Bang’ and ‘Beard’. If we focus on the result of the ‘Bang’ feature in Figure 6a, there are no dramatic changes in the proportion of features between the original and the unlearned model. However, the changes with the baseline model are relatively larger than that of the unlearned model. Since the baseline model excludes all images that contain the target feature, the model inevitably loses the other features contained in the excluded images. Unlike the ‘Bang’ case, unlearning ‘Beard’ alter the dis- tribution more. As shown in Figure 6b, the model unlearns ‘Mustache’ and ‘Male’ features when the target feature is ‘Beard’. The reason for this result can be explained by the Bang Male High Cheekbones Smiling Beard Features 0.0 0.1 0.2 0.3 0.4 0.5Feature ratio Feature ratio comparison (CelebA: Bang) Original generator Unlearn generator Baseline generator (a) Bang Beard Mustache Male Smiling High Cheekbones Features 0.0 0.1 0.2 0.3 0.4 0.5Feature ratio Feature ratio comparison (CelebA: Beard) Original generator Unlearn generator Baseline generator (b) Beard Figure 6: The feature ratio of sampled images with different generators on the CelebA dataset with GAN. We visualize ﬁve features whose proportion changed the most after unlearning. (a) Removing ‘Bang’ feature does not change the proportion of the other features compared to the baseline model. Note that the proportion of the baseline model changes relatively more than our framework since the baseline unlearns the entire subset of data containing the target feature. (b) Removing ‘Beard’ feature changes the proportion of the other features due to the correlation between different features. For example, the CelebA dataset does not have an instance of a woman with a beard. Hence, the proportion of male images is dramatically reduced after unlearning. difﬁculty in obtaining an accurate target vector from the implicit feedback provided by the user. The implicit feed- back mechanism cannot distinguish a spurious correlation between the target feature and other features. In the case of ‘Bang’, the dataset contains both men and women with bangs. However, since there are no women with beards in the dataset, the target vector becomes entangled with other features when identifying the target feature. The causal in- ference [20] may help to identify the spurious correlation, but we leave this for future work. Choices of hyper-parameter. We analyze the results with varying hyper-parameter α on unlearning the ‘Bang’ feature from GAN. As shown in Table 5, the importance of the unlearning objective becomes more signiﬁcant as α increases. Consequently, increasing α results in a decrease in the target ratio, which successfully erases the target fea- ture from the generated images. However, we observe the trade off between the target feature ratio and the quality of the generated images. Therefore, careful selection of α is important to achieve the desired balance between effective unlearning and preserving image quality. Computational efﬁciency. Training VAE and DCGAN on MNIST requires approximately 30 minutes on a single GPU. Training ProgGAN on CelebA takes around three days using eight GPUs, and VDVAE takes about two days using four GPUs. In contrast, our unlearning framework takes only approximately one minute to unlearn MNIST on a single GPU and approximately 30 minutes to unlearn CelebA using eight GPUs. Although we assume that relearning is impossible due to the inaccessibility of the training set, nevertheless, even if relearning were possible, our method alpha Target ratio (↓) IS (↑) FID (↓) α = 1 0.0536 2.871 48.406 α = 2 0.0108 2.891 49.090 α = 3 0.0042 2.914 49.824 α = 4 0.0021 2.891 49.925 α = 5 0.0001 2.868 50.572 Table 5: Result of hyper-parameter analysis varying α is signiﬁcantly more time-efﬁcient with comparable results. We use NVIDIA GeForce RTX 3090 for all experiments. 5. Conclusion In conclusion, the recent success of generative models has brought exciting developments in various ﬁelds, such as computer vision, natural language processing, and art generation. However, the potential risks associated with the generation of harmful or private content through these models highlight the importance of developing effective un- learning algorithms. Our proposed unlearning algorithm for generative models shows promising results in preventing the generation of unwanted features, which can serve as a crucial tool in addressing sensitive or private content con- cerns. Future research can build upon this work to improve the efﬁciency and effectiveness of unlearning algorithms in other contexts, such as data privacy and fairness. Ulti- mately, the development of robust and reliable unlearning algorithms can maximize the beneﬁts of generative models while minimizing the associated risks. References [1] Thomas Baumhauer, Pascal Sch¨ottle, and Matthias Zeppelza- uer. Machine unlearning: Linear ﬁltration for logit-based classiﬁers. Machine Learning, 111(9):3203–3226, 2022. 1, 2 [2] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015 IEEE Symposium on Security and Privacy, pages 463–480. IEEE, 2015. 1, 2 [3] Daniel C. Castro, Jeremy Tan, Bernhard Kainz, Ender Konukoglu, and Ben Glocker. Morpho-MNIST: Quantita- tive assessment and diagnostics for representation learning. Journal of Machine Learning Research, 20(178), 2019. 5 [4] Rewon Child. Very deep vaes generalize autoregressive mod- els and can outperform them on images. In International Conference on Learning Representations, 2021. 2, 5 [5] Vikram S Chundawat, Ayush K Tarun, Murari Mandal, and Mohan Kankanhalli. Zero-shot machine unlearning. arXiv preprint arXiv:2201.05629, 2022. 1, 2 [6] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data deletion in machine learning. Advances in neural information processing systems, 32, 2019. 1, 2 [7] Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual deﬁnitions of cognitive image properties. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5744–5753, 2019. 2 [8] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304– 9312, 2020. 1, 2 [9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communi- cations of the ACM, 63(11):139–144, 2020. 2 [10] Tao Guo, Song Guo, Jiewei Zhang, Wenchao Xu, and Junxiao Wang. Efﬁcient attribute unlearning: Towards selective re- moval of input attributes from feature representations. arXiv preprint arXiv:2202.13295, 2022. 2 [11] Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Shariﬁ-Malvajerdi, and Chris Waites. Adaptive machine un- learning. Advances in Neural Information Processing Systems, 34:16319–16330, 2021. 1, 2 [12] Erik H¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and Syl- vain Paris. Ganspace: Discovering interpretable gan controls. Advances in Neural Information Processing Systems, 33:9841– 9850, 2020. 2 [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern- hard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu- tional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 5 [15] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial net- works. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125–1134, 2017. 4 [16] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceed- ings, Part II 14, pages 694–711. Springer, 2016. 4 [17] Minguk Kang, Joonghyuk Shin, and Jaesik Park. Studiogan: A taxonomy and benchmark of gans for image synthesis. arXiv preprint arXiv:2206.09479, 2022. 6 [18] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 2, 5, 6 [19] Diederik P Kingma and Max Welling. Auto-encoding vari- ational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 5 [20] Murat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. Causalgan: Learning causal implicit generative models with adversarial training. arXiv preprint arXiv:1709.02023, 2017. 8 [21] Zhifeng Kong and Kamalika Chaudhuri. Data redaction from pre-trained gans. In Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022. 2 [22] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. In International conference on machine learning, pages 1558–1566. PMLR, 2016. 2 [23] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recog- nition. Proceedings of the IEEE, 86(11):2278–2324, 1998. 5 [24] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba) dataset. Retrieved August, 15(2018):11, 2018. 5 [25] Quoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet. Variational bayesian unlearning. Advances in Neural Information Processing Systems, 33:16025–16036, 2020. 1, 2, 5 [26] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. arXiv preprint arXiv:2209.02299, 2022. 2 [27] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Feature learn- ing by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2536–2544, 2016. 4 [28] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper- vised representation learning with deep convolutional genera- tive adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 2, 3, 5 [29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image genera- tion with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1 [30] Jeffrey Rosen. The right to be forgotten. Stan. L. Rev. Online, 64:88, 2011. 2 [31] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 6 [32] Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1532–1540, 2021. 2 [33] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Inter- preting the latent space of gans for semantic face editing. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9243–9252, 2020. 2 [34] Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine unlearning. arXiv preprint arXiv:2111.08947, 2021. 1, 2 [35] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre- sentation learning gan for pose-invariant face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1415–1424, 2017. 2 [36] Christos Tzelepis, Georgios Tzimiropoulos, and Ioannis Pa- tras. Warpedganspace: Finding non-linear rbf paths in gan latent space. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6393–6402, 2021. 2 [37] Andrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan latent space. In Inter- national conference on machine learning, pages 9786–9796. PMLR, 2020. 2 [38] Binxu Wang and Carlos R Ponce. The geometry of deep generative image models and its applications. arXiv preprint arXiv:2101.06006, 2021. 2 [39] Tom White. Sampling generative networks. arXiv preprint arXiv:1609.04468, 2016. 3 [40] Youngsik Yoon, Jinhwan Nam, Hyojeong Yun, Dongwoo Kim, and Jungseul Ok. Few-shot unlearning by model inver- sion. arXiv preprint arXiv:2205.15567, 2022. 1, 2 [41] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversar- ial examples: Attacks and defenses for deep learning. IEEE transactions on neural networks and learning systems, 30(9): 2805–2824, 2019. 2 [42] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, Octo- ber 11-14, 2016, Proceedings, Part III 14, pages 649–666. Springer, 2016. 4 [43] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural networks. IEEE Transactions on computational imaging, 3(1):47–57, 2016. 4","libVersion":"0.3.2","langs":""}