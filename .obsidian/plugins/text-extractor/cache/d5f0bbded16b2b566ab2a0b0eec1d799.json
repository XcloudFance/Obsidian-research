{"path":"Pasted image 20241015021339.png","text":"4 Mutual information Definition The mutual information between two discreet random variables X, Y jointly distributed according to p(z,y) is given by p(,y) I(X;Y) = p(z,y) log ——~ 24 () = 3 plealos o) e = H(X)-HX|Y) = H(Y)-H({Y[X) — H(X)+ H(Y) - H(XY). (25) 4 Tufts University EE194 — Network Information Theory Electrical and Computer Engineering Prof. Mai Vu ‘We can also define the analogous quantity for continuous variables. Definition The mutual information between two continuous random variables X, Y with joint p.d.f f(z,y) is given by f(zy) I(X;Y) = //f z,y) log —————~dzdy. 26 () (v 3 ) 0 For two variables it is possible to represent the different entropic quantities with an analogy","libVersion":"0.3.2","langs":"eng"}