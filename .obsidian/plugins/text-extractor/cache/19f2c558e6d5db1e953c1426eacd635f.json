{"path":"Pasted image 20241018094650.png","text":"Dataset/ Superfilter Pairwise T Huggingface Open LLM Leaderboard 1 AlpacaEval 1 Base Model Ratio(Size) ‘ Winning Score ‘ Average ARC HellaSwag MMLU Truthful QA ‘ Win Rate Alpaca/ 100% 1.000 55.25 54.35 78.65 47.02 40.98 27.75 LLaMA2-7B 5%(2,600) 1.133 55.67 56.57 80.15 45.21 40.74 33.04 10%(5,200) 1.101 56.97 58.02 80.57 47.16 42.14 - 15%(7,800) 1.193 56.61 56.23 80.29 46.73 43.21 - Alpaca/ 100% 1.000 58.78 57.59 81.98 54.05 41.49 35.00 LLaMA2-13B 5%(2,600) 1.174 60.96 61.60 83.84 55.79 42.63 45.71 10%(5,200) 1.069 61.11 62.12 83.74 55.09 43.50 - 15%(7,800) 1.142 60.90 6092 83.58 55.24 43.86 - Alpaca-GPT4/ 100% 1.000 58.71 54.69 80.05 47.89 52.21 71.32 LLaMA2-7B 5%(2,600) 1.014 59.66 56.74 81.19 46.80 53.92 72.13 10%(5,200) 1.064 59.80 57.42 81.79 45.67 54.33 - 15%(7,800) 1.078 60.02 57.00 81.21 46.15 55.72 - Alpaca-GPT4/ 100% 1.000 60.81 57.94 82.22 54.84 48.25 77.86 LLaMA2-13B 5%(2,600) 1.041 63.29 62.29 84.96 55.78 50.13 78.15 10%(5,200) 1.046 63.65 62.63 84.51 55.39 52.06 - 15%(7,800) 1.078 63.65 62.88 84.32 55.35 52.05 - Table 2: Comparison of Superfiltering with four data selection ratios (5%, 10%, 15%, 100%) when finetuning two LLMs (LLaMA2-7B/13B) on two datasets (Alpaca and Alpaca-GPT4). The finetuned models are evaluated by the pair-wise winning score (comparison to the baseline model finetuned on 100% data), Open LLM Leaderboard, and AlpacaEval. In the parathesis are the ratio of data being used and its exact number. The winning score is","libVersion":"0.3.2","langs":"eng"}