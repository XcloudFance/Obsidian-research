{"path":"GenAIUnleaning/DiffusionUnlearning/2024/R.A.C.E. Robust Adversa.pdf","text":"R.A.C.E. : Robust Adversarial Concept Erasure for Secure Text-to-Image Diffusion Model Changhoon Kim⋆1 Kyle Min⋆2 Yezhou Yang 1 1 Arizona State University 2 Intel Labs {kch,yz.yang}@asu.edu kyle.min@intel.com Abstract. In the evolving landscape of text-to-image (T2I) diffusion models, the remarkable capability to generate high-quality images from textual descriptions faces challenges with the potential misuse of re- producing sensitive content. To address this critical issue, we introduce Robust Adversarial Concept Erase (RACE), a novel approach designed to mitigate these risks by enhancing the robustness of concept erasure method for T2I models. RACE utilizes a sophisticated adversarial train- ing framework to identify and mitigate adversarial text embeddings, sig- nificantly reducing the Attack Success Rate (ASR). Impressively, RACE achieves a 30 percentage point reduction in ASR for the “nudity” concept against the leading white-box attack method. Our extensive evaluations demonstrate RACE’s effectiveness in defending against both white-box and black-box attacks, marking a significant advancement in protect- ing T2I diffusion models from generating inappropriate or misleading imagery. This work underlines the essential need for proactive defense measures in adapting to the rapidly advancing field of adversarial chal- lenges. Our code is publicly available: https://github.com/chkimmmmm/ R.A.C.E. Keywords: Concept Erasure · Responsible Image Generative Models · Secure T2I Diffusion Models 1 Introduction The field of text-to-image (T2I) diffusion models has garnered significant at- tention for their ability to produce high-quality images that can be adaptively generated from textual descriptions [44, 46]. This advancement is predicated on the training of T2I models with extensive datasets, often encompassing a range of content including copyrighted, explicit, and private materials [52–54]. Conse- quently, these models possess the capacity to inadvertently replicate protected images, potentially without user awareness [2, 53, 54]. The misuse of T2I models by malicious actors for misinformation or public opinion manipulation presents a significant concern [6, 34]. ⋆ These authors contributed equally to this work.arXiv:2405.16341v2 [cs.CV] 23 Jul 2024 2 C. Kim ⋆, K. Min⋆, Y. YangErasing“Nudity”Erasing “Van Gogh”Erasing “Church” ESD UnlearnDiff R.A.C.E.Original SD Added by authors* * * * * * Fig. 1: Comparative demonstration of concept erasure, red teaming, and robust era- sure within T2I diffusion models. The ESD method [10] removes targeted concepts from the original SD outputs, yet these concepts can be reconstructed using Unlearn- Diff [72]. Our proposed R.A.C.E. method showcases enhanced robustness against such red teaming reconstruction efforts. In response to the challenges posed by the malicious exploitation of generative models, Stable Diffusion (SD) [46] has integrated a safety checker [45] and advo- cates for the utilization of a watermarking module [71]. Despite these initiatives, the reliance on post-hoc interventions presents limitations due to their poten- tial for circumvention [8, 22]. Consequently, the research community is pivoting towards formulating methodologies that embed safety protocols directly within R.A.C.E. for Secure T2I Diffusion 3 the image generation pipeline. This approach ensures that content security is an intrinsic aspect of image creation [8, 22, 23, 37, 60, 66, 67]. Although these meth- ods are valuable for identifying the source of content after an incident, their reactive nature highlights the necessity for proactive strategies. This includes pioneering techniques attempting early removal of sensitive content from T2I models [10, 27], thereby preempting the production of inappropriate or harmful material. To address the challenge of removing undesirable content from T2I mod- els, even when users attempt to circumvent content restrictions, recent research has focused on the development of concept erasure techniques within T2I diffu- sion models [10, 13, 27, 69]. These techniques primarily aim to eliminate specific concepts (e.g., “nudity”) by altering the text embeddings associated with these concepts to neutral representations. Despite these efforts, there remains a vulner- ability wherein erased concepts can be reconstructed. This is achieved by iden- tifying text tokens that closely align with the visual embeddings of the targeted concepts, thus enabling the regeneration of prohibited content [4, 55, 59, 64, 72]. This issue is illustrated in Fig. 1, demonstrating that even with concept erasure, T2I models can be manipulated through prompt modification to regenerate the restricted content. This underscores the imperative for a more robust concept erasure methodology that can withstand such reconstruction attempts, ensuring the integrity of content generation within T2I models. Acknowledging the imperative for enhanced concept erasure methodologies within T2I models, we pose a critical question: “Is it feasible to develop a con- cept erasure approach that is resilient against reconstruction efforts?” In pur- suit of this, we introduce R.A.C.E. (Robust Adversarial Concept Erase), a novel strategy aimed at bolstering the resilience of concept erasure techniques against adversarial manipulations, as delineated in Algorithm 1. At the heart of RACE lies an adversarial training framework, leveraging insights from the effort of adversarial robustness [33]. Our method effectively identifies adversarial text embeddings capable of reconstructing erased concepts and then facilitates their integration into the T2I concept erasure workflow. A pivotal aspect of RACE is its ability to efficiently uncover adversarial text embeddings within a single time step of the diffusion process, an approach elabo- rated in Section 3.2. This efficiency not only streamlines the process of identifying adversarial examples but also facilitates the integration of our adversarial attack mechanism into the concept erasure workflow. To demonstrate the robustness of RACE, we have carried out an extensive array of experiments. These experiments validate the effectiveness of RACE in countering diverse red teaming strategies, with detailed results presented in Section 4. Our empirical investigations un- derscore the capacity of RACE to significantly enhance the robustness of T2I models against both white-box and black-box attacks across a broad spectrum of target concepts, including artistic, explicit, and object categories. We summarize the three main contributions here: 4 C. Kim ⋆, K. Min⋆, Y. Yang • We are the first to present, to our knowledge, an adversarial training ap- proach specifically designed to fortify concept erasure methods against prompt- based adversarial attacks without introducing additional modules. • Our method, RACE, implements a computationally efficient adversarial at- tack method that can be plugged into the concept erasing method. • We show RACE significantly improves T2I models’ robustness against prompts based on white/black box attacks. 2 Related Works For a comprehensive overview of additional related works, please refer to the supplementary material. Text-to-Image Synthesis. The field of generative models has seen remarkable advancements, notably extending their capabilities beyond generating photore- alistic images [7, 21] to include Text-to-Image (T2I) synthesis [36, 40, 44, 46, 49]. This progress has led to the development of fine-tuning techniques that allow for the customization of T2I models to user-specific needs [9, 28, 39, 48, 58], thereby enabling the creation of highly realistic images that align closely with textual prompts. However, the potential for misuse by malicious entities, using these models for purposes such as spreading misinformation [6, 34], raises significant concerns. This underscores the urgency of devising protective measures to miti- gate the risk of such exploitations. Advanced Techniques in Concept Erasure for T2I Diffusion Models. Within the realm of machine unlearning, concept erasure for T2I Diffusion mod- els has recently emerged as a critical area of research, focusing on the removal of sensitive or copyrighted concepts from T2I models. Methods to achieve this include guiding the image generation process or adjusting the model’s weights to exclude these elements [10,13,27,30,35,51,69]. Notably, techniques by Gandikota et al. [10] and Kumari et al. [27] involve mapping sensitive concepts to null enti- ties or benign equivalents by fine-tuning the weights of Stable Diffusion (SD) [46] models, effectively preventing the generation of undesirable content. Despite these advancements, red teaming methods have exposed potential loopholes, indicating that erased concepts might be regenerated through meticulously de- signed text prompts. Addressing this issue, our work contributes an adversarial training strategy aimed at bolstering the resilience of Stable Diffusion models against such text-prompt-based attacks [4,59,72], thereby enhancing the security and integrity of the content generation. Robustness Evaluation via Red Teaming in T2I Models. While various safety measures have been proposed to shield SD models from misuse, red team- ing strategies reveal vulnerabilities that still allow for circumvention. Research has demonstrated that techniques like Textual Inversion [9] can be exploited to regenerate content previously erased from SD models [41], prompting the devel- R.A.C.E. for Secure T2I Diffusion 5 Algorithm 1 Robust Adversarial Concept Erasure: RACE Algorithm Input: Diffusion Model Φθ, frozen diffusion model Φθ∗, scheduler S, target concept c, training steps M , adversarial steps N , perturbation limit ϵ, attack step size α for i = 0, . . . , M do Sample noise n ∼ N (0, 1), timestep t ∼ U (1, 1000) Initialize δ ∼ U (−ϵ, ϵ) Denoise zt = S(n, t, c) for j = 0, . . . , N do ▷ Perform targeted attack δ = δ + α · sign(∇δ − LSD(Φθ, zt, t, c, δ)) Clamp δ within [−ϵ, ϵ] end for θ = θ − ∇θLRACE(Φθ, Φθ∗ , zt, t, c, δ) end for return Φθ opment of countermeasures aimed at safeguarding against such inversions [63,73]. In real-world applications, T2I services such as Midjourney predominantly rely on user-provided text prompts, making them susceptible to prompt-based red teaming attacks [4, 55, 59, 64, 72]. These methods employ sophisticated prompt optimization techniques to restore images containing erased content, with their efficacy contingent upon the level of model access—categorized into white-box approaches, which utilize SD’s U-Net [47] for prompt optimization [4, 72], and black-box strategies, where such access is restricted [55,59,64]. Both approaches establish formidable benchmarks in attack success rates, as detailed in Tab. 2. However, the landscape lacks robust defense mechanisms against prompt-based red teaming, primarily due to the prohibitive computational demands associ- ated with identifying adversarial prompts—a challenge that renders traditional adversarial training approaches impractical. Addressing this gap, our work intro- duces a novel defense strategy tailored to counteract prompt-based red teaming attacks, marking a significant step forward in fortifying T2I diffusion models against adversarial threats. 3 Method Our methodology aims to expunge target concepts from T2I diffusion models through an adversarial training framework [33]. Initially, in Section 3.1, we es- tablish the foundation by formally introducing the notations and the rationale underpinning our approach. Following this, Section 3.2 details our proposed ad- versarial attack, specifically designed for robust concept erasure, and delineates its integration into the adversarial training regime. 3.1 Preliminaries Stable Diffusion Models. Our method is built upon the Stable Diffusion Model (SD) [46], which operates as the foundational architecture for our concept 6 C. Kim ⋆, K. Min⋆, Y. Yang 0.40 0.45 0.50 0.55 0.60ASR@𝑡 Time step 𝑡 0 200 400 600 800 1000 * Added by authors * * * * * *Attacked Images@𝑡 Fig. 2: Single-Timestep Adversarial Attack Efficacy. This figure illustrates the Attack Success Rate (ASR) across various timesteps, alongside representative images. Notably, even when the adversarial attack is applied at a singular timestep t ∗, the perturbed text embedding c + δt∗ successfully reproduces images containing the previously erased concept. For method details, see Sec. 3.2. erasure technique. The SD model is composed of two primary elements: firstly, an image autoencoder that has been pre-trained on a diverse and extensive im- age dataset [7]. Within this autoencoder, an encoder function E(·) transforms an input image x into a latent representation z = E(x). Conversely, a decoder function D(·) aims to reconstruct the input image from its latent form, where D(z) = ˆx ≈ x. The second element is a U-Net [47]-based diffusion model trained to craft latent representations within the acquired latent space. This model facilitates the conditioning on either class labels or text embeddings derived from training data. Let us denote by c = Etxt(y) the textual embedding encoded from a conditioning text prompt y, where Etxt symbolizes the text encoder, such as CLIP [43]. Under these constructs, the SD training objective is encapsulated by the loss function: LSD := En∼N (0,1),z,c,t [||n − Φθ(zt, t, c)|| 2 2] , (1) where t indexes the time step, n represents a noise sample drawn from a standard Gaussian distribution, zt is the perturbed version of z up to time step t, and Φθ is the denoising network based on a U-Net architecture. During inference, a random noise sample is procured from a Gaussian distribution and denoised R.A.C.E. for Secure T2I Diffusion 7 using Φθ following a scheduler S, operating over a sequence of predetermined time steps T . The resulting denoised latent, z0, is then decoded to produce the final image, ˆx = D(z0). Erase and Reconstruction of Target Concept. The objective of concept erasure is to remove specific concepts, such as “nudity”, from the latent space of a pre-trained diffusion model. The Erased Stable Diffusion (ESD) approach [10] introduces a method for excising these concepts from the latent representations within the Stable Diffusion framework. The erasure loss is formalized as follows: Lerase := ||Φθ(zt, t, c) − (Φθ∗ (zt, t) − η(Φθ∗ (zt, t, c) − Φθ∗ (zt, t)))|| 2 2, (2) where Φθ∗ represents the frozen denoising U-Net, and η denotes the guidance scale associated with classifier-free guidance [17]. ESD fine-tunes Lerase with respect to θ, guiding Φθ to produce outputs where the target concept is effec- tively nullified. Crucially, this process does not necessitate additional datasets; it operates successfully with only a concise textual description. Conversely, red teaming efforts aim to counteract concept erasure by crafting adversarial text prompts ˜y capable of resurrecting the erased concept in the gen- erated image. White-box methods [4, 72] engage in this adversarial prompt opti- mization by leveraging the gradients of Φθ. Meanwhile, black-box approaches [55, 59, 64] aim to achieve comparable outcomes without reliance on the gradients of Φθ. Both approaches pose significant computational demands, which presents challenges for their integration into an adversarial training framework. 3.2 Adversarial Training on Concept Erased Diffusion Models Motivation. The primary aim of T2I diffusion models is to generate high-quality images conditioned on specific prompts. Intriguingly, the SD model’s loss func- tion, as depicted in Eq.(1), can also facilitate image classification tasks [29]. This classification capability is derived by applying Bayes’ Theorem to the model’s predictions pθ(x|ci) and the prior distribution p(c) across a set of conditions ci, where each ci = Etxt(yi) represents a textual embedding of the prompt yi: pθ(ci|x) = p(ci)pθ(x|ci) ∑ j p(cj)pθ(x|cj) . (3) Notably, the prior terms p(c) can be disregarded when they are uniformly dis- tributed over the prompts ci (i.e., p(ci) = 1 N ). In the context of diffusion mod- els, directly computing pθ(x|ci) is computationally challenging, leading to the reliance on the computation of log pθ(x|ci) and the utilization of the Evidence Lower Bound (ELBO) for optimization purposes. Leveraging approximations in- troduced in [16], we can approximate the posterior distribution over prompts ci as follows: pθ(ci|x) = exp{−Ez,n,t [ ||n − Φθ(zt, t, ci)|| 2]} ∑ j exp{−Ez,n,t [||n − Φθ(zt, t, cj)||2]} . (4) 8 C. Kim ⋆, K. Min⋆, Y. Yang Building on this foundation, the Diffusion Classifier [29] proposes a method for estimating the class of a given image x by finding argmin of the following expression: argmin c T∑ t=1 [ ||n − Φθ(zt, t, c)|| 2] , (5) where zt = E(x) represents the latent encoding of the image x and the goal is to identify the label c from the available set of classes ci. A notable insight from the Diffusion Classifier is the feasibility of classifying image x even when computations are performed using a single time step t∗. Single-Timestep Adversarial Attacks on Erased T2I Models. Leveraging insights from the Diffusion Classifier, we re-conceptualize the SD model’s loss function in Eq.(1) as a classification mechanism. This perspective allows us to view Textual Inversion (TI) [9] as a form of targeted adversarial attack, where the objective is to optimize the conditioning text embedding c to regenerate the image x [41, 72]. Notably, TI is computationally intensive as it necessitates optimization across all time steps. Prompted by these considerations, our investigation centers around a critical inquiry: Can adversarial text embeddings be identified with just a single timestep? We investigate whether adversarial text embeddings can be effectively identified at a singular timestep t∗. Our approach is geared towards nullifying the embed- ding of a target concept c and its proximate embeddings that might facilitate the regeneration of an erased concept image ˜x, such as an explicit image. To this end, we devise a targeted adversarial attack to produce ˜x from a concept-erased dif- fusion model Φθ by introducing an adversarial perturbation δ. The perturbation δ is determined through the optimization: argmin ||δ||∞≤ϵ ||n − Φθ( ˜zt, t, c + δ)|| 2 2, (6) where ˜z = E(˜x) denotes the latent representation of image ˜x, ϵ is a small number, and c = Etxt(y) encodes the textual embedding of the targeted concept, for instance, y =“nudity”. The Projected Gradient Descent (PGD) algorithm [33] is employed to address this optimization problem. Specifically, when selecting timestep t ∗ = 500 as the critical adversarial point, ˜zt undergoes denoising via Φθ( ˜zt, t, c) transitioning from timesteps t = 1000 to t = 500. Subsequently, the targeted adversarial approach outlined in Eq. (6) is executed to determine δt∗ at t = 500. In subsequent denoising steps, ˜zt is denoised considering the introduced perturbation c + δt∗ . Our method introduces a distinctive single-timestep adversarial attack, con- trasting with prior approaches that required optimization over a wide range of timesteps [4,72]. This approach enables the seamless incorporation of our attack strategy into the adversarial training process specifically for T2I concept erasure. To assess the efficacy of our method, we execute tests on a Φθ model trained for removing the “nudity” concept via the ESD method. Utilizing 142 nudity- centric prompts from the I2P dataset [51], we systematically select timesteps R.A.C.E. for Secure T2I Diffusion 9 t ∗ at 100-step intervals within the [1, 1000] range and compute the Attack Suc- cess Rate (ASR), as formulated in Eq.(8), for each t ∗. The results, depicted in Fig. 2, unveil a notable capability: the reconstruction of the erased concept is feasible even with the attack confined to a single timestep t ∗. Interestingly, after t ∗ = 500, we observe an increasing trend in the ASR. The modified images re- sulting from the attack visually convey the transition back to the erased concept and reflect the ASR patterns. For additional insights into various concepts and their corresponding ASR trends across different timesteps t∗, the supplementary material offers further information. Table 1: Performance comparison of Lerase and LRACE Lerase LRACE I2P [51] 0.08 0.05 FID [15] 33.12 25.16 CLIP-Score [14] 0.726 0.745 Adversarial Training for T2I Concept Erasure. Motivated by the findings from our single-timestep adversarial attack experi- ments, we explore the potential of such attacks to enhance the robustness of concept erasure in T2I models, posing the question: Can ad- versarial attacks improve the resilience of con- cept erasure mechanisms? RACE distinguishes itself from existing approaches [10,27] by aiming to elim- inate not only the targeted concept’s embedding but also its adjacent embedding within the model’s latent space, which could otherwise lead to the inadvertent generation of the erased concept by Φθ. We incorporate our adversarial attack into the erasure loss function (Lerase), yielding an enhanced adversarial training loss: LRACE := ||Φθ(zt, t, c + δ) − (Φθ∗ (zt, t) − η(Φθ∗ (zt, t, c) − Φθ∗ (zt, t)))|| 2 2. (7) This method is deliberate, substituting the concept embedding c with c + δ within the trainable parameters of Φθ. This precise adjustment ensures enhanced fidelity of the generated images by mapping the ϵ-neighborhood of the concept embedding to its null representation. Comparative metrics between the direct substitution in Lerase and the strategic use of LRACE are provided in Tab. 1. The latter approach demonstrates promising reductions in ASR for “nudity” prompts within I2P dataset [51] and improvements in image quality metrics, as assessed on the MS-COCO [31]. The RACE methodology is comprehensively detailed in Algorithm 1. To validate RACE’s effectiveness, we evaluate the ASR against both white-box and black-box attacks, as elaborated in Sec. 4. 4 Experiments 4.1 Experimental Setting Datasets. Our assessment of the RACE framework spans various domains, in- cluding artistic styles, explicit concepts, and identifiable objects, in line with established benchmarks [4, 10, 72]. To ensure a uniform image generation pro- cess, we standardize key hyperparameters such as the scale of classifier-free guid- ance and random seeds. Artistic style evaluations leverage shared text prompts 10 C. Kim⋆, K. Min⋆, Y. Yang from ESD [10] and UnlearnDiff [72]. For explicit content, we draw from the in- appropriate image prompt benchmark [51], selecting a diverse set of prompts encompassing 142 nudity, 98 illegal acts, and 101 violence instances. Objects are curated from a subset of Imagenette [19], known for its distinct and recog- nizable classes, with text prompts synthesized via ChatGPT [38] following the approach by Kumari et al. [27]. It’s crucial to note that these datasets serve solely to gauge the effectiveness of adversarial attacks; the adversarial training component of RACE does not necessitate the use of these specific prompts. Training Details. In validating RACE, we choose the Erased Stable Diffusion (ESD) model [10] for its ability to erase a wide range of concepts, serving as an ideal testbed to showcase RACE’s efficacy. We integrate RACE with ESD, optimizing with attack parameters ϵ = 0.1 and α = ϵ/4. The optimization of model parameters θ utilizes the Adam optimizer [26] at a learning rate of 1e − 5, consistent with ESD. Additional details on attack parameter selection are avail- able in the supplementary material. Red Teaming Methods. To rigorously test RACE’s robustness, we deploy a comprehensive suite of adversarial attacks, spanning both white-box and black- box approaches. Initial assessments utilize the I2P red teaming prompt dataset [51]. In black-box scenarios, we employ PEZ [59], which crafts adversarial prompts via CLIP [43]. In the white-box scenario, methods like P4D [4] and UnlearnDiff [72] are used, which generate adversarial prompts by leveraging gradients from the SD. Evaluation. To gauge the robustness of RACE, we employ domain-specific clas- sifiers: a ViT-base model [61] pre-trained on ImageNet [5] and fine-tuned on WikiArt [50] for artistic styles, Nudenet [1] for explicit content, and ResNet- 50 [12] trained on ImageNet for object removal. We measure robustness using the Attack Success Rate (ASR): ASR = 1 N N∑ i=1 1 (f (SD( ˜yi)) = ˜yi) , (8) where ˜yi is the adversarial prompt, f is the classifier, and N is the number of prompts. Additionally, we assess image quality after applying RACE by gen- erating 5,000 images from the MS-COCO [31] test set, computing the Frechet Inception Distance (FID) score [15] and the CLIP score [14] to evaluate RACE’s impact on image fidelity while ensuring concept erasure. 4.2 Robust Concept Erase against Red Teaming In our comprehensive analysis, RACE undergoes a series of red teaming eval- uations [4, 51, 59, 72], encompassing both white-box and black-box techniques aimed at regenerating concepts targeted by RACE for erasure, as depicted in R.A.C.E. for Secure T2I Diffusion 11 Table 2: Attack Success Rate (ASR) against white (#)/black ( ) box attacks. We conduct experiments on artistic style (Van Gogh), explicit concepts (nudity, violence, and illegal acts), and objects (church, golf ball, and parachute). “ESD/RACE-Concept” denotes the concept erased from the model. We also measure the quality of T2I models. We can observe that RACE reduces ASR by over 30% (-0.30 or below) in absolute value on Van Gogh, nudity, and church for UnlearnDiff [72], which is the previous SOTA attack method. Prompts PEZ [59] P4D [4] UnlearnDiff [72] CLIP-Score [14] FID [15] White/Black Box # # - - ESD [10]-VanGogh 0.04 0.00 0.26 0.36 0.7997 19.16 ESD [10]-Nudity 0.14 0.08 0.75 0.80 0.7931 18.88 ESD [10]-Violence 0.27 0.13 0.84 0.79 0.7834 21.55 ESD [10]-Illegal 0.29 0.20 0.89 0.85 0.7854 21.50 ESD [10]-Church 0.16 0.00 0.58 0.68 0.7896 19.68 ESD [10]-GolfBall 0.04 0.00 0.16 0.16 0.7738 20.64 ESD [10]-Parachute 0.06 0.04 0.48 0.60 0.7865 19.72 RACE-VanGogh 0.00 (-0.04) 0.00 (-0.00) 0.00 (-0.26) 0.04 (-0.32) 0.8024 20.65 RACE-Nudity 0.05 (-0.09) 0.02 (-0.06) 0.49 (-0.26) 0.47 (-0.33) 0.7452 25.16 RACE-Violence 0.11 (-0.16) 0.08 (-0.05) 0.75 (-0.09) 0.68 (-0.11) 0.7374 28.71 RACE-Illegal 0.20 (-0.09) 0.13 (-0.07) 0.85 (-0.04) 0.80 (-0.05) 0.7591 24.87 RACE-Church 0.02 (-0.14) 0.00 (-0.00) 0.26 (-0.32) 0.38 (-0.30) 0.7730 23.92 RACE-GolfBall 0.00 (-0.04) 0.00 (-0.00) 0.10 (-0.06) 0.06 (-0.10) 0.7480 25.38 RACE-Parachute 0.02 (-0.04) 0.00 (-0.04) 0.24 (-0.24) 0.38 (-0.22) 0.7570 26.42 Fig.1. The comparative analysis of ASR presented in Tab.2 spans diverse con- ceptual domains, from “Van Gogh”-inspired artistry to explicit content such as “nudity” and “violence”, extending to tangible objects like “churches”, “golf balls”, and “parachutes”. Remarkably, RACE consistently diminishes ASR, notably sur- passing 30% for “Van Gogh” styles, “nudity”, and “church” categories, particu- larly outperforming UnlearnDiff [72], the current state-of-the-art in white-box adversarial methodologies. This marked decline in ASR underscores RACE’s heightened robustness and delineates its capability as a potent, computationally efficient defense mechanism for T2I diffusion models against intricate adversarial attacks. Crucially, RACE’s methodological advantage stems from its indepen- dence from external imagery or prompts, diverging from traditional red teaming techniques reliant on such data for prompt generation. As demonstrated in Tab.2, RACE achieves a significant 33% reduction in ASR for the “nudity” concept, underscoring its effectiveness. To further eluci- date how RACE enhances ASR, we analyze the specific categories or elements it targets for removal. Utilizing Nudenet, we enumerate the body parts gen- erated by various models—original SD, ESD, UnlearnDiff, and RACE—when prompted with nudity-related inputs from the I2P dataset. Illustrated in Fig.3, the analysis reveals that while ESD enhances the original SD’s resilience to such prompts, UnlearnDiff manages to bypass ESD’s defenses, reconstructing explicit content. In contrast, RACE maintains its robustness even against the sophis- ticated UnlearnDiff attacks, showcasing the advanced protective capabilities of our approach in safeguarding against the regeneration of sensitive content. One caveat to mention is that our experiments reveal a nuanced trade-off between robustness and image quality. While artistic style erasures maintain 12 C. Kim⋆, K. Min⋆, Y. Yang -57.7% -93.9% -81.3% -35.0% -58.3% -66.7% -50.0%# of exposed body parts Fig. 3: Although ESD significantly reduces the chance of generating images with ex- posed body parts, state-of-the-art red teaming methods, such as UnlearnDiff, can be used to bypass ESD’s defense and reconstruct explicit content. RACE and its variant can effectively defend the malicious attempts to reconstruct explicit content from the ESD model that erased the concept of nudity. quality metrics, erasures of other concepts inadvertently degrade image quality. This divergence could be attributed to methodological differences; erasing artis- tic styles predominantly involves fine-tuning the cross-attention layers of SD as per ESD guidelines, whereas erasing other concepts necessitates adjustments in non-cross-attention layers [10]. Another plausible explanation for the observed trade-off between robust concept erasure and overall image quality could relate to the inherent complexity of differentiating between closely related or overlap- ping concepts within the model’s latent space. As RACE intensifies adversarial robustness, it may inadvertently alter the delicate equilibrium within these con- ceptual overlaps, leading to unintended modifications in adjacent, non-targeted conceptual representations. This issue highlights the complex tension between precise concept erasure and maintaining the model’s overall integrity against attacks. To address the quality concerns arising from this trade-off, we explore a potential strategy for improvement in Sec. 4.4, aiming to reduce the trade-off between targeted erasure with high quality and the model’s defensive robustness. 4.3 Disentanglement Investigating RACE’s disentanglement performance, our study focuses on its ca- pability to precisely erase intended concepts without impacting other elements. The evaluation spans both qualitative and quantitative measures. On the quali- tative front, we configure separate RACE-enhanced models to specifically erase artistic imprints such as “Van Gogh”, “Thomas Kinkade”, and “Kilian Eng” from the SD. The illustrative outcomes, showcased in Fig. 4, underscore RACE’s era- sure precision, ensuring that the excision of one artistic style doesn’t lead to the collateral removal of others. This meticulous erasure extends to discrete object concepts like “church”, “golf ball”, and “parachute”, with the generated images further affirming the method’s discernment. R.A.C.E. for Secure T2I Diffusion 13 Original SD Erasing “Van Gogh” Erasing “Kilian Eng” Erasing “Thomas Kinkade”Peasant Woman by Vincent van GoghTranquil Lakeby Thomas KinkadeFuturistic Cityscape by Kilian EngPrompt Conditioning Original SD Erasing “Church” Erasing “Parachute” Erasing “Golf Ball”Church with snowy backgroundGolf ball on a course Skydiver with vibrant parachute Fig. 4: RACE’s Disentanglement in Concept Erasure. This figure highlights RACE’s precision in erasing specific concepts, as shown in diagonal images, while preserving unrelated concepts, which is evident in off-diagonal images. For reference, baseline images generated by the original Stable Diffusion (SD) model are also presented. Table 4: Ablation Studies for Performance Improvement. Using the “nudity” concept, we evaluate the effectiveness of adding weight regularization and close concept key- words. ASR and quality metrics are measured to assess performance. I2P [51] PEZ [59] P4D [4] UnleranDiff [72] CLIP-Score [14] FID [15] ESD 0.14 0.08 0.75 0.80 0.7931 18.88 RACE 0.05 0.02 0.49 0.47 0.7452 25.16 RACE+Reg. 0.07 0.02 0.60 0.62 0.7593 24.42 RACE+keywords 0.02 0.01 0.42 0.46 0.7201 30.97 Table 3: Accruacy of erased and non- erased classes in Imagenet [5]. We eval- uate the classification accuracy of ESD and RACE for the target erased con- cepts and other non-target concepts. Acc. Erased Acc. Others Erased Concept ESD RACE ESD RACE Church 0.16 0.02 0.57 0.53 Golf Ball 0.04 0.00 0.45 0.35 Parachute 0.06 0.02 0.57 0.41 Quantitatively, we generate 5,000 prompts with varied random seeds, such as “an image of a [class name]”, to pro- duce a diverse set of images, subsequently evaluated using a pre-trained ResNet-50 classifier for top-1 accuracy. As shown in Tab. 3, RACE showcases an improved ca- pability for object concept erasure com- pared to ESD. Despite this, there exists a slight decrement in classification accu- racy for non-target classes. This effect likely stems from RACE’s method of targeting the ϵ-neighborhood surrounding the intended concept, potentially influencing proximate concepts. Nevertheless, we can observe from Fig. 4 that RACE can precisely erase the target concept with minimal visual impacts on other concepts. 4.4 Discussion Potential Strategy to Improve the Robustness-Quality Trade-off. Our findings underscore the ability of RACE to significantly bolster the SD model’s 14 C. Kim⋆, K. Min⋆, Y. Yang defense against prompt-based adversarial attacks across a variety of concepts. However, as illustrated in Tab.2, enhancing robustness appears to inversely im- pact image quality. To address this dichotomy, we test a refined version of the RACE loss function incorporating a regularization term: LRACE+Reg. := LRACE + λ||θ − θ∗||1, (9) where θ and θ∗ represent the parameters of the RACE and original Stable Diffu- sion models, respectively, and λ is the regularization strength, set to 0.1 in our experiment. This regularization approach, as evidenced in Tab.4, helps to par- tially reconcile the robustness-quality trade-off, enhancing image quality while maintaining improved robustness over ESD. Enhancing Concept Erasure. In pursuit of further reducing the ASR, we explore strategies for more comprehensive concept erasure. Recognizing that a target concept may manifest in various synonymous forms, we extend RACE’s erasure scope to include semantically related concepts. Leveraging the CLIP text encoder embedded within Stable Diffusion, we identify and subsequently erase concepts closely related to the target, based on their proximity in the CLIP em- bedding space. For instance, alongside “nudity”, we also target synonymous con- cepts like “nude”, “nsfw”, and “bare”, identified as the top-3 semantically similar terms. In Fig. 3, we can observe that our method equipped with this expanded erasure strategy (denoted as RACE+keywords) is more effective in defending the malicious attempts to bypass the ESD by further reducing the number of exposed body parts. Tab. 4 also indicates that our strategy indeed fortifies the model’s robustness against red teaming tactics. Nonetheless, this broadened con- cept removal spectrum reaffirms the robustness-quality trade-off, manifesting as a decrement in image quality. This aspect opens an intriguing avenue for fu- ture enhancements to the RACE methodology, balancing the twin objectives of robust concept erasure and preserved image fidelity. 5 Conclusion In this work, we present RACE, a novel defense approach designed to protect the Text-to-Image Stable Diffusion models from prompt-based red teaming at- tacks. RACE effectively strengthens the model’s concept erasure capabilities while maintaining computational efficiency, offering a valuable enhancement to the current erasure framework and bolstering defenses against various adver- sarial techniques. We also observe the robustness-quality trade-off and discuss possible future directions to improve it. This initial contribution lays the ground- work for further exploration, underscoring the critical importance of developing sophisticated defenses in the rapidly evolving domain of generative AI. Acknowledgements. We would like to express our gratitude to Sangmin Jung for assisting with the experiments. This work is partially supported by the Na- tional Science Foundation under Grant No. 2038666 and No. 2101052. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the funding agencies and employers. R.A.C.E. for Secure T2I Diffusion 15 References 1. Bedapudi, P.: Nudenet: Neural nets for nudity classification, detection and selective censoring (12 2019) 10 2. Carlini, N., Hayes, J., Nasr, M., Jagielski, M., Sehwag, V., Tramèr, F., Balle, B., Ippolito, D., Wallace, E.: Extracting training data from diffusion models. ArXiv abs/2301.13188 (2023), https://api.semanticscholar.org/CorpusID: 256389993 1 3. Carlini, N., Wagner, D.A.: Towards evaluating the robustness of neural networks. 2017 IEEE Symposium on Security and Privacy (SP) pp. 39–57 (2016), https: //api.semanticscholar.org/CorpusID:2893830 20 4. Chin, Z.Y., Jiang, C.M., Huang, C.C., Chen, P.Y., Chiu, W.C.: Prompt- ing4debugging: Red-teaming text-to-image diffusion models by finding problematic prompts. arXiv preprint arXiv:2309.06135 (2023) 3, 4, 5, 7, 8, 9, 10, 11, 13, 20 5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large- scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp. 248–255. Ieee (2009) 10, 13 6. Devlin, K., Cheetham, J.: Fake trump arrest photos: How to spot an ai-generated image. BBC News (March 2023), https://www.bbc.com/news/technology- 68981525 1, 4 7. Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image synthesis. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 12873–12883 (2021) 4, 6 8. Fernandez, P., Couairon, G., Jégou, H., Douze, M., Furon, T.: The stable signature: Rooting watermarks in latent diffusion models. arXiv preprint arXiv:2303.15435 (2023) 2, 3 9. Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: An image is worth one word: Personalizing text-to-image gen- eration using textual inversion. ArXiv abs/2208.01618 (2022), https://api. semanticscholar.org/CorpusID:251253049 4, 8 10. Gandikota, R., Materzynska, J., Fiotto-Kaufman, J., Bau, D.: Erasing con- cepts from diffusion models. 2023 IEEE/CVF International Conference on Com- puter Vision (ICCV) pp. 2426–2436 (2023), https://api.semanticscholar.org/ CorpusID:257495777 2, 3, 4, 7, 9, 10, 11, 12, 20, 21 11. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. CoRR abs/1412.6572 (2014), https://api.semanticscholar.org/ CorpusID:6706414 20 12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 770–778 (2016) 10 13. Heng, A., Soh, H.: Selective amnesia: A continual learning approach to forget- ting in deep generative models. ArXiv abs/2305.10120 (2023), https://api. semanticscholar.org/CorpusID:258740988 3, 4 14. Hessel, J., Holtzman, A., Forbes, M., Bras, R.L., Choi, Y.: Clipscore: A reference- free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718 (2021) 9, 10, 11, 13, 22, 23 15. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained by a two time-scale update rule converge to a local nash equilibrium. In: Advances in Neural Information Processing Systems. pp. 6626–6637 (2017) 9, 10, 11, 13 16 C. Kim⋆, K. Min⋆, Y. Yang 16. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 6840–6851 (2020) 7 17. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598 (2022) 7 18. Hong, S., Lee, J., Woo, S.S.: All but one: Surgical concept erasing with model preservation in text-to-image diffusion models. ArXiv abs/2312.12807 (2023), https://api.semanticscholar.org/CorpusID:266374816 20 19. Howard, J., Gugger, S.: fastai: A layered api for deep learning. Inf. 11, 108 (2020), https://api.semanticscholar.org/CorpusID:211082837 10 20. Huang, C.P., Chang, K.P., Tsai, C.T., Lai, Y.H., Wang, Y.C.F.: Receler: Re- liable concept erasing of text-to-image diffusion models via lightweight erasers. ArXiv abs/2311.17717 (2023), https://api.semanticscholar.org/CorpusID: 265498506 20 21. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing and improving the image quality of stylegan. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 8110–8119 (2020) 4 22. Kim, C., Min, K., Patel, M., Cheng, S., Yang, Y.: Wouaf: Weight modulation for user attribution and fingerprinting in text-to-image diffusion models. arXiv preprint arXiv:2306.04744 (2023) 2, 3 23. Kim, C., Ren, Y., Yang, Y.: Decentralized attribution of generative models. In: International Conference on Learning Representations (2021) 3 24. Kim, S., Jung, S., Kim, B., Choi, M., Shin, J., Lee, J.: Towards safe self-distillation of internet-scale text-to-image diffusion models. ArXiv abs/2307.05977 (2023), https://api.semanticscholar.org/CorpusID:259837117 20 25. Kinfu, K.A., Vidal, R.: Analysis and extensions of adversarial training for video classification. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3416–3425 (2022) 20 26. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization (2014), http: //arxiv.org/abs/1412.6980, cite arxiv:1412.6980Comment: Published as a con- ference paper at the 3rd International Conference for Learning Representations, San Diego, 2015 10 27. Kumari, N., Zhang, B., Wang, S.Y., Shechtman, E., Zhang, R., Zhu, J.Y.: Ab- lating concepts in text-to-image diffusion models. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 22634–22645 (2023), https://api. semanticscholar.org/CorpusID:257687839 3, 4, 9, 10, 20 28. Kumari, N., Zhang, B., Zhang, R., Shechtman, E., Zhu, J.Y.: Multi-concept customization of text-to-image diffusion. 2023 IEEE/CVF Conference on Com- puter Vision and Pattern Recognition (CVPR) pp. 1931–1941 (2022), https: //api.semanticscholar.org/CorpusID:254408780 4 29. Li, A.C., Prabhudesai, M., Duggal, S., Brown, E., Pathak, D.: Your diffusion model is secretly a zero-shot classifier. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). pp. 2206–2217 (October 2023) 7, 8 30. Li, S., van de Weijer, J., Hu, T., Khan, F.S., Hou, Q., Wang, Y., Yang, J.: Get what you want, not what you don’t: Image content suppression for text-to-image dif- fusion models. ArXiv abs/2402.05375 (2024), https://api.semanticscholar. org/CorpusID:267547985 4 31. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision– ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. pp. 740–755. Springer (2014) 9, 10 R.A.C.E. for Secure T2I Diffusion 17 32. Lyu, M., Yang, Y., Hong, H., Chen, H., Jin, X., He, Y., Xue, H., Han, J., Ding, G.: One-dimensional adapter to rule them all: Concepts, diffusion models and erasing applications. ArXiv abs/2312.16145 (2023), https://api.semanticscholar. org/CorpusID:266551849 20 33. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017) 3, 5, 8, 20 34. Marcelo, P.: Fact focus: Fake image of pentagon explosion briefly sends jit- ters through stock market. Associated Press (May 2023), https://apnews. com / article / pentagon - explosion - fake - image - stock - market - jitters - 78d5913603bdf9f17cc7a8c77a72e59b 1, 4 35. Ni, M., Wu, C., Wang, X., Yin, S.S., Wang, L., Liu, Z., Duan, N.: Ores: Open- vocabulary responsible visual synthesis. ArXiv abs/2308.13785 (2023), https: //api.semanticscholar.org/CorpusID:261243073 4 36. Nichol, A.Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., Mcgrew, B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and edit- ing with text-guided diffusion models. In: International Conference on Machine Learning. pp. 16784–16804. PMLR (2022) 4 37. Nie, G., Kim, C., Yang, Y., Ren, Y.: Attributing image generative models using latent fingerprints. arXiv preprint arXiv:2304.09752 (2023) 3 38. OpenAI: Chatgpt. Online (2022), https://chat.openai.com/chat, accessed on February 24, 2024 10 39. Patel, M., Jung, S., Baral, C., Yang, Y.: λ-eclipse: Multi-concept personalized text- to-image diffusion models by leveraging clip latent space. ArXiv abs/2402.05195 (2024), https://api.semanticscholar.org/CorpusID:267547418 4 40. Patel, M., Kim, C.S., Cheng, S., Baral, C., Yang, Y.: Eclipse: A resource-efficient text-to-image prior for image generations. ArXiv abs/2312.04655 (2023), https: //api.semanticscholar.org/CorpusID:266149498 4 41. Pham, M., Marshall, K.O., Cohen, N., Mittal, G., Hegde, C.: Circumventing con- cept erasure methods for text-to-image generative models. In: The Twelfth Inter- national Conference on Learning Representations (2024), https://openreview. net/forum?id=ag3o2T51Ht 4, 8 42. Pinto, L., Davidson, J., Sukthankar, R., Gupta, A.: Robust adversarial reinforce- ment learning. In: International Conference on Machine Learning. pp. 2817–2826. PMLR (2017) 20 43. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748–8763. PMLR (2021) 6, 10 44. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text- conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022) 1, 4 45. Rando, J., Paleka, D., Lindner, D., Heim, L., Tramèr, F.: Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610 (2022) 2 46. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 10684– 10695 (June 2022) 1, 2, 4, 5, 21, 23 47. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed- ical image segmentation. In: Medical Image Computing and Computer-Assisted 18 C. Kim⋆, K. Min⋆, Y. Yang Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, Oc- tober 5-9, 2015, Proceedings, Part III 18. pp. 234–241. Springer (2015) 5, 6 48. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream- booth: Fine tuning text-to-image diffusion models for subject-driven genera- tion. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 22500–22510 (2022), https://api.semanticscholar.org/CorpusID: 251800180 4 49. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text- to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems 35, 36479–36494 (2022) 4 50. Saleh, B., Elgammal, A.: Large-scale classification of fine-art paintings: Learning the right metric on the right feature. ArXiv abs/1505.00855 (2015), https:// api.semanticscholar.org/CorpusID:14168099 10 51. Schramowski, P., Brack, M., Deiseroth, B., Kersting, K.: Safe latent diffusion: Mitigating inappropriate degeneration in diffusion models. 2023 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition (CVPR) pp. 22522–22531 (2022), https://api.semanticscholar.org/CorpusID:253420366 4, 8, 9, 10, 13, 23 52. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al.: Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402 (2022) 1 53. Somepalli, G., Singla, V., Goldblum, M., Geiping, J., Goldstein, T.: Diffusion art or digital forgery? investigating data replication in diffusion models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) pp. 6048–6058 (2022), https://api.semanticscholar.org/CorpusID:254366634 1 54. Somepalli, G., Singla, V., Goldblum, M., Geiping, J., Goldstein, T.: Understand- ing and mitigating copying in diffusion models. ArXiv abs/2305.20086 (2023), https://api.semanticscholar.org/CorpusID:258987384 1 55. Tsai, Y.L., Hsu, C.Y., Xie, C., Lin, C.H., Chen, J.Y., Li, B., Chen, P.Y., Yu, C.M., ying Huang, C.: Ring-a-bell! how reliable are concept removal methods for diffusion models? ArXiv abs/2310.10012 (2023), https://api.semanticscholar.org/ CorpusID:264146485 3, 5, 7, 20 56. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., Madry, A.: Robustness may be at odds with accuracy. arXiv: Machine Learning (2018), https://api. semanticscholar.org/CorpusID:52962648 20 57. Wang, Z., Guo, H., Zhang, Z., Liu, W., Qin, Z., Ren, K.: Feature importance-aware transferable adversarial attacks. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 7619–7628 (2021), https://api.semanticscholar. org/CorpusID:236493523 20 58. Wei, Y., Zhang, Y., Ji, Z., Bai, J., Zhang, L., Zuo, W.: Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. 2023 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 15897–15907 (2023), https://api.semanticscholar.org/CorpusID:257219968 4 59. Wen, Y., Jain, N., Kirchenbauer, J., Goldblum, M., Geiping, J., Goldstein, T.: Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. Advances in Neural Information Processing Systems 36 (2024) 3, 4, 5, 7, 10, 11, 13, 20 R.A.C.E. for Secure T2I Diffusion 19 60. Wen, Y., Kirchenbauer, J., Geiping, J., Goldstein, T.: Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030 (2023) 3 61. Wu, B., Xu, C., Dai, X., Wan, A., Zhang, P., Tomizuka, M., Keutzer, K., Vajda, P.: Visual transformers: Token-based image representation and processing for com- puter vision. ArXiv abs/2006.03677 (2020), https://api.semanticscholar. org/CorpusID:219531480 10 62. Wu, X., Guo, W., Wei, H., Xing, X.: Adversarial policy training against deep reinforcement learning. In: 30th USENIX Security Symposium (USENIX Security 21). pp. 1883–1900 (2021) 20 63. Wu, Y., Zhang, J., Kerschbaum, F., Zhang, T.: Backdooring textual inver- sion for concept censorship. ArXiv abs/2308.10718 (2023), https : / / api . semanticscholar.org/CorpusID:261049298 5 64. Yang, Y., Gao, R., Wang, X., Xu, N., Xu, Q.: Mma-diffusion: Multimodal attack on diffusion models. ArXiv abs/2311.17516 (2023), https : / / api . semanticscholar.org/CorpusID:265498727 3, 5, 7, 20 65. Yoo, J.Y., Qi, Y.: Towards improving adversarial training of nlp models. arXiv preprint arXiv:2109.00544 (2021) 20 66. Yu, N., Skripniuk, V., Abdelnabi, S., Fritz, M.: Artificial fingerprinting for genera- tive models: Rooting deepfake attribution in training data. In: Proceedings of the IEEE/CVF International conference on computer vision. pp. 14448–14457 (2021) 3 67. Yu, N., Skripniuk, V., Chen, D., Davis, L., Fritz, M.: Responsible disclosure of generative models using scalable fingerprinting. arXiv preprint arXiv:2012.08726 (2020) 3 68. Yuan, Z., Zhang, J., Jia, Y., Tan, C., Xue, T., Shan, S.: Meta gradient adversarial attack. 2021 IEEE/CVF International Conference on Computer Vision (ICCV) pp. 7728–7737 (2021), https://api.semanticscholar.org/CorpusID:236956844 20 69. Zhang, E., Wang, K., Xu, X., Wang, Z., Shi, H.: Forget-me-not: Learning to forget in text-to-image diffusion models. ArXiv abs/2303.17591 (2023), https://api. semanticscholar.org/CorpusID:257833863 3, 4 70. Zhang, H., Yu, Y., Jiao, J., Xing, E., Ghaoui, L.E., Jordan, M.: Theoretically princi- pled trade-off between robustness and accuracy. In: Chaudhuri, K., Salakhutdinov, R. (eds.) Proceedings of the 36th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 97, pp. 7472–7482. PMLR (09–15 Jun 2019), https://proceedings.mlr.press/v97/zhang19p.html 20 71. Zhang, K.A., Xu, L., Cuesta-Infante, A., Veeramachaneni, K.: Robust invisible video watermarking with attention (2019) 2 72. Zhang, Y., Jia, J., Chen, X., Chen, A., Zhang, Y., Liu, J., Ding, K., Liu, S.: To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images ... for now. ArXiv abs/2310.11868 (2023), https: //api.semanticscholar.org/CorpusID:264289091 2, 3, 4, 5, 7, 8, 9, 10, 11, 13, 20, 22, 23 73. Zheng, Y., Yeh, R.A.: Imma: Immunizing text-to-image models against malicious adaptation. ArXiv abs/2311.18815 (2023), https://api.semanticscholar. org/CorpusID:265506125 5 20 C. Kim⋆, K. Min⋆, Y. Yang Supplementary Material A Additional Related Works A.1 Adversarial Training Approaches Adversarial attacks [3,11,57,68] craft perturbed inputs, known as adversarial ex- amples, that can mislead models into making erroneous predictions. Adversarial training has emerged as a robust countermeasure, demonstrating that models can be fortified by incorporating these adversarial examples into the training process [33,70]. This method involves an iterative cycle of generating adversarial samples and utilizing them to update the model’s parameters, thereby instilling resilience against such attacks. Notably, studies like [56, 70] have highlighted a trade-off between standard accuracy and robustness stemming from adversarial training, adding an intriguing dimension to the field. The idea of adversarial training has spurred its adoption across various sec- tors to enhance model robustness [25,42,62,65]. The text-to-image (T2I) domain, for instance, has leveraged adversarial attacks to highlight its susceptibility to meticulously crafted inputs [4,55,59,64,72]. Despite the prevalence of adversarial attack strategies, there exists a notable scarcity of adversarial training method- ologies tailored for T2I models. Our work aims to bridge this gap by introducing a comprehensive adversarial training framework tailored for T2I models, as de- tailed in the main paper. A.2 Additional Works in Concept Erase Initial investigations into concept erasure demonstrate the capability to mod- ify representations in T2I models [10, 27]. Despite their groundbreaking contri- butions, these studies also unveil limitations such as performance drops when simultaneously erasing multiple concepts or inadvertently affecting nearby con- cepts [18, 20, 24, 32]. In response, Huang et al. [20] and Lyu et al. [32] introduce streamlined adapter layers with a loss function inspired by ESD’s [10]. Kim et al. [24] and Hong et al. [18] also craft approaches inspired by ESD’s foundational principles. While Huang et al. [20] investigate adversarial training customized for their adapter layer, their methodology is confined to this specific context and does not exhibit the versatility inherent in our proposed approach. Aligned with these developments, RACE is based on ESD, suggesting its compatibility with these recent advances. Our reliance on the innovative concept of single-timestep adversarial attacks presents a versatile solution adaptable to future T2I model enhancements, regardless of their direct association with ESD. This positions RACE as a significant contribution to reinforcing T2I models. B Additional Training Details For our experiments, we utilize pretrained concept-erased weights derived from ESD [10], adhering to their configuration where the default η value in Lerase is R.A.C.E. for Secure T2I Diffusion 21 (a) (b) Fig. 5: Additional Results of (a) Van Gogh and (b) church for Single-Timestep Ad- versarial Attack Efficacy. It is observed that the perturbed text embedding c + δt∗ can reproduce images containing the previously erased concept even when the adversarial attack is applied at a singular timestep t∗. set to η = 1. Consequently, we maintain this setting by also assigning η = 1 within LRACE for consistency. We train models using LRACE, allocating 3,000 iterations for style and ex- plicit concepts, and 2,000 iterations for object categories. These iteration counts are tailored to the specific nature of each target concept. Similar to the ESD framework, RACE does not require supplementary prompts or images for con- cept erasure. Consistent with ESD’s methodology, a brief textual description of the target concept suffices for its removal from the Stable Diffusion model [46], underscoring the efficiency and simplicity of our approach. C Additional Analysis of Single-Timestep Adversarial Attack In order to demonstrate the effectiveness of the proposed single-timestep ad- versarial attack, we perform additional experiments on Φθ models, each trained to erase the “Van Gogh” and “church” concepts via the ESD method [10]. The results, as illustrated in Fig. 5, demonstrate that a single-timestep adversarial attack can successfully reconstruct previously erased concepts. D Extended Visual Results from RACE In Figures Figs. 6 to 8, we present a supplementary collection of images to further demonstrate the capability of our method in erasing targeted concepts. It is important to note that the set of images in Fig. 7 contains explicit content. 22 C. Kim⋆, K. Min⋆, Y. YangOriginalSDR.A.C.E. Fig. 6: Comparison Between Original SD and RACE-“Van Gogh” Table 5: Attack Success Rate (ASR) when varying ϵ and the number of adversarial attack steps. For each entry, we report three ASR values when ϵ = 0.05, 0.1, and 0.2 using a tuple enclosed in parentheses. # of adversarial attack steps 10 20 RACE-VanGogh (0.88, 0.88, 0.88) (0.90, 0.92, 0.94) RACE-Nudity (0.94, 0.96, 0.99) (0.94, 0.99, 0.97) RACE-Violence (0.96, 0.98, 1.00) (0.95, 0.99, 0.99) RACE-Illegal (0.92, 0.97, 1.00) (0.95, 0.97, 0.97) RACE-Church (0.70, 0.86, 0.90) (0.74, 0.82, 0.96) RACE-GolfBall (0.74, 0.82, 0.96) (0.32, 0.56, 0.74) RACE-Parachute (0.60, 0.68, 0.88) (0.56, 0.76, 0.90) E Determining PGD Hyperparameters Within the main paper, we settle on ϵ = 0.1 and designate 10 steps for the adversarial attack. This section delves into the empirical analysis underpinning this selection, particularly focusing on the balance between ASR efficacy and hyperparameter tuning. As delineated in Tab. 5, an ϵ value of 0.2 marginally outperforms 0.1. However, as elaborated in the main manuscript, an elevated at- tack intensity with ϵ = 0.2 may compromise the equilibrium between adversarial robustness and the fidelity of generated images. Holding ϵ at 0.1, we observe a minimal variance between 10 and 20 attack steps, reinforcing our decision to configure PGD parameters at ϵ = 0.1 and 10 steps for an optimal trade-off. F Challenges in Erasing Violence and Illegal Act Table 6: Comparison of ASR decrease and CLIP-Score for different keywords Keyword Nudity Violence Illegal Act ASR Decrease -30% -11% -5% CLIP-Score [14] 0.658 0.533 0.526 The data presented in Table 2 of the main manuscript indicate that “violence” and “illegal act” concepts exhibit less pronounced reductions in Attack Success Rate (ASR) against UnlearnDiff white box attack [72]. Specifically, “nudity” sees R.A.C.E. for Secure T2I Diffusion 23Original SDR.A.C.E. Illegal ActOriginal SDR.A.C.E. ViolenceOriginal SDR.A.C.E. * * * * * * * Nudity Added by authors * Fig. 7: Comparison Between Original SD and RACE: For representations involving explicit or sensitive content, manual modifications have been applied (e.g., addition of censoring boxes or application of significant blurring) to ensure appropriateness for submission. a 30% decrease in ASR, while “violence” and “illegal act” experience reductions of 11% and 5%, respectively. To align with the experimental framework of Un- learnDiff [72], we employed “violence” and “illegal act” as keywords for con- cept removal. It is postulated that these terms may not optimally represent the range of hazardous imagery producible by Stable Diffusion (SD) [46] through the prompts in the I2P dataset [51]. To examine this hypothesis, we calculate the CLIP-score [14] for explicit images generated by a baseline SD model, SD(˜y), where ˜y refers prompts from I2P and c includes “nudity”, “violence”, and “illegal act” (i.e. CLIP-score(SD(˜y),c)). As illustrated in ??, a higher CLIP-score, indi- cating better keyword alignment with the dangerous images, correlates with a more substantial decrease in ASR. Conversely, keywords that poorly match the hazardous image content tend to result in less significant ASR reductions. This underscores the critical importance of selecting highly representative keywords for concept erasure in T2I models, as their alignment with the generated content significantly influences the effectiveness of concept erase. 24 C. Kim⋆, K. Min⋆, Y. YangOriginalSDR.A.C.E.OriginalSDR.A.C.E.OriginalSDR.A.C.E. Church Golf Ball Parachute Fig. 8: Comparison Between Original SD and RACE.","libVersion":"0.3.2","langs":""}