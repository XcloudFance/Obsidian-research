{"path":"GenAIUnleaning/DiffusionUnlearning/2024/Erasing Concepts from.pdf","text":"Erasing Concepts from Diffusion Models Rohit Gandikota ∗,1 Joanna Materzy´nska∗,2 Jaden Fiotto-Kaufman 1 David Bau 1 1Northeastern University 2Massachusetts Institute of Technology 1[gandikota.ro, fiotto-kaufman.j, davidbau]@northeastern.edu 2jomat@mit.edu * Erasing Nudity Original Model Edited Model Erased from model: “Nudity” * * * * * Erasing Artistic Style Erasing Objects Original Model Edited Model Erased from model: “Car” Original Model Edited Model Erased from model: “Van Gogh”* Added by authors for publication Figure 1: Given only a short text description of an undesired visual concept and no additional data, our method fine-tunes model weights to erase the targeted concept. Our method can avoid NSFW content, stop imitation of a specific artist’s style, or even erase a whole object class from model output, while preserving the model’s behavior and capabilities on other topics. Abstract Motivated by concerns that large-scale diffusion models can produce undesirable output such as sexually explicit content or copyrighted artistic styles, we study erasure of specific concepts from diffusion model weights. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually ex- plicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments eras- ing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at erasing.baulab.info. *Equal contribution 1. Introduction Recent text-to-image generative models have attracted at- tention due to their remarkable image quality and seemingly infinite generation capabilities. These models are trained on vast internet datasets, which enables them to imitate a wide range of concepts. However, some concepts learned by the model are undesirable, including copyrighted content and pornography, which we aim to avoid in the model’s out- put [27, 16, 29]. In this paper, we propose an approach for selectively removing a single concept from a text-conditional model’s weights after pretraining. Prior approaches have focused on dataset filtering [30], post-generation filtering [29], or inference guiding [38]. Unlike data filtering meth- ods, our method does not require retraining, which is pro- hibitive for large models. Inference-based methods can cen- sor [29] or steer the output away from undesired concepts effectively [38], but they can be easily circumvented. In contrast, our approach directly removes the concept from the model’s parameters, making it safe to distribute its weights. The open-source release of the Stable Diffusion text-to-arXiv:2303.07345v3 [cs.CV] 21 Jun 2023 image diffusion model has made image generation technol- ogy accessible to a broad audience. To limit the generation of unsafe images, the first version was bundled with a simple NSFW filter to censor images if the filter is triggered [29], yet since both the code and model weights are publicly avail- able, it is easy to disable the filter [43]. In an effort to prevent the generation of sensitive content, the subsequent SD 2.0 model is trained on data filtered to remove explicit images, an experiment consuming 150,000 GPU-hours of computa- tion [32] over the 5-billion-image LAION dataset [39]. The high cost of the process makes it challenging to establish a causal connection between specific changes in the data and the capabilities that emerge, but users report that removing explicit images and other subjects from the training data may have had a negative impact on the output quality [30]. And despite the effort, explicit content remains prevalent in the model’s output: when we evaluate generation of images using prompts from the 4,703 prompts of the Inappropriate Image Prompts (I2P) benchmark [38], we find that the popu- lar SD 1.4 model produces 796 images with exposed body parts identified by a nudity detector, while the new training- set-restricted SD 2.0 model produces 417 (Figure 7). Another major concern regarding the text-to-image mod- els is their ability to imitate potentially copyrighted content. Not only is the quality of the AI-generated art on par with the human-generated art [34], it can also faithfully replicate an artistic style of real artists. Users of Stable Diffusion [31] and other large-scale text-to-image synthesis systems have discovered that prompts such as “art in the style of [artist]” can mimic styles of specific artists, potentially devaluing original work. Copyright concerns of several artists has led to a lawsuit against the makers of Stable Diffusion [1], raising new legal issues [41]; the courts have yet to rule on these cases. Recent work [42] aims to protect the artist by applying an adversarial perturbation to artwork before posting it online to prevent the model from imitating it. That approach, however, cannot remove a learned artistic style from a pretrained model. In response to safety and copyright infringement con- cerns, we propose a method for erasing a concept from a text-to-image model. Our method, Erased Stable Diffusion (ESD), fine-tunes the model’s parameters using only unde- sired concept descriptions and no additional training data. Unlike training-set censorship approaches, our method is fast and does not require training the whole system from scratch. Furthermore, our method can be applied to existing models without the need to modify input images [42]. Un- like the post-filtering [29] or simple blacklisting methods, erasure cannot be easily circumvented, even by users who have access to the parameters. We benchmark our method on removing offensive content and find that it is as effective as Safe Latent Diffusion [38] for removing offensive images. We also test the ability of our method to remove an artistic style from the model. We conduct a user study to test the impact of erasure on user perception of the remove artist’s style in output images, as well as the interference with other artistic styles and their impact on image quality. Finally, we also test our method on erasure of complete object classes. 2. Related Works Undesirable image removal. Previous work to avoid undesirable image output in generative models has taken two main approaches: The first is to censor images from the training set, for example, by removing all people [25], or by more narrowly curating data to exclude undesirable classes of images [39, 27, 33]. Dataset removal has the disadvantage that the resources required to retrain large models makes it a very costly way to respond to problems discovered after training; also large-scale censorship can produce unintended effects [26]. The second approach is post-hoc, modifying output after training using classifiers [3, 21, 29], or by adding guidance to the inference process [38]; such methods are efficient to test and deploy, but they are easily circumvented by a user with access to parameters [43]. We compare both previous approaches including Stable Diffusion 2.0 [30], which is a complete retraining of the model on a censored training set, and Safe Latent Diffusion [38], which the state- of-the-art guidance-based approach. The focus of our current work is to introduce a third approach: we tune the model parameters using a guidance-based model-editing method, which is both fast to employ and also difficult to circumvent. Image cloaking. Another approach to protecting images from imitation by large models is for an artist to cloak im- ages by adding adversarial perturbations before posting them on the internet. Cloaking allows artists to effectively hide their work from a machine-learned model during training or inference by adding perturbations that cause the model to confuse the cloaked image with an unrelated image [36] or an image with a different artistic style [42]; the method is a promising way for an artist to self-censor their own content from AI training sets while still making their work visible to humans. Our paper addresses a different problem than the problem addressed by cloaking: we ask how a model creator can erase an undesired visual concept without active self-censorship by content providers. Model editing. As the cost of training grows, there has been increasing interest in lightweight model-editing meth- ods that alter the behavior of large-scale generative models given little or no new training data. In text generators, a model’s knowledge of facts can be edited based on a single statement of the fact by modifying specific neurons [7] or layers [23], or by using hypernetworks [8, 24]. In image syn- thesis, a generative adversarial network (GAN) can be edited using a handful of words [14], a few sketches [46], warping gestures [47], or copy-and-paste [2]. Recently, it has been shown that text-conditional diffusion models can be edited ϵθ(xt, c, t) ϵθ*(xt, t) ϵθ*(xt, c, t) Frozen Original SD θ* ϵθ*(xt, t) − η[ϵθ*(xt, c, t) − ϵθ*(xt, t)] (xt, c, t) (xt, c0, t) (xt, c, t) Fine Tune ESD θ “Van Gogh”, concept to erasext c “ ”c0 L2 Loss , generated by θ Time step sampled uniformlyt Figure 2: The optimization process for erasing undesired visual concepts from pre-trained diffusion model weights involves using a short text description of the concept as guidance. The ESD model is fine-tuned with the conditioned and unconditioned scores obtained from frozen SD model to guide the output away from the concept being erased. The model learns from its own knowledge to steer the diffusion process away from the undesired concept. by associating a token for a new subject trained using only a handful of images [13, 20, 35]. Unlike previous methods that add or modify the appearance of objects, the goal of our current work is to erase a targeted visual concept from a diffusion model given only a single textual description of the concept, object, or style to be removed. Memorization and unlearning. While the traditional goal of machine learning is to generalize without memo- rization, large models are capable of exact memorization if specifically trained to do so [48], and unintentional memo- rization has also been observed in large-scale settings [6, 5], including diffusion models [45]. The possibility of such ex- act memorization has driven privacy and copyright concerns and has led to work in machine unlearning [40, 4, 15], which aims to modify a model to behave as if particular training data had not been present. However, these methods are based on the assumption that the undesired knowledge corresponds to an identifiable set of training data points. The problem we tackle in this paper is very different from the problem of un- learning specific training data because rather than simulating the removal of a known training item, our goal is to erase a high-level visual concept that may have been learned from a large and unknown subset of the training data, such as the appearance of nudity, or the imitation of an artist’s style. Energy-based composition. Our work is inspired by the observation [10, 11] that set-like composition can be performed naturally on energy-based models and diffusion counterparts [22] naturally via arithmetic on the score or the noise predictions. Score-based composition is also the basis for classifier-free-guidance [18]. Like previous works, we treat “A and not B” as the difference between log probability densities for A and B; a similar observation has been used to reduce the undesirable output of both language models [37] and vision generators [38]. Unlike previous work that applies composition at inference time, we introduce the use of score composition as a source of unsupervised training data to teach a fine-tuned model to erase an undesired concept from model weights. 3. Background 3.1. Denoising Diffusion Models Diffusion models are a class of generative models that learn the distribution space as a gradual denoising pro- cess [44, 17]. Starting from sampled Gaussian noise, the model gradually denoises for T time steps until a final image is formed. In practice, the diffusion model predicts noise ϵt at each time step t that is used to generate the intermedi- ate denoised image xt; where xT corresponds to the initial noise and x0 corresponds to the final image. This denoising process is modeled as a Markov transition probability. pθ(xT :0) = p(xT ) 1∏ t=T pθ(xt−1|xt) (1) 3.2. Latent Diffusion Models Latent diffusion models (LDM) [31] improve efficiency by operating in a lower dimensional latent space z of a pre- trained variational autoencoder with encoder E and decoder D. During training, for an image x, noise is added to its encoded latent, z = E(x) leading to zt where the noise level increases with t. LDM process can be interpreted as a se- quence of denoising models with identical parameters θ that learn to predict the noise ϵθ(zt, c, t) added to zt conditioned on the timestep t as well as a text condition c. The following objective function is optimized: L = Ezt∈E(x),t,c,ϵ∼N (0,1)[∥ϵ − ϵθ(zt, c, t)∥ 2 2] (2) Classifier-free guidance is a technique employed to regu- late image generation, as described in Ho et al. [18]. This method involves redirecting the probability distribution to- wards data that is highly probable according to an implicit classifier p(c|zt). This approach is used during inference and requires that the model be jointly trained on both con- ditional and unconditional denoising. The conditional and unconditional scores are both obtained from the model dur- ing inference. The final score ˜ϵθ(zt, c, t) is then directed towards the conditioned score and away from the uncondi- tioned score by utilizing a guidance scale α > 1. ˜ϵθ(zt, c, t) = ϵθ(zt, t) + α(ϵθ(zt, c, t) − ϵθ(zt, t)) (3) The inference process starts from a Gaussian noise zT ∼ N (0, 1) and is denoised with the ˜ϵθ(zT , c, T ) to get zT −1. This process is done sequentially till z0 and is transformed to image space using the decoder x0 ← D(z0). 4. Method The goal of our method is to erase concepts from text- to-image diffusion models using its own knowledge and no additional data. Therefore, we consider fine-tuning a pre- trained model rather than training a model from scratch. We focus on Stable Diffusion (SD) [31], an LDM that consists of 3 subnetworks: a text encoder T , a diffusion model (U-Net) θ∗ and a decoder model D. We shall train new parameters θ. Our approach involves editing the pre-trained diffusion U-Net model weights to remove a specific style or concept. We aim to reduce the probability of generating an image x according to the likelihood that is described by the concept, scaled by a power factor η. Pθ(x) ∝ Pθ∗ (x) Pθ∗ (c|x)η (4) Where Pθ∗ (x) represents the distribution generated by the original model and c represents the concept to erase. Expand- ing P (c|x) = P (x|c)P (c) P (x) , the gradient of the log probability ∇ log Pθ(x) would be proportional to: ∇ log Pθ∗ (x) − η(∇ log Pθ∗ (x|c) − ∇ log Pθ∗ (x)) (5) Based on Tweedie’s formula [12] and the reparametriza- tion trick of [17], we can introduce a time-varying noising process and express each score (gradient of log probability) as a denoising prediction ϵ(xt, c, t). Thus Eq. 5 becomes: ϵθ(xt, c, t) ← ϵθ∗ (xt, t) − η[ϵθ∗ (xt, c, t) − ϵθ∗ (xt, t)] (6) This modified score function moves the data distribution to minimize the generation probability of images x that can be labeled as c. The objective function in Equation 6 fine-tunes the parameters θ such that ϵθ(xt, c, t) mimics the negatively guided noise. That way, after the fine-tuning, the edited model’s conditional prediction is guided away from the erased concept. Figure 2 illustrates our training process. We exploit the model’s knowledge of the concept to synthesize training samples, thereby eliminating the need for data collection. Training uses several instances of the diffusion model, with “car” Prompt (a) Cross-attention (b) Self-attention (c) Generated image “ ” Figure 3: When comparing generation of two similar car images conditioned on different prompts, self-attention (b) contributes to the features of a car regardless of the pres- ence of the word “car” in the prompt, while the contribution of cross-attention (a) is linked to the presence of the word. Heatmaps show local contributions of the first attention mod- ules of the 3rd upsampling block of the Stable Diffusion U-net while generating the images (c). one set of parameters frozen (θ∗) while training the other set of parameters (θ) to erase the concept. We sample partially denoised images xt conditioned on c using θ, then we per- form inference on the frozen model θ∗ twice to predict the noise, once conditioned on c and the other unconditioned. Finally, we combine these two predictions linearly to negate the predicted noise associated with the concept, and we tune the new model towards that new objective. 4.1. Importance of Parameter Choice The effect of applying the erasure objective (6) depends on the subset of parameters that is fine-tuned. The main distinction is between cross-attention parameters and non- cross-attention parameters. Cross-attention parameters, illus- trated in Figure 3a, serve as a gateway to the prompt, directly depending on the text of the prompt, while other parameters (Figure 3b) tend to contribute to a visual concept even if the concept is not mentioned in the prompt. Therefore we propose fine tuning the cross attentions, ESD-x, when the erasure is required to be controlled and specific to the prompt, such as when a named artistic style should be erased. Further, we propose fine tuning uncondi- tional layers (non-cross-attention modules), ESD-u, when the erasure is required to be independent of the text in the prompt, such as when the global concept of NSFW nudity should be erased. We refer to cross-attention-only fine- tuning as ESD-x-η (where η refers to the strength of the negative guidance), and we refer to the configuration that tunes only non-cross-attention parameters as ESD-u-η. For simplicity, we write ESD-x and ESD-u when η = 1. The effects of parameter choices on artist style removal are illustrated in Figure 4: when erasing the “Van Gogh” style ESD-u and other unconditioned parameter choices Original SD All Layers ESD-u Self Attentions ESD-xErasure of artist (More change is better)Interference with other artists (Less change is better)The Great Wave of Kanagawa by Hokusai Girl with a pearl earring by Johannes VermeerThe scream by Edvard MunchThe Bedroom in Arles by Van GoghThe Starry Night by Van Gogh Figure 4: Modifying the cross-attention weights, ESD-x, shows negligible interference with other styles (bottom 3 rows) and is thus well-suited for erasing art styles. In con- trast, altering the non-cross-attention weights, ESD-u, has a global erasure effect (all rows) on the visual concept and is better suited for removing nudity or objects. erase aspects of the style globally, erasing aspects of Van Gogh’s style from many artistic styles other than Van Gogh’s. On the other hand, tuning the the cross-attention parameters only (ESD-x) erases the distinctive style of Van Gogh specif- ically when his name is mentioned in the prompt, keeping the interference with other artistic styles to a minimum. Conversely, when removing NSFW content it is important that the visual concept of “nudity” is removed globally, espe- cially in cases when nudity is not mentioned in the prompt. To measure those effects we evaluate on a data set that in- clude many prompts that do not explicitly mention NSFW terms (Section 5.2). We find that ESD-u performs best in this application; full quantitative ablations over different parameter sets are included in Appendix E.2. 5. Experiments We train all our models for 1000 gradient update steps on a batch size of 1 with learning rate 1e-5 using the Adam optimizer. Depending on the concept we want to remove (4.1), the ESD-x method fine-tunes the cross-attention and ESD-u fine tunes the unconditional weights of the U-Net module in Stable Diffusion (our experiments use version 1.4 unless specified otherwise). Baseline methods are: • SD (pretrained Stable Diffusion), • SLD (Safe Latent Diffusion) [38], to adapt the method to our experiment, we substitute the concept we want to erase from the model for the original safety concepts. • SD-Neg-Prompt (Stable Diffusion with Negative Prompts), an inference technique in the community, that aims to steer away from unwanted effects in an image. We adapt this method by using the artist’s name as the negative prompt. 5.1. Artistic Style Removal 5.1.1 Experiment Setup To analyze imitation of art among contemporary practicing artists, we consider 5 modern artists and artistic topics; Kelly McKernan, Thomas Kinkade, Tyler Edlin, Kilian Eng and the series “Ajin: Demi-Human,” which have been reported to be imitated by Stable Diffusion. While we did not observe the model making direct copies of specific original artwork, it is undeniable that these artistic styles have been captured by the model. To study this effect, we demonstrate quali- tative results in Fig 5 and conduct a user study to measure the human perception on the artistic removal effect. Our experiments validate the observation that the particular artist- specific style is removed from the model, while the content and structure of the prompt is preserved (Fig 5) with min- imal interference on other artistic styles. For more image examples, please refer to Appendix E. 5.1.2 Artistic Style Removal User Study To measure the human perception of the effectiveness of the removed style, we conducted a user study. For each artist, we collect 40 images of art created by those artists, using Google Image Search to identify top-ranked works. Then for each artist, we also compose 40 generic text prompts that invoke the artist’s style, and we use Stable Diffusion to generate images for each artist using these prompts, for example: “Art by [artist]”, “A design of [artist]”, “An image in the style of [artist]”, “A reproduction of the famous art of [artist]”. We also evaluate images from edited diffusion models, as described in Section 5.1.1 as well as the baseline models. The images were generated with 4 seeds per prompt (same seeds were used for all methods) resulting in a dataset of 1000 images. Furthermore, we also included images from a similar human artist for each of the five artists. We pair real artist with similar real artist as follows: (Kelly McKer- nan, Kirbi Fagan), (Thomas Kinkade, Nicky Boehme), (Ajin: Demi Human, Tokyo Ghoul), (Tyler Edlin, Feng Zhu), (Kil- ian Eng, Jean Giraud). For each similar artist, we collected 12-25 works to use in our study. In our study, participants were presented with a set of five real artwork images along with an additional image. The additional image was either a real artwork from the same artist or a similar artist, or a synthetic image generated using a prompt involving the artist name with our method (ESD- x) or other baseline methods (SLD and SD-Neg-Prompt) applied to remove the artist or a random different artist. Original SD Our Method Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Safe Concept “Thomas Kinkade” Safe Concept “Kilian Eng” Safe Concept “Kelly McKernan” SLDThomas Kinkade inspired depiction of a peaceful parkPost-apocalyptic landscape by Kilian EngWhimsical creatures with floral elements by Kelly McKernanPrompt Conditioning Figure 5: Our method has a better erasure on intended style with a minimal interference compared to SLD [38]. The images enclosed in blue dotted borders are the intended erasure, and the off-diagonal images show effect on untargeted styles. (a) Baselines (b) Erasing targeted style (lower is better) (c) Preserving other styles (higher is better) SDSimilar Artist Real Artist ESD-x (Ours) SLD SD-Neg Prompt ESD-x (Ours) SLD SD-Neg PromptAvg. Rating 2.5 3.5 3.0 4.0 2.0 1.5 1.0 Figure 6: User study ratings (with ± 95% confidence inter- vals) show that our method erases the intended style better than the baselines. The rating (1-5) represent the similarity of the images compared to original artist style (5 being most similar). With higher ratings for images from similar style artists, the study shows that style is highly subjective. Participants were asked to estimate, on a five-point Likert scale, their confidence level that the experimental image was created by the same artist as the five real artworks. Our study involved 13 total participants, with an average of 170 responses per participant. We evaluated the effec- tiveness of the ESD-x method for removing the style of five modern artists and measuring the resemblance of generated artistic images to real images. Additionally, we assessed the amount of interference introduced by our method in com- parison to other baseline methods, measuring the extent to which other artistic styles were affected. The findings of our investigation are presented in Figure 6. Interestingly, even for the genuine artwork, there is some level of uncertainty about its authenticity. The average rat- ing for the original images is 3.85, the average rating for artists similar to the selected artists is 3.16, while the average rating for the AI-generated art is 3.21, indicating that the AI-duplicates are rated higher than similar genuine artwork. These outcomes reinforce our observations that the model effectively captures the style of an artist. All three removal techniques effectively decrease the perceived artistic style, with average ratings of 1.12, 2.00, and 2.22 for ESD-x, SLD and SD-Neg-Prompt respectively. In addition to assessing the removal of an artistic style, we are also interested in evaluating the interference of our method with different artistic styles. To accomplish this, we present our research participants with images produced using a text prompt referring to an artist that was not erased in a model generated with an erased artist. We compare our method to SLD and SD-Neg-Prompt in this experiment. The outcomes, presented in Figure 5, indicate that users are most likely to consider images generated using our method to be genuine artwork, as opposed to those generated using other removal techniques, indicating that our method does not interfere with other artistic styles as a result of removing an artist. It is worth noting that, unlike the two baselines, our method modifies the model permanently, rather than being an inference approach. Our method can also be applied to erase single works of Figure 7: Our method effectively removes nudity content from Stable Diffusion on I2P data, outperforming inference method, SLD [38] and model trained on NSFW filtered dataset, SD-V2.0 [30]. (SD-V2.1, also shown, filters less aggressively.) The figure shows the percentage change in the nudity-classified samples compared to the original SD-V1.4 model. The SD v1.4 produces 796 images with exposed body parts on the test prompts, and our method reduces this total to 134. Method FID-30k CLIP REAL - 0.1561 SD 14.50 0.1592 SLD-Medium 1 16.90 0.1594 SLD-Max 18.76 0.1583 ESD-u 13.68 0.1585 ESD-u-3 17.27 0.1586 Table 1: Our method shows better image fidelity perfor- mance compared to SLD and Stable Diffusion on COCO 30k images generated with inference guidance α = 7.5. All the methods show good CLIP score consistency with SD. art rather than entire artistic styles; we describe and analyze this variant in Appendix C. 5.2. Explicit Content Removal Recent works have addressed the challenge of NSFW con- tent restriction either through inference modification [38], post-production classification based restriction [31] or re- training the entire model with NSFW restricted subset of LAION dataset [30]. Inference and post-production clas- sification based methods can be easily circumvented when the models are open-sourced [43]. Retraining the models on filtered data can be very expensive, and we find that such models (Stable Diffusion V2.0) are still capable of generat- ing nudity, as seen in Figure 7. Since the erasure of unsafe content like nudity requires the effect to be global and independent of text embeddings, we use ESD-u to erase \"nudity\". In Figure 7, we compare the percentage change in nudity classified samples with respect to Stable Diffusion v1.4. We study the effectiveness of our method with both inference method (SLD [38]) and filtered 1These numbers are taken from the SLD paper [38]. In our experiments, we find the FIDs to be 18.71 and 25.29 for Medium and Max respectively re-training methods (SD V2.0 [30]). For all the models, 4703 images are generated using I2P prompts from [38]. The images are classified into various nudity classes using the Nudenet [28] detector. For this analysis, we show results for our weak erasure scale of η = 1. We find that across all the classes, our method has a more significant effect in erasing nudity2. For a more similar comparison study that was done by [38], please refer to Appendix E.2. To ensure that the erased model is still effective in generat- ing safe content, we compare all the methods’ performance on COCO 30K dataset prompts. We measure the image fidelity to show the quality and CLIP score to show speci- ficity of the model to generate conditional images in table 1. ESD-u refers to soft erasure with η = 1 and ESD-u-3 refers to stronger erasure with η = 3. Since COCO is a well curated dataset without nudity, this could be a reason for our method’s better FID compared to SD. All the methods have similar CLIP scores as SD showing minimal effect specificity. 5.3. Object Removal In this section, we investigate the extent to which method can also be used to erase entire object classes from the model. We prepare ten ESD-u models, each removing one class name from a subset of the ImageNet classes [9] (we invest- giate the Imagenette [19] subset which comprises ten easily identifiable classes). To measure the effect of removing both the targeted and untargeted classes, we generate 500 images of each class using base Stable Diffusion as well as each of the ten fine-tuned models using the prompt “an image of a [class name]”; then we evaluate the results by examining the top-1 predictions of a pretrained Resnet-50 Imagenet classifier. Table 2 displays quantitative results, comparing 2We note that Nudenet [28] has a higher false positive rate for male genitalia class. With manual cross verification, we find that both our method and SLD completely erases male genitalia, while SD V2.0 erases by 60% Class name Accuracy of erased class Accuracy of other classes SD ESD-u SD ESD-u cassette player 15.6 0.60 85.1 64.5 chain saw 66.0 6.0 79.6 68.2 church 73.8 54.2 78.7 71.6 gas pump 75.4 8.6 78.5 66.5 tench 78.4 9.6 78.2 66.6 garbage truck 85.4 10.4 77.4 51.5 English springer 92.5 6.2 76.6 62.6 golf ball 97.4 5.8 76.1 65.6 parachute 98.0 23.8 76.0 65.4 French horn 99.6 0.4 75.8 49.4 Average 78.2 12.6 78.2 63.2 Table 2: Our method can cleanly erase many object concepts from a model, evidenced here a significant drop in classifica- tion accuracy of the concept while keeping the other class scores high. We measure the extent to which erasing an ob- ject class from the model affects the scores of other classes classification accuracy of the erased class in both the origi- nal Stable Diffusion model and our ESD-u model trained to eliminate the class. The table also shows the classification accuracy when generating the remaining nine classes. It is evident that our approach effectively removes the targeted classes in most cases, although there are some classes such as “church” that are more difficult to remove. Accuracy of un- targeted classes remains high, but there is some interference, for example, removing “French horn” adds noticible distor- tions to other classes. Images showing the visual effects of object erasure are included in Appendix E.3. 5.4. Effect of η on Interference To measure the effect of η on interference, we test three different \"nudity\" erased ESD-u-η models’ performance on 1000 images per each of the 10 Imagenette [19] classes. Setting η = 10 erases 92% of nudity cases but reduces 1000-way classification accuracy by 34% on object images, while η = 3 erases 88% and impacts objects by 14%, and η = 1 erases 83% and impacts objects by 7%. These results suggest that reducing the value of η can mitigate interference, although reducing η also reduces the efficacy of erasing the targeted concept. Reducing η also improves image quality, as indicated in Table 1, so the appropriate η to choose will depend on the application. We also show that using generic prompts can erase the synonymous concepts in Appendix D. 5.5. Limitations For both NSFW erasure and artistic style erasure, we find that our method is more effective than baseline approaches on erasing the targeted visual concept, but when erasing large concepts such as entire object classes or some par- ticular styles, our method can impose a trade-off between Erasure Interference with Unrelated Artistic Style Original Model Erasing “Church” Incomplete Concept Erasure Original Model Erasing “Parachute” Erasing “Rembrandt” Erasing “Van Gogh” Original Model “Van Gogh Art” Original Model “Picasso Art” Figure 8: Cases of incomplete concept erasures and style interference with our method. When erasing concepts from Stable Diffusion, the model, sometimes, tends to erase only the main elements like crosses in case of church and texture in case of parachute. Also, with artistic style erasure, our method occasionally tends to interfere with other styles. This has been reflected in the user study in Section 5.1.2 complete erasure of a visual concept and interference with other visual concepts. In Figure 8 we illustrate some of the limitations. We quantify the typical level of interference dur- ing art erasure in the user study conducted in Section 5.1.2. When erasing entire object classes, our method will fail on some classes, erasing only particular distinctive attributes of concepts (such as crosses from churches and ribs from parachutes) while leaving the larger concepts unerased. Eras- ing entire object classes creates some interference to other classes, which is quantified in Section 5.3. 6. Conclusion This paper proposes an approach for eliminating specific concepts from text-to-image generation models by editing the model weights. Unlike traditional methods that require extensive dataset filtering and system retraining, our ap- proach does not involve manipulating large datasets or un- dergoing expensive training. Instead, it is a fast and efficient method that only requires the name of the concept to be removed. By removing the concept directly from the model weights, our method eliminates the need for post-inference filters and enables safe distribution of parameters. We demonstrate the efficacy of our approach in three different applications. Firstly, we show that our method can successfully remove explicit content with comparable results to the Safe Latent Diffusion method. Secondly, we demonstrate how our approach can be used to remove artistic styles, and support our findings with a thorough human study. Lastly, we illustrate the versatility of our method by applying it to concrete object classes. Code Source code, data sets, and fine-tuned model weights for reproducing our results are available at the project web- site https://erasing.baulab.info and the GitHub repository https://github.com/rohitgandikota/erasing. Acknowledgments Thanks to Antonio Torralba for valuable advice, discus- sions and support, and thanks to Fern Keniston for organizing the event where the team developed the work. RG, DB are supported by grants from Open Philanthropy and Signify. JM partially funded by ONR MURI grant N00014-22-1-2740 References [1] Sarah Andersen. et al v. Stability AI Ltd. et al. Case No. 3:2023cv00201. US District Court for the Northern District of California. Jan 2023. 2 [2] David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, and Antonio Torralba. Rewriting a deep generative model. In Proceedings of the European Conference on Computer Vision (ECCV), 2020. 2 [3] Praneeth Bedapudi. NudeNet: Neural nets for nudity detec- tion and censoring, 2022. 2 [4] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), pages 141–159. IEEE, 2021. 3 [5] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Kather- ine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. 3 [6] Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing un- intended memorization in neural networks. In 28th USENIX Security Symposium (USENIX Security 19), pages 267–284, 2019. 3 [7] Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 8493–8502, 2022. 2 [8] Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing fac- tual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491–6506, 2021. 2 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 7 [10] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models. Advances in Neural Information Processing Systems, 33:6637–6647, 2020. 3 [11] Yilun Du, Shuang Li, Yash Sharma, Josh Tenenbaum, and Igor Mordatch. Unsupervised learning of compositional en- ergy concepts. Advances in Neural Information Processing Systems, 34:15608–15620, 2021. 3 [12] Bradley Efron. Tweedie’s formula and selection bias. Journal of the American Statistical Association, 106(496):1602–1614, 2011. 4 [13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh Interna- tional Conference on Learning Representations, 2023. 3 [14] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip- guided domain adaptation of image generators. ACM Trans- actions on Graphics (TOG), 41(4):1–13, 2022. 2 [15] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9304– 9312, 2020. 3 [16] Google. Imagen, unprecedented photorealism x deep level of language understanding, 2022. 1 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. 3, 4 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 3 [19] Jeremy Howard and Sylvain Gugger. Fastai: A layered api for deep learning. Information, 11(2):108, 2020. 7, 8 [20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht- man, and Jun-Yan Zhu. Multi-concept customization of text- to-image diffusion. 2023. 3 [21] Gant Laborde. NSFW detection machine learning model, 2022. 2 [22] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual genera- tion with composable diffusion models. arXiv preprint arXiv:2206.01714, 2022. 3 [23] Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. In Advances in Neural Information Processing Systems, 2022. 2 [24] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2021. 2 [25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2 [26] Ryan O’Connor. Stable diffusion 1 vs 2 - what you need to know, 2022. 2 [27] OpenAI. DALL-E 2 preview - risks and limitations, 2022. 1, 2 [28] Bedapudi Praneeth. Nudenet: Neural nets for nudity classifi- cation, detection and selective censoring, 12 2019. 7 [29] Javier Rando, Daniel Paleka, David Lindner, Lennard Heim, and Florian Tramèr. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610, 2022. 1, 2 [30] Robin Rombach. Stable diffusion 2.0 release, Nov 2022. 1, 2, 7 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), 2022. 2, 3, 4, 7 [32] Robin Rombach and Patrick Esser. Stable diffusion v1-4 model card, 2022. 2 [33] Robin Rombach and Patrick Esser. Stable diffusion v2 model card, 2022. 2 [34] Kevin Roose. An a.i.-generated picture won an art prize. artists aren’t happy., 2022. 2 [35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven gen- eration. arXiv preprint arXiv:2208.12242, 2022. 3 [36] Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. arXiv preprint arXiv:2302.06588, 2023. 2 [37] Timo Schick, Sahana Udupa, and Hinrich Schütze. Self- diagnosis and self-debiasing: A proposal for reducing corpus- based bias in nlp. Transactions of the Association for Compu- tational Linguistics, 9:1408–1424, 2021. 3 [38] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. Safe latent diffusion: Mitigating inap- propriate degeneration in diffusion models. arXiv preprint arXiv:2211.05105, 2022. 1, 2, 3, 5, 6, 7 [39] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- man, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In Thirty-sixth Confer- ence on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 2 [40] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to for- get: Algorithms for machine unlearning. Advances in Neural Information Processing Systems, 34:18075–18086, 2021. 3 [41] Riddhi Setty. Ai art generators hit with copyright suit over artists’ images, 1 2023. 2 [42] Shawn Shan, Jenna Cryan, Emily Wenger, Haitao Zheng, Rana Hanocka, and Ben Y Zhao. Glaze: Protecting artists from style mimicry by text-to-image models. arXiv preprint arXiv:2302.04222, 2023. 2 [43] SmithMano. Tutorial: How to remove the safety filter in 5 seconds, 8 2022. 2, 7 [44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Confer- ence on Machine Learning, pages 2256–2265. PMLR, 2015. 3 [45] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. arXiv preprint arXiv:2212.03860, 2022. 3 [46] Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Sketch your own gan. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14050–14060, 2021. 2 [47] Sheng-Yu Wang, David Bau, and Jun-Yan Zhu. Rewriting geometric rules of a gan. ACM Transactions on Graphics (TOG), 41(4):1–16, 2022. 2 [48] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) re- quires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021. 3 Appendix In these supplementary materials, we show the visualiza- tion of our objective function as a motivation in Section A. In Section B we discuss the dataset details, the implementa- tion details of both the baselines and our methods. We then briefly discuss the image erasure experiment in Section C which was introduced in the main paper. We also show some visual results corresponding to artist erasure and object era- sures in Section E. Finally, we provide the details of the user study in Section F. A. Visual Motivation Our training objective is a reconstruction loss between the edited model’s (θ) conditioned noise and the negatively guided noise from frozen model (θ∗). ϵθ(xt, c, t) ← ϵθ∗ (xt, t) − η[ϵθ∗ (xt, c, t) − ϵθ∗ (xt, t)] (7) This can be interpreted as teaching the model to erase the residual noise that corresponds to the concept ϵθ∗ (xt, c, t) − ϵθ∗ (xt, t). To clearly understand this, Figure B.1 shows visual representation of the residual noise that corresponds to a particular concept. All the noises are sampled at t=10 with condition c shown in quotes. We amplify the residual noise by 10 folds and pass it through the VAE decoder D. We find that the styles and attributes of concepts are well represented within the residual scores. Negating this from unconditional noise will naturally lead to distribution without the concepts. B. Implementation Details B.1. Artist Style Erasure Method We use ESD-x, with negative guidance 1 fine- tuned for 1000 iterations with 1e-5 learning rate as our main method. We use the name of the artist as the prompt to condition for erasing the style. Baselines For baselines, we use SLD-Medium, Stable dif- fusion v1.4 and, Stable Diffusion with negative prompt (SD- NegPrompt). We use the official source code for SLD3 and diffusers implementation of Stable Diffusion 4. For SLD, we replace the default safety concept with artist name. For SD- NegPrompt, we use the artist name as the negative prompt. Dataset For user study, we use a generic dataset with prompts like \"art in the style of \". We generated a total of 1000 images using such prompts for both the erased artists and corresponding similar artists. To eval- uate them against the actual work of the artists, we 3https://github.com/ml-research/safe-latent-diffusion 4https://huggingface.co/blog/stable_diffusion Final Image ϵθ*(xt, c, t) − ϵθ*(xt, t)c: “Van Gogh”c: “Pablo Picasso”c: “Church”c: “Garbage Truck”Final Image ϵθ*(xt, c, t) − ϵθ*(xt, t) *For all the residual images: t=10 Figure B.1: Visually analysing the residual of the condi- tional and unconditional scores in image domain shows the styles/patterns representing a concept. We show both the final image and the residual noise after passing through the VAE decoder. All the scores are sampled from timestep 10 out of 50 ddim steps. The conditional scores are obtained using the prompt shown for each image. also show snapshots of the original artwork done by the artist used in the dataset. For qualitative results, we also generated 500 images using prompts from chatGPT5. We collected 20 prompts per artist by prompting chat- GPT with \"can you provide the prompts to generate images in the style of artist\". B.2. Nudity Erasure Method For nudity erasure, we present our main method, ESD-u, with negative guidance 1. For fine tuning, we use the prompt \"nudity\" and train the model for 1000 epochs with learning rate 1e-5. Baselines For baselines, we use SLD (Weak, Medium, and Max) and Stable Diffusion (v1.4, v2.0, and v2.1). We use the official source code for SLD and diffusers implementation of SD. Dataset We use the i2p dataset proposed by SLD. We use the prompts and seeds from the dataset with classifier-free guidance of 7.5 to generate 4703 images. Evaluation We use the Nudenet detector6 which detects several nudity classes in an image. We show the percentage change in number of nudity detected images compared to original SD-v1.4. Out of the 16 classes with both covered and exposed body parts, we show the effect of erasing nudity on a subset of 9 classes with exposed body parts. We used the CLIP score to measure the text-to-image alignment in our models and the baseline models and the FID score to measure 5https://chat.openai.com 6https://github.com/notAI-tech/NudeNet the image quality. We compute the FID score using the COCO-30k validation subset and the clean-fid 7 open- source implementation of the FID score. B.3. Object Erasure For object erasure, we present our main method, ESD-u, with negative guidance 1. We use the Imagnette8 subset of the imagenet dataset, which contains 10 selected classes of the original dataset. We train 10 models, each one erasing a class from the model. The classes are: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute. For fine- tuning, we use the class name as the prompt and train the model for 1000 epochs with learning rate 1e-5. C. Single Image Erasure Instead of using the model to generate the conditional images for training, ground truth images can be used for image specific erasure. We use the same algorithm but use the original image to generate partially denoised image using forward process. In Figure C.1, we show the effect of erasing the Starry Night from stable diffusion on other memorized artwork and Van Gogh’s style in general. The method has minimal effect on both Van Gogh’s style and other memorized artwork displaying a finer level of erasure effect. However, we find that simultaneously erasing multiple images from stable diffusion starts effecting other memo- rized artwork alone. It has miminal interference with stable diffusion’s ability to generate non-artwork images. This can be seen in Figure C.2 where Learned Perceptual Image Patch Similarity (LPIPS) is used to compare original image and the edited model’s image. We use the default Alex-net based settings from their source code9. Higher LPIPS score rep- resents greater change in the images. Clearly, erasing more art images has an effect on other memorized artwork while minimal effect on non-artwork generations. D. Generic Prompts for Erasure Erasing a generic prompt will reduce generation of a syn- onymous concept, even using ESD-x. We explored concept removal by applying ESD-x to five different prompts that paraphrased the concept of the Eiffel Tower: [‘Parisian Iron Lady’, ‘Famous Tower in Paris’, ‘A famous landmark of Paris’, ‘Paris’s Iconic Monument’, ‘The metallic lacework giant of Paris’]. We generated 100 images that directly ref- erenced ‘Eiffel Tower’. The original SD results contained 7https://github.com/GaParmar/clean-fid 8https://github.com/fastai/imagenette 9https://github.com/richzhang/PerceptualSimilarity 10In this analysis we also use Q16 classifier, which classifies all the toxic categories as unsafe. Erased Artwork Corresponding Artist Style Other Memorized Artwork Original Model Erasing “Starry Night” Original Model Erasing “Starry Night” Original Model Erasing “Starry Night” Figure C.1: Erasing a single artwork image does not effect the corresponding artist style or other memorized artworks. The edited model after erasing \"Starry Night\" clearly has minimal effect on Van Gogh style and other memorized artwork while effectively erasing \"Starry Night\". Figure C.2: Erasing multiple artwork images from stable diffusion has adverse effects on other unrelated memorized artworks, but has a minimal impact on the non-artwork gen- erations. We use LPIPS score to measure the distortion in images before and after the model is edited. The higher LPIPS score represents more change 79 images with the Eiffel Tower, whereas on average, the images generated with the erased models had only 38 im- ages depicting the Eiffel Tower. For a visual representation, please refer to Figure C.3, where the top row represents ‘Eif- fel Tower‘ images from SD and the bottom row are results from ESD. This suggests that our method does not overfit to specific words but instead responds to their underlying mean- ing. We have also conducted the reverse experiment (impact of removing the main wording on synonymous prompts), with similar results. SLD SLD \"nudity\" \"nudity\" \"nudity\" \"nudity\" \"i2p\" Category SD Medium Max ESD-u-1 ESD-u-3 ESD-u-10 ESD-x-3 ESD-u-1 Hate 0.40 0.20 0.09 0.25 0.19 0.13 0.30 0.17 Harrasment 0.34 0.17 0.09 0.16 0.18 0.15 0.29 0.16 Violence 0.43 0.23 0.14 0.37 0.34 0.26 0.41 0.24 Self-harm 0.40 0.16 0.07 0.32 0.24 0.18 0.35 0.22 Sexual 0.35 0.14 0.06 0.16 0.12 0.08 0.23 0.17 Shocking 0.52 0.30 0.13 0.41 0.32 0.27 0.46 0.16 Illegal activity 0.34 0.14 0.06 0.29 0.19 0.16 0.32 0.22 Table C.1: Erasing nudity with our method considerably restricts10the content from Stable Diffusion using just the prompt ”nudity”. The average probabilities of unsafe content presented here are predicted by a combined Q16/NudeNet classifier for various categories in I2P benchmark dataset. For comparison, we use standard Stable Diffusion v1.4 (SD) and Safe Latent Diffusion (SLD-Medium; SLD-Max). Figure C.3: Top: Images generate with Stable Diffusion, bottom: images generated with ESD when removing \"A famous landmark of Paris\". The prompts from left to right: \"A musician playing in front of the Eiffel Tower\", \"People biking towards the Eiffel Tower\", \"People having dinner, the Eiffel Tower in the background\". E. Extended Experimental results E.1. Artistic Style To observe the interference of a style erasure with other unrelated styles, we quantitatively measure the Learned Perceptual Image Patch Similarity (LPIPS) between the unedited and edited images in Table E.1. We do this analysis both on the erased style and unrelated styles for each artist. For erased styles, the lpips score is high (more difference) and less for unrelated art styles (less difference). We also show the analysis for 5 famous artists (Andy Warhol, Van Gogh, Pable Picasso, Rembrandt and Caravaggio). . Figures F.4-F.11 show some additional results of our model editing method to erase artistic style. Each figure illustrates both the intended erasure and undesired interfer- Erased Artist Style LPIPS Intended Undesired Ajin: Demi Human 0.46 0.15 Kelly McKernan 0.37 0.21 Kilian Eng 0.32 0.21 Thomas Kinkade 0.40 0.22 Tyler Edlin 0.34 0.22 Andy Warhol 0.41 0.19 Vincent Van Gogh 0.35 0.23 Pablo Picasso 0.32 0.21 Rembrandt 0.47 0.26 Caravaggio 0.31 0.21 Table E.1: We measure the style erasure in terms of LPIPs distance metric between the edited model image and original SD image. The higher the metric, the farther away are the images. Our method erases intended style with minimal undesired interference with other styles. ence. The figures also show the SLD performance on the style erasures for comparison. For each of the figures, the first column shows the unedited model’s generation and the remaining columns represent the images from both our method and SLD with same prompt and seed. E.2. Nudity We also compare the models across larger categories of inappropriate classes like hate, harassment, violence, self-harm, sexual, shocking and, illegal activity in Table C.1. Using a combination of Nudenet and Q1611 classifier, we show the proportion of images in a category that are classified as inappropriate by the dual classifier. Our methods that are trained to erase \"nudity\", also to some extent, reduces the inappropriateness across these broader categories. Q16 is a conservative classifier that classifies 11https://github.com/ml-research/Q16 an image as inappropriate if the image represents any of the categories mentioned above. We classify an image as inappropriate if any of the classifiers classifies as positive. To understand the effect of fine tuning different parameters for nudity erasure, we compare different settings of our method against the baselines SLD (Weak, Medium, Max) and SD (v2.0, v2.1) in Figure E.1. We find that the ESD-u-1 with \"nudity\" erased has a better efficiency in erasing nudity across all the categories. We also compare a model which erases default concept used by SLD \"hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty\"; we call it ESD-u-1 (\"i2p\"). E.3. Objects We present the class removal as well as interference with other classes in Figure F.12 and in Figure F.13,F.14 we show the intended erasure over various object classes using our method. F. User Study F.1. Design The user study was designed to measure both the effec- tiveness of our method in removing artists’ styles as well as interference with the styles of the other artists. For a given artist, participants are shown five images randomly selected from the artists real works, in order to provide points of ref- erence for the artist’s style. Participants are then also shown a singular image and asked to rate on a scale from one to five how confident they are that the image is also a real work from the chosen artist. With 36 evaluations per artist and 5 artists, participants are asked to rate 180 images. To create the batch of thirty-six evaluations for a given artist, images are grouped into nine buckets. Images are randomly sampled from these buckets and are shown to the users to rate. Two of the buckets are reference images of real art (1 from artist we erase and the other from similar artist). One is the original SD generation. Three buckets are from the models where the current artist is erased (ESD-x, SLD, and SDNG). Three more buckets to test interference of the 3 methods. These interference buckets are images of the current artist’s style, using models in which the style of other artists were removed. For example if Thomas Kinkade is the artist that is currently being evaluated, we’d show images generated in his style from models edited to remove the style of Tyler Edlin. Source Style Removal Interference SD 3.21(±.15) - ESD-x (Ours) 1.12(±.06) 2.92(±.18) SLD 2.00(±.14) 2.50(±.16) SDNG 2.22(±.16) 2.44(±.15) Real Artist 3.85(±.15) - Similar Artist 3.16(±.18) - Table F.1: The average user rating (with 95% error margin shown in paranthesis) show that our method generates least similar images compared to the original art style that is erased. While keeping the similarities high with other art styles. F.2. User Interface The participants are met with a request for participation at the outset of the user study and instructions on how to navigate as shown in Figure F.1. It explains who can partici- pate in the study as well as detailing the aspects that make it IRB compliant and must acknowledge their receipt of this information to continue. The layout is divided into two sections separated hori- zontally. The left section displays the example images for the currently selected artist as shown in Figure F.2. The right section shows the current image to be evaluated, four radio-buttons that indicate a rating as shown in Figure F.3 . Participants can also see both how many cases remain for the current artist as well as how many artists are left. F.3. Analysis We show analytical results of the study in Table F.1 with 95% confidence interval shown in paranthesis. ESD-x (our method) shows the minimum similarity for styles that are erased and maximum for the styles that are not (showing minimal interference). Figure E.1: ESD-u is more efficient in erasing nudity compared to ESD-x. I2P benchmark data consist of prompts with out explicit mention of nudity, for this reason unconditional fine tuning dominates in erasure efficiency. Apart from erasing \"nudity\" alone, we also erase longer prompt from SLD. Our method with longer prompt outperforms SLD-Medium in all categories. Our strongest guidance method (ESD-u-10), outperforms SLD-Max in all categories. Figure F.1: User study request for participation and instruc- tions to guide through the user study. Figure F.2: Reference Images shown to the users. Figure F.3: User study screenshot for the user to rate an image. Original SD Our Method SLDVan GoghRembrandtPablo PicassoArtistic Style in Prompts Conditioned Erasing “Van Gogh” Erasing “Rembrandt” Erasing “Pablo Picasso” Safety Concept “Van Gogh” Safety Concept “Rembrandt” Safety Concept “Pablo Picasso” Figure F.4: Our method has a significant erasure effect compared to SLD in erasing famous artistic styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference. Original SD Our Method Erasing “Tyler Edlin” Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Erasing “Ajin:Demi Human” SLDTyler Edlin inspired sci-fi landscape with floating islandsThomas Kinkade inspired depiction of a peaceful parkOtherworldly landscape by Kilian EngWhimsical creatures with floral elements by Kelly McKernanFantasy world with Ajin: Demi Human elementsPrompt Conditioning Safety Concept “Tyler Edlin” Safety Concept “Thomas Kinkade” Safety Concept “Kilian Eng” Safety Concept “Kelly McKernan” Safety Concept “Ajin:Demi Human” Figure F.5: Our method demonstrates a complete erasure of intended style and minimal interference with other styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference. Original SD Our Method SLDTyler EdlinThomas KinkadeKilian EngKelly McKernanAjin:Demi HumanArtistic Style in Prompts Conditioned Erasing “Tyler Edlin” Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Erasing “Ajin:Demi Human” Safety Concept “Tyler Edlin” Safety Concept “Thomas Kinkade” Safety Concept “Kilian Eng” Safety Concept “Kelly McKernan” Safety Concept “Ajin:Demi Human” Figure F.6: Our method demonstrates a complete erasure of intended style and minimal interference with other styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference. Original SD Our Method SLDTyler EdlinThomas KinkadeKilian EngKelly McKernanAjin:Demi HumanArtistic Style in Prompts Conditioned Erasing “Tyler Edlin” Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Erasing “Ajin:Demi Human” Safety Concept “Tyler Edlin” Safety Concept “Thomas Kinkade” Safety Concept “Kilian Eng” Safety Concept “Kelly McKernan” Safety Concept “Ajin:Demi Human” Figure F.7: Our method demonstrates a complete erasure of intended style and minimal interference with other styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference. Original SD Our Method SLDTyler EdlinThomas KinkadeKilian EngKelly McKernanAjin:Demi HumanArtistic Style in Prompts Conditioned Erasing “Tyler Edlin” Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Erasing “Ajin:Demi Human” Safety Concept “Tyler Edlin” Safety Concept “Thomas Kinkade” Safety Concept “Kilian Eng” Safety Concept “Kelly McKernan” Safety Concept “Ajin:Demi Human” Figure F.8: Our method demonstrates a complete erasure of intended style and minimal interference with other styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference. Original SD Our Method SLDTyler EdlinThomas KinkadeKilian EngKelly McKernanAjin:Demi HumanArtistic Style in Prompts Conditioned Erasing “Tyler Edlin” Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Erasing “Ajin:Demi Human” Safety Concept “Tyler Edlin” Safety Concept “Thomas Kinkade” Safety Concept “Kilian Eng” Safety Concept “Kelly McKernan” Safety Concept “Ajin:Demi Human” Figure F.9: Our method demonstrates a complete erasure of intended style and minimal interference with other styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference. Original SD Our Method SLDTyler EdlinThomas KinkadeKilian EngKelly McKernanAjin:Demi HumanArtistic Style in Prompts Conditioned Erasing “Tyler Edlin” Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Erasing “Ajin:Demi Human” Safety Concept “Tyler Edlin” Safety Concept “Thomas Kinkade” Safety Concept “Kilian Eng” Safety Concept “Kelly McKernan” Safety Concept “Ajin:Demi Human” Figure F.10: Our method demonstrates a complete erasure of intended style and minimal interference with other styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference. Original SD Our Method SLDTyler EdlinThomas KinkadeKilian EngKelly McKernanAjin:Demi HumanArtistic Style in Prompts Conditioned Erasing “Tyler Edlin” Erasing “Thomas Kinkade” Erasing “Kilian Eng” Erasing “Kelly McKernan” Erasing “Ajin:Demi Human” Safety Concept “Tyler Edlin” Safety Concept “Thomas Kinkade” Safety Concept “Kilian Eng” Safety Concept “Kelly McKernan” Safety Concept “Ajin:Demi Human” Figure F.11: Our method demonstrates a complete erasure of intended style and minimal interference with other styles. The blue dotted boxes show images with intended style erased. The off-diagonal images show the unintended interference.Original SDErasing “Tench”Erasing “English Springer”Erasing “Cassette Player”Erasing “Chain Saw”Erasing “Church”Erasing “French. Horn”Erasing “Garbage Truck”Erasing “Gas Pump”Erasing “Golf Ball”Erasing “Parachute” Figure F.12: Object removal in Stable Diffusion. The first row represents the original SD generations. From the later rows, the diagonal images represent the intended erasures while the off-diagonal images represent the interference.Erasing “Tench”Erasing “English Springer”Erasing “Cassette Player”Erasing “Chain Saw”Erasing “Church”Original SDOriginal SDOriginal SDOriginal SDOriginal SD Figure F.13: We show the intended erasure of objects by our method (Part 1). The rows in red-dotted box represent erasure of an object while the row above each of the red boxes represent their corresponding original SD image using the same seed and prompts.Erasing “French. Horn”Erasing “Garbage Truck”Erasing “Gas Pump”Erasing “Golf Ball”Erasing “Parachute”Original SDOriginal SDOriginal SDOriginal SDOriginal SD Figure F.14: We show the intended erasure of objects by our method (Part 2). The rows in red-dotted box represent erasure of an object while the row above each of the red boxes represent their corresponding original SD image using the same seed and prompts.","libVersion":"0.3.2","langs":""}