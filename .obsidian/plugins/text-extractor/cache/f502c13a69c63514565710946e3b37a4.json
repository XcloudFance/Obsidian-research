{"path":"GenAIUnleaning/LLM_unlearning/Wood Fisher.pdf","text":"WoodFisher: Efﬁcient Second-Order Approximation for Neural Network Compression Sidak Pal Singh∗ ETH Zurich, Switzerland contact@sidakpal.com Dan Alistarh IST Austria & Neural Magic, Inc. dan.alistarh@ist.ac.at Abstract Second-order information, in the form of Hessian- or Inverse-Hessian-vector prod- ucts, is a fundamental tool for solving optimization problems. Recently, there has been signiﬁcant interest in utilizing this information in the context of deep neural networks; however, relatively little is known about the quality of existing approximations in this context. Our work examines this question, identiﬁes issues with existing approaches, and proposes a method called WoodFisher to compute a faithful and efﬁcient estimate of the inverse Hessian. Our main application is to neural network compression, where we build on the classic Optimal Brain Damage/Surgeon framework. We demonstrate that WoodFisher signiﬁcantly outperforms popular state-of-the-art methods for one- shot pruning. Further, even when iterative, gradual pruning is considered, our method results in a gain in test accuracy over the state-of-the-art approaches, for pruning popular neural networks (like RESNET-50, MOBILENETV1) trained on standard image classiﬁcation datasets such as ImageNet ILSVRC. We examine how our method can be extended to take into account ﬁrst-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime. The code is available at the following link, https://github.com/IST-DASLab/WoodFisher. 1 Introduction The recent success of deep learning, e.g. [1, 2], has brought about signiﬁcant accuracy improvement in areas such as computer vision [3, 4] or natural language processing [5, 6]. Central to this performance progression has been the size of the underlying models, with millions or even billions of trainable parameters [4, 5], a trend which seems likely to continue for the foreseeable future [7]. Deploying such large models is taxing from the performance perspective. This has fuelled a line of work where researchers compress such parameter-heavy deep neural networks into “lighter”, easier to deploy variants. This challenge is not new, and in fact, results in this direction can be found in the early work on neural networks, e.g. [8–10]. Thus, most of the recent work to tackle this challenge can ﬁnd its roots in these classic references [11], and in particular in the Optimal Brain Damage/Surgeon (OBD/OBS) framework [8, 10]. Roughly, the main idea behind this framework is to build a local quadratic model approximation based on the second-order Taylor series expansion to determine the optimal set of parameters to be removed. (We describe it precisely in Section 4.) A key requirement to apply this approach is to have an accurate estimate of the inverse Hessian matrix, or at least have access to accurate inverse-Hessian-vector-products (IHVPs). In fact, IHVPs are a central ingredient in many parts of machine learning, most prominently for optimization [12–15], but ∗Work done while at IST Austria. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.arXiv:2004.14340v5 [cs.LG] 25 Nov 2020 also in other applications such as inﬂuence functions [16] or continual learning [17]. Applying second- order methods at the scale of model sizes described above might appear daunting, and so is often done via coarse-grained approximations (such as diagonal, block-wise, or Kronecker-factorization). However, relatively little is understood about the quality and scalability of such approximations. Motivation. Our work centers around two main questions. The ﬁrst is analytical, and asks if second-order approximations can be both accurate and scalable in the context of neural network models. The second is practical, and concerns the application of second-order approximations to neural network compression. In particular, we investigate whether these methods can be competitive with both industrial-scale methods such as (gradual) magnitude-based pruning [18], as well as with the series of non-trivial compression methods proposed by researchers over the past couple of years [19–24]. Contribution. We ﬁrst examine second-order approximation schemes in the context of convolu- tional neural networks (CNNs). In particular, we identify a method of approximating Hessian-Inverse information by leveraging the observed structure of the empirical Fisher information matrix to ap- proximate the Hessian, which is then used in conjunction with the Woodbury matrix identity [25] to provide iteratively improving approximations of Inverse-Hessian-vector products. We show that this method, which we simply call WoodFisher, can be computationally-efﬁcient, and that it faithfully represents the structure of the Hessian even for relatively low sample sizes. We note that early variants of this method have been considered previously [10, 26], but we believe we are the ﬁrst to consider its accuracy, efﬁciency, and implementability in the context of large-scale deep models, as well as to investigate its extensions. To address the second, practical, question, we demonstrate in Section 4 how WoodFisher can be used together with variants of the OBD/OBS pruning framework, resulting in state-of-the-art compression of popular convolutional models such as RESNET-50 [4] and MOBILENET [27] on the ILSVRC (ImageNet) dataset [28]. We investigate two practical application scenarios. The ﬁrst is one-shot pruning, in which the model has to be compressed in a single step, without any re-training. Here, WoodFisher easily outperforms all previous methods based on approximate second- order information or global magnitude pruning. The second scenario is gradual pruning, allowing for re-training between pruning steps. Surprisingly, even here WoodFisher either matches or outperforms state-of-the-art pruning approaches, including recent dynamic pruners [24, 29]. Our study focuses on unstructured pruning, but we can exhibit non-trivial speed-ups for real-time inference by running on a CPU framework which efﬁciently supports unstructured sparsity [30]. WoodFisher has several useful features and extensions. Since it approximates the full Hessian inverse, it can provide a global measure of parameter importance, and therefore removes the need for manually choosing sparsity targets per layer. Second, it allows us to apply compression in the limited-data regime, where either e.g. 99% of the training is unavailable, or no data labels are available. Third, we show that we can also take into account the ﬁrst-order (gradient) term in the local quadratic model, which leads to further accuracy gain, and the ability to prune models which are not fully converged. 2 Background Deterministic Setting. We consider supervised learning, where we are given a training set S = { (xi, yi)} N i=1, comprising of pairs of input examples x ∈ X and outputs y ∈ Y. The goal is to learn a function f : X ↦→ Y, parametrized by weights w ∈ Rd, such that given input x, the prediction f (x; w) ≈ y. We consider the loss function ℓ : Y × Y ↦→ R to measure the accuracy of the prediction. The training loss L is deﬁned as the average over training examples, i.e., L(w) = 1 N ∑N n=1 ℓ(yn, f (xn; w) ). The Hessian Matrix. For a twice differentiable loss L, the Hessian matrix H = ∇2 wL, takes into account the local geometry of the loss at a given point w and allows building a faithful approximation to it in a small neighbourhood δw surrounding w. This is often referred to as the local quadratic model for the loss and is given by L(w + δw) ≈ L(w) + ∇wL ⊤δw + 1 2 δw⊤H δw. Probabilistic Setting. An alternative formulation is in terms of the underlying joint distribution Qx,y = Qx Qy|x. The marginal distribution Qx is generally assumed to be well-estimated by the 2 empirical distribution ̂Qx over the inputs in the training set. As our task is predicting the output y given input x, training the model is cast as learning the conditional distribution Py|x, which is close to the true Qy|x. If we formulate the training objective as minimizing the KL divergence between these conditional distributions, we obtain the equivalence between losses ℓ (yn, f (xn; w)) = − log (pw(yn|xn) ), where pw is the density function corresponding to the model distribution. The Fisher Matrix. In the probabilistic view, the Fisher information matrix F of the model’s conditional distribution Py|x is deﬁned as, F = EPx,y [ ∇w log pw(x, y) ∇w log pw(x, y) ⊤] . (1) In fact, it can be proved that the Fisher F = EPx,y [ −∇2 w log pw(x, y)] . Then, by expressing Py,x = QxPy|x ≈ ̂QxPy|x and under the assumption that the model’s conditional distribution Py|x matches the conditional distribution of the data ̂Qy|x, the Fisher and Hessian matrices are equivalent. The Empirical Fisher. In practical settings, it is common to consider an approximation to the Fisher matrix introduced in Eq. (1), where we replace the model distribution Px,y with the empirical training distribution ̂Qx,y. Thus we can simplify the expression of empirical Fisher as follows, ˆF = E ̂Qx [E ̂Qy|x [∇ log pw(y|x)∇ log pw(y|x)⊤]] (a) = 1 N N∑ n=1 ∇ℓ (yn, f (xn; w)) ︸ ︷︷ ︸ ∇ℓn ∇ℓ (yn, f (xn; w))⊤ where (a) uses the equivalence of the loss between the probabilistic and deterministic settings. In the following discussion, we will use a shorthand ℓn to denote the loss for a particular training example (xn, yn), and refer to the Fisher as true Fisher, when needed to make the distinction relative to empirical Fisher. For a detailed exposition, we refer the reader to [31–33]. 3 Efﬁcient Estimates of Inverse-Hessian Vector Products Second-order information in the form of Inverse-Hessian Vector Products (IHVP) has several uses in optimization and machine learning [12–17]. Since computing and storing the Hessian and Fisher matrices directly is prohibitive, we will focus on efﬁcient ways to approximate this information. As we saw above, the Hessian and Fisher matrices are equivalent if the model and data distribution match. Hence, under this assumption, the Fisher can be seen as a reasonable approximation to the Hessian. Due to its structure, the Fisher is positive semideﬁnite (PSD), and hence can be made invertible by adding a small diagonal dampening term. This approximation is therefore fairly common [15, 20, 34], although there is relatively little work examining the quality of this approximation in the context of neural networks. Further, one can ask whether the empirical Fisher is a good approximation of the true Fisher. The latter is known to converge to the Hessian as the training loss approaches zero via relation to Gauss- Newton [32, 35]. The empirical Fisher does not enjoy this property, but is far more computationally- efﬁcient than the Fisher, as it can be obtained after a limited number of back-propagation steps2. Hence, this second approximation would trade off theoretical guarantees for practical efﬁciency. In the next section, we examine how these approximations square off in practice for neural networks. 3.1 The (Empirical) Fisher and the Hessian: A Visual Tour We consider the Hessian (H) and empirical Fisher ( ˆF ) matrices for neural networks trained on stan- dard datasets like CIFAR10 and MNIST. Due to practical considerations (involving the computation of full Hessian), we consider relatively small models: on CIFAR10, we utilize a fully-connected network with two hidden layers 3072 → 16 → 64 → 10, which we refer to as CIFARNET. Figure 1 compares the Hessian and empirical Fisher blocks corresponding to the second and third hidden layers of this network. Visually, there is a clear similarity between the structure of these two 2In comparison to empirical Fisher, the true Fisher requires m× more back-propagation steps for each sampled y or k× more steps to compute the Jacobian when the hessian Hℓ of the loss ℓ with respect to the network output f can be computed in a closed-form (like for exponential distributions) 3 (a) H2,2 (b) ̂F2,2 (c) H3,3 (d) ̂F3,3 Figure 1: Similarity in structures of the Hessian and empirical Fisher: Hessian (H) and empirical Fisher ( ̂F ) blocks for CIFARNET (3072 → 16 → 64 → 10) corresponding to second and third hidden layers when trained on CIFAR10. Figures have been smoothened slightly with a Gaussian kernel for better visibility. Both Hessian and empirical Fisher have been estimated over a batch of 100 examples in all the ﬁgures. matrices, for both the layers. A similar trend holds for the ﬁrst hidden layer as well as the cross-layer blocks, and likewise can be noted in the case of MNIST. Surprisingly, this behaviour occurs even if the network is not at full convergence, where we would expect the data and model distribution to match, but even at early stages of training (e.g., after one epoch of training). Please see Appendix S3 for full experimental results. This observation is consistent with recent work [36] ﬁnding high cosine similarity between the Hessian and empirical Fisher matrices just after a few gradient steps. As can be noted from the Figure 1, the main difference between these matrices is not in terms of structure, but in terms of scale. As a result, we could consider that the empirical Fisher ˆF ∝ H, modulo scaling, as long as our target application is not scale-dependent, or if we are willing to adjust the scaling through hyper-parametrization. Assuming we are willing to use the empirical Fisher as a proxy for the Hessian, the next question is: how can we estimate its inverse efﬁciently? 3.2 The WoodFisher Approximation The Woodbury Matrix Identity. Clearly, direct inversion techniques would not be viable, since their runtime is cubic in the number of model parameters. Instead, we start from the Woodbury matrix identity [25], which provides the formula for computing the inverse of a low-rank correction to a given invertible matrix A. The Sherman-Morrison formula is a simpliﬁed variant, given as (A + uv⊤)−1 = A−1 − A−1uv⊤A −1 1+v⊤A−1u . We can then express the empirical Fisher as the following recurrence, ̂Fn+1 = ̂Fn + 1 N ∇ℓn+1∇ℓn+1⊤, where ̂F0 = λId. Above, λ denotes the dampening term, i.e., a positive scalar λ times the identity Id to render the empirical Fisher positive deﬁnite. Then, the recurrence for calculating the inverse of empirical Fisher becomes: ̂F −1 n+1 = ̂F −1 n − ̂F −1 n ∇ℓn+1∇ℓ ⊤ n+1 ̂F −1 n N + ∇ℓ⊤ n+1 ̂F −1 n ∇ℓn+1 , where ̂F −1 0 = λ−1Id. (2) 4 Finally, we can express the inverse of the empirical Fisher as ̂F −1 = ̂F −1 N , where N denotes the number of examples from the dataset over which it is estimated. Stretching the limits of naming convention, we refer to this method of using the empirical Fisher in place of Hessian and computing its inverse via the Woodbury identity as WoodFisher. (a) LAYER1.0.CONV1 (b) LAYER2.0.CONV1 (c) LAYER3.0.CONV1 Figure 2: Approximation quality of the loss suggested by the local quadratic model using WoodFisher with respect to the actual training loss. The three plots measure the quality of local quadratic model along three different directions, each corresponding to pruning the respective layers to 50% sparsity. The ‘blue’ curve denotes the actual training loss along the pruning direction, while the ‘orange’ curve is the training loss predicted by the local quadratic model using WoodFisher. Approximation quality for the local quadratic model. To evaluate the accuracy of our local quadratic model estimated via WoodFisher, we examine how the loss predicted by it compares against the actual training loss. (Since we use a pre-trained network, the ﬁrst-order gradient term is ignored; we will revisit this assumption later.) We test the approximation on three different directions, each corresponding to the pruning direction (of the form H −1δw) obtained when compressing a particular layer to 50% sparsity. We choose three layers from different stages of a pre-trained RESNET-20 on CIFAR10, and Figure 2 presents these results. In all three cases, the local quadratic model using WoodFisher predicts an accurate approximation to the actual underlying loss. Further, it is possible to use the dampening λ to control whether a more conservative or relaxed estimate is needed. Overall, this suggests that the approach might be fairly accurate, a hypothesis we examine in more detail in Section 4. Computational Efﬁciency and Block-wise Approximation. While the expression in Eq. (2) goes until n = N , in practice, we found the method only needs a small subset of examples, m, typically ranging between 100 to 400. The runtime is thus reduced from cubic to quadratic in d, which can still be excessive for neural networks with millions of parameters. Thus, for large models we will need to employ a block-wise approximation, whereby we maintain and estimate limited-size blocks (‘chunks’) on the diagonal and ignore the off-diagonal parts. This block-wise simpliﬁcation is is motivated by the observation that Hessians tend to be diagonally- dominant, and has been employed in previous work, e.g. [37, 38]. Assuming uniform blocks of size c × c along the diagonal, the runtime of this inversion operation becomes O(mcd), and hence linear in the dimension d. This restriction appears necessary for computational tractability. 3.3 Context and Alternative Methods There is a large body of work utilizing second-order information in machine learning and optimization, to the extent that, it is infeasible to discuss every alternative in detail here. We therefore highlight the main methods for estimating inverse Hessian-vector products (IHVPs) in our context of neural networks. See Appendix S2 for detailed discussion. A tempting ﬁrst approach is the diagonal approximation, which only calculates the elements along the diagonal, and inverts the resulting matrix. Variants of this approach have been employed in optimization [13, 14] and model compression [20]. Yet, as we show experimentally (Figure 3a), this local approximation can be surprisingly inaccurate. By comparison, WoodFisher costs an additional constant factor, but provides signiﬁcantly better IHVP estimates. Hessian-free methods are another approach, which forgoes the explicit computation of Hessians [39] in favour of computing IHVP with a vector v by solving the linear system Hx = v for some given x. Unfortunately, a disadvantage of these methods, which we observed practically, is that they require many iterations to converge, 5 since the underlying Hessian matrix can be ill-conditioned. Neumann-series-based methods [40, 41] exploit the inﬁnite series expression for the inverse of a matrix with eigenspectrum in [0, 1]. This does not hold by default for the Hessian, and requires using the Power method to estimate the largest and smallest absolute eigenvalues, which increases cost substantially, while the Power method may fail to return the smallest negative eigenvalue. K-FAC. Kronecker-factorization (K-FAC) methods [15, 42] replace the expectation of a Kronecker product between two matrices (that arises in the formulation of Fisher blocks between two layers) as the Kronecker product between the expectations of two matrices. While this is known to be a signiﬁcant approximation [15], the beneﬁt of K-FAC is that the inverse can be efﬁciently computed [21, 22, 43, 44]. However, a signiﬁcant drawback is that the Kronecker factorization form only exists naturally for fully-connected networks. When applied to convolutional or recurrent neural networks, additional approximations [45, 46] are required in order to have a Kronecker structure, thus limiting its applicability. Furthermore, even in regards to its efﬁciency, often approximations like the chunking the layer blocks or channel-grouping are required [47]. For a quantitative comparison against WoodFisher, please refer to Section 5.1 where we show how WoodFisher can outperform K-FAC even for fully-connected networks. WoodFisher. In this context, with WoodFisher, we propose a new method of estimating second- order information that addresses some of the shortcomings of previous methods, and validate it in the context of network pruning. We emphasize that a similar approach was suggested in the early works of [10, 26] for the case of a one-hidden layer neural network. Our contribution is in extending this idea, and showing how it can scale to modern network sizes. The Woodbury matrix identity was also used in L-OBS [38] by deﬁning separate layer-wise objectives, and was applied to carefully-crafted blocks at the level of neurons. Our approach via empirical Fisher is more general, and we show experimentally that it yields better approximations at scale (c.f. Figure S1 in Section 5.1). Use of Empirical Fisher. Kunstner et al. [32] questioned the use of empirical Fisher since, as the training residuals approach zero, the empirical Fisher goes to zero while the true Fisher approaches the Hessian. However, this rests on the assumption that each individual gradient vanishes for well- optimized networks, which we did not ﬁnd to hold in our experiments. Further, they argue that a large number of samples are needed for the empirical Fisher to serve as a good approximation—in our experiments, we ﬁnd that a few hundred samples sufﬁce for our applications (e.g. Figure 1). 4 Model Compression This area has seen an explosion of interest in recent years–due to space constraints, we refer the reader to the recent survey of [11] for an overview, and here we mainly focus on closely related work on unstructured pruning. Broadly, existing methods can be split into four classes: (1) methods based on approximate second-order information, e.g. [20, 22, 38], usually set in the classical OBD/OBS framework [8, 10]; (2) iterative methods, e.g. [18, 19, 48], which apply magnitude-based weight pruning in a series of incremental steps over fully- or partially-trained models; (3) dynamic methods, e.g. [23, 24, 29, 49], which prune during regular training and can additionally allow the re-introduction of weights during training; (4) variational or regularization-based methods, e.g. [50, 51]. Recently, pruning has also been linked to intriguing properties of neural network training [52]. WoodFisher belongs to the ﬁrst class of methods, but can be used together with both iterative and dynamic methods. Optimal Brain Damage. We start from the idea of pruning (setting to 0) the parameters which, when removed, lead to a minimal increase in training loss. Denote the dense weights by w, and the new weights after pruning as w + δw. Using the local quadratic model, we seek to minimize δL = L(w + δw) − L(w) ≈ ∇wL ⊤δw + 1 2 δw⊤H δw. It is often assumed that the network is pruned at a local optimum, which eliminates the ﬁrst term, and the expression simpliﬁes to Eq. (3) ahead: δL ≈ 1 2 δw⊤H δw. (3) 6 4.1 Removing a single parameter wq Remember, our goal in pruning is to remove parameters that do not change the loss by a signiﬁcant amount. For now, let us consider the case when just a single parameter at index q is removed. The corresponding perturbation δw can be expressed by the constraint e ⊤ q δw + wq = 0, where eq denotes the qth canonical basis vector. Then pruning can be formulated as ﬁnding the optimal perturbation that satisﬁes this constraint, and the overall problem can written as follows: min δw ∈ Rd ( 1 2 δw⊤ H δw) , s.t. e ⊤ q δw + wq = 0. (4) In order to impose the best choice for the parameter to be removed, we can further consider the following constrained minimization problem. min q ∈ [d] { min δw ∈ Rd ( 1 2 δw⊤ H δw) , s.t. e⊤ q δw + wq = 0 }. (5) However, let us ﬁrst focus on the inner problem, i.e., the one from Eq. (4). As this is a constrained optimization problem, we can consider the Lagrange multiplier λ for the constraint and write the Lagrangian L(δw, λ) as follows, L(δw, λ) = 1 2 δw⊤ H δw + λ (e ⊤ q δw + wq) . (6) The Lagrange dual function g(λ), which is the inﬁmum of the Lagrangian in Eq. (6) with respect to w, can be then obtained by ﬁrst differentiating Eq. 6 and setting it to 0, and then substituting the obtained value of δw. These steps are indicated respectively in Eq. (7) and Eq. (8) below. Hδw + λeq = 0 =⇒ δw = −λH−1eq. (7) g(λ) = λ 2 2 e ⊤ q H −1eq − λ 2e ⊤ q H −1eq + λwq = − λ 2 2 e ⊤ q H −1eq + λwq. (8) Now, maximizing with respect to λ, we obtain that the optimal value λ∗ of this Lagrange multiplier as λ∗ = wq e⊤ q H−1eq = wq [H−1]qq . (9) The corresponding optimal perturbation, δw∗, so obtained is as follows: δw∗ = −wqH −1eq [H−1]qq . (10) Finally, the resulting change in loss corresponding to the optimal perturbation that removes parameter wq is, δL ∗ = w2 q 2 [H−1]qq . Going back to the problem in Eq. (5), the best choice of q corresponds to removing that parameter wq which has the minimum value of the above change in loss. We refer to this change in loss as the pruning statistic ρ, see Eq. (11), which we compute for all the parameters and then sort them in the descending order of its value. ρq = w2 q 2 [H−1]qq . (11) investigation of this direction is left for a future work. 7 4.2 Removing multiple parameters at once For brevity, consider that we are removing two distinct parameters, q1 and q2, without loss of generality. The constrained optimization corresponding to pruning can be then described as follows, min q1 ∈ [d], q2 ∈[d] { min δw ∈ Rd ( 1 2 δw⊤ H δw) , s.t. e ⊤ q1 δw + wq1 = 0, e ⊤ q2 δw + wq2 = 0, }. (12) We can see how the search space for the best parameter choices (q1, q2) grows exponentially. In general, solving this problem optimally seems to be out of hand. Although, it could be possible that the analysis might lead to a tractable computation for the optimal solution. Unfortunately, as described below, this is not the case and we will have to resort to an approximation to make things practical. L(δw, λ1, λ2) = 1 2 δw⊤ H δw + λ1 (e ⊤ q1δw + wq1) + λ2 (e ⊤ q2 δw + wq2 ) . (13) The Lagrange dual function g′(λ1, λ2), which is the inﬁmum of the Lagrangian in Eq. (13) with respect to w, can be then obtained by ﬁrst differentiating Eq. 13 and setting it to 0, and then substituting the obtained value of δw. These steps are indicated respectively in Eq. (14) and Eq. (15) below. Hδw + λ1eq1 + λ2eq2 = 0 =⇒ δw = −λ1H −1eq1 − λ2H−1eq2. (14) As a remark, we denote the Lagrange dual function here by g′ instead of g to avoid confusion with the notation for Lagrange dual function in case of a single multiplier. g′(λ1, λ2) = − λ2 1 2 e ⊤ q1H −1eq1 + λ1wq1 − λ2 2 2 e⊤ q2H−1eq2 + λ2wq2 − λ1λ2e ⊤ q1 H −1eq2. (15) Comparing this with the case when a single parameter is removed, c.f. Eq. (8), we can rewrite Lagrange dual function as follows, g′(λ1, λ2) = g(λ1) + g(λ2) − λ1λ2 e⊤ q1H−1eq2 . (16) We note that dual function is not exactly separable in terms of the dual variables, λ1 and λ2, unless the off-diagonal term in the hessian inverse corresponding to q1, q2 is zero, i.e., [H −1]q1q2 = 0. To maximize the dual function in Eq. (16) above, we need to solve a linear system with the Lagrange multipliers λ1, λ2 as variables. The equations for this system program correspond to setting the respect partial derivatives to zero, as described in Eq. (17) below, ∂g′ ∂λ1 = −λ1e ⊤ q1 H −1eq1 − λ2e ⊤ q1H−1eq2 + wq1 = 0 ∂g′ ∂λ2 = −λ1e ⊤ q1H−1eq2 − λ2e ⊤ q2H −1eq2 + wq2 = 0    Solve to obtain λ∗ 1, λ ∗ 2 (17) Hence, it is evident that exactly solving this resulting linear system will get intractable when we consider the removal of many parameters at once. 4.3 Discussion Pruning Direction. As a practical approximation to this combinatorially hard problem, when removing multiple parameters, we sort the parameters by the pruning statistic ρq, removing those with the smallest values. The overall weight perturbation in such a scenario is computed by adding the optimal weight update, Eq. (10), for each parameter that we decide to prune. (We mask the weight update at the indices of removed parameters to zero, so as to adjust for adding the weight updates separately.) We call this resulting weight update the pruning direction. If the Hessian is assumed to be diagonal, we recover the pruning statistic of optimal brain damage [8], δL ∗ OBD = 1 2 w2 q [H]qq. Further, if we let the Hessian be isotropic, we obtain the case of (global) magnitude pruning, as the statistic amounts to δL ∗ Mag = 1 2 w2 q . Note, magnitude pruning based methods, when combined with intermittent retraining, form one of the leading practical methods [18]. 8 Pruning using WoodFisher. We use WoodFisher to get estimates of the Hessian inverse required in Eq. (10). Next, the decision to remove parameters based on their pruning statistic can be made either independently for every layer, or jointly across the whole network. The latter option allows us to automatically adjust the sparsity distribution across the various layers given a global sparsity target for the network. As a result, we do not have to perform a sensitivity analysis for the layers or use heuristics such as skipping the ﬁrst or the last layers, as commonly done in prior work. We refer to the latter as joint (or global)-WoodFisher and the former as independent (or layerwise)-WoodFisher. 5 Experimental Results We now apply WoodFisher to compress commonly used CNNs for image classiﬁcation. We consider both the one-shot pruning case, and the gradual case, as well as investigate how the block-wise assumptions and the number of samples used for estimating Fisher affect the quality of the approxi- mation, and whether this can lead to more accurate pruned models. 5.1 One-shot pruning Assume that we are given a pre-trained neural network which we would like to sparsify in a single step, without any re-training. This scenario might arise when having access to limited data, making re-training infeasible. Plus, it allows us to directly compare approximation quality. (a) (b) Figure 3: (a) Comparing the one-shot pruning performance of WoodFisher on RESNET-20 and CIFAR10. (b) An ablation showing the effect of chunk size used for WoodFisher. RESNET-20, CIFAR10. First, we consider a pre-trained RESNET-20 [4] network on CIFAR10 with ∼ 300K parameters. We compute the inverse of the diagonal blocks corresponding to each layer. Figure 3a contains the test accuracy results for one-shot pruning in this setting, averaged across four seeds, as we increase the percentage of weights pruned. Despite the block-wise approximation, we observe that both independent- and joint-WoodFisher variants signiﬁcantly outperform magnitude pruning and diagonal-Fisher based pruning. We also compare against the global version of magnitude pruning, which can re-adjust sparsity across layers. Still, we ﬁnd that the global magnitude pruning is worse than WoodFisher-independent until about 60% sparsity, beyond which it is likely that adjusting layer-wise sparsity is essential. WoodFisher-joint performs the best amongst all the methods, and is better than the top baseline of global magnitude pruning - by about 5% and 10% in test accuracy at the 70% and 80% sparsity levels. Finally, diagonal-Fisher performs worse than magnitude pruning for sparsity levels higher than 30%. This ﬁnding was consistent, and so we omit it in the sections ahead. (We used 16,000 samples to estimate the diagonal Fisher, whereas WoodFisher performs well even with 1,000 samples.) EFFECT OF CHUNK SIZE. For networks which are much larger than RESNET-20, we also need to split the layerwise blocks into smaller chunks along the diagonal, to maintain efﬁciency. So, here we discuss the effect of this chunk-size on the performance of WoodFisher. We take the above setting of RESNET-20 on CIFAR10 and evaluate the performance for chunk-sizes in the set, {20, 100, 1000, 5000, 12288, 37000}. Note that, 37000 corresponds to the complete blocks across 9 all the layers. Figure 3b illustrate these results for WoodFisher in the joint mode. We observe that performance of WoodFisher increases monotonically as the size of the blocks (or chunk-size) is increased. This ﬁts well with our expectation that a large chunk-size would lead to a more accurate estimation of the inverse. Similar trend is observed for the independent mode, and is presented in Figure S8 of the Appendix S4.1. Figure 4: One-shot sparsity results for RESNET-50 on IMAGENET. In addition, we show here the effect of ﬁsher subsample size as well as how the performance is improved if we allow for recomputation of the Hessian (still no retraining). The numbers corresponding to tuple of values called ﬁsher samples refers to (ﬁsher subsample size, ﬁsher mini-batch size). A chunk size of 1000 was used for this experiment. RESNET-50, IMAGENET. Likewise, we performed a one-shot pruning experiment for the larger RESNET-50 model (25.5M parameters) on ImageNet, which for efﬁciency we break into layer-wise blocks (chunks) of size 1K. We found that this sufﬁces for signiﬁcant performance gain over layer- wise and global magnitude pruning, c.f. Figure 4. In addition, we observed that it is useful to replace individual gradients, in the deﬁnition of the empirical Fisher, with gradients averaged over a mini- batch of samples. Typically, we use 80 or 240 (i.e., ‘ﬁsher subsample size’) such averaged gradients over a mini-batch of size 100 (i.e., ‘ﬁsher mini-batch size’). This allows us to exploit mini-batch times more samples, without affecting the cost of WoodFisher, as only the ﬁsher subsample size dictates the number of Woodbury iterations. Besides, we found that accuracy can be further improved if we allow recomputation of the Hessian inverse estimate during pruning (but without retraining), as the local quadratic model is valid otherwise only in a small neighbourhood (or trust-region). These results correspond to curves marked with ‘Recompute’ in Figure 4, where the effect of increasing the recomputation steps is also shown. Further results can be found in Appendix S4.1, including one-shot pruning results of MOBILENETV1 on IMAGENET , as well as ablation for the effect of chunk-size and number of samples used for Fisher computations. Comparison against L-OBS. To facilitate a consistent comparison with L-OBS [38], we consider one-shot pruning of RESNET-50 on IMAGENET, and evaluate the performance in terms of top-5 accuracy as reported by the authors. (Besides this ﬁgure, all other results for test-accuracies are top-1 accuracies.) Here, all the layers are pruned to equal amounts, and so we ﬁrst compare it with WoodFisher independent (layerwise). Further, in comparison to L-OBS, our approach also allows to automatically adjust the sparsity distributions. Thus, we also report the performance of WoodFisher joint (global) The resulting plot is illustrated in Figure S1, where we ﬁnd that both independent and joint WoodFisher outperform L-OBS at all sparsity levels, and yield improvements of up to ∼ 3.5% and 20% respectively in test accuracy over L-OBS at the same sparsity level of 65%. Comparison against K-FAC based pruner. To quantitatively compare the approximation quality of the inverses (or IHVPs) yielded by K-FAC and WoodFisher, we consider the scenario of one-shot 10 (a) (b) Figure 5: (a) Top-5 test accuracy comparison of L-OBS and WoodFisher on IMAGENET for RESNET-50. (b) WoodFisher vs K-FAC: one-shot sparsity results for MLPNET on MNIST. K-FAC results are shown for both when the dampening π is taken into account and otherwise. pruning of MLPNET on MNIST. For both, we utilize a block-wise estimate of the Fisher (with respect to the layers, i.e., no further chunking). Figure 5b illustrates these results for the ‘joint’ pruning mode (however, similar results can observed in the ‘independent’ mode as well). The number of samples used for estimating the inverse is the same across K-FAC and WoodFisher (i.e., 50, 000 samples)3. This highlights the better approximation quality provided by WoodFisher, which unlike K-FAC does not make major assumptions. Note that, for convolutional layers, K-FAC needs to make additional approximations, so we can expect WoodFisher results to further improve over K-FAC. 5.2 Gradual Pruning Setup. So far, the two best methods we identiﬁed in one-shot tests are WoodFisher (joint/global) and global magnitude pruning. We now compare these methods extensively against several previous methods for unstructured pruning. To facilitate consistent comparison, we demonstrate our results on the pre-trained RESNET-50 and MOBILENETV1 models of the STR method [29], which claims state-of-the-art results. As in [29], all our IMAGENET experiments are run for 100 epochs on 4 NVIDIA V100 GPUs (i.e., ∼ 2.5 days for RESNET-50 and ∼ 1 day for MOBILENETV1). In terms of the pruning schedule, we follow the polynomial scheme of [19] (see illustration in Figures 6b and 6a), and run WoodFisher and global magnitude in identical settings. Note, all the sparsity percentages are with respect to the weights of all the layers present, as none of the methods prune batch-norm parameters. Also, see Appendix S1 for further details on the pruning schedule, hyperparameters, and runtime costs for WoodFisher. RESNET-50. Table 1 presents our results with comparisons against numerous baselines for pruning RESNET-50 at the sparsity levels of 80%, 90%, 95% and 98%. To take into account that some prior work uses different dense baselines, we also report the relative drop. WoodFisher outperforms all baselines, across both gradual and dynamic pruning approaches, in every sparsity regime. Compared to STR [29], WoodFisher improves accuracy at all sparsity levels, with a Top-1 test accuracy gain of ∼ 1%, 1.5%, and 3.3% respectively at the 90%, 95%, and 98% sparsity levels. We also ﬁnd that global magnitude (GM) is quite effective, surpassing many recent dynamic pruning methods, that can adjust the sparsity distribution across layers [24, 29, 49]. Comparing GM and WoodFisher, the latter outperforms at all sparsity levels, with larger gain at higher sparsities, e.g., ∼ 1.3% boost in accuracy at 98% sparsity. WoodFisher also outperforms Variational Dropout (VD) [50], the top-performing regularization-based method, on all sparsity targets, with a margin of ∼ 1.5% at 80% and 90% sparsity. VD is also known to be quite sensitive to initialization and hyperparameters [18]. This can be partly seen from its results 3For the sake of efﬁciency, in the case of WoodFisher, we utilize 1000 averaged gradients over a mini-batch size of 50. But, even if no mini-batching is considered and say, we considered 1000 samples for both, we notice similar gains over K-FAC. 11 Top-1 accuracy (%) Relative Drop Sparsity Remaining Method Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params DSR [53] 74.90 71.60 -4.41 80.00 5.10 M Incremental [19] 75.95 74.25 -2.24 73.50 6.79 M DPF [24] 75.95 75.13 -1.08 79.90 5.15 M GMP + LS [18] 76.69 75.58 -1.44 79.90 5.15 M Variational Dropout [50] 76.69 75.28 -1.83 80.00 5.12 M RIGL + ERK [49] 76.80 75.10 -2.21 80.00 5.12 M SNFS + LS [23] 77.00 74.90 -2.73 80.00 5.12 M STR [29] 77.01 76.19 -1.06 79.55 5.22 M Global Magnitude 77.01 76.59 -0.55 80.00 5.12 M DNW [54] 77.50 76.20 -1.67 80.00 5.12 M WoodFisher 77.01 76.76 -0.32 80.00 5.12 M GMP + LS [18] 76.69 73.91 -3.62 90.00 2.56 M Variational Dropout [50] 76.69 73.84 -3.72 90.27 2.49 M RIGL + ERK [49] 76.80 73.00 -4.94 90.00 2.56 M SNFS + LS [23] 77.00 72.90 -5.32 90.00 2.56 M STR [29] 77.01 74.31 -3.51 90.23 2.49 M Global Magnitude 77.01 75.15 -2.42 90.00 2.56 M DNW [54] 77.50 74.00 -4.52 90.00 2.56 M WoodFisher 77.01 75.21 -2.34 90.00 2.56 M GMP [18] 76.69 70.59 -7.95 95.00 1.28 M Variational Dropout [50] 76.69 69.41 -9.49 94.92 1.30 M Variational Dropout [50] 76.69 71.81 -6.36 94.94 1.30 M RIGL + ERK [49] 76.80 70.00 -8.85 95.00 1.28 M DNW [54] 77.01 68.30 -11.31 95.00 1.28 M STR [29] 77.01 70.97 -7.84 94.80 1.33 M STR [29] 77.01 70.40 -8.58 95.03 1.27 M Global Magnitude 77.01 71.72 -6.87 95.00 1.28 M WoodFisher 77.01 72.12 -6.35 95.00 1.28 M GMP + LS [18] 76.69 57.90 -24.50 98.00 0.51 M Variational Dropout [50] 76.69 64.52 -15.87 98.57 0.36 M DNW [54] 77.01 58.20 -24.42 98.00 0.51 M STR [29] 77.01 61.46 -20.19 98.05 0.50 M STR [29] 77.01 62.84 -18.40 97.78 0.57 M Global Magnitude 77.01 64.28 -16.53 98.00 0.51 M WoodFisher 77.01 65.55 -14.88 98.00 0.51 M Table 1: Comparison of WoodFisher gradual pruning results with the state-of-the-art approaches. WoodFisher and Global Magnitude results are averaged over two runs. For their best scores, please refer to Table S4. LS denotes label smoothing, and ERK denotes the Erd˝os-Renyi Kernel. in the 95% regime, where a slight change in hyperparameters amounts to 0.02% difference in sparsity and affects the obtained accuracy by over 2%. Besides, WoodFisher signiﬁcantly outperforms gradual magnitude pruning (GMP) [18], in all the sparsity regimes. Speciﬁcally, this amounts to a gain in test accuracy of ∼ 1 − 2% at sparsity levels of 80%, 90%, and 95%, with a gain of ≥ 7% at higher sparsities such as 98%. In fact, many of these GMP [18] results employ label smoothing (LS) which we do not need, but this should only improve WoodFisher’s performance further. Note, here the authors also report the results when GMP is run for an extended duration (∼ 2× longer compared to other methods). However, to be fair we do not compare them with other methods presented in Table 1, as followed in prior work [29]. Nevertheless, even under such an extended pruning scenario, the ﬁnal test accuracy of their models is often less than that obtained via WoodFisher, where we additionally pruned the ﬁrst convolutional layer and our whole procedure took about half the number of epochs. Lastly, WoodFisher does not require industry-scale extensive hyperparameter tuning, unlike [18]. ADDITIONAL COMPARISONS (a) With DPF. Besides the comparison in Table 1, we further compare against another recent state-of-the-art DPF [24] in a more commensurate setting by starting from a similarly trained baseline. 12 We follow their protocol and prune all layers except the last: see Table 2. In this setting as well, WoodFisher signiﬁcantly outperforms DPF and the related baselines. Top-1 accuracy (%) Relative Drop Sparsity Remaining Method Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params Incremental 75.95 73.36 -3.41 82.60 4.45 M SNFS 75.95 72.65 -4.34 82.00 4.59 M DPF 75.95 74.55 -1.84 82.60 4.45 M WoodFisher 75.98 75.20 -1.03 82.70 4.41 M Table 2: Comparison with state-of-the-art DPF [24] in a more commensurate setting by starting from a similarly trained dense baseline. The numbers for Incremental & SNFS are taken from [24]. (b) With GMP. In Gale et al. [18] (GMP), the authors empirically ﬁnd that by keeping the ﬁrst convolutional layer dense, pruning the last fully-connected layer to 80%, and then pruning rest of the layers in the network equally to the desired amount, the performance of magnitude pruning is signiﬁcantly improved and serves as a state-of-the-art. To show that there is further room for improvement even when such sparsity proﬁles are utilized, we consider WoodFisher and Global Magnitude for automatically adjusting the sparsity distribution for the intermediate layers while pruning them. Table 3 shows the results for this setting where the intermediate layers are pruned to an overall sparsity of 90%. Top-1 accuracy (%) Relative Drop Sparsity Remaining Method Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params GMP 77.01 75.13 -2.45 89.1 2.79 M Global Magnitude 77.01 75.45 -2.03 89.1 2.79 M WoodFisher 77.01 75.64 -1.78 89.1 2.79 M Table 3: Comparison of WoodFisher, Global Magnitude and Magnitude (GMP) pruning for RESNET-50 on IMAGENET. Namely, this involves skipping the (input) ﬁrst convolutional layer, and pruning the last fully connected layer to 80%. The rest of the layers are pruned to an overall target of 90%. So, here GMP prunes all of them uniformly to 90%, while WoodFisher and Global Magnitude use their obtained sparsity distributions for intermediate layers at the same overall target of 90%. MOBILENETV1. MobileNets [27] are a class of parameter-efﬁcient networks designed for mobile applications, and so is commonly used as a test bed to ascertain generalizability of unstructured pruning methods. In particular, we consider the gradual pruning setting as before on MOBILENETV1 which has ∼ 4.2M parameters. Following STR [29], we measure the performance on two sparsity levels: 75% and 90% and utilize their pre-trained dense model for fair comparisons. Top-1 accuracy (%) Relative Drop Sparsity Remaining Method Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params Incremental [19] 70.60 67.70 -4.11 74.11α 1.09 M STR [29] 72.00 68.35 -5.07 75.28 1.04 M Global Magnitude 72.00 69.90 -2.92 75.28 1.04 M WoodFisher 72.00 70.09 -2.65 75.28 1.04 M Incremental [19] 70.60 61.80 -12.46 89.03α 0.46 M STR [29] 72.00 62.10 -13.75 89.01 0.46 M Global Magnitude 72.00 63.02 -12.47 89.00 0.46 M WoodFisher 72.00 63.87 -11.29 89.00 0.46 M Table 4: Comparison of WoodFisher gradual pruning results for MobileNetV1 on ImageNet in 75% and 90% sparsity regime. (α) next to Incremental [19] is to highlight that the ﬁrst convolutional and all depthwise convolutional layers are kept dense, unlike the other shown methods. The obtained sparsity distribution and other details can be found in the section S7.2. Table 4 shows the results for WoodFisher and global magnitude along with the methods mentioned in STR. Note that the Incremental baseline from [19] keeps the ﬁrst convolutional and the (important) depthwise convolutional layers dense. However, in an aim to be network-agnostic, we let the 13 global variant of WoodFisher to automatically adjust the sparsity distributions across the layers. Nevertheless, we observe that WoodFisher outperforms [19] as well as the other baselines: STR and global magnitude, in each of the sparsity regimes. Further, it can be argued that the results in Table 4 are from just single runs. Hence, in order to check the statistical signiﬁcance of the results, we run both Global Magnitude and WoodFisher for 5 times 4 (at each of the sparsity levels) and carry out a two-sided student’s t-test. We observe that WoodFisher results in a statistically signiﬁcant gain over Global Magnitude (as well as other baselines) for both the sparsity regimes. The results for this comparison can be found in Table 5. Top-1 accuracy (%) Relative Drop Sparsity Remaining p-value Signiﬁcant Method Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params (at α = 0.05) Global Magnitude 72.00 69.93 ± 0.07 -2.88 75.28 1.04 M 0.02979 3 WoodFisher 72.00 70.04 ± 0.06 -2.72 75.28 1.04 M Global Magnitude 72.00 63.08 ± 0.12 -12.39 89.00 0.46 M 0.00003 3 WoodFisher 72.00 63.69 ± 0.11 -11.54 89.00 0.46 M Table 5: WoodFisher and Global Magnitude gradual pruning results, reported with mean and standard deviation across 5 runs, for MobileNetV1 on ImageNet in 75% and 90% sparsity regime. We also run a two-sided student’s t-test to check if WoodFisher signiﬁcantly outperforms Global Magnitude, which we ﬁnd to be true at a signiﬁcance level of α = 0.05. SUMMARY. To sum up, results show that WoodFisher outperforms state-of-the-art approaches, from each class of pruning methods, in all the considered sparsity regimes, for both RESNET-50 and MOBILENETV1 setting a new state-of-the-art in unstructured pruning for these common benchmarks. 5.3 What goes on during gradual pruning? Next, to give some further insights into these results, we illustrate, in Figures 6b and 6a, how WoodFisher and global magnitude behave during the course of gradual pruning, for RESNET-50 and MOBILENETV1 respectively. Here, we see the rationale behind the improved performance of WoodFisher. After almost every pruning step, WoodFisher provides a better pruning direction, and even with substantial retraining in between and after, global magnitude fails to catch up in terms of accuracy. The trend holds for other sparsity levels as well, and these ﬁgures can be found in the Apendix S4.2. This shows the beneﬁt of using the second order information via WoodFisher to perform superior pruning steps. (a) RESNET-50, IMAGENET (b) MOBILENETV1, IMAGENET Figure 6: The course of gradual pruning with points annotated by the corresponding sparsity amounts. Magnitude pruning, which prunes all layers equally, performs even worse, and Figure S12 in the Apendix S4.2 showcases the comparison between pruning steps for all the three: WoodFisher, Global Magnitude, and Magnitude. 4Doing 5 runs for all experiments is feasible in the MOBILENETV1 setting in contrast to RESNET50, as the overall gradual pruning time is much shorter for the former. 14 5.4 Actual inference speed-up Further, it is interesting to consider the actual speed-up which can be obtained via these methods, as the theoretical FLOP counts might not reveal the complete picture. And, to test the speed-up at inference, we use the inference framework of [30], which supports efﬁcient execution of unstructured sparse convolutional models on CPUs. We execute their framework on an Amazon EC2 c4.8xlarge instance with an 18-core Intel Haswell CPU. Sparse models are exported and executed through a modiﬁed version of the ONNX Runtime [55]. Experiments are averaged over 10 runs, and have low variance. The full results are given in Table 6. Inference Time (ms) Top-1 Acc. Compression Batch 1 Batch 64 Dense 7.1 296 77.01% STR-81.27% 5.6 156 76.12% WF-Joint-80% 6.3 188 76.73% STR-90.23% 3.8 144 74.31% WF-Independent-89.1% 4.3 157 75.23% WF-Joint-90% 5.0 151 75.26% Table 6: Comparison of inference times at batch sizes 1 and 64 for various sparse models, executed on the framework of [30], on an 18-core Haswell CPU. The table also contains the Top-1 Accuracy for the model on the ILSVRC validation set. We brieﬂy summarize the results as follows. First, note that all methods obtain speed-up relative to the dense baseline, with speed-up being correlated with increase in sparsity. At the same time, the standard WoodFisher variants (WF) tend to have higher inference time in comparison to STR, but also a higher accuracy for the same sparsity budget. A good comparison point is at ∼90% global sparsity, where WF-Joint has 0.95% higher Top-1 accuracy, for a 1.2ms difference in inference time at batch size 1. However, here the WF-Independent-89.1% model 5 offers a better trade-off, with similar accuracy improvement, but only 0.5ms difference in inference time with respect to STR. Further, in Section S6, we also consider a variant that can optimize for FLOPs as well. 6 Extensions 6.1 WoodFisher for structured pruning Let’s start by reformulating the pruning statistic in Eq. (11) for removing a single parameter as follows, ρq = w2 q 2 [H−1]qq = (w⊤eq)⊤(w⊤ eq) 2 e⊤ q H−1eq = e ⊤ q ww⊤ eq 2 e⊤ q H−1eq . Then to remove a parameter group/block (given by the set Q), we can either just consider the sum of pruning statistics over the parameters in Q, i.e., ρQ = ∑ q∈Q e ⊤ q ww⊤ eq 2 e⊤ q H−1eq . Or, we can take into account the correlation, which might be helpful for the case of structured pruning (like when removing ﬁlters) via the formulation given below: ρQ = ˜e ⊤ Q ww⊤ ˜eQ 2 ˜e⊤ QH−1˜eQ , (18) 5This refers to the WoodFisher version, where following [18], the ﬁrst convolutional layer is not pruned and the last fully-connected layer is pruned to 80%, while the rest of layers are pruned to 90%. We note that this latter model has lower overall sparsity than WF-Joint, since it does not prune the ﬁrst and last layers, following the recipe from [18]. 15 where, ˜eQ = 1{q ∈ Q}, i.e., it is the one-hot vector denoting which parameters are present in the set Q. Depending on the choice of the pruning statistic, we can modify the optimal perturbation δw∗ accordingly. More rigorously, the above formulation can be obtained by relaxing the problem of removing multiple parameters in Eq. (12) via summing the individual constraints into one combined constraint. min Q { min δw ∈ Rd ( 1 2 δw⊤ H δw) , s.t. ˜e ⊤ Qδw + w⊤˜eQ = 0 }. Then using the procedure of Lagrange multipliers as done earlier, we obtain the pruning statistic as in Eq. (18), and the optimal weight perturbation is given by the following, δw∗ = −(w⊤˜eQ)H −1˜eQ ˜e⊤ QH−1˜eQ . Lastly, we leave a detailed empirical investigation of structured pruning for future work. 6.2 WoodTaylor: Pruning at a general point Pruning analysis. Incorporating the ﬁrst-order gradient term in the Optimal Brain Damage frame- work should result in a more faithful estimate of the pruning direction, as many times in practice, the gradient is not exactly zero. Or it might be that pruning is being performed during training like in dynamic pruning methods. Hence, we redo the analysis by accounting for the gradient (see Appendix S9) and we refer to this resulting method as ‘WoodTaylor’. Essentially, this modiﬁes the problem in Eq. (4) to as follows: min δw ∈ Rd ( ∇wL ⊤δw + 1 2 δw⊤ H δw) , s.t. e ⊤ q δw + wq = 0. The corresponding Lagrangian can be then written as: L(δw, λ) = ∇wL ⊤δw + 1 2 δw⊤ H δw + λ (e ⊤ q δw + wq) . (19) Taking the derivative of which with respect to δw yields, ∇wL + Hδw + λeq = 0 =⇒ δw = −λH −1eq − H −1∇wL. The Lagrange dual function g(λ) can be then computed by putting the above value for δw in the Lagrangian in Eq. (19) as follows: g(λ) = −λ∇wL ⊤H −1eq − ∇wL ⊤H −1∇wL + 1 2 (λH −1eq + H −1∇wL)⊤H (λH −1eq + H −1∇wL) + λ ( − λe ⊤ q H −1eq − e⊤ q H−1∇wL + wq) = − λ 2 2 e ⊤ q H −1eq − λe⊤ q H−1∇wL + λwq − 1 2 ∇wL ⊤H −1∇wL . (20) As before, maximizing with respect to λ, we obtain that the optimal value λ ∗ of this Lagrange multiplier as follows: λ ∗ = wq − e ⊤ q H −1∇wL e⊤ q H−1eq = wq − e⊤ q H−1∇wL [H−1]qq . Note, if the gradient was 0, then we would recover the same λ∗ as in Eq. (9). Next, the corresponding optimal perturbation, δw∗, so obtained is as follows: δw∗ = − (wq − e ⊤ q H−1∇wL ) H −1eq [H−1]qq − H−1∇wL . (21) 16 In the end, the resulting change in loss corresponding to the optimal perturbation that removes parameter wq can be written as (after some calculations 6), δL ∗ = w2 q 2 [H−1]qq + 1 2 (e ⊤ q H −1∇wL)2 [H−1]qq − wq e ⊤ q H −1∇wL [H−1]qq − 1 2 ∇wL⊤H−1∇wL . (22) Lastly, choosing the best parameter wq to be removed, corresponds to one which leads to the minimum value of the above change in loss. As in Section 4.1, our pruning statistic ρ in this setting can be similarly deﬁned, in addition by excluding the last term in the above Eq. (22) since it does not involved the choice of removed parameter q. This is indicated in the Eq. (23) below. ρq = w2 q 2 [H−1]qq + 1 2 (e ⊤ q H −1∇wL )2 [H−1]qq − wq e ⊤ q H−1∇wL [H−1]qq . (23) Results. We present some initial results for the case when the model is far from the optimum, and hence the gradient is not close to zero. This setting will allow us to clearly see the effect of incorporating the ﬁrst-order gradient term, considered in the WoodTaylor analysis. In particular, we consider an MLPNET on MNIST, which has only been trained for 2 epochs. 0.0 0.2 0.4 0.6 0.8 1.0 Target sparsity level 10 20 30 40 50 60 70 80 90Test accuracy Mean performance computed across 4 seeds. Magnitude Global Magnitude Diagonal Fisher WoodTaylor Joint Figure 7: Comparing one-shot sparsity results for WoodTaylor and baselines on the partially trained MLPNET on MNIST. Figure 7 presents the results for performing one-shot compression for various sparsity levels at this stage in the training. Similar to the results in past, we ﬁnd that WoodTaylor is signiﬁcantly better than magnitude or diagonal Fisher based pruning as well as the global version of magnitude pruning. But the more interesting aspect is the improvement brought about by WoodTaylor, which in fact also improves over the accuracy of the initial dense model by about 5%, up to sparsity levels of ∼ 90%. Here, the number of samples used for Fisher in both WoodTaylor and Diagonal Fisher is 8000. A dampening of 1e − 1 was used for WoodTaylor, and Figures S14,S15 of the Appendix S9.1 present relevant ablation studies. Further, even on pre-trained models such as RESNET-20 on CIFAR-10, we ﬁnd that WoodTaylor can also outperform WoodFisher, since the gradient is usually not exactly zero (c.f. Appendix S9.1). Applicability with dynamic methods. Note, an advantage of dynamic pruning methods is that the pruning is performed during the training itself, although we have seen in Table 1, better results are obtained when pruning is performed post-training. Current dynamic pruning methods like DPF [24] prune via global magnitude, and a possible future work would be to use WoodTaylor instead. 6It’s easier to put the optimal value of λ ∗ in the dual function (Eq. (20)) and use duality, than substituting the optimal perturbation δw∗ in the primal objective. 17 6.3 Sampled Fisher: The case of unlabelled data While empirical Fisher inherently uses the label information when computing gradients, it is possible to avoid that and instead use a single sample from the model distribution, thus making it applicable to unlabeled data. (Appendix 6.3 shows this does not impact the results much). Top-1 accuracy (%) Sparsity Dense empirical WoodFisher sampled WoodFisher 20% 76.13 76.10 ± 0.04 76.16 ± 0.02 40% 75.22 ± 0.07 75.31 ± 0.05 60% 69.21 ± 0.05 69.29 ±0.20 70% 48.35 ± 0.22 48.74 ±1.03 80% 5.48 ± 0.45 5.77 ± 0.34 Table 7: Comparison of one-shot pruning performance of WoodFisher, when the considered Fisher matrix is empirical Fisher or one-sample approximation to true Fisher, for RESNET-50 on IMAGENET. The results are averaged over three seeds. The ‘empirical’ WoodFisher denotes the usual WoodFisher used throughout the paper. All the presented results in the paper are based on this setting. Whereas, the ‘sampled WoodFisher’ refers to sampling the output from the model’s conditional distribution instead of using the labels, in order to compute the gradients. As a result, the latter can be utilized in an unsupervised setting. Note that, ideally we would want to use the true Fisher, and none of the above approximations to it. But, computing the true Fisher would require m additional back-propagation steps for each sampled y or k more steps to compute the Jacobian. This would make the whole procedure m× or k× more expensive. Hence, the common choices are to either use m = 1 samples as employed in K-FAC [15] or simply switch to empirical Fisher (which we choose). The presented results are based on the following hyperparameters: chunk size = 1000, ﬁsher subsample size = 240, ﬁsher mini batch-size = 100. In Table 7, we contrast the performance of these two options: ‘empirical’ and ‘sampled’ WoodFisher, when performing one-shot pruning 7 of RESNET-50 on IMAGENET in the joint mode of WoodFisher. We ﬁnd that results for both types of approximations to Fisher (or whether we use labels or not) are in the same ballpark. The sampled WoodFisher does, however, have slightly higher variance which is expected since it is based on taking one sample from the model’s distribution. Nevertheless, it implies that we can safely switch to this sampled WoodFisher when labels are not present. 7 Discussion Future Work. Some of the many interesting directions to apply WoodFisher, include, e.g., struc- tured pruning which can be facilitated by the OBD framework (as in e.g., Section 6.1), compressing popular models used in NLP like Transformers [56], providing efﬁcient IHVP estimates for inﬂuence functions [16], etc. Conclusion. In sum, our work revisits the theoretical underpinnings of neural network pruning, and shows that foundational work can be successfully extended to large-scale settings, yielding state-of-the-art results. We hope that our ﬁndings can provide further momentum to the investigation of second-order properties of neural networks, and be extended to applications beyond compression. Acknowledgements This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No 805223 ScaleML). Also, we would like to thank Alexander Shevchenko, Alexandra Peste, and other members of the group for fruitful discussions. 7As in the one-shot experiments, we use the RESNET-50 model from TORCHVISION as the dense baseline. 18 References [1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015. [2] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural networks, 61:85–117, 2015. [3] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 91–99. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/ 5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks. pdf. [4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2016. doi: 10.1109/cvpr. 2016.90. URL http://dx.doi.org/10.1109/CVPR.2016.90. [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec- tional transformers for language understanding, 2018. [6] Alec Radford. Improving language understanding by generative pre-training. 2018. [7] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimization towards training a trillion parameter models. ArXiv, Oc- tober 2019. URL https://www.microsoft.com/en-us/research/publication/ zero-memory-optimization-towards-training-a-trillion-parameter-models/. [8] Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In D. S. Touretzky, editor, Advances in Neural Information Processing Systems 2, pages 598–605. Morgan-Kaufmann, 1990. URL http://papers.nips.cc/paper/250-optimal-brain-damage.pdf. [9] Michael C Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a network via relevance assessment. In Advances in neural information processing systems, pages 107–115, 1989. [10] Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain sur- geon. In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Pro- cessing Systems 5, pages 164–171. Morgan-Kaufmann, 1993. URL http://papers.nips.cc/paper/ 647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf. [11] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? arXiv preprint arXiv:2003.03033, 2020. [12] Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical Programming, 108(1):177–205, 2006. [13] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, 2010. [14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. [15] James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate curvature, 2015. [16] Pang Wei Koh and Percy Liang. Understanding black-box predictions via inﬂuence functions, 2017. [17] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521–3526, 2017. ISSN 0027-8424. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/content/114/13/3521. [18] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks, 2019. [19] Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efﬁcacy of pruning for model compression, 2017. [20] Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Huszár. Faster gaze prediction with dense networks and ﬁsher pruning, 2018. 19 [21] Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang. Eigendamage: Structured pruning in the kronecker-factored eigenbasis, 2019. [22] Wenyuan Zeng and Raquel Urtasun. MLPrune: Multi-layer pruning for automated neural network compression, 2019. URL https://openreview.net/forum?id=r1g5b2RcKm. [23] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance, 2019. [24] Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning with feedback. In International Conference on Learning Representations, 2020. URL https://openreview. net/forum?id=SJem8lSFwB. [25] Max A. Woodbury. Inverting modiﬁed matrices. SRG Memorandum report ; 42. Princeton, NJ: Department of Statistics, Princeton University, 1950. [26] Shun-ichi Amari, Hyeyoung Park, and Kenji Fukumizu. Adaptive method of realizing natural gradi- ent learning for multilayer perceptrons. Neural Computation, 12(6):1399–1409, 2000. doi: 10.1162/ 089976600300015420. URL https://doi.org/10.1162/089976600300015420. [27] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. ArXiv, abs/1704.04861, 2017. [28] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211–252, 2015. [29] Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham M. Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. ArXiv, abs/2002.03231, 2020. [30] Neural Magic Inc. Early access signup for the sparse inference engine, 2020. URL https:// neuralmagic.com/earlyaccess/. [31] James Martens. New insights and perspectives on the natural gradient method, 2014. [32] Frederik Kunstner, Lukas Balles, and Philipp Hennig. Limitations of the empirical ﬁsher approximation for natural gradient descent, 2019. [33] Sidak Pal Singh. Efﬁcient second-order methods for model compression. EPFL Master Thesis, 2020. URL http://infoscience.epfl.ch/record/277227. [34] Shun-ichi Amari. Natural gradient works efﬁciently in learning. Neural Computation, 10(2):251–276, 1998. doi: 10.1162/089976698300017746. URL https://doi.org/10.1162/089976698300017746. [35] Nicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent. Neural Computation, 14(7):1723–1738, 2002. doi: 10.1162/08997660260028683. URL https://doi.org/10. 1162/08997660260028683. [36] Valentin Thomas, Fabian Pedregosa, Bart van Merriënboer, Pierre-Antoine Mangazol, Yoshua Bengio, and Nicolas Le Roux. On the interplay between noise and curvature and its effect on optimization and generalization, 2019. [37] Nicolas Le Roux, Pierre-Antoine Manzagol, and Yoshua Bengio. Topmoumoute online natural gradi- ent algorithm. In NIPS2007, January 2008. URL https://www.microsoft.com/en-us/research/ publication/topmoumoute-online-natural-gradient-algorithm/. [38] Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon, 2017. [39] James Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th International Conference on International Conference on Machine Learning, ICML’10, page 735–742, Madison, WI, USA, 2010. Omnipress. ISBN 9781605589077. [40] Shankar Krishnan, Ying Xiao, and Rif. A. Saurous. Neumann optimizer: A practical optimization algorithm for deep neural networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=rkLyJl-0-. 20 [41] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for machine learning in linear time, 2016. [42] Tom Heskes. On natural learning and pruning in multilayered perceptrons. Neural Computation, 12: 881–901, 2000. [43] Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using kronecker- factored approximations. 2016. [44] Kazuki Osawa, Yohei Tsuji, Yuichiro Ueno, Akira Naruse, Rio Yokota, and Satoshi Matsuoka. Large- scale distributed second-order optimization using kronecker-factored approximate curvature for deep convolutional neural networks. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2019. doi: 10.1109/cvpr.2019.01264. URL http://dx.doi.org/10.1109/CVPR.2019. 01264. [45] Roger Grosse and James Martens. A kronecker-factored approximate ﬁsher matrix for convolution layers, 2016. [46] James Martens, Jimmy Ba, and Matt Johnson. Kronecker-factored curvature approximations for recurrent neural networks. In International Conference on Learning Representations, 2018. URL https:// openreview.net/forum?id=HyMTkQZAb. [47] César Laurent, Thomas George, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. An evaluation of ﬁsher approximations beyond kronecker factorization, 2018. URL https://openreview.net/forum? id=ryVC6tkwG. [48] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2015. [49] Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Making all tickets winners. ArXiv, abs/1911.11134, 2019. [50] Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsiﬁes deep neural networks, 2017. [51] Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0 regularization, 2017. [52] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks, 2018. [53] Hesham Mostafa and Xin Wang. Parameter efﬁcient training of deep convolutional neural networks by dynamic sparse reparameterization, 2019. [54] Mitchell Wortsman, Ali Farhadi, and Mohammad Rastegari. Discovering neural wirings, 2019. [55] Microsoft Corporation. The onnx runtime, 2002. [56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017. [57] Yuhuai Wu, Elman Mansimov, Roger B. Grosse, Shun Liao, and Jimmy Ba. Second- order optimization for deep reinforcement learning using kronecker-factored approxi- mation. In NIPS, pages 5285–5294, 2017. URL http://papers.nips.cc/paper/ 7112-second-order-optimization-for-deep-reinforcement-learning-using-kronecker-factored-approximation. [58] Barak A. Pearlmutter. Fast exact multiplication by the hessian. Neural Comput., 6(1):147–160, January 1994. ISSN 0899-7667. doi: 10.1162/neco.1994.6.1.147. URL https://doi.org/10.1162/neco. 1994.6.1.147. 21 Appendix Contents S1 Experimental Details 23 S1.1 Pruning schedule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 S1.2 WoodFisher Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 S1.3 Run time costs for WoodFisher pruning steps . . . . . . . . . . . . . . . . . . . . 24 S2 More on the related work for IHVPs 24 S2.1 K-FAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 S2.2 Other methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 S3 Visual tour detailed 27 S3.1 All ﬁgures for Hessian and empirical Fisher comparison on CIFARNET . . . . . . 27 S3.2 Across different stages of training . . . . . . . . . . . . . . . . . . . . . . . . . . 28 S4 Detailed Results 30 S4.1 One-shot Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 S4.2 Gradual Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 S5 Gradual pruning scores at best runs 36 S6 FLOPs and Inference Costs 37 S7 Sparsity distributions 39 S7.1 ResNet-50 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 S7.2 MobileNetV1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 S8 FLOPs-aware sparsity distributions 41 S9 WoodTaylor Results 43 S9.1 Pre-trained model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 S9.2 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 22 S1 Experimental Details S1.1 Pruning schedule We use Stochastic Gradient Descent (SGD) as an optimizer during gradual pruning, with a learning rate = 0.005, momentum = 0.9, and weight decay = 0.0001. We run the overall procedure for 100 epochs, similar to other works [29].Retraining happens during this procedure, i.e., in between the pruning steps and afterwards, as commonly done when starting from a pre-trained model. For RESNET-50, we carry out pruning steps from epoch 1, at an interval of 5 epochs, until epoch 40S1. Once the pruning steps are over, we decay the learning rate in an exponential schedule from epoch 40 to 90 by a factor of 0.6 every 6 epochs. For MOBILENETV1, we carry out pruning steps from epoch 4, at an interval of 4 epochs, until epoch 24. Once the pruning steps are over, we decay the learning rate in an exponential schedule from epoch 30 to 100 by a factor of 0.92 every epoch. The amount by which to prune at every step is calculated based on the polynomial schedule, suggested in [19], with the initial sparsity percentage set to 0.05. The same pruning schedule is followed for all: WoodFisher, Global Magnitude, and Magnitude. In fact, this whole gradual pruning procedure was originally made for magnitude pruning, and we simply replaced the pruner by WoodFisher. S1.2 WoodFisher Hyperparameters The hyperparameters for WoodFisher are summarized in the Table S1. Fisher subsample size refers to the number of outer products considered for empirical Fisher. Fisher mini-batch size means the number of samples over which the gradients are averaged, before taking an outer product for Fisher. This was motivated from computation reasons, as this allows us to see a much larger number of data samples, at a signiﬁcantly reduced cost. The chunk size refers to size of diagonal blocks based on which the Hessian (and its inverse) are approximated. For RESNET-50, we typically use a chunk size of 2000, while for MOBILENETV1 we use larger chunk size of 10, 000 since the total number of parameters is less in the latter case (allowing us to utilize a larger chunk size). A thorough ablation study is carried out with respect to these hyperparameters for WoodFisher in Section S4.1. Model Sparsity (%) Fisher Chunk size Batch Size subsample size mini-batch size RESNET-50 80.00 400 400 2000 256 90.00 400 400 2000 180 95.00 400 400 2000 180 98.00 400 400 1000 180 MOBILENETV1 75.28 400 2400 10,000 256 89.00 400 2400 10,000 180 Table S1: Detailed hyperparameters for the gradual pruning results presented in Tables 1, 4. Note, we used a batch size of 256 or 180, depending upon whether the GPUs we were running on had 16GB memory or less. Anyhow, the same batch size was used at all the respective sparsity levels for Global Magnitude to ensure consistent comparisons. Besides, the dampening λ used in WoodFisher, to make the empirical Fisher positive deﬁnite, is set to 1e − 5 in all the experiments. S1All the epoch numbers are based on zero indexing. 23 S1.3 Run time costs for WoodFisher pruning steps WoodFisher indeed incurs more time during each pruning step. However, the additional time taken for these pruning steps (which are also limited in number, ∼ 6 to 8 in our experiments) pales in comparison to the overall 100 epochs on IMAGENET in the gradual pruning procedure. The exact time taken in each pruning step, depends upon the value of the ﬁsher parameters and chunk size. So more concretely, for RESNET-50 this time can vary as follows e.g., ∼ 15 minutes for ﬁsher subsample size = 80, ﬁsher subsample size = 400, chunk size = 1000 to ∼ 47 minutes for ﬁsher subsample size = 160, ﬁsher subsample size = 800, chunk size = 2000. However, as noted in Tables S2, S3, the gains from increasing these hyperparameter values is relatively small (after a threshold), so one can simply trade-off the accuracy in lieu of time, or vice versa. Most importantly, one has to keep in mind that compressing a particular neural network will only be done once. The pruning cost will be amortized over its numerous runs in the future. As a result, the extra test accuracy provided via WoodFisher is worthwhile the slightly increased running cost. Lastly, note that, currently in our implementation we compute the inverse of the Hessian sequentially across all the blocks (or chunks). But, this computation is amenable to parallelization and a further speed-up can easily obtained for use in future work. S2 More on the related work for IHVPs S2.1 K-FAC In the recent years, an approximation called K-FAC [15, 42] has been made for the Fisher that results in a more efﬁcient application when used as a pre-conditioner or for IHVPs. Consider we have a fully-connected network with l layers. If we denote the pre-activations of a layer i by si, we can write them as si = Wiai−1, where Wi is the weight matrix at the i th layer and ai−1 denotes the activations from the previous layer (which the i th layer receives as input). By chain rule, the gradient of the objective L with respect to the weights in layer i, is the following: ∇WiL = vec( gia⊤ i−1 ). Here, gi is the gradient of the objective with respect to the pre-activations si of this layer, so gi = ∇siL. Using the fact that vec(uv⊤) = v ⊗ u, where ⊗ denotes the Kronecker product, we can simplify our expression of the gradient with respect to Wi as ∇WiL = a ⊤ i−1 ⊗ gi. Then, we can then write the Fisher block corresponding to layer i and j as follows, Fi,j = E [ ∇Wi L ∇Wj L ⊤] = E [(ai−1 ⊗ gi) (aj−1 ⊗ gj) ⊤] (a) = E [ (ai−1 ⊗ gi) (a⊤ j−1 ⊗ g⊤ j )] (b) = E [ ai−1a⊤ j−1 ⊗ gig⊤ j ] , where, in (a) and (b) we have used the transpose and mixed-product properties of Kronecker product. The expectation is taken over the model’s distribution as in the formulation of Fisher. The Kronecker Factorization (K-FAC) based approximation ̃F thus used by the authors can be written as, ̃Fi,j = E [ ai−1a⊤ j−1] ⊗ E [ gig⊤ j ] = ̃Ai−1,j−1 ⊗ ̃Gi,j Essentially, we have moved the expectation inside and do it prior to performing the Kronecker product. As mentioned by the authors, this is a major approximation since in general the expectation of a Kronecker product is not equal to the Kronecker product of the expectations. We will refer to ̃F as the Fisher matrix underlying K-FAC or the K-FAC approximated Fisher. The advantage of such an approximation is that it allows to compute the inverse of K-FAC approx- imated Fisher quite efﬁciently. This is because the inverse of a Kronecker product is equal to the Kronecker product of the inverses. This implies that instead of inverting one matrix of bigger size ni−1ni × nj−1nj, we need to invert two smaller matrices ̃Ai,j and ̃Gi,j of sizes ni−1 × nj−1 and ni × nj respectively (here, we have denoted the number of neurons in layer ℓ by nℓ). 24 As a result, K-FAC has found several applications in the last few years in: optimization [43, 44], pruning [21, 22], reinforcement-learning [57], etc. However, an aspect that has been ignored is the accuracy of this approximation, which we discuss in Section 5.1 in the context of pruning. Besides, there are a couple more challenges associated with the Kronecker-factorization based approaches. Extending to different network types. Another issue with K-FAC is that it only naturally exists for fully-connected networks. When one proceeds to the case of convolutional or recurrent neural networks, the Kronecker structure needs to be specially designed by making further approximations [45, 46]. Whereas, a WoodFisher based method would not suffer from such a problem. Application to larger networks. Furthermore, when applied to the case of large neural networks like RESNET-50, often further approximations like the chunking of block size as we consider or channel-grouping as called by [47], or assuming spatially uncorrelated activations are anyways required. Thus, in lieu of these aspects, we argue that WoodFisher, i.e., (empirical) Fisher used along with Woodbury-based inverse, is a better alternative (also see the quantitative comparison with K-FAC in Section 5.1). S2.2 Other methods Double back-propagation. This forms the naive way of computing the entire Hessian matrix by explicitly computing each of its entries. However, such an approach is extremely slow and would require O(d 2) back-propagation steps, each of which has a runtime of O(md), where m is the size of the mini-batch considered. Thus, this cubic time approach is out of the question. Diagonal Hessian. If we assume the Hessian to be diagonal, this allows us to compute the inverse very easily by simply inverting the elements of the diagonal. But, even if we use the Pearlmutter’s trick [58], which lets us compute the exact Hessian-vector product in linear time, we need a total of O(d) such matrix-vector products to estimate the diagonal, which results in an overall quadratic time. Diagonal Fisher. Diagonal estimate for the empirical Fisher is really efﬁcient to build, since it just requires computing the average of the squared gradient across the training samples, for each dimension. If the mini-batch size is m, we just need O(md) time to build this estimate. This approach has been widely used in optimization by adaptive methods [13, 14], as well for model compression by the work called Fisher-pruning [20]. However as we show ahead, by simply paying a small additional factor of c in the runtime, we can estimate the inverse and IHVPs more accurately. This leads to a performance which is signiﬁcantly better than that obtained via diagonal Fisher. Hessian-Free methods. Another line of work is to completely forgo the explicit computation of Hessians [39], by posing the problem of computing IHVP with a vector v as solving the linear system Hx = v for x. Such methods rely on conjugate-gradients based linear-system solvers that only require matrix-vector products, which for neural networks can be obtained via Pearlmutter’s trick [58]. However, a big disadvantage of these methods is that they can require a lot of iterations to converge since the underlying Hessian matrix is typically very ill-conditioned. Further, this whole procedure would have to be repeated at least O(d) times to build just the diagonal of the inverse, which is the minimum needed for application in model compression. Neumann series expansion. These kind of methods [40, 41] essentially exploit the following result in Eqn. (24) for matrices A which have an eigen-spectrum bounded between 0 and 1, i.e., 0 < λ(A) < 1. A−1 = ∞∑ i=0(I − A)i (24) This can be then utilized to build a recurrence of the following form, A−1 n ≜ I + (I − A)A−1 n−1, which allows us to efﬁciently estimate (unbiased) IHVP’s via sampling. However, an important issue here is the requirement of the eigen-spectrum to be between 0 and 1, which is not true by default 25 for the Hessian. This implies that we further need to estimate the largest absolute eigenvalue (to scale) and the smallest negative eigenvalue (to shift). Hence, requiring the use of the Power method which adds to the cost. Further, the Power method might not be able to return the smallest negative eigenvalue at all, since when applied to the Hessian (or its inverse) it would yield the eigenvalue with the largest magnitude (or smallest magnitude). Woodbury-based methods. In prior work, Woodbury-based inverse has been considered for the case of a one-hidden layer neural network in Optimal Brain Surgeon (OBS, [10]), where the analyti- cal expression of the Hessian can be written as an outer product of gradients. An extension of this approach to deeper networks, called L-OBS, was proposed in [38], by deﬁning separate layer-wise objectives, and was applied to carefully-crafted blocks at the level of neurons. Our approach via em- pirical Fisher is more general, and we show ahead experimentally that it yields better approximations at scale (Figure S1). To facilitate a consistent comparison with L-OBS, we consider one-shot pruning of RESNET-50 on IMAGENET, and evaluate the performance in terms of top-5 accuracy as reported by the authors. (Besides this ﬁgure, all other results for test-accuracies are top-1 accuracies.) Here, all the layers are pruned to equal amounts, and so we ﬁrst compare it with WoodFisher independent (layerwise). Further, in comparison to L-OBS, our approach also allows to automatically adjust the sparsity distributions. Thus, we also report the performance of WoodFisher joint (global) The resulting plot is illustrated in Figure S1, where we ﬁnd that both independent and joint WoodFisher outperform L-OBS at all sparsity levels, and yield improvements of up to ∼ 3.5% and 20% respectively in test accuracy over L-OBS at the same sparsity level of 65%. Figure S1: Top-5 test accuracy comparison of L-OBS and WoodFisher on IMAGENET for RESNET- 50. 26 S3 Visual tour detailed (a) First-layer sub matrices averaged across over diagonal blocks of 6144 × 6144 for illustration purposes. (b) Second-layer sub-matrices. (c) Third-layer sub matrices. Figure S2: Hessian and empirical Fisher blocks for CIFARNET (3072 → 16 → 64 → 10) on the diagonal corresponding to different layers when trained on CIFAR10. Figures have been smoothened slightly with a Gaussian kernel for better visibility. Both Hessian and empirical Fisher have been estimated over a batch of 100 examples in all the ﬁgures. Hessian blocks are in the left-column, while empirical Fisher blocks are displayed in the right-column. S3.1 All ﬁgures for Hessian and empirical Fisher comparison on CIFARNET We consider a fully connected network, CIFARNET, with two hidden layers. Since, CIFAR10 consists of 32 × 32 RGB images, so we adapt the size of network as follows: 3072 → 16 → 64 → 10. Such a size is chosen for computational reasons, as the full Hessian exactly is very expensive to compute. We follow a commonly used SGD-based optimization schedule for training this network on CIFAR10, with a learning rate 0.05 which is decayed by a factor of 2 after every 30 epochs, momentum 0.9, and train it for a total of 300 epochs. The checkpoint with best test accuracy is used as a ﬁnal model, and 27 (a) Cross-matrices between ﬁrst-layer and third-layer averaged across over blocks of 6144 × 640 for illustration purposes. (b) Cross-matrices between second-layer and third-layer. Figure S3: Off-Diagonal Hessian and empirical Fisher blocks for CIFARNET (3072 → 16 → 64 → 10) corresponding to different layers when trained on CIFAR10. Figures have been smoothened slightly with a Gaussian kernel for better visibility. Both Hessian and empirical Fisher have been estimated over a batch of 100 examples in all the ﬁgures. Hessian blocks are in the left-column, while empirical Fisher blocks are displayed in the right-column. this test accuracy S2 is 41.8%. However, this low test accuracy is not a concern for us, as we are more interested in investigating the structures of the Hessian and the empirical Fisher matrices. The plots in the Figure S2 illustrate the obtained matrices for the diagonal sub-matrices corresponding to the ﬁrst, second, and the third layers. We observe that empirical Fisher possesses essentially the same structure as observed in the Hessian. Further, Figure S3 presents the result for the off-diagonal or cross blocks of these two matrices, where also we ﬁnd a similar trend. Thus, we conclude that the empirical Fisher shares the structure present in the Hessian matrix. S3.2 Across different stages of training While previously we compared the Hessian and the empirical Fisher at convergence, in this section our aim is to show that such a trend can be observed across various stages of training, including as early as 0.5 epochs. For our experimental setup, we ﬁrst consider a fully-connected network trained on the MNIST digit recognition dataset. This fully-connected network has two hidden layers of size 40 and 20. MNIST consists of 28 × 28 grayscale images for the digits 0 − 9. Thus the overall shape of this network can be summarized as 784 → 40 → 20 → 10. Note, here our purpose is not to get the best test accuracy, but rather we would like to inspect the structures of the Hessian and the empirical Fisher matrices. As a result, we choose the network with a relatively small size so as to exactly compute the full Hessian via double back-propagation. We use stochastic gradient descent (SGD) with a constant learning rate of 0.001 and a momentum of 0.5 to S2In fact, this CIFARNET model with 41.8% test accuracy, is far from ensuring that the model and data distribution match, yet the empirical Fisher is able to faithfully capture the structure of the Hessian. 28 (a) At 0.5 epochs. Test accuracy at this stage is 63.9%. (b) At 5 epochs. Test accuracy at this stage is 85.5%. (c) At 50 epochs. Test accuracy at this stage is 90.6%. Figure S4: Last-layer Hessian and empirical Fisher blocks for MLPNET (784 → 40 → 20 → 10) at different points of training on MNIST. Both Hessian and empirical Fisher have been estimated over a batch of 64 examples in all the ﬁgures. Hessian blocks are in the left-column, while empirical Fisher blocks are displayed in the right-column. train this network. The training set was subsampled to contain 5000 examples in order to prototype faster, and the batch size used during optimization was 64. In Figure S4, we compare the last-layer sub-matrices of sizes 200 × 200 for both Hessian and empirical Fisher at different stages of training. We see that both these matrices share a signiﬁcant amount of similarities in their structure. In other words, if we were to compute say the correlation or cosine similarity between the two matrices, it would be quite high. In these plots, the number of samples used to build the estimates of Hessian and empirical Fisher was 16, and similar trends can be observed if more samples are taken. Thus, we can establish that the empirical Fisher shares the same underlying structure as the Hessian, even at early stages of the training, where theoretically the model and data distribution still do not match. 29 S4 Detailed Results S4.1 One-shot Pruning In all the one-shot experiments, we use the TORCHVISION models for RESNET-50 and MO- BILENETV1 as dense baselines. RESNET-50 on IMAGENET. Here, all layers except the ﬁrst convolutional layer are pruned to the respective sparsity values in a single shot, i.e., without any re-training. Figure S5 shows how WoodFisher outperforms Global Magnitude and Magnitude with as few as 8,000 data samples. Increasing the ﬁsher subsample size from 80 to 240 further helps a bit, and Table S2 properly investigates the effect of the ﬁsher parameters, namely ﬁsher subsample size and ﬁsher mini-batch size, on the performance. Figure S5: One-shot sparsity results for RESNET-50 on IMAGENET. In addition, we show here the effect of ﬁsher subsample size as well as how the performance is greatly improved if we allow for recomputation of the Hessian (still no retraining). This is because the local quadratic model is only valid in a small neighbourhood (trust-region), beyond which it is not guaranteed to be accurate. The numbers corresponding to tuple of values called ﬁsher samples refers to (ﬁsher subsample size, ﬁsher mini-batch size). A chunk size of 1000 was used for this experiment. Importantly, in Figure S5, we observe that if we allow recomputing the Hessian inverse estimate during pruning (but without retraining), it leads to signiﬁcant improvements, since the local quadratic model is valid otherwise only in a small neighbourhood or trust-region. Note, this kind of recomputation can also be applied in the settings discussed below. However, having shown here for the primary example of RESNET-50 on IMAGENET, we skip the recomputation results for other secondary settings detailed below. MOBILENETV1 on IMAGENET. Similarly, we perform one-shot pruning experiments for MO- BILENETV1 on IMAGENET. Here, also we ﬁnd that WoodFisher outperforms both Global Magnitude and Magnitude. Next, Figure S6 shows the effect of ﬁsher mini batch size, across various values of ﬁsher subsample size, for this scenario. We notice that ﬁsher mini-batch serves as a nice trick, which helps us take advantage of larger number of samples in the dataset at a much less cost. Further, in Table S3 presents the exact numbers for these experiments Effect of chunk size. For networks which are much larger than RESNET-20, we also need to split the layerwise blocks into smaller chunks along the diagonal. So, here we study the effect of this chunk-size on the performance of WoodFisher. We take the setting of RESNET-20 on CIFAR10 and 30 Sparsity (%) Fisher mini-batch size Dense: Top-1 accuracy (%) Pruned: Top-1 accuracy (%) Fisher subsample size 80 160 30 1 76.13 55.81 ± 3.28 57.53 ± 1.62 30 75.17 ± 0.09 75.26 ± 0.17 100 75.73 ± 0.03 75.77 ± 0.08 400 75.80 ± 0.01 75.80 ± 0.04 800 75.74 ± 0.04 75.71 ± 0.08 2400 75.76 ± 0.04 75.72 ± 0.06 50 1 76.13 48.76 ± 4.95 51.23 ± 3.11 30 73.02 ± 0.09 73.27 ± 0.25 100 73.70 ± 0.13 73.80 ± 0.08 400 73.66 ± 0.02 73.73 ± 0.02 800 73.43 ± 0.08 73.47 ± 0.06 2400 73.32 ± 0.10 73.30 ± 0.04 Table S2: Effect of ﬁsher subsample size and ﬁsher mini-batch size on one-shot pruning performance of WoodFisher, for RESNET-50 on IMAGENET. A chunk size of 1000 was used for this experiment. The resutlts are averaged over three seeds. Figure S6: Effect of ﬁsher mini batch size, across various values of ﬁsher subsample size, on the one-shot pruning performance of WoodFisher for MOBILENETV1 on IMAGENET. evaluate the performance for chunk-sizes in the set, {20, 100, 1000, 5000, 12288, 37000}. Note that, 37000 corresponds to the size of the block for the layer with the most number of parameters. Thus, this would correspond to taking the complete blocks across all the layers. Figures S7 and S8 illustrate the impact of the block sizes used on the performance of WoodFisher in joint and independent mode respectively. We observe that performance of WoodFisher increases monotonically as the size of the blocks (or chunk-size) is increased, for both the cases. This ﬁts well with our expectation that a large chunk-size would lead to a more accurate estimation of the inverse. However, it also tells us that even starting from blocks of size as small as 100, there is a signiﬁcant gain in comparison to magnitude pruning. Effect of the dampening parameter. Regarding λ, we selected a small value so that the Hessian is not dominated by the dampening. We note that the algorithm is largely insensitive to this dampening value, Fig S9. 31 Sparsity (%) Fisher mini-batch size Dense: Top-1 accuracy (%) Pruned: Top-1 accuracy (%) Fisher subsample size 160 400 10 1 71.76 43.11 ± 4.34 41.99 ± 6.75 10 66.86 ± 0.79 67.69 ± 0.53 30 70.88 ± 0.12 70.89 ± 0.13 100 71.56 ± 0.05 71.59 ± 0.14 400 71.75 ± 0.04 71.79 ± 0.05 2400 71.79 ± 0.01 71.77 ± 0.08 30 1 71.76 38.60 ± 3.76 38.60 ± 6.05 10 62.40 ± 1.03 63.54 ± 0.74 30 66.90 ± 0.11 66.92 ± 0.17 100 67.55 ± 0.12 67.71 ± 0.08 400 67.88 ± 0.06 67.96 ± 0.12 2400 67.88 ± 0.05 67.99 ± 0.06 50 1 71.76 17.64 ± 1.62 18.25 ± 2.96 10 28.91 ± 1.30 31.75 ± 0.74 30 33.71 ± 0.42 32.10 ± 0.42 100 32.15 ± 1.70 31.37 ± 0.34 400 32.30 ± 0.40 31.46 ± 0.13 2400 32.39 ± 0.67 32.06 ± 0.65 Table S3: Effect of ﬁsher subsample size and ﬁsher mini-batch size on one-shot pruning performance of WoodFisher, for MOBILENETV1 on IMAGENET. A chunk size of 10, 000 was used for this experiment. The results are averaged over three seeds. Figure S7: Effect of chunk size on one-shot sparsity results of WoodFisher joint for RESNET-20 on CIFAR10. 32 Figure S8: Effect of chunk size on one-shot sparsity results of WoodFisher independent for RESNET- 20 on CIFAR10. Figure S9: Effect of the dampening λ on one-shot pruning results of WoodFisher (RESNET-20, CIFAR10) (avg over 4 seeds). As one would expect, the lower dampening value of 1e − 5 performs slightly better on average than the other values. This also highlights that the performance of WoodFisher is insensitive to the dampening λ. 33 S4.2 Gradual Pruning (a) Target sparsity: 80% (b) Target sparsity: 90% (c) Target sparsity: 95% (d) Target sparsity: 98% Figure S10: The course of gradual pruning with points annotated by the corresponding sparsity amounts, for RESNET-50 on IMAGENET across the different sparsity regimes. (a) Target sparsity: 75.3% (b) Target sparsity: 89% Figure S11: The course of gradual pruning with points annotated by the corresponding sparsity amounts, for MOBILENETV1 on IMAGENET across the different sparsity regimes. 34 Figure S12: Comparison of the pruning phase during gradual pruning for WoodFisher, Global Magnitude, and Magnitude when compressing MOBILENETV1 to 60% on IMAGENET. The labels on the line plot indicate the corresponding sparsity level. We observe that after each pruning step WoodFisher outperforms both Global Magnitude and Magnitude. Besides, magnitude pruning, which prunes all layers equally performs even worse, and Figure S12 showcases the comparison between pruning steps for all the three: WoodFisher, Global Magnitude, and Magnitude. Such a trend is consistent and this is why we omit the results for magnitude pruning, and instead compare mostly with global magnitude. 35 S5 Gradual pruning scores at best runs In the main text, we reported results which were averaged across two runs for both WoodFisher and Global Magnitude. Here, we present the best run results for each method in Tables S4 and S5. We can see that even here as well, Global Magnitude (or Magnitude pruning) does not perform better than WoodFisher, across any of the pruning scenarios. Top-1 accuracy (%) Relative Drop Sparsity Remaining Method Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params DSR [53] 74.90 71.60 -4.41 80.00 5.10 M Incremental [19] 75.95 74.25 -2.24 73.50 6.79 M DPF [24] 75.95 75.13 -1.08 79.90 5.15 M GMP + LS [18] 76.69 75.58 -1.44 79.90 5.15 M Variational Dropout [50] 76.69 75.28 -1.83 80.00 5.12 M RIGL + ERK [49] 76.80 75.10 -2.21 80.00 5.12 M SNFS + LS [23] 77.00 74.90 -2.73 80.00 5.12 M STR [29] 77.01 76.19 -1.06 79.55 5.22 M Global Magnitude 77.01 76.60 -0.53 80.00 5.12 M DNW [54] 77.50 76.20 -1.67 80.00 5.12 M WoodFisher 77.01 76.78 -0.30 80.00 5.12 M GMP + LS [18] 76.69 73.91 -3.62 90.00 2.56 M Variational Dropout [50] 76.69 73.84 -3.72 90.27 2.49 M RIGL + ERK [49] 76.80 73.00 -4.94 90.00 2.56 M SNFS + LS [23] 77.00 72.90 -5.32 90.00 2.56 M STR [29] 77.01 74.31 -3.51 90.23 2.49 M Global Magnitude 77.01 75.20 -2.36 90.00 2.56 M DNW [54] 77.50 74.00 -4.52 90.00 2.56 M WoodFisher 77.01 75.26 -2.27 90.00 2.56 M GMP [18] 76.69 70.59 -7.95 95.00 1.28 M Variational Dropout [50] 76.69 69.41 -9.49 94.92 1.30 M Variational Dropout [50] 76.69 71.81 -6.36 94.94 1.30 M RIGL + ERK [49] 76.80 70.00 -8.85 95.00 1.28 M DNW [54] 77.01 68.30 -11.31 95.00 1.28 M STR [29] 77.01 70.97 -7.84 94.80 1.33 M STR [29] 77.01 70.40 -8.58 95.03 1.27 M Global Magnitude 77.01 71.79 -6.78 95.00 1.28 M WoodFisher 77.01 72.16 -6.30 95.00 1.28 M GMP + LS [18] 76.69 57.90 -24.50 98.00 0.51 M Variational Dropout [50] 76.69 64.52 -15.87 98.57 0.36 M DNW [54] 77.01 58.20 -24.42 98.00 0.51 M STR [29] 77.01 61.46 -20.19 98.05 0.50 M STR [29] 77.01 62.84 -18.40 97.78 0.57 M Global Magnitude 77.01 64.39 -16.38 98.00 0.51 M WoodFisher 77.01 65.63 -14.78 98.00 0.51 M Table S4: At best runs: Comparison of WoodFisher gradual pruning results with the state-of-the-art approaches. LS denotes label smoothing, and ERK denotes the Erd˝os-Renyi Kernel. Top-1 accuracy (%) Relative Drop Sparsity Remaining Method Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params GMP 77.01 75.18 -2.38 89.1 2.79 M WoodFisher (independent) 77.01 75.23 -2.31 89.1 2.79 M Table S5: At best runs: comparison of WoodFisher and magnitude pruning with the same layer-wise sparsity targets as used in [18] for RESNET-50 on IMAGENET. Namely, this involves skipping the ﬁrst convolutional layer, pruning the last fully connected layer to 80% and the rest of the layers equally to 90%. 36 S6 FLOPs and Inference Costs In the discussion so far, our objective has been to prune the model parameters in such a way that the accuracy is preserved or decreases minimally. For several practical use-cases, another important criterion to consider while pruning is to reduce the inference costs. However, our current procedure 4 is solely designed to maintain the accuracy of the pruned model. FLOPs-aware pruning statistic. Thus, to take into account the inference costs, we normalize the pruning statistic by the respective theoretical FLOP (ﬂoating point operation) cost for that parameter. The FLOP cost for a parameter (FLOPs-per-paramq) is considered as the FLOP cost for the layer it is present in divided by the number of parameters in that layer. Overall, we can deﬁne the FLOPs-aware pruning statistic as follows, ρFLOPs q = ρq (FLOPs-per-paramq)β . where, ρq refers to the usual pruning statistic (as in Eq. (11)) and β ≥ 0 is a hyper-parameter that controls the inﬂuence of FLOP costs in the net pruning statistic (for β = 0, we recover the pruning statistic ρq). Intuitively, we are normalizing the sensitivity (of the loss) to removal of parameter q (in other words, the pruning statistic ρq) by its corresponding FLOP cost. Note, during the course of gradual pruning, both the FLOP costs and the number of parameters are measured with respect to the active (non-pruned) parameters in the corresponding layer. The beneﬁts of such a formulation are that we still do not need to manually design a FLOPs-aware sparsity distribution for the model and we can control the FLOPs-accuracy trade-off as required, plus the fact that the rest of pruning procedure remains exactly the same. To illustrate the effect of this FLOPs-aware pruning statistic, we consider one of the sparsity regimes for both RESNET50 and MOBILENETV1 and compare the FLOP costs in Table S6. Method Model Top-1 accuracy (%) Relative Drop Sparsity Remaining FLOPs Dense (D) Pruned (P ) 100 × (P −D) D (%) # of params STR [29] RESNET50 77.01 74.31 -3.51 90.23 2.49 M 343 M STR [29] 77.01 74.01 -3.90 90.55 2.41 M 341 M WoodFisher (β = 0.00) 77.01 75.26 -2.27 90.00 2.56 M 594 M WoodFisher (β = 0.30) 77.01 74.34 -3.47 90.23 2.49 M 335 M STR [29] MOBILENETV1 72.00 68.35 -5.07 75.28 1.04 M 101 M WoodFisher (β = 0.00) 72.00 70.09 -2.65 75.28 1.04 M 159 M WoodFisher (β = 0.30) 72.00 69.26 -3.81 75.28 1.04 M 101 M WoodFisher (β = 0.35) 72.00 68.69 -4.60 75.28 1.04 M 92 M Table S6: Comparison of WoodFisher (WF) and STR across theoretical FLOP counts for RESNET50 and MOBILENETV1 on IMAGENET in 90% and 75% sparsity regime respectively. First, we observe that interestingly while STR [29] performs signiﬁcantly worse than WoodFisher in terms of the pruned model accuracy, the theoretical FLOP costs for STR are much better than WoodFisher (or Global Magnitude). Roughly, this is because STR leads to sparsity proﬁles that are relatively more “uniform” across layers, whereas WoodFisher and Global Magnitude may in theory arbitrarily re-distribute sparsity across layers. (In practice, we note that the sparsity proﬁles generated by these methods do correlate layer sparsity with the number of parameters in the layer.) However, if we incorporate the FLOPs-aware pruning statistic (with β = 0.30) for WoodFisher, then it improves over STR simultaneously along the axis of accuracy and FLOP costs. E.g., we see that for both the settings of RESNET50 (β = 0.30) and MOBILENETV1 (β = 0.35), WoodFisher has a higher accuracy than STR as well as lower FLOP costs. Plus, for the same FLOP costs of 101 M in case of MOBILENETV1, WoodFisher results in ≈ 1% accuracy gain over STR. This resulting improvement over STR illustrates the beneﬁt of using FLOPs-aware pruning statistic for WoodFisher, as STR is claimed to be a state-of-the-art method for unstructured pruning that in addition minimizes FLOP costs. 37 The sparsity proﬁles for the WoodFisher pruned models with lower FLOP counts can be found in Section S8. Inference Time (ms) Top-1 Acc. Compression Batch 1 Batch 64 Dense 7.1 296 77.01% STR-81.27% 5.6 156 76.12% WF-Joint-80% 6.3 188 76.73% STR-90.23% 3.8 144 74.31% FLOPs-aware WF-Joint-90.23% 3.8 116 74.34% WF-Independent-89.1% 4.3 157 75.23% WF-Joint-90% 5.0 151 75.26% Table S7: Comparison of inference times at batch sizes 1 and 64 for various sparse models, executed on the framework of [30], on an 18-core Haswell CPU. The table also contains the Top-1 Accuracy for the model on the ILSVRC validation set. The difference with respect to the Table 6 is that here we include the results of FLOPs-aware variant too. Effect of FLOPs-Aware on actual inference times. In Table S7, we see that by considering the FLOPs-aware pruning statistic from the discussion before (with β = 0.30), this results in 20% relative improvement over STR on inference time at batch size 64. 38 S7 Sparsity distributions As followed in the literature [24, 29], we prune only the weights in fully-connected and convolutional layers. This means that none of the batch-norm parameters or bias are pruned if present. The sparsity percentages in our work and others like [29] are also calculated based on this. S7.1 ResNet-50 Module Fully Dense Params Sparsity (%) WoodFisher Global Magni WoodFisher Global Magni WoodFisher Global Magni WoodFisher Global Magni Overall 25502912 80% 90% 95% 98% Layer 1 - conv1 9408 37.04 37.61 44.97 45.72 51.65 51.86 60.63 60.09 Layer 2 - layer1.0.conv1 4096 46.31 49.12 58.30 60.69 66.02 68.70 75.39 77.83 Layer 3 - layer1.0.conv2 36864 68.18 68.69 79.48 80.19 86.81 87.54 93.03 93.45 Layer 4 - layer1.0.conv3 16384 61.43 62.77 72.16 73.77 79.68 81.51 86.91 89.32 Layer 5 - layer1.0.downsample.0 16384 56.10 57.78 66.31 68.08 74.10 75.67 81.68 83.82 Layer 6 - layer1.1.conv1 16384 66.12 66.82 77.44 78.33 85.05 85.53 91.47 92.35 Layer 7 - layer1.1.conv2 36864 71.19 71.78 82.65 83.01 89.15 89.52 94.19 94.76 Layer 8 - layer1.1.conv3 16384 71.69 72.98 80.57 82.43 86.60 88.04 91.19 93.15 Layer 9 - layer1.2.conv1 16384 60.47 61.04 73.18 74.16 81.82 83.04 89.84 90.91 Layer 10 - layer1.2.conv2 36864 59.54 60.04 73.37 74.00 82.92 83.04 90.80 91.47 Layer 11 - layer1.2.conv3 16384 72.05 73.14 79.29 80.76 84.31 86.17 89.09 91.46 Layer 12 - layer2.0.conv1 32768 58.69 59.70 71.70 73.05 80.73 81.65 88.53 90.12 Layer 13 - layer2.0.conv2 147456 71.07 71.44 83.83 84.42 90.77 91.20 95.70 96.25 Layer 14 - layer2.0.conv3 65536 73.68 74.59 82.89 83.84 88.41 89.45 93.03 94.33 Layer 15 - layer2.0.downsample.0 131072 80.55 81.24 88.98 89.66 93.31 94.04 96.60 97.22 Layer 16 - layer2.1.conv1 65536 80.91 81.47 89.38 89.68 93.74 94.32 96.84 97.35 Layer 17 - layer2.1.conv2 147456 77.50 77.66 87.38 87.63 92.82 92.95 96.57 96.71 Layer 18 - layer2.1.conv3 65536 76.52 77.78 85.39 86.41 90.42 91.59 94.53 95.68 Layer 19 - layer2.2.conv1 65536 72.53 73.08 84.34 85.05 90.64 91.40 95.21 96.06 Layer 20 - layer2.2.conv2 147456 75.94 76.14 87.02 87.27 92.55 92.98 96.48 96.88 Layer 21 - layer2.2.conv3 65536 69.96 70.99 82.19 83.21 88.65 89.91 93.77 95.04 Layer 22 - layer2.3.conv1 65536 70.29 70.74 82.43 83.14 89.11 89.94 94.29 95.17 Layer 23 - layer2.3.conv2 147456 72.67 72.75 84.80 85.04 91.15 91.53 95.85 96.23 Layer 24 - layer2.3.conv3 65536 73.87 74.68 84.12 85.37 89.84 90.91 94.23 95.49 Layer 25 - layer3.0.conv1 131072 62.86 63.53 76.43 77.31 84.79 85.61 91.73 92.67 Layer 26 - layer3.0.conv2 589824 81.91 82.22 91.43 91.80 95.57 95.95 98.10 98.41 Layer 27 - layer3.0.conv3 262144 72.28 72.95 84.20 85.00 90.76 91.48 95.39 96.13 Layer 28 - layer3.0.downsample.0 524288 87.11 87.26 94.23 94.43 97.15 97.38 98.85 99.04 Layer 29 - layer3.1.conv1 262144 85.79 85.99 93.09 93.43 96.38 96.76 98.32 98.64 Layer 30 - layer3.1.conv2 589824 85.63 85.73 93.25 93.37 96.61 96.79 98.52 98.72 Layer 31 - layer3.1.conv3 262144 77.65 78.16 88.41 89.09 93.59 94.24 96.99 97.58 Layer 32 - layer3.2.conv1 262144 83.75 83.92 92.21 92.51 95.95 96.23 98.17 98.50 Layer 33 - layer3.2.conv2 589824 84.99 84.97 93.31 93.42 96.73 96.94 98.63 98.86 Layer 34 - layer3.2.conv3 262144 78.29 78.65 88.91 89.40 94.06 94.57 97.31 97.85 Layer 35 - layer3.3.conv1 262144 81.17 81.27 90.86 91.14 95.15 95.52 97.86 98.21 Layer 36 - layer3.3.conv2 589824 85.06 84.94 93.32 93.42 96.77 96.96 98.68 98.88 Layer 37 - layer3.3.conv3 262144 80.29 80.71 89.93 90.42 94.54 95.08 97.54 98.01 Layer 38 - layer3.4.conv1 262144 80.07 80.17 90.20 90.44 94.87 95.19 97.73 98.04 Layer 39 - layer3.4.conv2 589824 84.99 84.95 93.24 93.37 96.75 96.92 98.65 98.88 Layer 40 - layer3.4.conv3 262144 79.24 79.66 89.26 89.77 94.23 94.87 97.47 97.93 Layer 41 - layer3.5.conv1 262144 75.83 75.99 87.68 87.91 93.44 93.81 97.07 97.48 Layer 42 - layer3.5.conv2 589824 84.07 84.07 92.72 92.86 96.45 96.67 98.52 98.75 Layer 43 - layer3.5.conv3 262144 75.90 76.42 87.00 87.59 92.85 93.52 96.74 97.38 Layer 44 - layer4.0.conv1 524288 68.48 68.82 82.43 82.75 90.41 90.78 95.93 96.27 Layer 45 - layer4.0.conv2 2359296 87.47 87.64 94.87 95.04 97.77 97.96 99.20 99.36 Layer 46 - layer4.0.conv3 1048576 75.56 75.85 87.88 88.12 94.33 94.56 97.90 98.14 Layer 47 - layer4.0.downsample.0 2097152 90.08 89.97 96.30 96.29 98.60 98.66 99.54 99.63 Layer 48 - layer4.1.conv1 1048576 79.00 79.16 90.34 90.39 95.69 95.80 98.40 98.58 Layer 49 - layer4.1.conv2 2359296 87.10 87.27 94.85 94.97 97.92 98.05 99.32 99.43 Layer 50 - layer4.1.conv3 1048576 76.30 76.64 88.78 88.75 95.11 95.19 98.37 98.51 Layer 51 - layer4.2.conv1 1048576 69.19 69.42 84.27 84.19 92.98 92.85 97.63 97.69 Layer 52 - layer4.2.conv2 2359296 87.68 87.73 95.85 95.92 98.53 98.63 99.56 99.64 Layer 53 - layer4.2.conv3 1048576 78.33 77.79 91.36 91.17 96.56 96.57 98.85 99.01 Layer 54 - fc 2048000 54.95 53.28 70.49 68.55 83.24 80.79 93.17 90.49 Table S8: The obtained distribution of sparsity across the layers by WoodFisher and Global Magnitude when sparsifying RESNET-50 to 80%, 90%, 95%, 98% levels on IMAGENET. 39 S7.2 MobileNetV1 Module Fully Dense Params Sparsity (%) WoodFisher GlobalMagni WoodFisher GlobalMagni Overall 4209088 75.28 89.00 Layer 1 864 50.93 51.16 55.56 57.99 Layer 2 (dw) 288 47.57 50.00 52.08 55.56 Layer 3 2048 74.02 75.93 81.20 83.40 Layer 4 (dw) 576 18.75 21.01 26.04 30.21 Layer 5 8192 60.05 60.79 73.44 74.34 Layer 6 (dw) 1152 30.30 31.86 39.84 43.75 Layer 7 16384 58.16 58.69 73.55 74.29 Layer 8 (dw) 1152 07.64 07.90 15.02 17.45 Layer 9 32768 65.53 65.94 80.06 80.71 Layer 10 (dw) 2304 33.64 35.68 45.70 48.13 Layer 11 65536 67.88 68.36 82.99 83.45 Layer 12 (dw) 2304 16.02 15.41 25.43 27.26 Layer 13 131072 76.40 76.71 88.93 89.28 Layer 14 (dw) 4608 38.26 38.85 51.22 51.58 Layer 15 262144 80.23 80.33 92.20 92.40 Layer 16 (dw) 4608 49.87 51.65 64.11 65.84 Layer 17 262144 79.29 79.58 91.92 92.04 Layer 18 (dw) 4608 49.80 51.19 64.37 66.43 Layer 19 262144 77.42 77.66 90.90 91.14 Layer 20 (dw) 4608 43.40 44.88 60.31 61.98 Layer 21 262144 74.51 74.67 89.47 89.65 Layer 22 (dw) 4608 30.71 31.62 50.11 51.89 Layer 23 262144 71.09 71.15 87.93 88.18 Layer 24 (dw) 4608 17.12 18.12 41.71 44.34 Layer 25 524288 80.30 80.42 92.62 92.70 Layer 26 (dw) 9216 62.96 64.45 79.37 82.56 Layer 27 1048576 87.58 87.57 96.67 96.80 Layer 28 (fc) 1024000 61.11 60.69 79.91 79.27 Table S9: The obtained distribution of sparsity across the layers by WoodFisher and Global Magnitude when sparsifying MOBILENETV1 to 75%, 89% levels on IMAGENET. 40 S8 FLOPs-aware sparsity distributions Module Fully Dense Params FLOPs-aware (β) Overall 4209088 β = 0.00 (usual) β = 0.30 Layer 1 - conv1 9408 44.97 62.83 Layer 2 - layer1.0.conv1 4096 58.30 75.15 Layer 3 - layer1.0.conv2 36864 79.48 91.88 Layer 4 - layer1.0.conv3 16384 72.16 86.17 Layer 5 - layer1.0.downsample.0 16384 66.31 80.66 Layer 6 - layer1.1.conv1 16384 77.44 90.70 Layer 7 - layer1.1.conv2 36864 82.65 93.73 Layer 8 - layer1.1.conv3 16384 80.57 90.96 Layer 9 - layer1.2.conv1 16384 73.18 89.09 Layer 10 - layer1.2.conv2 36864 73.37 89.84 Layer 11 - layer1.2.conv3 16384 79.29 89.34 Layer 12 - layer2.0.conv1 32768 71.70 87.52 Layer 13 - layer2.0.conv2 147456 83.83 92.60 Layer 14 - layer2.0.conv3 65536 82.89 90.05 Layer 15 - layer2.0.downsample.0 131072 88.98 94.45 Layer 16 - layer2.1.conv1 65536 89.38 94.91 Layer 17 - layer2.1.conv2 147456 87.38 94.12 Layer 18 - layer2.1.conv3 65536 85.39 91.99 Layer 19 - layer2.2.conv1 65536 84.34 92.60 Layer 20 - layer2.2.conv2 147456 87.02 94.08 Layer 21 - layer2.2.conv3 65536 82.19 90.64 Layer 22 - layer2.3.conv1 65536 82.43 91.20 Layer 23 - layer2.3.conv2 147456 84.80 93.00 Layer 24 - layer2.3.conv3 65536 84.12 91.62 Layer 25 - layer3.0.conv1 131072 76.43 87.23 Layer 26 - layer3.0.conv2 589824 91.43 94.65 Layer 27 - layer3.0.conv3 262144 84.20 89.21 Layer 28 - layer3.0.downsample.0 524288 94.23 96.50 Layer 29 - layer3.1.conv1 262144 93.09 95.76 Layer 30 - layer3.1.conv2 589824 93.25 95.89 Layer 31 - layer3.1.conv3 262144 88.41 92.44 Layer 32 - layer3.2.conv1 262144 92.21 95.19 Layer 33 - layer3.2.conv2 589824 93.31 96.04 Layer 34 - layer3.2.conv3 262144 88.91 92.97 Layer 35 - layer3.3.conv1 262144 90.86 94.33 Layer 36 - layer3.3.conv2 589824 93.32 96.13 Layer 37 - layer3.3.conv3 262144 89.93 93.66 Layer 38 - layer3.4.conv1 262144 90.20 94.02 Layer 39 - layer3.4.conv2 589824 93.24 96.11 Layer 40 - layer3.4.conv3 262144 89.26 93.31 Layer 41 - layer3.5.conv1 262144 87.68 92.40 Layer 42 - layer3.5.conv2 589824 92.72 95.83 Layer 43 - layer3.5.conv3 262144 87.00 91.79 Layer 44 - layer4.0.conv1 524288 82.43 88.34 Layer 45 - layer4.0.conv2 2359296 94.87 95.23 Layer 46 - layer4.0.conv3 1048576 87.88 88.61 Layer 47 - layer4.0.downsample.0 2097152 96.30 96.64 Layer 48 - layer4.1.conv1 1048576 90.34 90.97 Layer 49 - layer4.1.conv2 2359296 94.85 95.23 Layer 50 - layer4.1.conv3 1048576 88.78 89.41 Layer 51 - layer4.2.conv1 1048576 84.27 85.22 Layer 52 - layer4.2.conv2 2359296 95.85 96.25 Layer 53 - layer4.2.conv3 1048576 91.36 92.18 Layer 54 - fc 2048000 70.49 50.50 Total Sparsity 90.00% 90.23% FLOPs 594 M 335 M Table S10: The obtained distribution of sparsity across the layers by WoodFisher when sparsifying RESNET50 to 90% sparsity level with FLOPs-aware hyperparameter β = 0.00, 0.30 on IMAGENET. 41 Module Fully Dense Params FLOPs-aware (β) Overall 4209088 β = 0.00 (usual) β = 0.30 β = 0.35 Layer 1 864 50.93 62.15 64.35 Layer 2 (dw) 288 47.57 57.29 58.33 Layer 3 2048 74.02 85.01 86.57 Layer 4 (dw) 576 18.75 31.25 33.16 Layer 5 8192 60.05 77.81 80.57 Layer 6 (dw) 1152 30.30 47.22 50.00 Layer 7 16384 58.16 78.97 81.96 Layer 8 (dw) 1152 07.64 16.32 18.92 Layer 9 32768 65.53 79.57 81.77 Layer 10 (dw) 2304 33.64 47.61 50.35 Layer 11 65536 67.88 82.74 84.92 Layer 12 (dw) 2304 16.02 20.36 21.18 Layer 13 131072 76.40 83.70 84.96 Layer 14 (dw) 4608 38.26 46.18 47.31 Layer 15 262144 80.23 87.68 88.87 Layer 16 (dw) 4608 49.87 58.79 60.76 Layer 17 262144 79.29 87.11 88.49 Layer 18 (dw) 4608 49.80 59.64 61.09 Layer 19 262144 77.42 85.76 87.17 Layer 20 (dw) 4608 43.40 55.73 57.44 Layer 21 262144 74.51 83.81 85.29 Layer 22 (dw) 4608 30.71 47.57 50.24 Layer 23 262144 71.09 81.08 82.82 Layer 24 (dw) 4608 17.12 24.11 24.74 Layer 25 524288 80.30 81.96 82.39 Layer 26 (dw) 9216 62.96 60.45 61.56 Layer 27 1048576 87.58 88.60 88.94 Layer 28 (fc) 1024000 61.11 45.05 42.13 FLOPs 159 M 101 M 92 M Table S11: The obtained distribution of sparsity across the layers by WoodFisher when sparsifying MO- BILENETV1 to 75.28% sparsity level with FLOPs-aware hyperparameter β = 0.00, 0.30, 0.35 on IMA- GENET. 42 S9 WoodTaylor Results S9.1 Pre-trained model Next, we focus on the comparison between WoodFisher and WoodTaylor for the setting of ResNet-20 pre-trained on CIFAR10, where both the methods are used in their ‘full-matrix’ mode. In other words, no block-wise assumption is made, and we consider pruning only the ‘layer1.0.conv1’, ‘layer1.0.conv2’ and ‘layer2.0.conv1’. In Figures S13, S14, we present the results of one-shot experiments in this setting. We observe that WoodTaylor (with damp=1e−3) outperforms WoodFisher (across various dampening values) for almost all levels of target sparsity. This conﬁrms our hypothesis of factoring in the gradient term, which even in this case where the model has relatively high accuracy, can lead to a gain in performance. However, it is important to that in comparison to WoodFisher, WoodTaylor is more sensitive to the choice of hyper-parameters like the dampening value, as reﬂected in the Figure S13. This arises because now in the weight update, Eqn. (21), there are interactions between the Hessian inverse and gradient terms, due to which the scaling of the inverse Hessian governed by this dampening becomes more important. To give an example, in the case where damp=1e − 5, the resulting weight update has about 10× bigger norm than that of the original weight. Figure S13: Comparing one-shot sparsity results for WoodTaylor and WoodFisher on CIFAR-10 for ResNet-20. Figure S14: A simpliﬁed comparison of one-shot sparsity results for WoodTaylor and WoodFisher on CIFAR-10 for ResNet-20. This can be easily adjusted via the dampening, but unlike WoodFisher, it is not hyper-parameter free. Also, for these experiments, the number of samples used was 50,000, which is higher in comparison to our previously used values. 43 S9.2 Ablation Study In order to better understand the sensitivity of WoodTaylor with respect to these hyper-parameter choices, we present an ablation study in Figure S15 that measures their effect on WoodTaylor’s performance. Figure S15: Ablation study for WoodTaylor that shows the effect of dampening and the number of samples used on the performance. In the end, we conclude that incorporating the ﬁrst-order term helps WoodTaylor to gain in perfor- mance over WoodFisher, however, some hyper-parameter tuning for the dampening constant might be required. Future work would aim to apply WoodTaylor in the setting of gradual pruning discussed in Section S4.2. 44","libVersion":"0.3.2","langs":""}