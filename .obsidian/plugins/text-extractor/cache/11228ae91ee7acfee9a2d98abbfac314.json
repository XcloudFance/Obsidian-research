{"path":"SJTU/Data Curation - Compression - Efficiency - Filtering - Distillation/images/image.png","text":"Study Domain Model Parameter Count Total Unique CPT Tokens Minerva (Lewkowycz et al., 2022) STEM 8B, 62B, 540B 26B-38.5B MediTron (Chen et al., 2023) Medicine 7B, 70B 46.7B Code Llama (Roziere et al., 2024) Code 7B, 13B, 34B 520B-620B Llemma (Azerbayev et al., 2024) Math 7B, 34B 50B-55B DeepSeekMath (Shao et al., 2024) Math 7B 500B SaulLM-7B (Colombo et al., 2024b) Law 7B 30B SaulLM-{54, 141}B (Colombo et al., 2024a) Law 54B, 141B 520B HEAL (Yuan et al., 2024a) Medicine 13B 14.9B Our setting Articles & Books 7B 1.3M Table 1: Comparing the scale of modern continued pretraining (CPT) works with our small corpus setting. Prior work adapts language models to broad domains with diverse, large-scale corpora. We aim to downscale continued pretraining to small corpora; we use a corpus that is 10,000x smaller than the smallest modern corpus for domain-adaptive CPT.","libVersion":"0.3.2","langs":"eng"}