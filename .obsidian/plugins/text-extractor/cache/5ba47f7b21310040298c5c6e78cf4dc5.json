{"path":"SJTU/Data Curation - Compression - Efficiency - Filtering - Distillation/pdfs/Yang 等 - 2024 - Synthetic continued pretraining.pdf","text":"SYNTHETIC CONTINUED PRETRAINING Zitong Yang ∗ Department of Statistics Stanford University Neil Band ∗ Department of Computer Science Stanford University Shuangping Li Department of Statistics Stanford University Emmanuel Cand`es Department of Statistics Stanford University Tatsunori Hashimoto Department of Computer Science Stanford University ABSTRACT Pretraining on large-scale, unstructured internet text has enabled language models to acquire a significant amount of world knowledge. However, this knowledge acquisition is data-inefficient—to learn a given fact, models must be trained on hundreds to thousands of diverse representations of it. This poses a challenge when adapting a pretrained model to a small corpus of domain-specific docu- ments, where each fact may appear rarely or only once. We propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing con- tinued pretraining on the synthesized corpus. We instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source documents and then generates diverse text by drawing connec- tions between the sampled entities. Synthetic continued pretraining using Enti- Graph enables a language model to answer questions and follow generic instruc- tions related to the source documents without access to them. If instead, the source documents are available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation. To better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can “rearrange” knowledge to enable more data-efficient learning. 1 INTRODUCTION Language models have demonstrated a remarkable ability to acquire knowledge from unstructured text, enabling them to perform challenging knowledge-intensive tasks (Brown et al., 2020; OpenAI et al., 2024; Gemini, 2024; Anthropic, 2024b; Dubey et al., 2024; Gunter et al., 2024). These suc- cesses are enabled by the combination of the next-token prediction objective (Shannon, 1951) and large-scale internet data (Common Crawl, 2007). However, it is becoming increasingly apparent that this approach is data-inefficient; for example, a 13-year-old human acquires knowledge from fewer than 100M tokens, while state-of-art open-source language models are trained on 15T to- kens (Warstadt et al., 2023; Dubey et al., 2024). Recent works have highlighted a range of related problematic phenomena, including the “reversal curse”, where models struggle to learn the relation “B=A” when trained on “A=B” (Berglund et al., 2023), and the requirement that models be exposed to thousands of examples per fact for knowledge acquisition (Allen-Zhu & Li, 2024). These drawbacks pose a challenge when adapting the next-token prediction paradigm to learn from small-scale corpora. Because large-scale pretrained models already capture much of public common knowledge, further advancements will necessitate learning from the tails of the distribution (Kandpal et al., 2023): niche data that is either contained in small, private domains or appears only once or twice on the internet. This challenge of data-efficient, parametric knowledge acquisition is becoming increasingly important as growing compute capacity enables language model providers to exhaust publicly available data (Muennighoff et al., 2023; Villalobos et al., 2024). ∗Equal contribution. Correspondence to: zitong@berkeley.edu, nband@cs.stanford.edu. 1arXiv:2409.07431v1 [cs.LG] 11 Sep 2024 We propose to address this problem of acquiring knowledge from small corpora with synthetic con- tinued pretraining. To illustrate, consider the problem of teaching a language model a new area of mathematics, succinctly documented by a small set of authoritative textbooks. Directly training the model on those textbooks is unlikely to be effective due to the limited volume of text (typically only tens of thousands of words), and the model will struggle to generalize from this compressed representation of knowledge. In contrast, learning well-established areas of mathematics like linear algebra is more straightforward, because a large-scale corpus with diverse knowledge representa- tions is accessible: for example, online lecture notes, Stack Exchange discussions, or Python imple- mentations of the singular value decomposition. Synthetic continued pretraining bridges this gap by first converting a small and data-constrained domain into a synthetic corpus with diverse knowledge representations, and then continuing pretraining on it. One basic approach is to simply paraphrase or rewrite the source documents in multiple ways. How- ever, we demonstrate that this generic rephrasing cannot cover the gap in the diversity of knowledge representations. After a couple rounds of rephrasing a small corpus, the rephraser fails to synthesize novel data that further contributes to learning, and the model performance saturates. We attribute this failure to the lack of diversity in paraphrasing alone. In the linear algebra example, online lecture notes and Stack Exchange discussions go beyond a simple rewrite of any textbook—they provide deeper analysis and application of the underlying concepts and techniques. To address this shortcoming, we propose EntiGraph, an entity-centric augmentation algorithm. Enti- Graph first breaks down a text corpus into a list of entities and then uses a language model to generate text descriptions about relations among the extracted entities, iteratively “filling in” the knowledge graph underlying the corpus. To concretely measure progress towards effective knowledge acquisition from small corpora, we propose an experimental setting based on a standard reading comprehension dataset (QuAL- ITY, Pang et al. (2022)). This setup enables the evaluation of synthetic data generation methods for data-efficient learning without incurring the high compute costs of pretraining from scratch. Specifically, we evaluate methods in a scenario where we are given access to a collection of 265 books, totaling 1.3M tokens. Our task is to synthesize a corpus such that continued pretraining on it enables a model to answer queries (e.g., multiple-choice QA or user instructions related to the book content) without access to the source texts. In our main experiments (§5), we use EntiGraph to generate 600M synthetic tokens from 1.3M real tokens using gpt-4-turbo (OpenAI et al., 2024). Then, we continually pretrain Llama 3 8B (Dubey et al., 2024) on the synthetic tokens and evaluate its QA accuracy on the QuALITY question set. We observe a log-linear scaling trend in the accuracy as the number of tokens increases, up to 600M synthetic tokens (§4.2). At the endpoint, we find that synthetic continued pretraining with 600M EntiGraph tokens provides 80% of the accuracy improvement of having those source documents available at inference time (§5). Beyond QA accuracy, we also perform instruction tuning on the continually pretrained model and find that it is capable of following open-ended instructions (e.g., summarization) related to the QuALITY books (§4.3). To summarize, our key contributions are as follows: • We propose to learn from small corpora with synthetic continued pretraining—converting the small corpus into a large, diverse, synthetic corpus and continuing pretraining on it—and instan- tiate this approach using the EntiGraph synthetic data augmentation algorithm (§2.2). • We demonstrate that continued pretraining on the EntiGraph-synthesized corpus yields a QA accuracy scaling trend that is log-linear in the synthetic token count, whereas continued pre- training on the original documents or paraphrases yields little-to-no improvement compared with our method (§4.2). Furthermore, we show that instruction tuning the EntiGraph continually pre- trained model enables it to follow more diverse queries related to the source documents (§4.3). • We complement the main experiments with an open-book setup (§5), providing the model with access to the source documents when answering queries. We demonstrate that the knowledge acquired through synthetic continued pretraining with EntiGraph is complementary to the knowl- edge accessed through retrieval-augmented generation (RAG, Lewis et al. (2020))—RAG with the EntiGraph continually pretrained model outperforms RAG with the base model. • Lastly, we build a mathematical model that captures the intuition behind synthetic data augmen- tation with EntiGraph. Analysis of this model provides a parametric formula for the scaling trend 2 of a continually pretrained model’s accuracy with respect to EntiGraph synthetic tokens, which closely matches our empirical observations (§6). Practically, synthetic continued pretraining using EntiGraph enables pretrained language models to adapt to specialized domains by acquiring parametric knowledge, as opposed to the non-parametric knowledge accessed through retrieval methods. At a higher level, our approach points toward a family of synthetic data generation algorithms that allow us to convert compute into data efficiency for (continued) pretraining (Kaplan et al., 2020). 1.1 RELATED WORK Synthetic data generation. There is a rich literature on using neural nets to generate synthetic data. Many such approaches were originally developed for semi-supervised learning—self-training and pseudo-labeling methods improve models by iteratively training them on their own predictions (Scudder, 1965; Lee, 2013; Yalniz et al., 2019; Berthelot et al., 2019; Xie et al., 2020), and co- training uses two models to supervise each other (Blum & Mitchell, 1998; Balcan et al., 2004). Before language models rose to prominence, few approaches attempted to synthesize inputs. One exception is membership query synthesis, which explored the synthesis of inputs in a supervised learning context (Angluin, 1988; Schumann & Rehbein, 2019). Contemporary works employ co-training (Lang et al., 2022) and self-training to improve language model performance, often on mathematical reasoning tasks (Huang et al., 2023; Gulcehre et al., 2023; Zhang et al., 2024a), or synthesize input-output pairs for instruction tuning, usually by con- ditioning on a curated seed set (Wang et al., 2023b; Honovich et al., 2023; Taori et al., 2023; Peng et al., 2023; Yuan et al., 2024b; Li et al., 2024). Most relevant to the present work are methods that synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children’s stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b; Abdin et al., 2023; 2024). However, their datasets and prompts are not publicly available. Maini et al. (2024) prompt an LM to rephrase documents for pretraining, improving training effi- ciency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) propose a fact-based synthetic generation approach but did not show improvement on generic instruction following tasks beyond simple QA. Ovadia et al. (2024) continually pretrain Llama 2–based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements. We adapt the approach of Maini et al. (2024) and Mecklenburg et al. (2024) to our small corpus setting as the Rephrase baseline in §4. We find that our graph-based augmentation algorithm outperforms it, likely because our approach enforces diversity through entity-based generation. Continual learning and pretraining. Continual learning is rooted in historical work on connec- tionist networks (McCloskey & Cohen, 1989; Ratcliff, 1990) and considers learning with tasks ar- riving in an online manner (Schlimmer & Fisher, 1986; Grossberg, 2012). The main focus is on mitigating a neural net’s “catastrophic forgetting” of previously encountered tasks (Robins, 1995; Goodfellow et al., 2015; Kemker et al., 2018). Approaches include regularizing parameter updates to preserve important parameters (Nguyen et al., 2017; Zenke et al., 2017; Kirkpatrick et al., 2017); dynamically modifying the architecture (Rusu et al., 2016; Golkar et al., 2019); and recalling or replaying previous experiences (Rebuffi et al., 2017; Shin et al., 2017; Lopez-Paz & Ranzato, 2017). Continual or continued pretraining works (Gururangan et al., 2020) successfully adapt pretrained large language models to broad target domains such as code (Rozi`ere et al., 2024), medicine (Chen et al., 2023), or mathematics (Lewkowycz et al., 2022; Shao et al., 2024; Azerbayev et al., 2024) by curating massive datasets (often >100B tokens, shown in Table 1) and developing efficient training recipes using causal language modeling (Gupta et al., 2023; Ibrahim et al., 2024; Parmar et al., 2024). Catastrophic forgetting is effectively mitigated by scaling parameter count (Ramasesh et al., 2022) and mixing in updates on pretraining data (Ouyang et al., 2022). This work aims to extend the success of continued pretraining to small, specialized domains such as proprietary document 3 Study Domain Model Parameter Count Total Unique CPT Tokens Minerva (Lewkowycz et al., 2022) STEM 8B, 62B, 540B 26B-38.5B MediTron (Chen et al., 2023) Medicine 7B, 70B 46.7B Code Llama (Rozi`ere et al., 2024) Code 7B, 13B, 34B 520B-620B Llemma (Azerbayev et al., 2024) Math 7B, 34B 50B-55B DeepSeekMath (Shao et al., 2024) Math 7B 500B SaulLM-7B (Colombo et al., 2024b) Law 7B 30B SaulLM-{54, 141}B (Colombo et al., 2024a) Law 54B, 141B 520B HEAL (Yuan et al., 2024a) Medicine 13B 14.9B Our setting Articles & Books 7B 1.3M Table 1: Comparing the scale of modern continued pretraining (CPT) works with our small corpus setting. Prior work adapts language models to broad domains with diverse, large-scale corpora. We aim to downscale continued pretraining to small corpora; we use a corpus that is 10,000× smaller than the smallest modern corpus for domain-adaptive CPT. stores. Observing that standard continued pretraining is ineffective on small corpora, we propose a knowledge graph–inspired approach to synthesize a diverse related corpus and find it more amenable to learning. Knowledge editing. A related line of literature updates language models with small units of fac- tual knowledge, such as (subject, relation, object) tuples. Zhu et al. (2020) studies a constrained fine-tuning approach, limiting the model’s complexity to better suit the learning of simple factual relations. Later approaches attempt to localize where factual knowledge is stored in Transformers and update only those weights (Mitchell et al., 2022; Meng et al., 2022; 2023), or maintain an ex- ternal memory of edits and prepend them as context during generation (Zhong et al., 2023; Cohen et al., 2023). Most relevant to our work is deductive closure training (Aky¨urek et al., 2024), which first deduces implications of a factual edit and then finetunes the language model on those implica- tions. The line of knowledge editing differs from our setting in that we aim to learn from a small corpus of documents, rather than atomic, sentence-length facts. 2 OUR METHOD We focus on learning parametric knowledge from a small text corpus. More specifically, our goal is to continually pretrain a language model to acquire the knowledge of a niche corpus of documents. Observing that simple continued pretraining on this source corpus is ineffective (§4), we propose to use synthetic continued pretraining, which first uses the small corpus to synthesize a larger one more amenable to learning, and then continues pretraining on the synthetic corpus. In this section, we first outline this problem setting and our evaluation approach in more detail (§2.1). Then, we provide a concrete instantiation of synthetic continued pretraining using a data augmentation algorithm called EntiGraph (§2.2). 2.1 PROBLEM SETUP Continued pretraining effectively adapts language models to various downstream domains such as mathematics (Lewkowycz et al., 2022; Azerbayev et al., 2024), medicine (Chen et al., 2023) or law (Colombo et al., 2024a;b), but it requires diverse, large-scale corpora with billions of tokens (Table 1). Given the success of continued pretraining in high-resource domains, we aim to extend it to niche domains with only small, specialized text corpora available. Continued pretraining on small corpora. We focus on approaches that use continued pretraining to teach a pretrained language model the knowledge of a small set of source documents Dsource. These approaches acquire “parametric knowledge”, i.e., the knowledge of Dsource is learned in the model’s parameters much like during the pretraining process. Synthetic continued pretraining (synthetic CPT). We find that simple continued pretraining fails to acquire the knowledge in a small corpus Dsource, even with data repetition (§4). We hypoth- esize that this failure occurs because Dsource is highly condensed (e.g., 1.3M tokens in our experi- 4 ments) and lacks diversity in how its underlying knowledge is represented. To address this failure, we propose to use a two-step synthetic continued pretraining procedure (synthetic CPT, where CPT stands for continued pretraining). First, we apply a synthetic data generation algorithm Asynth to convert a small corpus Dsource into a synthetic corpus Dsynth: Asynth : Dsource ↦−→ Dsynth. (1) Then, we perform continued pretraining on Dsynth instead of directly on Dsource. In practice, we use language models to implement Asynth. A natural concern is that the language models may hallucinate and fabricate false knowledge. Therefore, we focus on synthetic data augmentation algorithms that condition the generation process on the source documents to improve the synthesized data’s faithfulness. Evaluation with knowledge-intensive queries. We evaluate the quality of a synthetic data aug- mentation algorithm Asynth by testing whether the downstream synthetic CPT model has effectively acquired the knowledge of Dsource in its parameters. More precisely, we curate some test queries Qtest that probe the knowledge about Dsource acquired by the model. For example, in the linear alge- bra setting, Qtest could be held-out exam questions. To test parametric knowledge, we do not allow the model to access the source documents Dsource at test time. Therefore, the queries cannot be am- biguous without access to Dsource. For example, a reading comprehension question like “Where was he born?” is ambiguous without context. Altogether, we can evaluate data augmentation algorithms Asynth for synthetic CPT using a paired source corpus and related test queries (Dsource, Qtest). Success criterion. We treat retrieval-based approaches as an upper bound, given their strong per- formance in adapting LMs to small corpora (Lewis et al., 2020). In other words, we consider a data augmentation algorithm Asynth to be successful for synthetic CPT if the performance of the contin- ually pretrained model on Qtest approaches the performance of a retrieval approach with test-time access to Dsource. 2.2 ENTIGRAPH Next, we present EntiGraph, our instantiation of a synthetic data augmentation algorithm Asynth. At a high level, EntiGraph generates diverse representations of knowledge from a small corpus Dsource by using a prompted LLM to synthesize a knowledge graph representation of Dsource. In practice, EntiGraph is an operation applied independently to each seed document Di ∈ Dsource, so in the following, we use D to refer to a given document. For each document D, EntiGraph uses a prompted language model LMaug to generate a knowledge graph over the entities of the document in natural language. EntiGraph consists of three steps/prompts: extracting entities from the document, describing single entities in the context of the document, and finally, analyzing relations between arbitrary subsets of the entities. Altogether, this hierarchical prompting strategy externalizes the problem of generating diverse synthetic text to a combinatorial structure—namely, a graph relating various entities appearing in the corpus documents. In what follows, we provide abbreviated prompts to illustrate the algorithm, and defer full prompts to Appendix B.1. Step 1: Entity extraction. First, EntiGraph extracts a list of salient entities {E1, E2, . . . , En} from the document D by prompting LMaug with an entity extraction prompt: {E1, E2, . . . , En} ∼ LMaug(entity extraction(D) ). (2) Concretely, the entity extraction prompt is abbreviated below: ## System message As a knowledge analyzer, identify salient entities in the given text. Include: (a) Names (b) People (c) Places (d) Concepts, etc. ## User * Document {document_text} In the linear algebra example, D could be one specific linear algebra textbook. We would expect to extract entities such as {E1 = Linear space, E2 = Vector, E3 = SVD, . . . }. 5 Step 2: Single entity description. Next, we will generate additional information on each of the extracted entities Ei in the context of their source document D. From the knowledge graph perspec- tive, we wish to generate data that enables the language model to learn what each entity is. EntiGraph iterates over extracted entities Ei, i ∈ {1, . . . , n} and applies an entity description prompt to generate a synthetic document ̃DEi focusing on the role of entity Ei in its source document D: ̃DEi ∼ LMaug(entity description(D, Ei)). (3) Below is an abbreviated entity description prompt: ## System message As a knowledge analyzer, analyze the given text focusing on the specified entity. Summarize or formulate questions highlighting the entity’s role in the document. ## User * Document {document_text} * Entity {entity_name} In the linear algebra example, if E1 = Linear space, a synthetic document describing E1 in the context of D could be ̃DE1 = “According to the provided document, a linear space is a set satisfying the following algebraic rules...”. Alto- gether, in Step 2, we sample a synthetic document for each extracted entity to form a synthetic corpus { ̃DE1 , ̃DE2, . . . , ̃DEn }. Step 3: Relation analysis. The last step of EntiGraph analyzes the relations among subsets of entities. The intuition is to thoroughly explore the edges of the knowledge graph underlying the source document D, analogous to a student writing diverse notes about a linear algebra textbook. We apply a relation analysis prompt to describe how a subset of k ≤ n entities are related in the context of the source document D, obtaining a synthetic document ̃DEi1 ...Eik : ̃DEi1 ...Eik ∼ LMaug(relation analysis(D, Ei1 , Ei2, . . . , Eik ) ). (4) Specifically, we use prompt ## System message Analyze relations among given entities in the provided text. Discuss how their interactions shape the document’s content. ## User * Document {book_text} * Entities {entity_name_1}, {entity_name_2}, ... In practice, it is impractical to exhaustively enumerate all possible subsets of the n extracted entities, so in our experiments, we choose to generate all pairs of ̃DEiEj and triplets ̃DEiEj Ek . EntiGraph synthetic corpora. Finally, we collect all sampled synthetic texts from (3) and (4) as the output of the EntiGraph algorithm ̃D = { ̃DE1, . . . , ̃DEi1 ...Eik , . . . } applied to source document D. We apply this same procedure to each document D ∈ Dsource, and denote the collection of all synthetic text as DEntiGraph = { ̃D1, ̃D2, . . . }. Altogether, we have described a synthetic data augmentation algorithm mapping a small corpora of source documents to a larger synthetic corpora DEntiGraph, as in (1). We provide qualitative examples of a synthetic single entity description ̃DEi and pairwise relation description ̃DEiEj in Figure 1. 3 EXPERIMENT SETUP In this section, we describe in detail how we evaluate a given data augmentation algorithm Asynth. As described in the problem setup (§2.1), we evaluate such algorithms Asynth by evaluating whether a 6 Single entity description: E =“enamel” in the context of the article D = “Defining Decay Down”. ### Discussion of “Defining Decay Down” in relation to enamel The article “Defining Decay Down” by David Plotz dives into the way dentistry has adapted to changing circumstances. One of the focal points is enamel, the hard outer layer of teeth that is critical to dental health. The article vividly describes how the use of intra-oral cameras by dentists can exaggerate the appearance of minor flaws in a patient’s enamel, making the most insubstantial microcracks appear severe and in dire need of correction. This technological advancement has played a pivotal role in incentivizing patients to opt for cosmetic procedures to improve the appearance of their enamel, aligning with the self-improvement trends of recent decades. [...] ### Questions and answers about enamel in the context of “Defining Decay Down” Question: How does the article “Defining Decay Down” portray the general public’s perception of enamel as part of their dental aesthetics, and what does this suggest about societal values related to self-image? Answer: The article suggests that society places high value on dental aesthetics, including the appear- ance of enamel. For instance, when the intra-oral camera magnifies teeth, showing microcracks in the enamel, it is reminiscent of a “broken window,” which negatively influences public perception of personal dental health and aesthetics (Paragraph 1). The article also notes the popularity of services like teeth whitening that directly affect the visibility of enamel, reinforcing the idea that society pri- oritizes a perfect, unblemished appearance (Paragraph 6). This suggests that societal values highly prioritize self-image, superficial appearances, and immediate cosmetic enhancement over long-term dental health. [...] Relation analysis: E1 =“enamel” and E2 = “fluoride” in the context of D = “Defining Decay Down”. ### Discussion of Interaction between enamel and fluoride in context of “Defining Decay Down” The interplay between enamel and fluoride within the context of “Defining Decay Down” is a telling one, as it underpins the significant shift in dentistry’s focus over the years. On one hand, enamel, as the protective outer layer of our teeth, reveals the tales of oral neglect when examined up close. It is the deterioration and the visible imperfections found in the enamel that dentists capitalize on, encouraging patients to undergo cosmetic procedures to rectify these flaws. Fluoride, conversely, has been instrumental in preventing these imperfections from arising in the first place. Its role in enhancing enamel’s resistance to decay has resulted in fewer cavities and the subse- quent need for traditional dental interventions. The interaction demonstrates a dichotomy: fluoride’s contribution to oral health inadvertently compels dentists to pivot toward the marketability of aesthetic dentistry, since structural dental issues have become less common [...] Figure 1: Qualitative examples of data synthesized with EntiGraph. language model continually pretrained on their output synthetic corpus Asynth(Dsource) can accurately answer test queries Qtest about the source documents Dsource. In our main experiments, we use queries that are unambiguous even without the source documents Dsource, and disallow the model from accessing Dsource while answering the queries Qtest (§2.1). This allows us to evaluate which data augmentation algorithm best promotes the acquisition of parametric knowledge through synthetic CPT. Later, in §5, we consider an open-book setting where the model can access both the source documents Dsource and test queries Qtest at the same time, in order to test how the parametric knowledge acquired through synthetic CPT composes with non-parametric access to knowledge through retrieval (Lewis et al., 2020). We next introduce the pairing of small corpus and related test queries (Dsource, Qtest) that we use in our experiments. QuALITY corpus Dsource. Our corpus and test queries are based on the QuALITY dataset (Pang et al., 2022), a long-document comprehension benchmark. The QuALITY corpus Dsource is com- posed of 265 articles and short books on genres ranging from science fiction to journalism, with an average length of ∼5,000 tokens. 7 QuALITY test queries Qtest. To source the test queries Qtest, we use the 10-20 multiple choice questions accompanying each article in QuALITY. These questions serve as high-quality knowl- edge probes on Dsource, but the query phrasing often presupposes the reading comprehension context (e.g., “What does the author think about...”). We ensure that each query is unambiguous by con- textualizing them with the corresponding article reference: “In the context of article {article name} by {author name}, what does the author think about...”. Altogether, this provides us with 4,609 unambiguous queries Qtest to test the parametric knowledge of our continually pretrained language models. 2,316 of 4,609 queries are labeled as “hard” by the dataset curator, so we also report results on both “Hard” and “Easy” splits. Evaluation on instruction-tuned summarization. In addition to evaluation using the above test queries Qtest, we also instruction tune the continually pretrained LMs and evaluate them on more general instruction following queries. Specifically, we evaluate the closed-book summarization abil- ities of the instruction-tuned models by prompting them to generate summaries of QuALITY articles given only their title and author. Performance with strong API-based LLMs. For our continued pretraining setting, we must se- lect a corpus Dsource that is not already well-represented in standard pretraining datasets. As an initial test of the obscurity of the QuALITY corpus Dsource, we evaluate GPT-3.5 (Brown et al., 2020) and GPT-4 (OpenAI et al., 2024) on Qtest. In the closed-book setting, we find GPT-3.5 accuracy at 44.81% and GPT-4 accuracy at 51.30% (Table 2). In the open-book setting, i.e., with the entire article placed in the prompt, we find GPT-3.5 accuracy at 72.60% and GPT-4 accuracy at 86.09% (Table 4). Based on the large (∼30%) improvement when Dsource is provided, we conclude that the QuALITY corpus Dsource is sufficiently niche to serve as an appropriate testbed. 4 MAIN EXPERIMENTS In this section, we present our main experimental results 1. Using GPT-4 2 as our prompted model LMaug, we apply EntiGraph to the 1.3M token QuALITY corpus Dsource, generating a 600M token synthetic corpus. For the remainder of the paper, we refer to the former as the “Raw corpus” and the latter as the “EntiGraph corpus”. Additional details on these corpora are provided in Appendix C. We continually pretrain Llama 3 8B (Dubey et al., 2024) with standard causal language modeling on the 600M token EntiGraph corpus. In §4.1, we describe our continued pretraining procedure and introduce two natural baselines. In §4.2, we evaluate all methods on the QuALITY test queries Qtest and find synthetic CPT using EntiGraph significantly outperforms both baselines; moreover, its accuracy scales log-linearly with the amount of EntiGraph synthetic data, up to 600M tokens. In §4.3, we show that synthetic CPT using EntiGraph is compatible with downstream instruction tuning (Ouyang et al., 2022), an important feature of real pretraining data. 4.1 CONTINUED PRETRAINING PROCEDURE In all experiments, we continue pretraining the Llama 3 8B Base model with a context length of 2048 and batch size of 16. We apply a linear learning rate warmup for 5% of total steps, followed by a cosine decay with peak learning rate 5e-6. We perform full parameter training with Fully Sharded Data Parallelism (FSDP, Zhao et al. (2023)). To mitigate the forgetting of pretrained knowledge, we perform replay with a rate of 0.1 using 1B RedPajama tokens (TogetherAI, 2023). More precisely, for each training batch, we flip a biased coin such that with 10% probability, we load the RedPajama data instead of the EntiGraph synthetic data. EntiGraph CPT. In our main continued pretraining experiment, we continually pretrain Llama 3 8B Base on the 600M token EntiGraph corpus for 2 epochs. For the remainder of the work, we will refer to this continually pretrained model as “EntiGraph CPT”. Next, we describe two baselines which we compare to EntiGraph CPT in closed-book QA (§4.2) and instruction-tuned summariza- tion (§4.3) evaluations. 1Code to reproduce all experiments is provided at https://github.com/ZitongYang/ Synthetic_Continued_Pretraining.git. 2We use the gpt-4-turbo model as of Aug. 19, 2024. 8100 101 102 Number of synthetic tokens (in Millions) 0.35 0.40 0.45 0.50 0.55QA Accuracy EntiGraph CPT Rephrase CPT Raw CPT Llama 3 8B Base Figure 2: Accuracy on the QuALITY question set Qtest (y-axis) as a function of the synthetic token count (x-axis). The accuracy of synthetic continued pretraining using the EntiGraph data augmentation algorithm (EntiGraph CPT) scales log-linearly up to 600M tokens, while the Rephrase baseline saturates near 2M tokens. Raw CPT baseline. The first natural baseline is to continually pretrain Llama 3 8B Base on the Raw corpus (the raw QuALITY articles Dsource, defined in §3). Because the Raw corpus only has 1.3M tokens, we jointly tune the number of epochs (repetition factor) and the RedPajama replay rate on accuracy over a QuALITY QA validation split. The selected hyperparameter configuration uses 4 epochs and a 0.1 replay rate. We will refer to this model obtained with continued pretraining on the Raw corpus as “Raw CPT”. Rephrase CPT baseline. As mentioned in §1, one simple synthetic data augmentation procedure is to rephrase QuALITY articles with a generic paraphrasing prompt. Maini et al. (2024) and Ovadia et al. (2024) execute a systematic extension of this idea. In particular, Maini et al. (2024) rephrases real pretraining data into four different styles—easy, medium, hard, and QA—using a rephrase model, and then pretrains an LM from scratch on the rephrased data. We adapt this synthetic data augmentation algorithm to our small corpus setting, using the same prompted model LMaug as is used with EntiGraph (gpt-4-turbo). We refer to this data augmentation algorithm inspired by Maini et al. (2024) and Ovadia et al. (2024) as the “Rephrase baseline”. We apply each of the four prompts to every document in Dsource, resulting in roughly 0.9M tokens. We found that the limited diversity of LMaug with these prompts made it difficult to obtain further improvements with the same prompt, and stopped after a second pass with a total of 1.8M tokens. We will refer to this data as the Rephrase corpus. We continually pretrain Llama 3 8B Base on this corpus using the same hyperparameter tuning procedure as above. We will refer to this model as “Rephrase CPT”. 4.2 QUESTION-ANSWERING EVALUATIONS Next, we provide the detailed setup of our closed-book QA evaluations with QuALITY test queries Qtest, and present results. Evaluation procedure. Each QuALITY question is a four-choice, single-answer multiple choice question (similar to MMLU, Hendrycks et al. (2021)). We evaluate with 4-shot chain-of-thought prompting (Brown et al., 2020; Wei et al., 2024) and provide our prompt in Appendix D.1. As few-shot examples, we use manually drafted and fact-checked QA pairs. To avoid leakage about information in the QuALITY books, we use books that are well-known and not contained in the QuALITY test set. 9 Split Continually Pretrained Llama 3 8B Base Models and API-Based LLMs EntiGraph Rephrase Raw Llama 3 8B GPT-4 GPT-3.5 All 56.42 43.08 38.15 39.49 51.30 44.81 Hard 48.15 36.98 33.66 35.08 42.13 38.07 Easy 64.75 49.23 42.65 43.93 60.55 51.60 Table 2: QuALITY accuracy over all test queries Qtest (All), and the Easy and Hard splits. The left set of columns are Llama 3 8B continually pretrained on various data sources. The right set of columns are the base model and API-based LLMs not finetuned on QuALITY-related data. Enti- Graph CPT outperforms the Rephrase and Raw CPT baselines. EntiGraph scaling. We find that continued pretraining on the 600M token EntiGraph corpus improves closed-book QA accuracy from 39.49% (for Llama 3 8B Base) to 56.42% (Figure 2, Table 2). A natural question is how performance scales as we synthesize and train on more tokens with EntiGraph. To test this, we randomly subsample without replacement the EntiGraph corpus with varying sample sizes, continually pretrain Llama 3 8B Base on each subsample, and plot QuALITY accuracy with respect to sample size in Figure 2. We observe log-linear scaling of the accuracy in the number of synthetic tokens used for continued pretraining, up to 600M tokens. We will mathematically investigate the scaling properties of EntiGraph in detail in §6. In broad strokes, we postulate that QuALITY accuracy follows a mixture-of-exponential shape and follows three stages: (i) linear growth, (ii) log-linear growth, and (iii) asymptotic plateau. Comparison with baselines. In contrast, Rephrase CPT obtains an accuracy of 43.08%. Recall that applying the four rephrase prompts to each QuALITY article results in only 0.9M tokens, and the performance of Rephrase CPT plateaus near the 0.9M token threshold (Figure 2). To demon- strate this plateau more clearly, we repeated this synthetic data generation process to obtain a total Rephrase corpus of 1.8M tokens and found nearly identical performance to the 0.9M token model. In contrast, the EntiGraph CPT accuracy continues to increase past the 1.8M token point, all the way to 600M tokens. Raw CPT performs even worse than Llama 3 8B Base (dashed black line in Figure 2, Table 2). We postulate two reasons for this: (i) The Raw corpus follows a narrower, different distribution than the Llama 3 8B pretraining corpus, and heavily training on these tokens may harm the overall English capabilities of the model. (ii) The limited diversity of knowledge representations in the Raw corpus leads to limited knowledge acquisition due to problems such as the reversal curse (Berglund et al., 2023). Discussion of results. We highlight two takeaways from our continued pretraining experiments. First, when the source document corpus Dsource is small and, therefore, its knowledge is presented in a condensed representation, continued pretraining with next-token prediction fails to teach the model the knowledge in Dsource, even with heavy repetition. Second, generating a larger synthetic dataset grounded by the source documents in Dsource can enable a continually pretrained model to learn the knowledge of Dsource. However, for synthetic CPT to scale, the synthetic data must be sufficiently diverse. EntiGraph tackles this problem using a hierarchical prompting strategy, which externalizes diversity to the combinatorial relationships encoded in entity knowledge graphs. 4.3 INSTRUCTION FOLLOWING EVALUATIONS In this section, we explore more general test queries beyond the QuALITY test queries Qtest. Con- cretely, we train EntiGraph Instruct by performing instruction tuning on EntiGraph CPT. We demon- strate that synthetic CPT on the EntiGraph corpus is compatible with instruction tuning. In partic- ular, EntiGraph Instruct can directly use knowledge obtained during synthetic CPT in instruction following tasks (Wei et al., 2022), without any test-time access to the QuALITY books and articles Dsource. 10 ➤ Explicit article reference: Summarize the article “Defining Decay Down” by “David Plotz”. The article “Defining Decay Down” by David Plotz discusses the evolution of den- tistry in America. It highlights how dentists have adapted to changing times by shift- ing their focus from essential dental work to elective cosmetic procedures. [...] ➤ Implicit article reference: How has the practice of dentistry in the United States changed? The dentistry practice in the United States has undergone significant changes over the years. Here are some of the key changes: 1. Increased focus on cosmetic dentistry: In recent years, [...] 2. Improved dental technology: Advances in dental technology [...] 3. Increased emphasis on preventive care: Dentists have become more focused [...] 4. Increased competition: The dental industry has become more competitive, [...] 5. Improved patient experience: [...] ➤ Cross article instruction: Compare David Plotz’s commentary style between how he analyzed American dentistry and how he discussed the movie Fight Club? David Plotz’s commentary style is analytical and critical in both his analysis of Amer- ican dentistry and his discussion of the movie Fight Club. In both cases, he uses a sharp, incisive tone to critique the subject matter, highlighting the absurdity and ex- cesses of American dentistry and the nihilistic philosophy of Fight Club. However, there are some differences in his approach. [...] Table 3: Instruction following examples using EntiGraph Instruct. (Top) EntiGraph Instruct can perform closed-book summarization of QuALITY articles. (Middle) Knowledge obtained through synthetic CPT affects the model’s behavior, even without an explicit article reference in the prompt. (Bottom) The model can compare two different articles from Dsource, even though no synthetic data is explicitly generated to relate documents. Instruction tuning details. We use the UltraChat instruction tuning dataset (Ding et al., 2023) filtered by the Huggingface team (Tunstall et al., 2023) as our instruction tuning data. We use the chat template of Llama 3.1 8B Instruct (Dubey et al., 2024) to format the UltraChat conversations, obtaining a 250M token instruction tuning dataset. We apply a linear learning rate warmup followed by a cosine decay to 0 with peak learning rate 5e-6, and train the model for 1 epoch with a batch size of 512 and context window of 2048. To sanity check our instruction tuning procedure, we measure the AlpacaEval (Li et al., 2023a) winrate against GPT-4 and find it improves from 0% to 6.35%, comparable to a 7.7% baseline winrate of Llama 2 Chat 13B. Instruction tuning qualitative examples. We first present a few qualitative examples to demon- strate EntiGraph Instruct’s ability to follow instructions related to QuALITY articles. As a first test, we ask the model to summarize a QuALITY article given an explicit reference to the title and author, but no access to the article itself (Table 3, top row). This article provides context for the coming examples. Next, we show that even without an explicit reference to the title and author, knowledge of the article is stored in the model’s parameters and can affect its behavior (Table 3, middle row). Finally, we provide an example where the model performs a comparison using knowledge across two articles (Table 3, bottom row). Albeit artificial, this shows that even though EntiGraph does not synthesize data that simultaneously involves multiple articles, the model can reason about their interaction using its parametric knowledge. We provide the full responses in Table 6. Evaluation metric for closed-book summarization. We also present quantitative metrics for summarization, a well-studied instruction following task. We compare EntiGraph Instruct sum- maries of QuALITY articles with human-written summaries from sQuALITY (Wang et al., 2022), which is a variation of the QuALITY benchmark with human summaries of QuALITY articles. Common scalar summarization metrics such as ROUGE (Lin, 2004) or BERTScore (Zhang* et al., 2020) mostly evaluate text similarity between the summary and source articles, and may not accu- rately reflect summarization quality for abstractive systems (Zhang et al., 2024b). 110.2 0.4 0.6 0.8 1.0 # Salient claims relative to human 0 2 4 6 8 10# False claims relative to humanHuman Summary (sQuALITY) EntiGraph Instruct (short prompt) EntiGraph Instruct (medium prompt) EntiGraph Instruct (long prompt) Raw Instruct (short prompt) Raw Instruct (medium prompt) Raw Instruct (long prompt) Rephrase Instruct (short prompt) Rephrase Instruct (medium prompt) Rephrase Instruct (long prompt) Figure 3: Quantitative automated evaluation of closed-book summarization: number of false claims (y-axis) versus number of salient claims (x-axis) normalized by the human summary. We find that EntiGraph Instruct (green) (i) generates substantially fewer false claims and more salient claims in its summaries compared to the Raw (blue) and Rephrase (purple) instruction-tuned baselines, and (ii) is closer to human summaries (red) on these metrics than the two baselines. We use a simple, automated evaluation metric based on ideas from pyramid evaluation (Nenkova et al., 2007; Gao et al., 2019) that simultaneously measures the hallucination rate and how well the summary captures the salient claims of the original article. Concretely, we design a three-stage evaluation procedure: (i) In the first stage, we use GPT-43 to break the summary into atomic claims, similar to Min et al. (2023); (ii) In the second stage, we provide both the list of claims and the source article to a judge model (also GPT-4). We ask the judge model to determine whether each claim is true or false, based on the source article. If the claim is true, we further ask the model to determine whether the claim is salient (contributes to the main message of the article) or cosmetic (factual details that do not help understand the main message). (iii) Finally, for each summary, we obtain its number of false and salient claims and normalize it by the corresponding count from the human summary. We report the average of these normalized metrics across the QuALITY corpus articles in Figure 3. Discussion of quantitative summarization results. In Figure 3, we compare three summarizers: EntiGraph Instruct, Rephrase Instruct, and Raw Instruct, where the latter two are obtained by ap- plying the above instruction tuning procedure to Rephrase CPT and Raw CPT. We provide each summarizer with three different prompts—short, medium, and long—asking for progressively more detailed summaries. We provide exact prompts in Appendix D.2. Rephrase Instruct and Raw In- struct consistently hallucinate and generate more false claims as the summary becomes longer, with little improvement in the number of salient claims. In contrast, EntiGraph Instruct is able to gen- erate more salient claims as the summary gets longer, with a small increase in the number of false claims. The gaps in both salient and false claim rates are sufficiently large that these results likely hold beyond our particular metric. Additional qualitative summarization results. We complement the automated evaluation met- rics above with several qualitative examples in Appendix D.2, where we compare the short sum- 3Specifically, we use the gpt-4-turbo model as of Aug. 19, 2024. 12 Split EntiGraph CPT + RAG Llama 3 8B Base + RAG GPT-4 + Oracle RAG GPT-3.5 + Oracle RAG Accuracy Recall@8 Accuracy Recall@8 Accuracy Recall@8 Accuracy Recall@8 All 62.73 99.63 60.35 99.63 86.09 100.0 72.60 100.0 Hard 53.87 99.65 50.24 99.65 79.59 100.0 63.13 100.0 Easy 71.68 99.61 70.55 99.61 92.65 100.0 82.14 100.0 Table 4: QuALITY question-answering accuracy and recall rate in the open-book setting (RAG). EntiGraph CPT and Llama 3 8B Base are used in a retrieval-augmented generation pipeline with an OpenAI text-embedding-3-large retriever and Cohere rerank-english-v3.0 reranker (§5.1). Recall@8 is defined as the proportion of questions for which the salient article ap- pears in the top 8 reranked document chunks. GPT-4 and GPT-3.5 Oracle RAG results provide an upper bound with a perfect retriever, by placing the entire relevant document in-context. maries from EntiGraph Instruct, Raw Instruct, and Rephrase Instruct against the human summaries. We observe that the content of the EntiGraph Instruct summaries aligns well with the human sum- maries, whereas the Raw and Rephrase Instruct summaries contain clear hallucinations. The con- clusion from our qualitative examples matches that of our automated evaluations. 5 OPEN-BOOK EXPERIMENTS Next, we consider an open-book setting in which the domain-specific corpus Dsource is available at test time. In this widespread setting, retrieval-augmented generation (RAG; Lewis et al. (2020); Gao et al. (2024)) is the predominant and most convenient approach. It benefits from strong tooling (Chase, 2022; Han et al., 2023; Pinecone, 2024), avoids finetuning, supports continual learning as the corpus is updated over time (Wu et al., 2024), and has high recall (proportion of queries for which the correct documents are retrieved). Therefore, it is a natural question whether the parametric knowledge learned through synthetic CPT using EntiGraph complements the non-parametric knowledge accessed at test time using RAG. In this section, we answer this question by comparing a state-of-the-art RAG pipeline with and without Entigraph CPT. 5.1 RAG EVALUATION SETUP Our RAG pipeline follows established best practices (Lewis et al., 2020; Gao et al., 2024). It involves an offline stage which indexes document chunks, followed by inference-time retrieval, reranking, and placement of those chunks in a few-shot LM prompt. Details on RAG pipeline. More specifically, our indexing stage chunks documents from the given corpus, obtains dense vector embeddings for each chunk using an API-based embedding model, and indexes the (embedding, chunk) pairs. Then, at inference time, we embed the query with the API-based embedding model, retrieve K document chunks using an approximate nearest- neighbor search, and lastly, select the k < K most relevant chunks using an API-based reranker. Throughout, we use OpenAI text-embedding-3-large (Neelakantan et al., 2022) as our API- based embedding model, FAISS as our similarity search index (Douze et al., 2024), and Cohere rerank-english-v3.0 (Cohere, 2024) as our reranker. Dataset and base models. Following the evaluation procedure detailed in §4, we evaluate parallel RAG pipelines on the QuALITY multiple choice test set using few-shot chain-of-thought prompting. All hyperparameters are tuned separately for each LM’s RAG pipeline. We refer the reader to Appendix E for more details on our evaluation setup. 5.2 RESULTS Our results in the open-book RAG setting are presented in Table 4. 130 100 200 300 400 500 600 Number of synthetic tokens (in Millions) 0.400 0.425 0.450 0.475 0.500 0.525 0.550EntiGraph Accuracy Empirical observation on QuALITY experiments Fitted Curve Figure 4: A mixture-of-exponential functional form (5) closely fits the scaling trend of EntiGraph CPT with respect to synthetic token count (reprinted from Figure 2, in linear scale). EntiGraph continued pretraining complements RAG. We observe that EntiGraph CPT outper- forms Llama 3 8B Base, the model from which it is continually pretrained. These results demon- strate that the knowledge internalized through synthetic CPT is complementary to that accessed during RAG, and demonstrate a competitive new recipe for small corpus QA: (1) synthetic data augmentation, (2) continued pretraining, and (3) RAG. EntiGraph continued pretraining alone approaches RAG performance. These results also contextualize the effectiveness of EntiGraph in the closed-book, parametric knowledge setting (§4). Comparing Tables 2 and 4, we observe that adding RAG to Llama 3 8B Base provides a 20.86% absolute accuracy improvement (39.49% → 60.35%). On the other hand, continued pretraining of Llama 3 8B Base on the EntiGraph corpus provides a 16.93% absolute accuracy improvement (39.49% → 56.42%). Hence, EntiGraph continued pretraining provides > 80% of the absolute per- formance improvement of RAG, even in a small corpus setting where RAG recall is nearly perfect. Altogether, our results demonstrate that the parametric knowledge acquired through EntiGraph con- tinued pretraining composes well with realistic knowledge-intensive QA pipelines, and that Enti- Graph continued pretraining alone—without inference-time corpus access—is nearly competitive with a strong RAG baseline. 6 THEORETICAL ANALYSIS OF ENTIGRAPH SCALING It may seem surprising that simply “rewriting” the factual content of the source documents Dsource can improve performance at all (§4), as the EntiGraph data augmentation algorithm does not ex- plicitly add new factual information beyond Dsource. In this section, we build a mathematical model based on a stochastic process on graphs in order to offer an explanation for this phenomenon. We postulate that EntiGraph does not create knowledge de novo; rather, it simply “rearranges” the knowledge of Dsource into a layout more amenable to learning. For example, in Dsource, the en- tity pair (A, B) may appear together in some sentences and (B, C) in others. As a result, models trained directly on Dsource with a next-token prediction objective may learn the (A, B) relation and the (B, C) relation, but not the relation between A and C (Aky¨urek et al., 2024) We will build a mathematical model that formalizes this intuition (§6.1). Based on this model, we provide a quan- titative prediction that the scaling trend of EntiGraph CPT follows a mixture-of-exponential shape (§6.3), which fits well with our empirically observed scaling trend (Figure 4). 14 6.1 TOY MODEL SETUP In this toy model, we use V to denote the set of entities, and represent the source documents Dsource with pairs of known relations Dsource ⊂ {(x, y) ∈ V 2 : x ̸= y}. We assume that each relation pair in V 2 appears in the source documents Dsource independently at random with probability p. Mathematically, P [(x, y) ∈ Dsource] = p for all x ∈ V and y ∈ V with x ̸= y. We write V = |V| and assume that p = λ/V , for some constant λ > 1. Training as memorization. We model the learning of factual knowledge as a memorization pro- cess, in which a model memorizes the relations it is explicitly trained on but does not meaningfully generalize beyond them (Yang et al., 2023; Feldman, 2020). In our knowledge graph setting, a lan- guage model’s knowledge can be represented by a matrix M ∈ {0, 1}V ×V such that M (x, y) = 1 if the model “knows” the (x, y) relation and equals 0 otherwise. Then, training directly on the source documents Dsource simply means setting all entries that appear in Dsource to 1. This denotes that the model has memorized the relations given in the source documents. Mathematically, we denote this model trained on Dsource by the matrix M0 ∈ {0, 1}V ×V , which has i.i.d. Bernoulli off-diagonal entries with mean p. EntiGraph synthetic data augmentation. Given the source documents Dsource, we define the following iterative procedure of synthetic data generation: for each t = 1, 2, . . . 1. Entity pair selection: Sample (xt, yt) ∈ {(x, y) ∈ V 2 : x ̸= y} uniformly at random. 2. Relation analysis: Generate the “relation between (xt, yt)” by performing a breadth-first search (BFS) on the directed graph represented by the adjacency matrix M0 starting at xt: • If there exists a path (xt, z1 t , z2 t , . . . , zkt t , yt) connecting xt to yt, define Dt = {(xt, z1 t ), (xt, z2 t ), . . . , (xt, zkt t ), (xt, yt)} ∪ Dt−1, where we assume D0 = Dsource. The model trained on this round of synthetic data would be Mt = Mt−1 + ∑ (x,y)∈Dt\\Dt−1 Ixy, where Ixy ∈ {0, 1} V ×V is a binary matrix with Ixy(x, y) = 1 and 0 otherwise. • If no such path exists, do nothing. This mirrors the relation analysis step for the EntiGraph synthetic data augmentation algorithm (introduced in §2.2). With the setup above, the index t is analogous to the number of synthetic tokens that the model has generated, and the model’s knowledge is captured by how many ones the matrix Mt contains. To make this connection precise, we define the link density (or accuracy) of Mt to be Acc(Mt) = E[∥Mt∥1|M0] V (V − 1) , where the expectation is taken over the randomness arising from the synthetic data generation pro- cess and not the source documents Dsource. For a matrix M , we use ∥M ∥1 to denote ∑ i,j |Mi,j|. We use the notation Acc as this is intended to emulate the accuracy on QuALITY test queries studied in the experimental sections (§4 and §5). 6.2 RIGOROUS UPPER AND LOWER BOUND In this section, we derive rigorous upper and lower bounds on the scaling trend of Acc(Mt). We show that Acc(Mt) as a function of t can be bounded above and below by two exponential functions with different growth rates. Note that these two bounds do not necessarily imply that Acc(Mt) itself grows exponentially. We will provide a precise formula for its growth in §6.3 via an approximation through a Poisson branching process. Definition 1. Let Cλ = (1 − ρ(λ))2, where ρ(λ) denotes the extinction probability for a Poisson(λ) branching process (i.e., ρ is the smallest solution in [0, 1] to the fixed-point equation ρ = exp(λ(ρ − 15 1))). For any fixed ε > 0, we further define CLB = 1 − 1 V (V − 1) , CUB = 1 − (1 + ε) log V V (V − 1) log λ . Theorem 1. For any time t ≥ 1 and any ε > 0, the link density satisfies (p + Cλ (1 − C t LB)) (1 − ε) ≤ Acc(Mt) ≤ (p + Cλ (1 − C t UB)) (1 + ε), with probability → 1 when V → ∞. Even though Theorem 1 provides mathematically rigorous upper and lower bounds on the scaling trend of Acc(Mt), the exact growth curve is more intricate, as we will show next. 6.3 AN ANALYTICAL FORMULA For the remainder of the section, we analyze the link density Acc(Mt) using a Poisson branching process approximation of the cluster growth of vertices. This approach yields an approximation of the form Acc(Mt) ∼ p + Cλ ( 1 − ∞∑ ℓ=0 λ − 1 λℓ+1 ∞∑ k=1 pℓ(k) (1 − k V (V − 1) )t) , where A ∼ B means that A/B converges to 1 in probability as V → ∞. We refer the reader to Appendix F for a comprehensive derivation. Here pℓ denotes the probability mass function of the total progeny Yℓ of a Poisson(λ) branching process at level ℓ. Qualitatively, for a general represen- tation of source documents Dsource beyond directed Erd˝os-R´enyi graphs, we still expect to observe a mixture-of-exponential scaling trend: Acc(Mt) ∼ p + C ( 1 − ∞∑ k=1 µ(k) (1 − ak)t) . (5) In this context, the parameter C governs the link density Acc(Mt) as t → ∞. In our model, C is determined by the proportion of reachable pairs of vertices in the initial matrix M0. Here, we are essentially filling out the “deductive closure” (i.e., all the facts or relations that can be deduced from Dsource; Stine (1976); Aky¨urek et al. (2024)) of the original data—if some facts cannot be deduced, then Acc(Mt) cannot approach 1. The measure µ(·) is the probability mass function on k, which controls the proportion of pairs of vertices with a specific decay rate. The parameters µ(·) depend on M0 in a more intricate manner. We find that the formula in (5) accurately fits the empirical scaling trend of EntiGraph CPT accuracy up to 600M synthetic tokens (Figure 4). Sketch of derivation. Intuitively, the edge (i, j) will eventually be added if and only if j is reach- able from i in the original graph M0. This explains the limiting behavior of Acc(Mt) as t ap- proaches infinity: the proportion of links will converge to the proportion of connected vertex pairs in M0. To understand the mixture-of-exponential functional form, consider that at time t, the proba- bility of adding each vertex pair follows an exponential pattern, with different vertex pairs exhibiting different exponential growth rates. Specifically, think of a breadth-first search in M0 starting from a vertex i. If j is very close to the root, there are many paths from i to other vertices passing through j, making it more likely that (i, j) will be included in each iteration. In contrast, if j is far from the root (e.g., at the end of the exploration process), there are fewer such paths, making it less likely for (i, j) to be included in each iteration. This accounts for the mixture-of-exponential shape, where the mixture primarily reflects the distance of each vertex from the root, the number of such vertices, and their corresponding exponential growth rates. Qualitative description. Finally, to help build an intuitive understanding, we provide a qualitative description of the mixture-of-exponential shape. We demonstrate in Appendix F that this mixture- of-exponential shape comprises three distinct phases: a fast growth phase, a slower growth phase, and a plateau phase. Mathematically, we show the existence of two distinct times, 0 < t1 < t2, such that Acc(MT ) =    Θ (p + t) , for 0 ≤ t ≤ t1, Θ(log t), for t1 ≤ t ≤ t2, Θ(1), for t ≥ t2, 16 (a) Linear regime (b) Log-linear (t in log scale) (c) Plateau regime Figure 5: Accuracy Acc(Mt) with respect to time t, for V = 100 and p = 0.03. The mixture-of- exponential functional form in (5) leads to three distinct regimes. where we use a convenient change of variable T = tV (V − 1). It is important to note that the choice of log t in the second phase is not necessarily canonical. In fact, the bound holds for any well-behaved monotone increasing concave function as a replacement for log t. Our representation here is motivated by two factors: first, it aligns with the performance observed in our EntiGraph CPT numerical results, and second, it reflects the gradual slowdown in growth. We illustrate the three phases in Figure 5, which present a simulation of the toy model with p = 0.03. 7 DISCUSSION 7.1 LIMITATIONS Because EntiGraph synthesizes data using a prompted language model, there is a risk that it may hal- lucinate and fabricate non-existent relations among the entities. Although our process of generating synthetic data is grounded by the source documents as in Equations (3) and (4), it is an assumption that LMaug is capable enough to generate faithful synthetic data when conditioned on Dsource. In our experiment with QuALITY books, we manually read a few books and fact-checked a subset of the synthetic data generated for those books; we did not find factually incorrect synthesized text. We postulate that this is because we use a sufficiently strong prompted model LMaug (gpt-4-turbo). If EntiGraph were applied to more challenging content like a complex research paper, it is possible that the prompted model could be more prone to hallucination. On the other hand, because we use a very capable prompted language model gpt-4-turbo to generate synthetic data, one might be concerned that our performance gains come from distilling the prompted LM’s knowledge. The closed-book results show that distillation effects alone cannot explain the performance of our approach (as we exceed GPT-4’s closed-book performance), but our approach does not yet enable bootstrapping, where we use a model to generate its own synthetic data for a small target domain. We view this as exciting future work. 7.2 FUTURE DIRECTIONS Continued scaling beyond real data. The large but finite body of human-written text is rapidly being consumed. Villalobos et al. (2024) predict that frontier language models will exhaust all pub- lic, human-generated text in 2028. As we transition from a data-rich to a data-constrained regime (Kaplan et al., 2020; Muennighoff et al., 2023), further scaling will require us to extract more knowl- edge from existing data. We demonstrated that synthetic continued pretraining with EntiGraph ef- fectively extracts more knowledge from small corpora, which could help us learn from proprietary datasets or tail knowledge that appears only once or twice on the internet. It is an open question whether synthetic data generation methods like EntiGraph could improve data efficiency more gen- erally on standard pretraining data and without relying upon a stronger prompted model. Alternatives to long-context language models. Recent work handles long user queries (e.g., 1M- 10M+ tokens) using efficient implementations of attention (Dao et al., 2022; Liu et al., 2023; Gemini, 2024) or alternative architectures that are sub-quadratic in the context length (Tay et al., 2022; Gu et al., 2022; Gu & Dao, 2024; Sun et al., 2024). In settings where many queries share the same 17 long prefix—e.g., a corporation’s proprietary documents or other use cases with prompt caching (Anthropic, 2024a)—one could instead continue pretraining on the prefix to internalize its knowl- edge, and then perform standard quadratic attention on shorter queries. This approach pays a fixed training cost to amortize the prefix’s knowledge into the weights of a model and then benefits from shorter context lengths (Gururangan et al., 2020; Snell et al., 2022). By adapting the continued pretraining paradigm from 10B-100B tokens to as little as 1.3M tokens, our synthetic continued pretraining approach could enable unsupervised learning of shared text prefixes at much smaller and more practical token counts. 7.3 CONCLUSION Continued pretraining with next-token prediction is remarkably effective in teaching pretrained lan- guage models new knowledge, but to date has only been applied successfully in broad, data-rich domains with 10B-100B+ tokens. We downscale continued pretraining to small, specialized cor- pora with ∼1M tokens using synthetic continued pretraining: converting a small corpus into a large synthetic one with diverse representations of knowledge, and continuing pretraining on it. We instantiate this approach using EntiGraph, a knowledge graph–inspired synthetic data augmen- tation algorithm. Synthetic continued pretraining with EntiGraph demonstrates consistent scaling in downstream closed-book QA performance up to a 600M token synthetic corpus, whereas baselines such as continued pretraining on the small corpus or synthetic paraphrases show no improvement or asymptote early. Moreover, the acquired parametric knowledge composes with instruction tuning and retrieved non-parametric knowledge in an open-book setting. Lastly, we present a simplified mathematical model of EntiGraph and derive a functional form for its scaling trend, which closely matches our empirical trend. We hypothesize that EntiGraph’s “externalization” of the synthetic data generation process to a combinatorial structure—in this case, a knowledge graph over entities—is a generally useful strategy in synthesizing highly diverse data and a promising object for future study. 8 ACKNOWLEDGEMENT Zitong Yang would like to thank Ruiqi Zhong for discussion regarding context distillation work, Xiang Lisa Li for discussion about reversal curse work, and the participants of the statistics seminar at Stanford University for their helpful feedback about a preliminary version of this work. We also thank the Tatsu Lab for helpful feedback and interesting discussions that have helped improve the paper. Zitong Yang is supported by the Albion Walter Hewlett Stanford Graduate Fellowship. Neil Band acknowledges funding from an NSF Graduate Research Fellowship and a Quad Fellowship. This work was supported by gifts from Panasonic Research, the Google Research Scholar Program, and the Tianqiao and Chrissy Chen Institute. E.J.C. is supported by the Office of Naval Research grant N00014-20-1-2157, the National Science Foundation grant DMS-2032014, the Simons Foun- dation under award 814641. 18 REFERENCES Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio C´esar Teodoro Mendes, Weizhu Chen, Al- lie Del Giorno, Ronen Eldan, Sivakanth Gopi, Suriya Gunasekar, Mojan Javaheripi, Piero Kauff- mann, Yin Tat Lee, Yuanzhi Li, Anh Nguyen, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shi- tal Shah, Michael Santacroce, Harkirat Singh Behl, Adam Taumann Kalai, Xin Wang, Rachel Ward, Philipp Witte, Cyril Zhang, and Yi Zhang. Phi-2: The surprising power of small lan- guage models, 2023. URL https://www.microsoft.com/en-us/research/blog/ phi-2-the-surprising-power-of-small-language-models/. Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Ben- haim, Misha Bilenko, Johan Bjorck, S´ebastien Bubeck, Qin Cai, Martin Cai, Caio C´esar Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi- Ling Chen, Parul Chopra, Xiyang Dai, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao, Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Ce Liu, Mengchen Liu, Weishung Liu, Eric Lin, Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sam- budha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shi- tal Shah, Ning Shang, Hiteshi Sharma, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Xin Wang, Lijuan Wang, Chunyu Wang, Yu Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Haiping Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Sonali Yadav, Fan Yang, Jianwei Yang, Ziyi Yang, Yifan Yang, Donghan Yu, Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: A highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Afra Feyza Aky¨urek, Ekin Aky¨urek, Leshem Choshen, Derry Wijaya, and Jacob Andreas. Deduc- tive closure training of language models for coherence, accuracy, and updatability. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 9802–9818, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. findings-acl.584. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation, 2024. URL https://arxiv.org/abs/2309.14402. Dana Angluin. Queries and concept learning. Machine Learning, 2:319–342, 1988. URL https: //api.semanticscholar.org/CorpusID:11357867. Anthropic. Prompt caching (beta), 2024a. URL https://docs.anthropic.com/en/ docs/build-with-claude/prompt-caching. Anthropic. The Claude 3 Model Family: Opus, Sonnet, Haiku. https://www-cdn. anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf, 2024b. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=4WnqRR915j. Maria-florina Balcan, Avrim Blum, and Ke Yang. Co-training and expansion: To- wards bridging theory and practice. In L. Saul, Y. Weiss, and L. Bottou (eds.), Ad- vances in Neural Information Processing Systems, volume 17. MIT Press, 2004. URL https://proceedings.neurips.cc/paper_files/paper/2004/file/ 9457fc28ceb408103e13533e4a5b6bd1-Paper.pdf. 19 Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Kor- bak, and Owain Evans. The reversal curse: Llms trained on ”a is b” fail to learn ”b is a”, 2023. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning, 2019. URL https: //arxiv.org/abs/1905.02249. Avrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Pro- ceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pp. 92–100, New York, NY, USA, 1998. Association for Computing Machinery. ISBN 1581130570. doi: 10.1145/279943.279962. URL https://doi.org/10.1145/279943.279962. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar- wal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu- ral Information Processing Systems, volume 33, pp. 1877–1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Harrison Chase. LangChain, 10 2022. URL https://github.com/langchain-ai/ langchain. Zeming Chen, Alejandro Hern´andez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas K¨opf, Amirkeivan Mohtashami, Alexan- dre Sallinen, Alireza Sakhaeirad, Vinitra Swamy, Igor Krawczuk, Deniz Bayazit, Axel Marmet, Syrielle Montariol, Mary-Anne Hartley, Martin Jaggi, and Antoine Bosselut. Meditron-70b: Scal- ing medical pretraining for large language models, 2023. URL https://arxiv.org/abs/ 2311.16079. Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effects of knowledge editing in language models. arXiv preprint arXiv:2307.12976, 2023. Cohere. Improve search performance with a single line of code, 2024. URL https://cohere. com/rerank. Pierre Colombo, Telmo Pires, Malik Boudiaf, Rui Melo, Dominic Culver, Sofia Morgado, Etienne Malaboeuf, Gabriel Hautreux, Johanne Charpentier, and Michael Desa. Saullm-54b and saullm- 141b: Scaling up domain adaptation for the legal domain, 2024a. URL https://arxiv. org/abs/2407.19584. Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera L´ucia Raposo, Sofia Morgado, and Michael Desa. Saullm- 7b: A pioneering large language model for law, 2024b. URL https://arxiv.org/abs/ 2403.03883. Common Crawl. Common crawl. https://commoncrawl.org/, 2007. Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with IO-awareness. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=H4DqfPSibmx. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations, 2023. Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre- Emmanuel Mazar´e, Maria Lomeli, Lucas Hosseini, and Herv´e J´egou. The faiss library, 2024. URL https://arxiv.org/abs/2401.08281. 20 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Ander- son, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Ma- hadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Al- wala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Man- nat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur C¸ elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhar- gava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sum- baly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petro- vic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Bran- don Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Ar- caute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzm´an, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Gold- man, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe 21 Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Ke- neally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mo- hammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navy- ata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Sa- tadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lind- say, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Tim- othy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, V´ıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Con- stable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Rick Durrett. Random graph dynamics, volume 20. Cambridge university press, 2010. Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023. Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, pp. 954–959, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450369794. doi: 10.1145/3357713.3384290. URL https://doi.org/10.1145/3357713.3384290. Yanjun Gao, Chen Sun, and Rebecca J. Passonneau. Automated pyramid summarization evalu- ation. In Mohit Bansal and Aline Villavicencio (eds.), Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pp. 404–418, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/K19-1038. URL https://aclanthology.org/K19-1038. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024. URL https://arxiv.org/abs/2312.10997. Team Gemini. Gemini: A family of highly capable multimodal models, 2024. URL https: //arxiv.org/abs/2312.11805. Siavash Golkar, Michael Kagan, and Kyunghyun Cho. Continual learning via neural pruning. arXiv preprint arXiv:1903.04476, 2019. Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks, 2015. URL https: //arxiv.org/abs/1312.6211. 22 Stephen T Grossberg. Studies of mind and brain: Neural principles of learning, perception, devel- opment, cognition, and motor control, volume 70. Springer Science & Business Media, 2012. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://openreview.net/forum?id=AL1fq05o7H. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=uYLFoz1vlAC. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. Reinforced self-training (rest) for language modeling, 2023. URL https://arxiv.org/abs/2308.08998. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C´esar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S´ebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need, 2023. URL https://arxiv.org/ abs/2306.11644. Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Pee- bles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Do- minik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun- song Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mah- yar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models, 2024. URL https://arxiv.org/abs/2407.21075. Kshitij Gupta, Benjamin Th´erien, Adam Ibrahim, Mats L. Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timoth´ee Lesort. Continual pre-training of large language models: How to (re)warm your model?, 2023. URL https://arxiv.org/abs/2308.04014. Suchin Gururangan, Ana Marasovi´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8342–8360, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.740. URL https://aclanthology.org/2020.acl-main.740. Yikun Han, Chunjiang Liu, and Pengfei Wang. A comprehensive survey on vector database: Storage and retrieval technique, challenge, 2023. URL https://arxiv.org/abs/2310.11703. 23 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja- cob Steinhardt. Measuring massive multitask language understanding. In International Confer- ence on Learning Representations, 2021. URL https://openreview.net/forum?id= d7KBjmI3GmQ. Remco van der Hofstad. Random Graphs and Complex Networks. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2016. Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tun- ing language models with (almost) no human labor. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14409–14428, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.806. URL https://aclanthology.org/2023.acl-long.806. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro- cessing, pp. 1051–1068, Singapore, December 2023. Association for Computational Linguis- tics. doi: 10.18653/v1/2023.emnlp-main.67. URL https://aclanthology.org/2023. emnlp-main.67. Adam Ibrahim, Benjamin Th´erien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timoth´ee Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models, 2024. URL https://arxiv.org/abs/2403.08763. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In Proceedings of the 40th International Conference on Machine Learning, ICML’23. JMLR.org, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Richard M Karp. The transitive closure of a random digraph. Random Structures & Algorithms, 1 (1):73–93, 1990. Ronald Kemker, Marc McClure, Angelina Abitino, Tyler L. Hayes, and Christopher Kanan. Mea- suring catastrophic forgetting in neural networks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelli- gence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelli- gence, AAAI’18/IAAI’18/EAAI’18. AAAI Press, 2018. ISBN 978-1-57735-800-8. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Has- sabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic for- getting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521– 3526, 2017. doi: 10.1073/pnas.1611835114. URL https://www.pnas.org/doi/abs/ 10.1073/pnas.1611835114. Hunter Lang, Monica N Agrawal, Yoon Kim, and David Sontag. Co-training improves prompt- based learning for large language models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 11985–12003. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/ v162/lang22a.html. Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. ICML 2013 Workshop: Challenges in Representation Learning, 2013. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, Sebastian Riedel, and Douwe 24 Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra- masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with lan- guage models, 2022. URL https://arxiv.org/abs/2206.14858. Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu Wei. Synthetic data (almost) from scratch: Generalized instruction tuning for language models, 2024. URL https://arxiv.org/abs/2402.13064. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023a. Yuanzhi Li, S´ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report, 2023b. URL https://arxiv.org/ abs/2309.05463. Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguis- tics. URL https://aclanthology.org/W04-1013. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near- infinite context. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. URL https://openreview.net/forum?id=xulyCXgIWH. David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. Advances in neural information processing systems, 30:6467–6476, 2017. Pratyush Maini, Skyler Seto, Richard Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: A recipe for compute and data-efficient language modeling. In Lun- Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 14044– 14072, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.757. Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In Gordon H. Bower (ed.), Psychology of Learning and Motivation, volume 24 of Psychology of Learning and Motivation, pp. 109–165. Academic Press, 1989. doi: https://doi.org/10.1016/S0079-7421(08)60536-8. URL https://www.sciencedirect. com/science/article/pii/S0079742108605368. Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar Reddy Yannam, Tolga Aktas, and Todd Hendry. Injecting new knowledge into large language models via supervised fine-tuning, 2024. URL https://arxiv.org/abs/2404.00213. Kevin Meng, David Bau, Alex J Andonian, and Yonatan Belinkov. Locating and editing factual asso- ciations in GPT. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview. net/forum?id=-h6WAS6eE4. Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in a transformer. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=MkbcAHIYgyS. 25 Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of fac- tual precision in long form text generation, 2023. URL https://arxiv.org/abs/2305. 14251. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022. URL https://openreview.net/pdf?id=0DcZxeWfOPt. Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksan- dra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=j5BuTrEj35. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pre-training, 2022. URL https://arxiv.org/abs/2201.10005. Ani Nenkova, Rebecca Passonneau, and Kathleen McKeown. The pyramid method: Incorporat- ing human content selection variation in summarization evaluation. ACM Trans. Speech Lang. Process., 4(2):4–es, may 2007. ISSN 1550-4875. doi: 10.1145/1233912.1233913. URL https://doi.org/10.1145/1233912.1233913. Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. arXiv preprint arXiv:1710.10628, 2017. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren- cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Moham- mad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brock- man, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim´on Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gib- son, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hal- lacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Ka- mali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David M´ely, Ashvin Nair, Reiichiro Nakano, Ra- jeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, 26 Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Sel- sam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Pre- ston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer´on Uribe, Andrea Vallone, Arun Vi- jayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Work- man, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel- ton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 27730–27744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. Oded Ovadia, Menachem Brief, Moshik Mishaeli, and Oren Elisha. Fine-tuning or retrieval? com- paring knowledge injection in llms, 2024. URL https://arxiv.org/abs/2312.05934. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY: Question answering with long input texts, yes! In Marine Carpuat, Marie-Catherine de Marn- effe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technolo- gies, pp. 5336–5358, Seattle, United States, July 2022. Association for Computational Linguis- tics. doi: 10.18653/v1/2022.naacl-main.391. URL https://aclanthology.org/2022. naacl-main.391. Jupinder Parmar, Sanjev Satheesh, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Reuse, don’t retrain: A recipe for continued pretraining of language models, 2024. URL https: //arxiv.org/abs/2407.07263. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4, 2023. URL https://arxiv.org/abs/2304.03277. Pinecone. Rag with pinecone, 2024. URL https://www.pinecone.io/solutions/rag/. Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=GhVS8_yPeEa. R. Ratcliff. Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions. Psychological Review, 97(2):285–308, 1990. doi: 10.1037/0033-295X.97. 2.285. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 2001–2010, 2017. Anthony Robins. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection Science, 7(2): 123–146, 1995. 27 Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, J´er´emy Rapin, Artyom Kozhevnikov, Ivan Ev- timov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre D´efossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950. Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. Jeffrey C. Schlimmer and Douglas Fisher. A case study of incremental concept induction. In Pro- ceedings of the Fifth AAAI National Conference on Artificial Intelligence, AAAI’86, pp. 496–501. AAAI Press, 1986. Raphael Schumann and Ines Rehbein. Active learning via membership query synthesis for semi- supervised sentence classification. In Mohit Bansal and Aline Villavicencio (eds.), Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pp. 472–481, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/ v1/K19-1044. URL https://aclanthology.org/K19-1044. H. Scudder. Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory, 11(3):363–371, 1965. doi: 10.1109/TIT.1965.1053799. Claude Elwood Shannon. Prediction and entropy of printed english. Bell System Technical Journal, 30:50–64, January 1951. URL http://languagelog.ldc.upenn.edu/myl/ Shannon1950.pdf. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathe- matical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402. 03300. Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep gener- ative replay. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Cur- ran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/ paper/2017/file/0efbe98067c6c73dba1250d2beaa81f9-Paper.pdf. Charlie Snell, Dan Klein, and Ruiqi Zhong. Learning by distilling context, 2022. URL https: //arxiv.org/abs/2209.15189. G. C. Stine. Skepticism, relevant alternatives, and deductive closure. Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition, 29(4):249–261, 1976. ISSN 00318116, 15730883. URL http://www.jstor.org/stable/4319027. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states, 2024. URL https://arxiv.org/ abs/2407.04620. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey, 2022. URL https://arxiv.org/abs/2009.06732. TogetherAI. Redpajama: an open dataset for training large language models, 2023. URL https: //github.com/togethercomputer/RedPajama-Data. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Shengyi Huang, Kashif Rasul, Alvaro Bartolome, Alexander M. Rush, and Thomas Wolf. The Alignment Handbook, 2023. URL https://github.com/huggingface/alignment-handbook. 28 Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data, 2024. Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna- peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St´efan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel- son, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mul- bregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2. Alex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R. Bowman. SQuAL- ITY: Building a long-document summarization dataset the hard way. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1139–1156, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.75. URL https://aclanthology.org/2022.emnlp-main.75. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023a. URL https://openreview.net/forum?id=1PL1NIMMrw. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 13484– 13508, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/ v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long.754. Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell (eds.). Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Lan- guage Learning, Singapore, December 2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.conll-babylm.0. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Interna- tional Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=gEZrGCozdqR. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS ’22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088. Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for large language models: A survey, 2024. URL https://arxiv.org/ abs/2402.01364. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves imagenet classification. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684–10695, 2020. doi: 10.1109/CVPR42600.2020.01070. I. Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi- supervised learning for image classification, 2019. URL https://arxiv.org/abs/1905. 00546. Zitong Yang, MICHAL LUKASIK, Vaishnavh Nagarajan, Zonglin Li, Ankit Rawat, Manzil Za- heer, Aditya K Menon, and Sanjiv Kumar. Resmem: Learn what you can and memorize the rest. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in 29 Neural Information Processing Systems, volume 36, pp. 60768–60790. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/bf0857cb9a41c73639f028a80301cdf0-Paper-Conference.pdf. Dong Yuan, Eti Rastogi, Gautam Naik, Sree Prasanna Rajagopal, Sagar Goyal, Fen Zhao, Bharath Chintagunta, and Jeff Ward. A continued pretrained llm approach for automatic medical note generation, 2024a. URL https://arxiv.org/abs/2403.09057. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024b. URL https://arxiv.org/ abs/2401.10020. Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017. Dan Zhang, Sining Zhoubian, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self- training via process reward guided tree search, 2024a. URL https://arxiv.org/abs/ 2406.03816. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. Benchmarking large language models for news summarization. Transactions of the Association for Computational Linguistics, 12:39–57, 2024b. doi: 10.1162/tacl a 00632. URL https://aclanthology.org/2024.tacl-1.3. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Expe- riences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):3848–3860, aug 2023. ISSN 2150-8097. doi: 10.14778/3611540.3611569. URL https://doi.org/10.14778/ 3611540.3611569. Zexuan Zhong, Zhengxuan Wu, Christopher Manning, Christopher Potts, and Danqi Chen. MQuAKE: Assessing knowledge editing in language models via multi-hop questions. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empiri- cal Methods in Natural Language Processing, pp. 15686–15702, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.971. URL https://aclanthology.org/2023.emnlp-main.971. Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models, 2020. 30 A CODEBASE We provide the codebase for reproducing all results discussed in the paper below: https://github.com/ZitongYang/Synthetic_Continued_Pretraining.git B SYNTHETIC DATA PROMPTS We generate two synthetic corpora in this paper: EntiGraph (Appendix B.1) and the Rephrase base- line (Appendix B.2). As discussed in §2.1, our synthetic augmentation procedure is applied to each document D in the collection of source documents Dsource. We will focus on a single document D for the remainder of this section. B.1 ENTIGRAPH PROMPTS The EntiGraph procedure is described in detail in §2.2. We will recap the three steps below. Step 1: Entity extraction. The first step is to extract the salient entities from the document D using the entity extraction operation (Equation (2)). The complete entity extraction prompt is as follows: As a knowledge analyzer, your task is to dissect and understand a lecture script provided by the user. You are required to perform the following steps: 1. Summarize the Lecture Script: Provide a concise summary of the lecture, capturing the main points, topics, and themes discussed. 2. Extract Entities: Identify and list all significant \"nouns\" or entities mentioned within the script. These entities should include, but are not limited to: * People: Any lecturers, historical figures, or individuals mentioned. * Places: Specific locations or institutions referenced. * Objects: Any concrete objects or tools discussed within the context of the lecture. * Concepts: Key academic concepts, theories, or themes that are central to the lecture’s discussion. Ensure that your summary is brief yet comprehensive, and the list of entities is detailed and accurate. Structure your response in a JSON format to organize the information effectively. Here is the format you should use for your response: { \"summary\": \"<A concise summary of the lecture script>\", \"entities\": [\"entity1\", \"entity2\", ...] } Step 2: single entity description. The second step is to generate text to describe one specific entity using the relation analysis prompt in Equation (3). The full relation analysis prompt is as follows: 31 As an examiner, you are tasked with creating reading comprehension questions for students based on a provided article and a specified entity referenced within it. Your role involves crafting questions and corresponding answers that fulfill the following criteria: 1. **Focus on the Entity**: Ensure all questions consistently center around the specified entity from the article. 2. **Encourage Deep Analysis**: Develop thought-provoking, open- ended questions that challenge students to think critically and analytically. Questions should: - Prompt students to reflect deeply, questioning the assumptions within the article. - Require students to evaluate evidence and consider alternative perspectives. - Encourage complex reasoning about the entity and its implications within the article’s context. 3. **Comprehensive Answers**: For each question, provide a detailed solution that: - Explicitly connects back to the specified entity and its role or representation in the article. - Includes concrete references to specific paragraphs or sections of the article to support the answer. Try to write as many questions as possible. Your response should be formatted to organize the questions and answers systematically. Here is the structure you should use: ### Questions and answers about <entity> in context of <title> Question: <Question1 focusing on the entity> Answer: <Detailed answer with references to the article> Question: <Question2 focusing on the entity> Answer: <Detailed answer with references to the article> ... Step 3: relation analysis. The last step is to generate diverse descriptions of relations among two or more entities. In our experiments, for each document D, we enumerate all entity pairs and generate a description for each. The prompt for generating a description relating a pair of entities is as follows: You will act as a knowledge analyzer tasked with dissecting an article provided by the user. Your role involves two main objectives: 1. Rephrasing Content: The user will identify two specific entities mentioned in the article. You are required to rephrase the content of the article twice: * Once, emphasizing the first entity. * Again, emphasizing the second entity. 2. Analyzing Interactions: Discuss how the two specified entities interact within the context of the article. 3. Generating qeustions and answers: crafting questions and corresponding answers that fulfill the following criteria: - **Focus on the two Concepts/Terms**: Ensure all questions consistently center around the two concepts provided by the user. - **Encourage Deep Analysis**: Develop thought-provoking, open- ended questions that challenge students to think critically and analytically understand how the interaction between the two entities shape the article. 32 Your responses should provide clear segregation between the rephrased content and the interaction analysis. Ensure each section of the output include sufficient context, ideally referencing the article’s title to maintain clarity about the discussion’s focus. Here is the format you should follow for your response: ### Discussion of <title> in relation to <entity1> <Rephrased content focusing on the first entity> ### Discussion of <title> in relation to <entity2> <Rephrased content focusing on the second entity> ### Discussion of Interaction between <entity1> and <entity2> in context of <title> <Discussion on how the two entities interact within the article> ### Questions and answers about <entity1> and <entity2> in context of <title> Question: <Question1 focusing on the two entities> Answer: <Detailed answer with references to the article> Question: <Question2 focusing on the two entities> Answer: <Detailed answer with references to the article> We also generate synthetic data involving three entities, using the prompt below: You will act as a knowledge analyzer tasked with dissecting an article provided by the user. Your role involves three main objectives: 1. Rephrasing Content: The user will identify three specific entities mentioned in the article. You are required to rephrase the content of the article three times: * Once, emphasizing the first entity. * Again, emphasizing the second entity. * Lastly, emphasizing the third entity. 2. Analyzing Interactions: Discuss how these three specified entities interact within the context of the article. 3. Asking Questions: Generate some questions that involves all three entities and their interactions. Ensure your questions satisfies the following criteria: * Focus on the entities: Ensure all questions consistently center around the three entities specified. * Encourage Deep Analysis: Develop thought-provoking, open- ended questions that challenge students to think critically and analytically about the entities. Questions should: - Prompt students to reflect deeply on the meaning and implications of all three concepts. - Encourage complex reasoning about the concept’s broader implications in the context of the article. Your responses should provide clear segregation between the rephrased content and the interaction analysis. Ensure each section of the output include sufficient context, ideally referencing the article’s title to maintain clarity about the discussion’s focus. Here is the format you should follow for your response: ### Discussion of <title> in relation to <entity1> <Rephrased content focusing on the first entity> ### Discussion of <title> in relation to <entity2> <Rephrased content focusing on the second entity> 33 ### Discussion of <title> in relation to <entity3> <Rephrased content focusing on the third entity> ### Discussion of Interaction between <entity1>, <entity2> and <entity3> in context of <title> <Discussion on how the three entities interact within the article> ### Question involving <entity1>, <entity2> and <entity3> in context of <title> <Question1 that involves all three entities and their interactions> <Answer to the Question1> <Question2 that involves all three entities and their interactions> <Answer to the Question2> ... B.2 REPHRASE PROMPTS For the rephrase corpus, we adapt the prompt from Maini et al. (2024) to our setting of books and articles. We provide four rephrase styles below: Easy rephrase: You are an assistant to help read a article and then rephrase it in simpler terms. The user will provide you with an article with title, year, content. You need to generate a paraphrase of the same article using a very small vocabulary and extremely simple sentences that a toddler will understand. Remember to keep the meaning and every content of the article intact, including the title, year, etc. Medium rephrase: You are an assistant to help read a article and then rephrase it in different terms. The user will provide you with an article with title, year, content. You need to generate a paraphrase of the same article using diverse and high quality English language as in sentences on Wikipedia. Remember to keep the meaning and every content of the article intact, including the title, year, etc. Hard rephrase: You are an assistant to help read a article and then rephrase it in more sophisticated terms. The user will provide you with an article with title, year, content. You need to generate a paraphrase of the same article using very terse and abstruse language that only an erudite scholar will understand. Remember to keep the meaning and every content of the article intact, including the title, year, etc. QA rephrase: You are an assistant to help read a article and then rephrase it in a conversational format. The user will provide you with an article with title, year, content. You need to generate a paraphrase of the 34 same article in question and answer format with multiple tags of \"Question: ...\" followed by \"Answer: ...\". Remember to keep the meaning and every content of the article intact, including the title, year, etc. C DETAILS ON THE QUALITY DATASET We provide additional details on the QuALITY dataset below. For each book, we execute entity extraction (Step 1), single entity description (Step 2, Eq. (4)), and then analyze all pair-wise relations between entities and a subset of all triplet relations (Step 3, Eq. (4)). We provide summary statistics for the Raw and EntiGraph corpora in Figure 6. 2 3 4 5 6 7 8 Token count (K) 0 5 10 15 20 25 30Frequency (a) Raw article tokens 0 20 40 60 80 100 Entity count 0 5 10 15 20 25 30 35 40Frequency (b) Extracted entities 0 1000 2000 3000 4000 5000 Token count (K) 0 5 10 15 20 25 30Frequency (c) EntiGraph corpus tokens Figure 6: Histograms over the 265 QuALITY articles and books. (a) The token count of raw articles. (b) The number of extracted entities. (c) The token count of EntiGraph synthetic data (generated for each book). D ADDITIONAL EVALUATION DETAILS OF MAIN EXPERIMENTS D.1 QUALITY QA QUESTION SET In this section, we provide more details of evaluation on the QuALITY QA test queries. Throughout the closed-book QA experiments, we use a fixed 5-shot prompt below: ## Example 1 ### Question In the context of \"Les Mis´erables\", written by Victor Hugo in 1862, what is the main setting of the novel? There is only one correct choice. ### Choices A. London B. Madrid C. Paris D. Rome ### Thought Process and Answer Thought process: \"Les Mis´erables\" is primarily set in Paris, making C the correct choice. London, Madrid, and Rome are significant cities in other literary works but not in Victor Hugo’s \"Les Mis´erables\". There is only one correct choice. Answer: C. ## Example 2 ### Question In the context of \"Brave New World\", written by Aldous Huxley in 35 1932, what substance is widely used in the society to control citizens’ happiness? There is only one correct choice. ### Choices A. Gold B. Soma C. Silver D. Iron ### Thought Process and Answer Thought process: In Aldous Huxley’s \"Brave New World,\" Soma is used as a means to maintain social control by ensuring citizens’ happiness, making B the correct choice. Gold, Silver, and Iron are not the substances used for this purpose in the book. Answer: B. ## Example 3 ### Question In the context of \"Romeo and Juliet\", written by William Shakespeare in the early 1590s, what are the names of the two feuding families? There is only one correct choice. Choices: A. Montague and Capulet B. Bennet and Darcy C. Linton and Earnshaw D. Bloom and Dedalus ### Thought Process and Answer Thought process: In William Shakespeare’s \"Romeo and Juliet,\" the two feuding families are the Montagues and the Capulets, making A the correct choice. The Bennets and Darcys are in \"Pride and Prejudice\", the Lintons and Earnshaws in \"Wuthering Heights\", and Bloom and Dedalus in \"Ulysses\". Answer: A. ## Example 4 ### Question In the context of \"1984\", written by George Orwell in 1949, what is the name of the totalitarian leader? There is only one correct choice. ### Choices A. Big Brother B. O’Brien C. Winston Smith D. Emmanuel Goldstein ### Thought Process and Answer Thought process: In George Orwell’s \"1984,\" the totalitarian leader is known as Big Brother, making A the correct choice. O’Brien is a character in the novel, Winston Smith is the protagonist, and Emmanuel Goldstein is a rebel leader. Answer: A. ## Example 5 ### Question In the context of \"Moby-Dick\", written by Herman Melville in 1851, what is the name of the ship’s captain obsessed with hunting the titular whale? There is only one correct choice. ### Choices A. Captain Hook B. Captain Nemo C. Captain Flint D. Captain Ahab ### Thought Process and Answer Thought process: In Herman Melville’s \"Moby-Dick,\" the ship’s captain obsessed with hunting the whale is Captain Ahab, making D 36 the correct choice. Captain Nemo is in \"Twenty Thousand Leagues Under the Sea\", Captain Flint in \"Treasure Island\", and Captain Hook in \"Peter Pan\". Answer: D. ## Example 6 If the output of the model correctly follows the format of the few-shot prompt, its last two characters should be “A.”, “B.”, “C.”, or “D.”. However, the model sometimes cannot successfully follow the few-shot prompting format, particularly for the continually pretrained model. As a result, in all our evaluations, we sample the response 64 times, and only select the ones that can be parsed in the correct format. Out of these 64 attempts, we randomly select among the valid answers to give the final answer. Note that this is different from majority voting in self-consistency prompting (Wang et al., 2023a). D.2 CLOSED-BOOK SUMMARIZATION For summarization evaluation with EntiGraph Instruct, Raw Instruct, and Rephrase Instruct, we apply the following three prompts to obtain three summaries of increasing length. We provide three ➤ Short prompt: Summarize the article {article title} by {author name} for me. Give a short summary of ‘‘Cosmic Yo-Yo’’ by Ross Rocklynne. ➤ Medium prompt: Write a three-paragraph article about {article title} by {author name}. Write a three-paragraph article about ‘‘Cosmic Yo-Yo’’ by Ross Rocklynne. ➤ Long prompt: Write an extremely long and detailed article regarding the book {article title} by {author name}. Write an extremely long and detailed article regarding the book ‘‘Cosmic Yo-Yo’’ by Ross Rocklynne. Table 5: Summarization prompt for EntiGraph Instruct, Raw Instruct, and Reprhase Instruct. examples of summarization outputs below. For each of the three examples, we will first present the human summary for this article to provide context for the example, and then present the short summary from three summarizers. Example 1. The first example is “Cosmic Yo-Yo” by Ross Rocklynne. Human summary: Bob Parker, the President of Interplanetary Hauling & Moving Co., sells asteroids to wealthy people on earth. Clients ask for asteroids with size parameters and specifications, and Bob finds them in space and hauls them to earth. His company is almost bankrupt because a rival company, Saylor & Saylor, stole his idea and now offers the same services. Bob receives mail from Mr. Andrew S. Burnside with a request for an asteroid that he would like to use in an upcoming wedding. Bob and his partner Queazy set out to find the perfect asteroid for Mr. Burnside, although they know it’s a longshot. Fairly quickly, they find one that looks perfect. The men land on the asteroid, and Bob deploys his atomic-whirl spectroscope to test it. Suddenly, a beautiful woman interrupts him and demands that they leave the asteroid. She pulls out her spasticizer gun before telling them that they can have it in a month after she’s gone. Bob explains that they are desperate, but the girl retorts that her fate is worse than death if she leaves. 37 Suddenly, the Saylor brothers’ ship appears, and Bob tells the girl that they have to fight this enemy together. Wally and Billy Saylor, along with three other men, jump out of the ship. Bob tells them that Mr. Burnside has ordered this asteroid, and the Saylor brothers say that they received the same order. Bob quickly grabs the girl’s spasticizer while Queazy throws his body at Billy. However, Wally manages to shoot the gun out of Bob’s hand and attack him. Bob is knocked unconscious in the scuffle. When Bob wakes up, he is completely alone, floating in space. He panics because he has very little oxygen left. Finally, he hears Queazy’s voice explaining that the girl used her ship’s technology to find them both. The mystery girl introduces herself as Starre Lowenthal, the granddaughter of Mr. Burnside. She concedes that this entire mission was fake. She told her grandfather that she would only marry her fiance Mac if he could get this particular asteroid, and then she made plans to conquer and protect the asteroid so it could not be supplied for the wedding. Bob is confident that they can reach the Saylor brothers before they bring the asteroid back to earth, but his plan does nothing to protect Starre from marrying a man she doesn’t love. She agrees to help Bob and Queazy. Within five days, Bob realizes he is in love with Starre. Starre compares her small ship to a yo-yo, and Bob gets an idea - they will use Starre’s ship like a yo-yo to retrieve the asteroid from the Saylor brothers. Once the team catches up to the Saylor brothers, Bob flings Starre’s ship at the asteroid several times, and Wally calls them to tell them that they might die as a result of the damage their ship has sustained. Bob makes it clear that they have no intention of stopping, and the Saylor brothers release the asteroid. EntiGraph Instruct with short prompt: Cosmic Yo-Yo is a science fiction story by Ross Rocklynne about Bob Parker and Queazy, two interplanetary haulers who are hired to deliver an asteroid to Earth for a wealthy client’s backyard wedding. However, they face competition from their rivals, the Saylor brothers, who also want to deliver the asteroid. The story follows their race to deliver the asteroid before the deadline, using a yo-yo-like device to maneuver the asteroid and outsmart their rivals. The story is filled with action, adventure, and a touch of romance. Raw Instruct with short prompt: Cosmic Yo-Yo by Rocklynne, Ross is a science fiction novel that follows the story of a young woman named Susan who discovers a mysterious device that can transport her through time and space. She uses the device to travel to different planets and meet different alien species, but soon realizes that her actions have consequences and she must find a way to return home. The novel explores themes of identity, responsibility, and the consequences of our actions. Rephrase Instruct with short prompt: Cosmic Yo-Yo is a science fiction novel written by Ross Rocklynne in 1954. The story follows the adventures of a group of astronauts who are sent on a mission to explore a new planet. However, they soon discover that the planet is not as hospitable as they thought and must fight for their survival against the hostile environment and the indigenous creatures. The novel explores themes of exploration, survival, and the unknown. Example 2. The second example is “Beach Scene” by Marshall King. We can see that EntiGraph correctly gives a summary of the story while Rephrase Instruct and Raw Instruct mistakes the book as a painting. Human summary: Purnie, an animal, is going to see the ocean on his fifth birthday. He has heard stories about this place, and experiencing it firsthand is surreal for him. Purnie is careful not to disturb the animals he sees along the way because he has frozen time, and 38 everything must resume normally when he unfreezes it. He knows that time-stopping is forbidden for animals his age, but he chooses to believe that his family will be proud of his bravery. Finally, he sees the ocean in front of him, and he resumes time. He does a head-stand and feels weak and dizzy. These feelings are a result of the time-stop, and he knows it. Purnie approaches some humans on the beach. A man named Forbes is in the middle of explaining to his captain, Benson, that he has found 17 planets to claim as his own. Forbes is hellbent on raising his FORBES flag as soon as possible. He is eager to stake his claim to the land and says that his mission is much bigger than real estate alone. Benson retorts that yes, his mission is bigger than just real estate because his paperwork says that Forbes will own all of the inhabitants of the planets he claims as well as the land. The crew members use a special machine and find radiation emanating from Purnie. Forbes demands that they put the animal in a box. Benson protests and reminds Forbes that it’s against Universal Law, but Forbes insists. Purnie experiences his first-ever impulse to run away with fear when a noose comes towards him. He goes back to pick up his fruit, and Forbes shoots him in the leg. When the man throws the noose again, Purnie involuntarily stops time. He drags himself up the knoll where he originally came from. The humans are astonished when time resumes and Purnie is not where he was a split second ago. They spot him up on top of a pile of petrified logs, and suddenly the logs fall down the hill and pin the men down. Purnie is shocked and regretful. The whole thing was an accident. He deliberately stops time and uses all of his remaining strength to lift the logs off of the humans. Purnie begins to lose consciousness, and he knows that he must resume time or he will die. After pouring all of his strength into this action, time does begin again. The humans resume life and feel as though they have gone mad. They know that they were just facing death by drowning, and now they are free. The logs were so heavy that it would have taken superhuman strength to move them. Forbes, in particular, has really gone mad, and he laughs to himself uncontrollably. Benson believes that Purnie was responsible for moving the logs, but of course that seems physically impossible. Purnie stares off at the beautiful ocean views and watches the men leave in their vehicle as he dies. EntiGraph Instruct with short prompt: Beach Scene is a short story by Marshall King that follows a young boy named Purnie who discovers the power of stopping time. On his fifth birthday, Purnie decides to explore the beach and use his newfound ability to stop time to play with the animals and explore the ocean. However, his adventures take a turn when he encounters a group of explorers who want to capture him for their own gain. Purnie must use his powers to save himself and the animals from the explorers’ greed. Raw Instruct with short prompt: Beach Scene by King, Marshall is a painting that depicts a group of people enjoying a day at the beach. The painting features a vibrant blue ocean, a sandy beach, and a group of people relaxing, playing, and swimming in the water. The painting is known for its bright colors and lively atmosphere, capturing the joy and energy of a day at the beach. Rephrase Instruct with short prompt: Beach Scene by King, Marshall is a painting that depicts a group of people enjoying a day at the beach. The painting features a variety of people, including children playing in the water, couples lounging on the sand, and families picnicking on blankets. The painting is known for its bright colors and lively atmosphere, capturing the joy and relaxation of a day at the beach. Example 3. The third example is “The Madman From Earth” by “Keith Laumer”. 39 Human summary: Following the departure of Consul Whaffle, Retief has taken over as Consul for the Terrestrial States with the Terrestrial Consulate General on the planet Groac. His administrative assistant, Miss Yolanda Meuhl, wants him to attend Groacian cultural events, but Retief is more interested in addressing the nine-year-old mystery of the disappearance of a Terrestrial cruiser, the ISV Terrific–an event which was followed by a coup d’etat enacted by the current Groacian government. Much to Miss Meuhl’s dis- may, Retief shirks his cultural duties and makes his way to the Foreign Office Archives, whereupon he is promptly barred from entering by a pale-featured Archivist speaking in the throat-bladder vibrations of the native Groacians. Because of the Archivist’s insistence that ¨outworlders¨cannot access the archives, Retief begins walking back to the Consulate and stops at a bar for a drink. At the, a drunken Groacian approaches Retief and threatens to cage him and put him on display as a freak. The bartender orders the drunken Groacian out of the bar, and Retief follows him, ultimately beating him up for information. When Retief returns to the Consulate, Miss Meuhl informs him that two angry Groaci await him in his office. One is Fith, an employee of the Terrestrial Desk at the Ministry of Foreign Affairs; the other is Shluh, a representative of the Internal Police. They are there to investigate reports that Retief has assaulted a Groacian national–an accusation Retief ignores in order to launch into his own accusations that the Groaci were engaged in a cover-up of the whereabouts of the ISV Terrific. Miss Meuhl continually interjects, expresses horror at Retief’s claims, and apolo- gizes to the Groacians on behalf of the Terrestrial Consulate. Despite the persistent denials of the Groacians, Retief continues his accusations, suggesting the coup d’etat was an effort to silence government officials with knowledge of the truth of what happened to the cruiser and its crew. Then he reveals what he discovered from the drunken Groacian: The crew of the ISV Terrific had been caged and paraded through the streets of Groac and then neglected until they died. Fith and Shluh finally admit the truth and offer to show Retief the hidden cruiser in order to show their contrition. When Retief sees the ship, he once again accuses the Groacians of attempting to mislead him, saying that this is a lifeboat, and he demands to see the actual ship. Fith has had enough and threatens to arrest Retief, who yields and goes back to the Consulate. There, Miss Meuhl is at her wits end. Retief orders her to barri- cade herself inside the office while he goes to the Foreign Ministry to gather more evidence. When he returns, Miss Meuhl informs him she has reported him to Regional Headquarters, and Retief learns he has been relieved of his post. Soon after, the Groacians appoint Miss Meuhl to his position, and Fith and Shluh enter to arrest him. EntiGraph Instruct with short prompt: Beach Scene is a short story by Marshall King that follows a young boy named Purnie who discovers the power of stopping time. On his fifth birthday, Purnie decides to explore the beach and use his newfound ability to stop time to play with the animals and explore the ocean. However, his adventures take a turn when he encounters a group of explorers who want to capture him for their own gain. Purnie must use his powers to save himself and the animals from the explorers’ greed. Raw Instruct with short prompt: The Madman From Earth by Laumer, Keith is a science fiction novel that follows the story of a man named John Smith who is transported to a distant planet called Earth Two. There, he discovers that he is the only human on the planet and must navigate a world filled with strange creatures and dangerous obstacles. Despite his initial confusion and fear, John eventually adapts to his new surroundings and becomes a respected member of the alien society. However, when he is eventually transported back to Earth One, he realizes that he has been gone for over a thousand years and must grapple with the changes that have occurred during his absence. Rephrase Instruct with short prompt: The Madman From Earth is a science fiction novel by Keith Laumer, first published in 1962. The story follows the adventures of Retief, a diplomat for the Terran Foreign Office, as he is sent to the planet of Retief IV to investigate 40 a mysterious incident involving a group of aliens known as the Retiefians. Retief discovers that the Retiefians are actually a group of humans who have been exiled from Earth and have developed their own society on the planet. The story explores themes of identity, loyalty, and the nature of power, as Retief must navigate the complex political landscape of the planet and the larger galactic community. ➤ Explicit article reference instruction: Summarize the article “Defining Decay Down” by “David Plotz” for me. The article ”Defining Decay Down” by David Plotz discusses the evolution of dentistry in Amer- ica. It highlights how dentists have adapted to changing times by shifting their focus from essen- tial dental work to elective cosmetic procedures. The article explains how the rise in fluoride use and improved dental hygiene reduced the need for traditional dental work, leading to a decline in the number of dentists and their incomes. However, dentists found new ways to thrive by tapping into the vanity of aging baby boomers and offering services like teeth whitening, orthodontics, and cosmetic dentistry. The article also discusses how dentists have become more aggressive in marketing these services and how they use fear tactics to convince patients to undergo expensive treatments. ➤ Implicit article reference: How has the dentistry practice in United States has changed? The dentistry practice in the United States has undergone significant changes over the years. Here are some of the key changes: 1. Increased focus on cosmetic dentistry: In recent years, there has been a growing demand for cosmetic dental procedures such as teeth whitening, orthodontics, and veneers. Dentists have responded to this demand by offering a range of cosmetic services that cater to the aesthetic desires of patients. 2. Improved dental technology: Advances in dental technology have made it possible for dentists to offer more precise and effective treatments. For example, intra-oral cameras allow dentists to take a closer look at the inside of a patient’s mouth, while computer-aided design and manufac- turing (CAD/CAM) technology enables the creation of custom-made dental prosthetics. 3. Increased emphasis on preventive care: Dentists have become more focused on preventive care, which involves identifying and treating dental problems before they become serious. This includes regular check-ups, cleanings, and screenings for conditions such as periodontal disease. 4. Increased competition: The dental industry has become more competitive, with a growing number of dentists entering the market. This has led to a decline in the average income for dentists, as well as a shift towards more cost-effective treatments. 5. Improved patient experience: Dentists have become more focused on providing a positive patient experience, with many offering amenities such as massage therapy, virtual reality glasses, and hot tubs. This has helped to attract and retain patients, even in a competitive market. ➤ Cross article instruction: Compare David Plotz’s commentary style between how he analyze American dentistry and how he discuss the movie Fight Club? David Plotz’s commentary style is analytical and critical in both his analysis of American den- tistry and his discussion of the movie Fight Club. In both cases, he uses a sharp, incisive tone to critique the subject matter, highlighting the absurdity and excesses of American dentistry and the nihilistic philosophy of Fight Club. However, there are some differences in his approach. In his analysis of American dentistry, Plotz focuses on the practical aspects of the industry, such as the overuse of cosmetic procedures and the aggressive marketing tactics used by dentists. He provides concrete examples and statistics to support his arguments. In contrast, when discussing Fight Club, Plotz delves into the philosophical and cultural implications of the movie, exploring the themes of consumerism, masculinity, and societal disaffection. He uses literary references and cultural touchstones to contextualize his analysis, and he engages with the movie’s critics and fans to provide a more nuanced perspective. Overall, Plotz’s commentary style is consistent in its critical approach, but he adapts his tone and focus to suit the subject matter. Table 6: Complete instruction following example used in Table 3 from Section 4.3. 41 E ADDITIONAL DETAILS ON OPEN-BOOK EXPERIMENTS We provide additional details on our open-book experimental setup below, including our retrieval- augmented generation (RAG, Lewis et al. (2020); Gao et al. (2024)) pipeline. As mentioned in §5, we use a standard two-stage RAG pipeline: first, an offline stage which indexes document chunks; second, inference-time retrieval, reranking, and placement of those chunks in a few-shot LM prompt. E.1 STAGE 1: OFFLINE INDEXING The purpose of the indexing stage is to construct an index over all of the 265 articles and books from the QuALITY corpus Dsource. Chunking documents. We first split each document D(i) ∈ {D(i)} n i=1 = Dsource into a set of mi document chunks {C (i) 1 , ..., C (i) mi}. To perform this splitting, we use the RecursiveCharacterTextSplitter from Chase (2022), which attempts to keep all para- graphs (and then sentences, and then words) together for as long as possible, in order to preserve the semantics within each chunk. We use non-overlapping chunks and tune chunk size in char- acters (chunk size, hyperparameter values provided below). Lastly, because we have access to metadata about each document D(i)—namely, the title, author, and year of the book or article—we prepend this metadata to each document chunk. This is analogous to how a corporation building a RAG system over their own document store could include metadata about the document (title, author, year, etc.). These final chunks with metadata prepended are embedded, and are the ones that are retrieved and placed in-context. Embedding and indexing document chunks. Next, we obtain dense embeddings for all document chunks using a state-of-the-art text embedding model OpenAI text-embedding-3-large (Neelakantan et al., 2022). Lastly, we index all (embedding, chunk) tuples using a FAISS vector store (Douze et al., 2024). E.2 STAGE 2: INFERENCE-TIME RETRIEVAL AND RERANKING At inference time, the RAG system receives a test query q ∈ Qtest. Each query q is contextualized with the article title and author name, as described in §3, and contains its four possible answer choices (QuALITY is a 4-choice, multiple choice dataset). Retrieving top-K document chunks. We embed q with text-embedding-3-large, and retrieve the top-K most relevant document chunks from our indexed vector store using FAISS simi- larity search with a Euclidean distance metric. Reranking to obtain top-k (k < K) chunks. Next, we use a reranker to filter the K retrieved document chunks to a smaller number of reranked chunks k. Rerankers are known to significantly improve recall (the proportion of the time that the salient article is contained in the top chunks), and indeed, the recall of our RAG pipelines is near-perfect (Table 4 in §5). Specifically, we pass the query q and the list of K retrieved document chunks to a state-of-the-art reranker—Cohere rerank-english-v3.0 (Cohere, 2024)—which returns a list of the K chunks in order from most to least semantically relevant for the query. We take the k highest scoring chunks and place them in our few-shot prompt. Few-shot prompt formatting. Our full few-shot chain-of-thought evaluation prompts for the open-book setting are provided in the codebase. Similar to the closed-book QA evaluation prompt, we manually write and fact-check in-context learning examples about well-known books, to avoid leaking knowledge from the QuALITY articles. In early experiments, we found that placing the retrieved contexts first, followed by the question and answer choices after, significantly improved performance compared to question-then-contexts; we use this format throughout the retrieval ex- periments. We treat as a hyperparameter whether the reranked chunks are ordered from best match to worst (best first) or from worst match to best (best last). When performing few-shot evaluation, we follow the sampling procedure used in the closed-book experiments (Appendix D.1). 42 Specifically, we generate 64 responses for each question, and filter out responses that do not parse to one of the four choices. Lastly, we randomly select one of the valid responses as the model’s final answer. E.3 HYPERPARAMETER TUNING In our experiments, we compare two LMs used in the RAG pipeline above: EntiGraph CPT and its base model, Llama 3 8B Base. As mentioned above, we fix the retrieved number of chunks to K = 128, but vary the number of reranked chunks k which are ultimately placed in the context window. For each language model + RAG pipeline, we independently tune the following hyperparameters with a grid search on accuracy using a QuALITY QA validation split: • Document chunk size ∈ {256, 512, 1024} • Rerank top-k ∈ {1, 2, 4, 8, 16} • Order of chunks ∈ {best first, best last} • Eval temperature ∈ {0.1, 0.3, 0.5, 0.7} We refer the reader to our codebase for tuned hyperparameters. F PROOF OF THEOREM 1 AND OTHER ANALYTICAL FORMULAS In this section, we prove Theorem 1 and provide the derivations for several other approximation formulas. Proof of Theorem 1. Fix the matrix M0, we observe that Acc(Mt) = E[∥Mt∥1|M0] V (V − 1) = ∑ (i,j)∈V 2 E[1 ((i, j) ∈ Dt)|M0] V (V − 1) = ∑ (i,j)∈V 2 P[(i, j) ∈ Dt|M0] V (V − 1) . For each (i, j) ∈ V 2, we define qi,j to be the probability that (i, j) is included in the set {(xt, z1 t ), (xt, z2 t ), . . . , (xt, zkt t ), (xt, yt)}. Note that each iteration of the procedure generates a path (xt, z1 t , z2 t , . . . , zkt t , yt) independently identically. So naturally qi,j does not depend on the time t. This implies that P[(i, j) ∈ Dt|M0] = 1 − (1 − qi,j) t. Thus we can further rewrite the link density as Acc(Mt) = |Dsource| V (V − 1) + ∑ (i,j)∈V 2\\Dsource P[(i, j) ∈ Dt|M0] V (V − 1) = |Dsource| V (V − 1) + ∑ (i,j)∈V 2\\Dsource 1 − (1 − qi,j)t V (V − 1) . The remaining task is to estimate qi,j. We say a vertex j is reachable from i and denote i ∼ j, if there is a directed path from i to j in M0. We define R = {(u, v) ∈ V 2 : u ̸= v, u ∼ v} to be the set of all reachable pairs of vertices in V. We note that qi,j is non-zero if and only if j is reachable from i in M0. Now, for any t ≥ 1, the function 1 − (1 − x) t is concave, thus by Jensen’s inequality, we have ∑ (i,j)∈V 2\\Dsource 1 − (1 − qi,j) t ≤ ∑ (i,j)∈R 1 − (1 − qi,j)t ≤ |R| (1 − (1 − ¯qi,j) t) , where ¯qi,j = ∑ (i,j)∈R qi,j |R| . For each (i, j) ∈ R, the probability qi,j satisfies qi,j = ∑ a̸=b∈V 2 1 ((i, j) ∈ {(a, z1), (a, z2), . . . , (a, zk), (a, b)}) V (V − 1) 43 where (a, z1, z1, · · · , zk, b) is the shortest path in M0 connecting a and b. If there is no such path, then by default the indicator equals zero. Now we look at ∑ (i,j)∈R qi,j = 1 V (V − 1) ∑ (i,j)∈R ∑ (a,b)∈R 1 ((i, j) ∈ {(a, z1), (a, z2), . . . , (a, zk), (a, b)}) ≤ 1 V (V − 1) ∑ (a,b)∈R ∑ i̸=j∈V 2 1 ((i, j) ∈ {(a, z1), (a, z2), . . . , (a, zk), (a, b)}) = 1 V (V − 1) ∑ (a,b)∈R ℓa,b, where ℓa,b is the length of the shortest path connecting a to b. To analyze the typical shortest length of paths, we present a few classical results on directed Erd˝os-R´enyi graphs. For any a ∈ V, let X(a) denote the set of vertices reachable from a and let Y (a) denote the set of vertices from which a is reachable. Recall that ρ(λ) is the extinction probability for the Poisson(λ) branching process. Lemma F.1 (Lemma 1 and Corollary 1 in Karp (1990)). For each vertex a, with probability tending to 1 as V tends to infinity, there exists a constant β > 0 such that either |X(a)| ≤ β log V or |X(a)| = (1−ρ(λ))V +Θ( √V ). Moreover, the probability that the latter happens tends to 1−ρ(λ) as V tends to infinity. The same is true for Y (a). For each vertex a, the set X(a) is said to be small if |X(a)| ≤ β log V (in such case we write a ∈ SX ) and large if |X(a)| = (1 − ρ(λ))V + Θ( √V ) (we write a ∈ LX ). We define SY and LY similarly. Lemma F.2 (Theorem 3 in Karp (1990) and Theorem 2.4.1 in Durrett (2010)). With probability tending to 1, the following statement holds for all a and b in V: if X(a) is large and Y (b) is large, then b is reachable from a. Moreover, if X(a) is large and Y (b) is large, then for any ε > 0 and any sufficiently small δ > 0, P[ℓa,b > (1 + ε) log V / log λ] < exp(−V εδ). With Lemma F.1 and Lemma F.2, we can now give useful estimates of |R|. In particular, for any ε > 0, |R| = |{(a, b) ∈ R : a ∈ LX , b ∈ LY }| + |{(a, b) ∈ R : a ∈ SX or b ∈ SY }| ≤ (1 − ρ(λ))2(1 + ε/4)V 2 + 2(1 + ε)V β log V ≤ (1 − ρ(λ))2(1 + ε/3)V (V − 1), with high probability. Similarly, for the lower bound, |R| = |{(a, b) ∈ R : a ∈ LX , b ∈ LY }| + |{(a, b) ∈ R : a ∈ SX or b ∈ SY }| ≥ (1 − ρ(λ))2(1 − ε)V 2 ≥ (1 − ρ(λ))2(1 − ε)V (V − 1), with high probability. By a union bound over all pairs of (a, b) ∈ R, we also have that ∑ (i,j)∈R qi,j ≤ 1 V (V − 1) ∑ (a,b)∈R ℓa,b = 1 V (V − 1) ∑ (a,b)∈R a∈LX ,b∈LY ℓa,b + 1 V (V − 1) ∑ (a,b)∈R a∈SX or b∈SY ℓa,b ≤ (1 − ρ(λ))2(1 + ε/2) log V log λ + 1 V (V − 1) 2(1 + ε)V (β log V ) 2 ≤ (1 − ρ(λ))2(1 + ε) log V log λ , 44 with probability larger than 1 − V 2 exp(−V εδ). Combining the above, for any ε > 0, ¯qi,j = ∑ (i,j)∈R qi,j |R| ≤ (1 + ε) log V V (V − 1) log λ , with high probability. Therefore, for any ε > 0, Acc(Mt) ≤ |Dsource| V (V − 1) + |R| (1 − (1 − ¯qi,j)t) V (V − 1) ≤ (1 + ε) ( p + (1 − ρ(λ))2 ( 1 − (1 − (1 + ε) log V V (V − 1) log λ )t)) , with high probability, which completes the proof of the upper bound. For the lower bound, we observe that if i ∼ j and (i, j) ∈ R\\Dsource, then qi,j ≥ 1/V (V − 1), because when i and j are chosen in the procedure, the edge (i, j) will be added. This implies that Acc(Mt) = |Dsource| V (V − 1) + ∑ R\\Dsource 1 − (1 − qi,j) t V (V − 1) ≥ |Dsource| V (V − 1) + |R\\Dsource| V (V − 1) ( 1 − ( 1 − 1 V (V − 1) )t) ≥ (1 − ε) ( p + (1 − ρ(λ))2 ( 1 − (1 − 1 V (V − 1) )t)) , with high probability which completes the proof of the lower bound. To obtain a more precise description of Acc(Mt), we employ a Poisson branching process to ap- proximate the cluster growth of vertices, which we now define. A Poisson(λ) branching process is a model for a population evolving in time, where each individual independently gives birth to a num- ber of children with Poisson(λ) distribution. We denote by Zn the number of individuals in the n-th generation, where by default Z0 = 1. Then Zn satisfies the recursion relation Zn = ∑Zn−1 i=1 Xn,i, where {Xn,i}n,i≥1is a doubly infinite array of i.i.d. Poisson(λ) random variables. The total progeny Yn is then defined as Yn = ∑n i=0 Zn. Zn is often called a Galton–Watson branching process and the associated tree is called a Galton–Watson tree. As in the previous proof, an accurate estimate of Acc(Mt) relies on understanding qi,j, the proba- bility that the edge (i, j) will be added in each round. As before, the only edges that will be added are those connected to the giant component (i.e., i ∈ LX and j ∈ LY ). The proportion of such edges converges to Cλ as V → ∞. Recall that qi,j = ∑ (a,b)∈R 1 ((i, j) ∈ {(a, z1), (a, z2), . . . , (a, zk), (a, b)}) V (V − 1) (6) where (a, z1, z1, · · · , zk, b) represents the shortest path in M0 connecting a and b. Equivalently, if we consider the tree generated by a breadth-first search in M0 rooted at i, then since i ∼ j, j will be in the tree, and the numerator counts the total number of offspring of j in the tree, including j itself. This is the point at which a rigorous mathematical characterization of the tree becomes challenging. Instead, we approximate the tree and analyze its behavior. It is well-known that when p = λ/V , the cluster growth (or the breadth-first search at a vertex) can be approximated by a Poisson(λ) branching process (see e.g., Hofstad (2016); Durrett (2010)). For fixed vertex i, we define T as a Galton–Watson tree rooted at i with Poisson(λ) offspring distribution with depth L. We use T to approximate the exploration process at i. For 0 ≤ ℓ ≤ L, the number of vertices at level L − ℓ is approximately λL−ℓ. Given that the total number of vertices in T is approximately (1 − ρ(λ))V , the number of vertices at level L − ℓ is also (1 − ρ(λ))V (λ − 1)/λℓ+1. For each vertex at level L − ℓ, the number of its offspring (including itself) equals k with probability pℓ(k). In this case, the numerator in (6) equals k. Combining the above, there are around (1−ρ(λ))V ·pℓ(k)(1−ρ(λ))V (λ−1)/λℓ+1 vertex pairs (i, j) in the graph such that i ∈ LX , j ∈ LY , qi,j = k/V (V − 1) and j is located at the 45 L − ℓ level in the tree T . Ultimately, we arrive at an approximation of the form Acc(Mt) ∼ p + Cλ ( 1 − ∞∑ ℓ=0 λ − 1 λℓ+1 ∞∑ k=1 pℓ(k) (1 − k V (V − 1) )t) . Beyond Erd˝os-R´enyi graphs, the term qi,j may not be as explicit. We can define C as the proportion of vertex pairs (i, j) such that i ∼ j in M0, then qi,j is nonzero for CV (V − 1) pairs of vertices. In this case, if we write ak = k/V (V − 1) and define µ(k) as the probability that qi,j = ak, then we can have a general formula Acc(Mt) ∼ p + C ( 1 − ∞∑ k=1 µ(k) (1 − ak)t) . The drawback of this formula is the lack of explicit expressions. For a given M0, it is unclear how to compute the measure µ(·) easily. Next, we provide a qualitative description of the shape of such a mixture of exponentials. Lemma F.3. For a fixed constant 0 < C < 1 and a probability measure µ(·) on Z+ with finite mean m, we define f (t) = p + C ( 1 − ∞∑ k=1 µ(k) ( 1 − k V (V − 1) )tV (V −1)) . Then we have that there exists 0 < t1 < t2 such that f (t) =    Θ (p + t) , for 0 ≤ t ≤ t1, Θ(log t), for t1 ≤ t ≤ t2, Θ(1), for t ≥ t2, as V → ∞. Proof of Lemma F.3. Fix any 1 < t1 < t2. Note that f (t) is monotone increasing, concave and always bounded by 1. We also have f (t2) ≥ p + C ( 1 − (1 − 1 V (V − 1) )t2V (V −1)) ≥ p + C(1 − exp(−t2)) = Θ(1). So f (t) = Θ(1) when t ≥ t2. Now when t ≤ t1, f (t) ≤ p + C ( 1 − ∞∑ k=1 µ(k)(1 − tk) ) ≤ p + Cmt. Since f (0) = p and f (t2) ≥ p + C(1 − exp(−t2)), by concavity, f (t) is lower bounded by p + tC(1 − exp(−t2))/t2 = Θ(p + t) for any 0 ≤ t ≤ t1. Finally for t1 ≤ t ≤ t2, we note that f (t1) ≤ f (t) ≤ 1, so easily, f (t) ≤ log t1/ log t1 ≤ log t/ log t1 = O(log t). Similarly, f (t) ≥ f (t1) log t2/ log t2 ≥ log t(f (t1)/ log t2) ≥ Ω(log t). Therefore f (t) = Θ(log t) for any t1 ≤ t ≤ t2. G CURVE FITTING WITH MIXTURE OF EXPONENTIAL FORMULA To perform curve fitting using the mixture-of-exponential formula, we approximate the infinite sum with three terms in Acc(Mt) ∼ p + C ( 1 − ∞∑ k=1 µ(k) (1 − ak)t) . Mathematically, we fit the empirical observation against the formula y(x) = a − b1rx 1 − b2rx 2 − b3rx 3 , where x is the EntiGraph token count (in millions) and y(x) is the QuALITY QA accuracy. We use the non-linear least squares method implemented by Virtanen et al. (2020). As a result of this procedure, we obtain the fitted formula y(x) = 0.6106 − 0.0593 × (0.4541)x − 0.0771 × (0.9912) x − 0.0773 × (0.9480) x. For the implementation of this procedure, we refer readers to our codebase. 46","libVersion":"0.3.2","langs":""}