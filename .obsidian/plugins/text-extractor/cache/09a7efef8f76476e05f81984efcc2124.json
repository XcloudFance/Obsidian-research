{"path":"GenAIUnleaning/DiffusionUnlearning/impo/Enhancing User-Centric Privacy Protection-An Interactive Framework through Diffusion Models and Machine Unlearning.pdf","text":"Enhancing User-Centric Privacy Protection: An Interactive Framework through Diffusion Models and Machine Unlearning Huaxi Huang 1*, Xin Yuan 1, Qiyu Liao 1, Dadong Wang 1, Tongliang Liu 2 1Data61, CSIRO, Australia 2The University of Sydney, Sydney NSW 2006, Australia Abstract In the realm of multimedia data analysis, the extensive use of image datasets has escalated concerns over privacy pro- tection within such data. Current research predominantly fo- cuses on privacy protection either in data sharing or upon the release of trained machine learning models. Our study pioneers a comprehensive privacy protection framework that safeguards image data privacy concurrently during data shar- ing and model publication. We propose an interactive im- age privacy protection framework that utilizes generative ma- chine learning models to modify image information at the attribute level and employs machine unlearning algorithms for the privacy preservation of model parameters. This user- interactive framework allows for adjustments in privacy pro- tection intensity based on user feedback on generated im- ages, striking a balance between maximal privacy safeguard- ing and maintaining model performance. Within this frame- work, we instantiate two modules: a differential privacy dif- fusion model for protecting attribute information in images and a feature unlearning algorithm for efficient updates of the trained model on the revised image dataset. Our ap- proach demonstrated superiority over existing methods on fa- cial datasets across various attribute classifications. Introduction Images inherently contain a wealth of private information, such as gender, race, and age in facial datasets to license plate numbers and vehicle types in car photographs. The extensive use of such images across social networks, gov- ernment databases, and industrial applications has signifi- cantly raised privacy risks, leading to profound public con- cerns (Times 2018; Liu et al. 2021a). This growing reliance on image data brings privacy issues to the forefront of global discussions and legislation. Countries around the world have responded by implementing stringent privacy laws, with the European General Data Protection Regulation (GDPR) (Eu- ropean Parliament and Council of the European Union) and the Australian Privacy Act 1988 (No 1988) being prime ex- amples. These laws emphasize the safeguarding of “personal data,” which includes any information linked to a specific or identifiable individual. Given this definition, images fall *Project lead. Huaxi Huang, Xin Yuan and Qiyu Liao are Co- first authors. Corresponding Author: Dadong Wang. This work was done when Huaxi was a CREC Research Fellow at Data61. under the category of personal data due to their frequent in- clusion of personal sensitive attributes such as faces, textual content, and license plates. This situation highlights the ur- gent need for effective methods to protect image data pri- vacy. Contemporary multimedia research has focused on dataset publication and model design, with a key chal- lenge being privacy protection. Traditional methods like pixel-level image obfuscation (Fan 2018, 2019) are inef- fective against advanced deep learning techniques, while recent approaches using generative adversarial networks (GANs) (Liu et al. 2021b; Wen et al. 2022; Li and Clifton) suffer from instability and poor image quality. Moreover, despite progress in preventing unauthorized access to sensi- tive model information, a unified solution addressing privacy in both dataset sharing and model deployment is lacking. This challenge is particularly relevant for companies pro- viding facial image applications, such as facial recognition and emotion detection services, which require comprehen- sive privacy solutions to manage user requests for the re- moval of private data, requiring adjustments to both datasets and models. Recognizing these theoretical and practical challenges, this paper proposes an integrated framework that bridges the gap by effectively managing privacy concerns in image data sharing and secure machine learning model release. Devel- oped from published datasets, these models are designed to prioritize privacy protection and can efficiently unlearn spe- cific private information upon user requests. This approach ensures a balance between privacy safeguarding and the util- ity and performance of the data and models. Fig. 1 provides a schematic overview of this framework, illustrating the in- teractive, user-centric process from customer input to model refinement, underscoring our strategy for dynamic privacy management in both data and models. Specifically, our framework addresses the privacy preser- vation challenge in image sharing by introducing differen- tial privacy (DP). To overcome the instability in GAN train- ing and issues with image quality, we have drawn inspi- ration from diffusion models (Ho, Jain, and Abbeel 2020; Preechakul et al. 2022) and developed a novel approach to differential image privacy. This method employs diffu- sion models for both the extraction of intermediate features and the generation of new images. The fundamental advan-arXiv:2409.03326v1 [cs.CV] 5 Sep 2024 tage of diffusion models lies in their process, which pri- marily involves adding noise to images and optimizing the subsequent denoising steps, circumventing the adversarial training intrinsic to GAN models and leading to more sta- ble model training. Existing research (Ho, Jain, and Abbeel 2020; Preechakul et al. 2022) indicates that diffusion models can achieve superior image quality compared to GANs. We design the Diffusion Differential Privacy model (Diffusion- DP). By utilizing a diffusion autoencoder model (Preechakul et al. 2022), we train an auxiliary classifier to disentangle in- termediate features, allowing precise control over target face images by injecting DP noises. This enhances the privacy protection of the shared images while maintaining their util- ity. To address the challenge of updating models in response to dataset changes, especially in erasing privacy information, our framework incorporates an advanced machine unlearn- ing (AMU) module. This module is designed for scenarios requiring the removal of specific private information from trained models due to dataset updates. Unlike traditional methods of retraining models from scratch, characterized by inefficiency and high costs, our method utilizes cutting- edge machine unlearning techniques (Bourtoule et al. 2021; Warnecke et al. 2023). We have adapted the machine fea- ture unlearning algorithm (Warnecke et al. 2023) specifi- cally for our task, allowing for rapid fine-tuning of models with minimal data input. This enables the models to effi- ciently unlearn specific information, significantly reducing the resources and time required while ensuring high per- formance. Integrated with the Diffusion-DP module and the AMU module, our framework offers an efficient solution for adapting models to dataset changes, rigorously safeguarding user privacy, and maintaining the utility and effectiveness of the models. In summary, this paper makes the following contributions: • We propose a user-centric interactive image privacy pro- tection framework designed to safeguard user privacy concurrently during image data sharing and model re- lease phases. This framework is interactive, enabling user- driven adjustments for personalized privacy considera- tions. • We instantiate two specific modules within our broader framework to separately address the processing of pri- vate information in data sharing and model updates. The Diffusion-DP module focuses on editing and generating image data with an emphasis on privacy preservation, while the AMU module enables efficient and rapid model adjustments in response to dataset updates, ensuring pri- vacy compliance. • Comprehensive experiments are conducted to evaluate the effectiveness of our proposed framework. The results demonstrate that our approach not only achieves signif- icant improvements in privacy protection but also main- tains high utility and performance. The rest of this paper is organized as follows. Section introduces related works. Section presents the proposed framework. In Section 6, we evaluate the proposed method on widely-used facial datasets. The conclusion is discussed in Section 6. Related Work Privacy Protection Methods for Image Data Traditional privacy-preserving methods like pixel-level im- age obfuscation are often ineffective against advanced deep learning techniques (Fan 2018, 2019), as they can signif- icantly degrade the utility of the image and fail to pro- tect against sophisticated attacks (McPherson, Shokri, and Shmatikov 2016). Recent research has proposed using deep GANs for image privacy protection, where DP noise is in- fused into intermediate features extracted from the encoder, which are then used by the GAN’s image generator to pro- duce new images (Liu et al. 2021b; Wen et al. 2022). While this approach offers a more robust solution, it faces chal- lenges such as GAN training instability, susceptibility to mode collapse, and the need for further improvement in the quality of generated images (Lucic et al. 2018). This challenge extends to contemporary multimedia re- search, which focuses on both dataset publication (Liu et al. 2015; Lee et al. 2020; Deng et al. 2009; Karras, Laine, and Aila 2019) and specific model design (He et al. 2016; Ren et al. 2015; Ronneberger, Fischer, and Brox 2015). Pri- vacy protection remains a critical concern, particularly in the publication and sharing of datasets. Although substantial progress has been made in developing privacy-preserving techniques for both datasets and models, a unified approach that addresses privacy concerns in both domains is still lack- ing. This gap is particularly evident in practical scenarios faced by companies specializing in facial image applica- tions (FaceApp.com 2020; Corporation 2023), where there is a growing demand for solutions that safeguard privacy while maintaining the functionality of AI services based on facial data, like facial recognition (Deng et al. 2019; Schroff, Kalenichenko, and Philbin 2015), age estimation (Pan et al. 2018; Gao et al. 2018; Li et al. 2022), gender identifi- cation (Eidinger, Enbar, and Hassner 2014; Kuprashevich and Tolstykh 2023), and emotion detection (Kollias and Zafeiriou 2019; Zhang et al. 2023; Savchenko 2023). When users request the retraction of their private data to prevent privacy violations, these companies must carefully adjust both datasets and corresponding AI models, underscoring the complexity and importance of a comprehensive privacy- preserving strategy. Machine Unlearning In the field of machine learning, both exact and approximate machine unlearning methods have been explored to address the challenge of completely removing the influence of cer- tain data segments from trained models. Exact machine un- learning aims to fully eliminate data influence, often neces- sitating some degree of model retraining. Early work by Cao et al. (Cao and Yang 2015) introduced a heuristic approach to convert machine learning algorithms into a summation format, facilitating the removal of data lineage. Subsequent methods, such as Sharded, Isolated, Sliced, and Aggregated training (SISA) by Bourtoule et al. (Bourtoule et al. 2021), proposed a sharded, isolated, sliced, and aggregated training approach to enhance retraining efficiency. DeltaGrad (Wu, Dobriban, and Davidson 2020), another exact method, ac- celerates retraining by counteracting the data designated for deletion, though it is limited to specific algorithms and can- not manage mini-batch sizes. On the other hand, approximate machine unlearning fo- cuses on minimizing the differences between models before and after data removal, aiming to preserve model perfor- mance. Notable methods include the certified-removal ap- proach by Guo et al. (Guo et al. 2020), which uses a Newton step to erase the influence of data points in L2-regularized linear models, and a scrubbing method for deep neural net- works proposed by Golatkar et al. (Golatkar, Achille, and Soatto 2020). These approaches often incorporate differen- tial privacy mechanisms to obscure residual information. Additionally, specialized methods, such as those by Ginart et al. (Ginart et al. 2019) for K-means, highlight the de- velopment of model-specific unlearning techniques. While approximate methods offer privacy safeguards and perfor- mance preservation, they face challenges in verifying im- plementation and aligning with legal requirements like the “right to be forgotten.” User-centric Interactive Image Privacy Protection Framework In this paper, we propose a user-centric interactive image privacy protection framework that converts a potentially risky model into a safe one by unlearning specific attribute features, as depicted in Fig. 1. The framework begins with a training dataset labeled as “risky” due to its inclusion of sensitive information. This dataset is used to train a risky model. The process involves identifying a sensitive subset within the training data, guided by a resource specification step, which isolates sensitive attributes or individuals. Con- currently, a transferring dataset, deemed “safe”, undergoes distribution matching to ensure alignment with the sensi- tive subset. This allows for the safe transfer of attribute fea- tures, producing an “attribute transferred sensitive subset” that mitigates privacy risks. The core of this framework is the advanced machine unlearning (AMU) module, which em- ploys a refined machine feature unlearning algorithm to ef- ficiently remove sensitive attributes from the model without the need for costly retraining. To ensure privacy preservation, the framework integrates a Diffusion-DP module, providing DP guarantees by train- ing on the transformed sensitive subset. The result is a “safe model” that can perform accurate inferences while safe- guarding sensitive information. This approach not only mit- igates the risks associated with sensitive data exposure but also maintains high model performance and utility, making it an efficient solution for adapting to dataset changes while rigorously protecting user privacy. In what follows, we will provide detailed insights into each module of the framework. Differential Privacy For an (ϵ, δ)-DP mechanism, ϵ > 0 denotes the distinguish- able bound of all outputs on adjacent datasets D and D′ in a dataset 1, and δ denotes the probability that the ratio of the probabilities of two adjacent datasets D and D′ cannot be bounded by exp(ϵ) after adding a privacy-preserving mech- anism (McSherry and Talwar 2007). The specific definition of DP is provided as follows. Definition 1 ((ϵ, δ)-DP (Abadi et al. 2016)) A randomized mechanism M: X → R with domain X and range R satis- fies (ϵ, δ)-DP, if Pr [M(D) ∈ S] ≤ e ϵ Pr [M(D′) ∈ S] + δ, (1) for all measurable sets S ⊆ R and for any two adjacent datasets D, D′ ∈ X . Definition 2 ((α, ϵ)-RDP (Mironov 2017)) A randomized mechanism M: X → R with domain X and range R satis- fies (α, ϵ)-RDP, if Dα(M(D)∥M(D′)) ≤ ϵ, (2) for all measurable sets S ⊆ R and for any two adjacent datasets D, D′ ∈ X . Lemma 1 (RDP to (ϵ, δ) − DP (Mironov 2017)) If a mechanism M: X → R satisfy (α, δ)-RDP, it also satisfies(ϵ + log(1/δ) α−1 , δ)-DP for any 0 < δ < 1. Moreover, M satisfies pure ϵ-DP. We analyze the sensitivity and privacy performance of the proposed time-varying DP noise perturbation mechanism. We use the ℓ2-norm sensitivity, as given by ∆s = max D,D′ ∥s(D) − s(D′)∥ , (3) where s(·) is a general function in D. Diffusion Differential Privacy Model To implement DP for images, we consider a diffusion mech- anism that generates images while incorporating noise to en- sure privacy. Definition 3 (Weighted Diffusion Mechanism) Assume X, X′ ∈ Rd, let M(X) be the mechanism that is a random function taking X as the input, and returning M(X) at the (t + 1)-the iteration: Xt+1(i) = st+1(Xt(i)) + wt c(i) · nt+1(i), (4) Xt+1 = Decoder(Encoder(Xt) + wt c(i) · nt+1(i)), (5) where st+1(·) is the contractive map, wt c = [wt c(1), · · · , wt c(i), · · · , wt c(d)] is the weighted vector obtained from a N -class classifier, and nt+1 ∼ OU (θ, ρ) at the (t + 1)-th iteration, i.e., nt = N (e−θtx, ρ2 θ (1 − e−2θt)Id). Let M(X′) be obtained from X ′ under the same mapping. For α ≥ 1, M(X) satisfies Dα(M(X)∥M(X′)) ≤ α∥X − X′∥2 2T ( wt c(i) ∑N i=1 wt c(i) · ρ2 θ (e2θt − 1)) . (6) 1Two datasets, D and D′, are adjacent if D′ can be built by adding or removing a single training example from D. Figure 1: A user-centric interactive image privacy protection framework, which protects sensitive information in image data for machine learning. It transforms a risky training dataset and model into a safe model using attribute feature unlearning and DP techniques, ensuring privacy by modifying sensitive attributes and matching data distributions while balancing privacy protection with model performance through user feedback and adjustments. Theorem 1 (Diffusive DP-Image) Suppose that an input image X0, and its corresponding output image XT . s(·) maps the image into its latent space (or, feature space), and s ′(·) is a 1-Lipschitz. Let s(·) have global L2-sensitivity ∆s and P = (Pt)t≥0 be the Ornstein-Uhlenbeck (OU) process with parameters θ and ρ. For any α > 1 and t > 0 the OU mechanism M s t (Xt) = Pt(s(Xt)) satisfies α, αθ∆2 s 2T ( wt c (i) ∑N i=1 wt c (i) ρ2(e2θt−1) )  -RDP. Proof 1 According to Definition 3, we have M(s(X)) for α ≥ 1 satisfies Dα (M(s(X))∥M(s(X′))) ≤ α∥s(X) − s(X′)∥ 2 2T ( wt c(i) ∑N i=1 wt c(i) · ρ2 θ (e2θt − 1)) (7a) ≤ α max{X,X′} ∥s(X) − s(X′)∥ 2 2 2T ( wt c(i) ∑N i=1 wt c(i) · ρ2 θ (e2θt − 1)) (7b) = α∆2 s 2T ( wt c(i) ∑N i=1 wt c(i) · ρ2 θ (e2θt − 1)) , (7c) where (7c) is obtained based on (3). Based on the definition of (α, ϵ)-RDP, we have Dα(M(X)∥M(X′)) ≤ ϵ. Therefore, the OU mechanism M s t (Xt) = Pt(s(Xt)) satisfies α, αθ∆ 2 s 2T ( wt c (i) ∑N i=1 wt c (i) ρ2(e2θt−1) )  -RDP. Algorithm 1: Diffusion-DP algorithm 1 Parameters: Noise coefficient σ, and number of iterations T . Input: The original image Xt. Output: Output Privacy-preserving image XT . 2 while 0 ≤ t < T do 3 zt = s(Xt) 4 zt+1 = s ′(zt) + wt c · nt, nt ∼ OU(θ, ρ) 5 Xt+1 = g(zt+1) 6 end Advanced Machine Unlearning This section presents our methodology for unlearning spe- cific attributes from input images using first-order and second-order optimization techniques, aiming to remove the influence of specific attributes from the model while pre- serving the overall performance of other features. First-Order Update To unlearn data, we aim to find an update ∆(X, ˜X) for our model w∗, where X and ˜X are the original data and its perturbed version. If the loss L is differ- entiable, we compute the first-order update as: ∆(X, ˜X) = −η   ∑ ˜x∈ ˜X ∇wL(˜x, w∗) − ∑ x∈X ∇wL(x, w∗)   , (8) where η is the unlearning rate. This update shifts the model parameters to minimize the loss on ˜x while removing the in- formation in x. It differs from gradient descent by moving the model based on the gradient difference between origi- nal and perturbed data. The gradients can be computed in O(p), with p being the number of parameters in the learning model (Warnecke et al. 2023). Second-Order Update The unlearning rate η can be elim- inated if L is twice differentiable and strictly convex. The influence of a single data point is approximated by: ∂w∗ ϕ,x→˜x ∂ϕ ∣ ∣ ∣ ∣ϕ=0 = −H −1 w∗ (∇wL(˜x, w∗) − ∇wL(x, w∗)) , (9) where H −1 w∗ is the inverse Hessian of the loss at w∗. This leads to a linear approximation, as given by w∗ x→˜x ≈ w∗ − H −1 w∗ (∇wL(˜x, w∗) − ∇wL(x, w∗)) . (10) Extending this to multiple data points gives the second-order update: ∆(X, ˜X) = −H −1 w∗   ∑ ˜x∈ ˜X ∇wL(˜x, w∗) − ∑ x∈X ∇wL(x, w∗)   . (11) This update does not require parameter calibration, as it derives from the inverse Hessian of the loss function. The second-order update is preferred for unlearning in models with strongly convex and twice differentiable loss functions, and can be easily calculated with common machine learning frameworks. Experiment We verify the effectiveness of our proposed method on the two manually corrupted datasets: CelebA-HQ dataset (Lee et al. 2020) and CelebA dataset (Liu et al. 2015). CelebA- HQ contains 30k high-resolution celebrity images with 40 attributes. We use this CelebA-HQ dataset to train our dif- fusion model. CelebA has 202,599 images with 40 same at- tributes as CelebA-HQ. We use this dataset to test our pro- posed method. Comparison Methods We compare our method against several SOTA methods. The image-manipulated methods involve processing the in- put images (224x224 pixels) using different techniques: (i) Random central removal of 100x100 pixels, which in- volves randomly removing a central portion of the image; (ii) Whole image mosaic, which applies a mosaic filter to the entire image, blurring detailed features; and (iii) our proposed diffusion-based method, which leverages diffusion processes to selectively alter specific attributes in the im- ages. For fine-tuning methods, we implement three unlearn- ing approaches: (i) Retraining, which involves training the model from scratch on a modified dataset; (ii) First-order unlearning, which adjusts the model parameters using gra- dient descent based on the attribute-specific loss; and (iii) Second-order unlearning, our proposed method, which em- ploys both gradient and curvature information to more effec- tively remove specific attributes from the model. Evaluation Metrics In our experiments, conducted on the CelebA dataset, we employed several evaluation metrics to assess the perfor- mance of attribute modification and image realism. Specif- ically, we focused on two primary metrics: Attribute Clas- sification Accuracy (ACC) and Structural Similarity Index (SSIM). For the assessment of attribute modification, we utilized the ACC. In this approach, we trained a linear binary classi- fier on the training set for each of the 40 attributes. This al- lowed us to evaluate the accuracy with which each attribute was correctly modified or maintained. The effectiveness of the attribute modifications across the entire image was fur- ther quantified by calculating the average ACC across all 40 attributes. This average ACC provides a comprehensive measure of the system’s performance in handling multiple attributes simultaneously, ensuring that modifications are not only accurate but also consistent across the dataset. In addition to ACC, we also assessed the realism of the generated images using the SSIM. Following the method- ologies presented in (Hinojosa, Niebles, and Arguello 2021; Liu et al. 2021b), SSIM was chosen due to its ability to mea- sure the perceptual quality of images. SSIM evaluates the similarity between two images based on luminance, contrast, and structural information, producing a similarity score that ranges from -1 to 1. Higher SSIM values indicate a greater degree of similarity between the images, reflecting the ex- tent to which the modified images retain their visual fidelity and align with human visual perception. Conversely, lower SSIM values suggest a decline in image quality or realism. Together, these metrics provide a robust framework for evaluating both the technical accuracy of attribute modifi- cations and the visual quality of the resulting images. The combination of ACC and SSIM ensures that our approach is not only effective in modifying attributes but also capable of generating realistic and visually pleasing images, which is critical for applications that require high levels of visual authenticity. Experimental Results Table 1 provides an extensive comparison of various image manipulation methods and their impact on attribute classi- fication performance under different levels of data change (i.e., 1%, 3%, and 5%). The methods compared include ‘Mo- saic’, ‘Random’, and ‘Ours’, with each method’s perfor- mance evaluated across multiple attributes such as Young, Arched, Bald, Bangs, and Blurry. Table 1 presents the results for attributes categorized un- der different percentages of data changes. Notably, in the case of 1% data change, our method achieves the highest average accuracy of 89.82% across 40 attributes, outper- forming both the Mosaic’ (82.38%) and ‘Random’ (84.32%) methods. Our method also shows strong performance in spe- cific attributes, such as Attract and Bald, indicating its effectiveness in handling minimal data changes. As the data change increases to 3% and 5%, the ‘Ours’ method consistently demonstrates superior performance, maintain- ing high accuracy in most attributes. For example, under 3% data change, our method reaches an accuracy of 89.07%, compared to 88.96% and 89.28% for ‘Mosaic’ and ‘Ran- dom’, respectively. This trend continues under the 5% data change scenario, where our method achieves an accuracy of Table 1: Comparison of different image manipulation methods and their impact on attribute classification performance. Change Methods Average Young Attract Bald Bangs Blurry Clean 89.75 85.81 79.58 98.59 94.78 95.73 1% Mosaic 89.06(-0.69) 82.38(-3.43) 74.1(-5.48) 98.53(-0.06) 94.6(-0.18) 95.79(+0.06) Random 89.31(-0.44) 84.32(-1.49) 76.04(-3.54) 98.52(-0.07) 94.55(-0.23) 95.9(+0.17) Ours 89.82(+0.07) 84.13(-1.68) 77.8(-1.78) 98.59(0.00) 94.72(-0.06) 95.78(+0.05) 3% Mosaic 88.96(-0.79) 82.94(-2.87) 74.84(-4.74) 98.57(-0.02) 94.56 95.67(-0.06) Random 89.28(-0.47) 84.16(-1.65) 75.56(-4.02) 98.58(-0.01) 94.56(-0.22) 95.6(-0.13) Ours 89.07(-0.68) 83.53(-2.28) 77.6(-1.98) 98.59(0.00) 94.56(-0.22) 95.66(-0.07) 5% Mosaic 88.96(-0.79) 79.94(-5.87) 78.84(-0.74) 98.57(-0.02) 94.56(-0.22) 95.67(-0.06) Random 88.91(-0.84) 82.87(-2.94) 71.07(-8.51) 98.57(-0.02) 81.97(-12.81) 95.42(-0.31) Ours 89.05(-0.7) 82.95(-2.86) 79.63(+0.05) 98.4(-0.19) 94.09(-0.69) 95.42(-0.31) 89.05%, again outperforming Mosaic (88.96%) and Random (88.91%). Overall, the results indicate that our method is particu- larly robust and adaptable, offering high accuracy even as the data change percentage increases. This consistent per- formance across different attributes and levels of data ma- nipulation highlights the method’s potential for robust at- tribute classification in dynamic datasets. Such reliability is crucial for real-world applications where data may vary sig- nificantly, and accurate attribute classification is essential. Our proposed method’s superior results in most categories, especially under higher data changes, highlight its effective- ness and utility in improving attribute classification perfor- mance. Table 2 provides a detailed comparison of various un- learning methods and their impact on attribute classifica- tion performance under different levels of data change (3%, 5%, and 10%). The methods evaluated include ‘Retrain- ing’, ‘First order’, and ‘Second order’, with their perfor- mance assessed across multiple attributes such as Young, Arched, Bald, Bangs, and Blurry. The differences from the ‘clean’ baseline are highlighted, with all selected attributes having an initial accuracy of less than 90%. In the clean data scenario, the ‘Retraining’ method method achieves the highest average accuracy of 89.75% across 40 attributes, followed closely by ‘First order’ (89.03%) and ‘Second order’ (89.01%). As the data change increases to 3%, 5%, and 10%, the ‘First order’ and ‘Sec- ond order’ methods exhibit competitive performance, often surpassing ‘Retraining’ in specific attributes. For instance, under a 3% data change, ‘Second order’ attains notable accuracies in attributes like Bald (98.29%) and Bangs (94.00%), with values close to those of the clean scenario, demonstrating robustness against minor data alterations. As the data change reaches 5% and 10%, the ‘First order’ and ‘Second order’ methods continue to demonstrate ro- bust performance. Specifically, at a 10% data change, ‘Second order’ maintains an average accuracy of 88.99%, excelling in challenging attributes like Bald (98.04%) and Bangs (91.15%). The ‘First order’ method, although slightly lower in accuracy, still achieves a respectable aver- age accuracy of 88.64%, showing reasonable adaptability in high data change scenarios. The method’s ability to maintain high accuracy under significant data changes underscores its robustness and effectiveness. Overall, these results demonstrate the efficacy of the ‘First order’ and ‘Second order’ unlearning methods in maintaining high attribute classification performance across varying levels of data change. The ‘Second order’ method, in particular, shows superior adaptability and accuracy in more challenging scenarios, suggesting its potential as a re- liable unlearning technique for robust attribute classification in dynamic and diverse datasets. These insights are valuable for selecting appropriate unlearning methods to enhance at- tribute classification performance in practical applications. Table 3 compares the performance of various image gen- eration models on the CelebA-HQ dataset, measured by the SSIM metric. Diff-AE leads with a near-perfect SSIM of 0.991, indicating exceptional retention of image details. Our method, with an SSIM of 0.951, ranks second, showcasing its strong capability to preserve image realism, highlight- ing its competitiveness and effectiveness in generating high- quality images after attribute modifications. Table 4 summarizes the training times for different un- learning methods. The ‘Clean’ method takes 13,322 sec- onds, while the ‘Retraining’ method, which likely involves comprehensive retraining, required the most time at 15,185 seconds. In contrast, the ‘First order’ and ‘Second order’ methods drastically reduce the training times to 174.41 sec- onds and 506.99 seconds, respectively. Notably, the ‘Sec- ond order’ method, while not as fast as ‘First order’, still offers a substantial reduction in time compared to ‘Clean’ and ‘Retraining’. This additional computational time for the ‘Second order’ method suggests it may involve a more de- tailed unlearning process, potentially leading to better ac- curacy, as indicated in Table 2. This efficiency, coupled with the possibility of improved accuracy, makes the ‘Sec- ond order’ method particularly valuable for applications where balancing computational resources and maintaining high performance is critical. The sharp decrease in training times with both ‘First order’ and ‘Second order’ methods demonstrates their potential utility in scenarios necessitat- ing quick updates or modifications, while the slight increase in computational time for the ‘Second order’ method could indicate a trade-off that favors accuracy without significantly compromising efficiency. Table 2: Comparison of different unlearning methods and their impact on attribute classification performance. Change Methods Average Young Attract Bald Bangs Blurry Clean Retraining 89.75 85.81 79.58 98.59 94.78 95.73 First order 89.03(-0.72) 78.07(-7.74) 67.67(-11.91) 98.31(-0.28) 91.13(-3.65) 95.03(-0.70) Second order 89.01(-0.74) 79.65(-6.16) 72.65(-6.93) 98.49(-0.10) 91.44(-3.34) 95.07(-0.66) 3% Retraining 89.07(-0.68) 83.35(-2.46) 76.98(-2.60) 98.46(-0.13) 94.47(-0.31) 95.60(-0.13) First order 89.00(-0.75) 82.80(-3.01) 60.62(-18.96) 98.07(-0.52) 92.53(-2.25) 95.14(-0.59) Second order 89.01(-0.74) 83.26(-2.55) 68.01(-11.57) 98.29(-0.30) 94.00(-0.78) 95.16(-0.57) 5% Retraining 89.01(-0.74) 83.91(-1.90) 76.96(-2.62) 98.37(-0.22) 93.94(-0.84) 95.62(-0.11) First order 88.66(-1.09) 81.18(-4.63) 67.93(-11.65) 98.01(-0.58) 92.01(-2.77) 95.1(-0.63) Second order 89.02(-0.73) 81.27(-4.54) 68.18(-11.40) 98.23(-0.36) 93.00(-1.78) 95.15(-0.58) 10% Retraining 89.05(-0.70) 82.95(-2.86) 73.98(-5.60) 98.40(-0.19) 94.09(-0.69) 95.42(-0.31) First order 88.64(-1.11) 79.10(-6.71) 61.10(-18.48) 98.03(-0.56) 91.19(-3.59) 95.05(-0.68) Second order 88.99(-0.76) 79.54(-6.27) 61.49(-18.09) 98.04(-0.55) 91.15(-3.63) 95.13(-0.60) Table 3: Image generation quality on CelebA-HQ (Lee et al. 2020). Model SSIM ↑ StyleGAN2 (W) (Karras et al. 2020) 0.677 StyleGAN2 (W+) (Karras et al. 2020) 0.827 VQ-GAN (Esser et al. 2021) 0.782 DDIM (T=100, 128 2) (Song, Meng, and Ermon 2020) 0.917 Diff-AE (T=100, 128 2) (Preechakul et al. 2022) 0.991 Ours (T=100, 128 2) 0.951 Table 4: Training times for different methods. Methods Training Time Clean 13322s Retraining 15185s First order 174.41s Second order 506.99s Conclusion This paper introduces a privacy protection framework for image data, designed to address challenges in both data sharing and model publication. By integrating a diffusion model for attribute modification with machine unlearning algorithms to secure model parameters, our approach suc- cessfully balances the dual objectives of privacy protection and model performance. The framework’s user-interactive design enables customized privacy settings, ensuring that high-quality image generation and precise attribute modi- fications are maintained. Our results demonstrate that this framework not only preserves image realism but also en- hances accuracy across facial datasets, representing a signif- icant advancement in privacy-preserving machine learning. References Abadi, M.; Chu, A.; Goodfellow, I.; McMahan, H. B.; Mironov, I.; Talwar, K.; and Zhang, L. 2016. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, 308–318. Bourtoule, L.; Chandrasekaran, V.; Choquette-Choo, C. A.; Jia, H.; Travers, A.; Zhang, B.; Lie, D.; and Papernot, N. 2021. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP), 141–159. IEEE. Cao, Y.; and Yang, J. 2015. Towards making systems forget with machine unlearning. In 2015 IEEE Symposium on Security and Privacy, 463–480. IEEE. Corporation, M. 2023. Microsoft Azure Face API. Web page. https: //azure.microsoft.com/en-us/services/cognitive-services/face/. Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248–255. Ieee. Deng, J.; Guo, J.; Xue, N.; and Zafeiriou, S. 2019. Arcface: Addi- tive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recog- nition, 4690–4699. Eidinger, E.; Enbar, R.; and Hassner, T. 2014. Age and gender estimation of unfiltered faces. IEEE Transactions on information forensics and security 9(12): 2170–2179. Esser, P.; Rombach, R.; Blattmann, A.; and Ommer, B. 2021. Im- agebart: Bidirectional context with multinomial diffusion for au- toregressive image synthesis. Advances in neural information pro- cessing systems 34: 3518–3532. European Parliament; and Council of the European Union. ???? Regulation (EU) 2016/679 of the European Parliament and of the Council. URL https://data.europa.eu/eli/reg/2016/679/oj. FaceApp.com. 2020. FaceAPP. URL https://www.faceapp.com/. Last accessed: Nov. 16, 2020. Fan, L. 2018. Image Pixelization with Differential Privacy. In 32th IFIP Annual Conference on DBSec, 148–162. Springer Inter- national Publishing. Fan, L. 2019. Practical image obfuscation with provable privacy. In ICME, 784–789. IEEE. Gao, B.-B.; Zhou, H.-Y.; Wu, J.; and Geng, X. 2018. Age Estima- tion Using Expectation of Label Distribution Learning. In IJCAI, 712–718. Ginart, A.; Guan, M.; Valiant, G.; and Zou, J. Y. 2019. Making AI forget you: Data deletion in machine learning. Advances in neural information processing systems 32. Golatkar, A.; Achille, A.; and Soatto, S. 2020. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9304–9312. Guo, C.; Goldstein, T.; Hannun, A.; and Van Der Maaten, L. 2020. Certified data removal from machine learning models. In Proceed- ings of the 37th International Conference on Machine Learning, 3832–3842. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual Learn- ing for Image Recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Hinojosa, C.; Niebles, J. C.; and Arguello, H. 2021. Learning privacy-preserving optics for human pose estimation. In Proceed- ings of the IEEE/CVF international conference on computer vision, 2573–2582. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion proba- bilistic models. NeurIPS 33: 6840–6851. Karras, T.; Laine, S.; and Aila, T. 2019. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recogni- tion, 4401–4410. Karras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.; and Aila, T. 2020. Analyzing and improving the image quality of style- gan. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, 8110–8119. Kollias, D.; and Zafeiriou, S. 2019. Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace. In 30th British Machine Vision Conference 2019, BMVC 2019, Cardiff, UK, September 9-12, 2019, 297. BMVA Press. Kuprashevich, M.; and Tolstykh, I. 2023. MiVOLO: Multi-input Transformer for Age and Gender Estimation . Lee, C.-H.; Liu, Z.; Wu, L.; and Luo, P. 2020. MaskGAN: To- wards Diverse and Interactive Facial Image Manipulation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Li, Q.; Wang, J.; Yao, Z.; Li, Y.; Yang, P.; Yan, J.; Wang, C.; and Pu, S. 2022. Unimodal-concentrated loss: Fully adaptive label dis- tribution learning for ordinal regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni- tion, 20513–20522. Li, T.; and Clifton, C. ???? Poster: Differentially Private Imag- ing. In The 42nd IEEE Symposium on Security and Privacy (S&P), 2021. Liu, B.; Ding, M.; Shaham, S.; Rahayu, W.; Farokhi, F.; and Lin, Z. 2021a. When machine learning meets privacy: A survey and outlook. ACM Computing Surveys (CSUR) 54(2): 1–36. Liu, B.; Ding, M.; Xue, H.; Zhu, T.; Ye, D.; Song, L.; and Zhou, W. 2021b. Dp-image: differential privacy for image data in feature space. arXiv preprint arXiv:2103.07073 . Liu, Z.; Luo, P.; Wang, X.; and Tang, X. 2015. Deep Learning Face Attributes in the Wild. In Proceedings of International Conference on Computer Vision (ICCV). Lucic, M.; Kurach, K.; Michalski, M.; Gelly, S.; and Bousquet, O. 2018. Are gans created equal? a large-scale study. NeurIPS 31. McPherson, R.; Shokri, R.; and Shmatikov, V. 2016. De- feating image obfuscation with deep learning. arXiv preprint arXiv:1609.00408 . McSherry, F.; and Talwar, K. 2007. Mechanism design via differ- ential privacy. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), 94–103. IEEE. Mironov, I. 2017. R´enyi differential privacy. In 2017 IEEE 30th computer security foundations symposium (CSF), 263–275. IEEE. No, C. 1988. Privacy Act 1988 . Pan, H.; Han, H.; Shan, S.; and Chen, X. 2018. Mean-variance loss for deep age estimation from a face. In Proceedings of the IEEE conference on computer vision and pattern recognition, 5285– 5294. Preechakul, K.; Chatthee, N.; Wizadwongsa, S.; and Suwa- janakorn, S. 2022. Diffusion autoencoders: Toward a meaningful and decodable representation. In CVPR, 10619–10629. Ren, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-cnn: To- wards real-time object detection with region proposal networks. Advances in neural information processing systems 28. Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Convolu- tional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, 234–241. Springer. Savchenko, A. 2023. Facial Expression Recognition with Adaptive Frame Rate based on Multiple Testing Correction. In Krause, A.; Brunskill, E.; Cho, K.; Engelhardt, B.; Sabato, S.; and Scarlett, J., eds., Proceedings of the 40th International Conference on Machine Learning (ICML), volume 202 of Proceedings of Machine Learn- ing Research, 30119–30129. PMLR. URL https://proceedings.mlr. press/v202/savchenko23a.html. Schroff, F.; Kalenichenko, D.; and Philbin, J. 2015. Facenet: A uni- fied embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, 815–823. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising Diffusion Im- plicit Models. In International Conference on Learning Represen- tations. Times, F. 2018. Facebook privacy breach. Retrieved January 6: 2022. Warnecke, A.; Pirch, L.; Wressnegger, C.; and Rieck, K. 2023. Ma- chine Unlearning of Features and Labels. In 30th Annual Network and Distributed System Security Symposium, NDSS 2023. The In- ternet Society. Wen, Y.; Liu, B.; Ding, M.; Xie, R.; and Song, L. 2022. Identi- tydp: Differential private identification protection for face images. Neurocomputing 501: 197–211. Wu, Y.; Dobriban, E.; and Davidson, S. 2020. Deltagrad: Rapid retraining of machine learning models. In International Conference on Machine Learning, 10355–10366. PMLR. Zhang, Z.; An, L.; Cui, Z.; Xu, A.; Dong, T.; Jiang, Y.; Shi, J.; Liu, X.; Sun, X.; and Wang, M. 2023. ABAW5 Challenge: A Facial Affect Recognition Approach Utilizing Transformer Encoder and Audiovisual Fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 5724–5733.","libVersion":"0.3.2","langs":""}