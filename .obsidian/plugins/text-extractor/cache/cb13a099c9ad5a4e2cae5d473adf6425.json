{"path":"GenAIUnleaning/DiffusionUnlearning/2024/All but One Surgical C.pdf","text":"All but One: Surgical Concept Erasing with Model Preservation in Text-to-Image Diffusion Models Seunghoo Hong 1*, Juhun Lee 1*, Simon S. Woo 1 1Department of Artificial Intelligence, Sungkyunkwan University, S. Korea hoo0681@g.skku.edu, josejhlee@g.skku.edu, swoo@g.skku.edu DDIM INVERSION Van Gogh Style Dry brushing = Image of pig Image of cow = Desktop Laptop = Style Object Pretrained Fine-tuned Concept a) Concept Erasing b) Concept Purification Figure 1: Our method erases a concept while leaving the model intact. To enable more controllability in the erasing procedure, the user may introduce alternative concepts. Also, it fixes many current issues such as spatial inconsistency, model degradation, and training inefficiency that have been the problems in many previous approaches. Interestingly, our model can even “purify” and censor concepts with DDIM inversion, which is currently hardly reproducible through other methods to practically erase nudity from real-world data. Abstract Text-to-Image models such as Stable Diffusion have shown impressive image generation synthesis, thanks to the utiliza- tion of large-scale datasets. However, these datasets may con- tain sexually explicit, copyrighted, or undesirable content, which allows the model to directly generate them. Given that retraining these large models on individual concept deletion requests is infeasible, fine-tuning algorithms have been de- veloped to tackle concept erasing in diffusion models. While these algorithms yield good concept erasure, they all present one of the following issues: 1) the corrupted feature space yields synthesis of disintegrated objects, 2) the initially syn- thesized content undergoes a divergence in both spatial struc- ture and semantics in the generated images, and 3) sub- optimal training updates heighten the model’s susceptibility to utility harm. These issues severely degrade the original utility of generative models. In this work, we present a new approach that solves all of these challenges. We take inspira- tion from the concept of classifier guidance and propose a sur- gical update on the classifier guidance term while constrain- ing the drift of the unconditional score term. Furthermore, our algorithm empowers the user to select an alternative to the erasing concept, allowing for more controllability. Our experimental results show that our algorithm not only erases the target concept effectively but also preserves the model’s generation capability. Introduction Recently, large-scale text-to-image models have demon- strated a remarkable ability to synthesize photo-realistic im- ages (Rombach et al. 2022; Saharia et al. 2022; Ramesh et al. 2021). This rise in generative models was elicited by the joint advancement of algorithms, computing resources, and the curation of large-scale datasets such as LAION (Schuh- mann et al. 2022). While these datasets offer rich features for training large-scale models (Brown et al. 2020; Dosovitskiy et al. 2020), many of them are curated with web-scraped ma- terial and, thus, lack the necessary preprocessing regarding safety, privacy, and bias (Mehrabi et al. 2021). Moreover, such datasets often contain sexually explicit content, copy- righted material, and personal images. Training generative models using these sensitive data means that the model’s generative capability is derived from these same images, and the model is capable of generating such inappropriate con- tent partially or entirely.arXiv:2312.12807v1 [cs.CV] 20 Dec 2023 To make things worse, due to the stochastic property and the capability to model complex distributions, there is al- ways a non-zero likelihood that the generated image will contain unsafe content even when conditioned on any un- related text token. This limits the usability of these genera- tive models in public settings. To alleviate this problem, re- searchers have inserted an NSFW safe-checking neural net- work (von Platen et al. 2022). Still, alongside a high false positive rate, their near-unpredictable masking rate of im- ages limits their applicational range, especially when the ap- plication relies on a continuous stream of data (Rando et al. 2022). Given these complications, both computation-wise and performance-wise, of retraining these “foundational” models with a heavily curated dataset, researchers have pro- posed to directly fine-tune foundation models such as Stable Diffusion to erase target concepts (Gandikota et al. 2023; Kumari et al. 2023; Zhang et al. 2023). While such fine-tuning algorithms for concept ablation are efficient in erasing itself, they significantly sacrifice much of the original generative power of the model to do so. This is far from the original motivation of this line of research. Through a closer examination of the utility aspect of these models, we identify three issues with the current fine-tuning algorithms: 1) Due to the corruption in the fea- ture space of the model, generated images prompted with any arbitrary concepts become unrecognizable or very dif- ferent from their original concept (see Fig. 2), 2) Generative models such as diffusion models rely on random seeds to output images. As a consequence of fine-tuning the model, the spatial structure and the semantics of the image from the same random seed change. If we regard the model before erasing as the oracle, then any unintended deviation in the output image is not aligned with the ultimate utility of ablat- ing concepts in the model, and 3) despite the model display- ing adequate erasing capabilities, certain methods demand a high number of iterations, thereby subjecting the model to increased utility harm. Recent algorithms (Gandikota et al. 2023; Kim et al. 2023) for erasing recommend around 1,000 update steps, which increases the exposure to the issues mentioned above. In this paper, we aim to address all of the aforementioned challenges. Our main motivation comes from the hypothe- sis that the task of erasing a concept while preserving the rest requires a surgical intervention, where we modify the concept of interest no more than needed. To achieve this, we first inherit from the idea of classifier guidance(Ho and Sal- imans 2022) to decompose the intermediate latent into the unconditional score and the guidance score term and solely apply updates to the latter term. In this region of update, we modify the target concept by introducing supervised and un- supervised erasing guidance, which shows that updating the guidance score is agnostic to the method of erasing guid- ance supervision method. Moreover, deriving from the La- grangian Multiplier method, we introduce a regularization on the unconditional score term so that it does not interfere with the update of the guidance score distribution. Our main contributions are summarized as follows: • We examine the possible societal and harmful effects of the latest generative models and approaches to mitigate issues via concept erasing, especially focusing on sexu- ally explicit content. We identify that current SoTA algo- rithms do not consider model utility enough when erasing a concept, and most of them fall short of being used for practice. • We formulate a fine-tuning algorithm that modifies the core of the target concept while keeping the model in- tact. Our approach naturally gives rise to a regularization term, where we effectively and safely control the trade- off between erasing strength and model preservation. • Through extensive experiments, we demonstrate that our surgical approach improves on spatial and semantic con- sistency, and training efficiency over the current base- lines with FID, KID, CLIP, and SSIM scores. Background We first describe the essential components used in this line of research before explaining relevant research works. Diffusion Models Diffusion models are a class of generative models that learn to reverse the Markov chain diffusion process. Let x0 rep- resent true data observations and xt represent intermediate noised data, when t = T , corresponding observations xT are noised to become Gaussian noise. More precisely, diffu- sion process (Ho, Jain, and Abbeel 2020; Song, Meng, and Ermon 2020) is defined as q (xt | xt−1) := N (xt; √αtxt−1, (1 − αt) I) q(xT |x0) ≈ N (xT ; 0, I), (1) where αt is a fixed (Ho, Jain, and Abbeel 2020) or learnable schedule (Sohl-Dickstein et al. 2015). According to Bayes’ rule, we can obtain the reverse diffusion, which can be inter- preted as an interpolation between xt and x0. Then, we can learn to predict this distribution by matching it with a pa- rameterized network and minimizing the KL divergence of the two distributions. The divergence of two Gaussian distri- butions can be formulated as the mean square error loss. In practice, we reparameterize xt so that we predict the epsilon ϵt (Ho, Jain, and Abbeel 2020) that was used to generate xt as follows: Ldiffusion = Ext,t,ϵ∼N (0,1) [ ∥ϵ − ϵθ (xt, t)∥2 2] (2) Text-to-Image Diffusion Models By diffusing in the latent space of powerful VAEs (Oord, Vinyals, and Kavukcuoglu 2017; Razavi, Van den Oord, and Vinyals 2019) and conditioning these models with text em- beddings (Ramesh et al. 2021), they take the form of La- tent Diffusion Models (LDM) (Rombach et al. 2022; Saharia et al. 2022) or commonly known as “text-to-image diffu- sion models”. With the addition of these two components, the loss can be formulated as follows: LLDM = Ezt∈E(x),t,c,ϵ∼N (0,1) [∥ϵ − ϵθ (zt, c, t)∥ 2 2] , (3) where zt is the noised latent embedding of image x through a VAE, and c is the text embedding encoded by text encoders such as CLIP (Radford et al. 2021). ESD SDD Ours Step 50 Step 100 Recommended Step 50 Step 100 Recommended Figure 2: Image erasing timeline: ESD and SDD’s images are from iterations 50, 100, and 1000. For our model, we sample from 50, 100, and 400, twice as many steps as we have recommended for the sake of comparison. Classifier guidance and Classifier-free guidance It is well known that score −σt∇zt log p(zt) and epsilon ϵθ(zt) are equivalent. Then, given that pθ(zt|c)pθ(c|zt) γ ∝ pθ(zt)pθ(c|zt)γ+1 (Ho and Salimans 2022; Song et al. 2020; Dhariwal and Nichol 2021), we can formulate classifier guidance as follows: ˜ϵθ (zt|c) = ϵθ (zt) − (γ + 1)σt∇zt log pθ (c | zt) ≈ −σt∇zt [log p (zt) + (γ + 1) log pθ (c | zt)] = −σt∇zt [log p (zt | c) + γ log pθ (c | zt)] . (4) With classifier-free guidance (CFG) (Ho and Salimans 2022), one can obtain ∇zt log p(c | zt) by composing the scores ϵθ(zt) and ϵθ(zt, c) as follows ∇zt log p(c | zt) = − 1 σt [ϵθ(zt, c) − ϵθ(zt)]. (5) Ultimately, we can sample an epsilon with guidance scale, γ, as follows: ˜ϵθ(zt, c) = (1 + γ)ϵθ(zt, c) − γϵθ(zt). (6) Related Work One of the early works in erasing fine-tuning is by Gandikota et al. (2023). Their work presents Erased Stable Diffusion (ESD), which updates the student network by mapping its output conditioned on the erasing concept ϵθ (zt, cs, t) to the output epsilon conditioned on the erasing concept to the epsilon with negative guidance ˜ϵθ⋆ (zt, cs, t) from the fixed teacher network. While it delivers substantial erasing capa- bility, it has the tendency to map the erasing concepts to completely non-related concepts and break the spatial and semantic consistency of non-related concepts. To address these issues, Safe self-Distillation Diffusion (SDD) (Kim et al. 2023) hypothesizes that the training in- stability of ESD is due to the dependency on the CFG term. Their goal is to map the erasing concept to the null (a.k.a. unconditional) concept directly, without introducing CFG in the supervision signal. Additionally, it incorporates self- distillation, where the teacher is the exponential moving average of the student (Zhang et al. 2019). Despite their impressive erasing results, they both present semantic and spatial corruption, and shifts in the spatial structure before achieving good erasing, as shown in Fig. 2. In Ablation (Ku- mari et al. 2023), the erasing concept is mapped to a broader ”anchor” concept. While their loss is effective, the effect of it leaks to nearby concepts, similar to Dreambooth (Ruiz et al. 2023). Likewise, they also use the Class-Specific Prior Preservation Loss to regularize the language drift (Lee, Cho, and Kiela 2019; Lu et al. 2020) due to the optimization. In Forget-Me-Not (Zhang et al. 2023), they use the cross- attention layer to erase concepts and directly apply a loss function on those layers. Formally, they penalize the model on the activation of the attention map for the erasing con- cept token. However, this type of direct manipulation of the internal activations can be detrimental to the model’s repre- sentation. Our Approach Erasing Signal Notation. We first define the notations used in our work for concept erasing. First, let c and c ′ be the target erased concept and replacing concept, respectively, containing ctext, tclow , tchigh; And, γ is the guidance scale; P· and ˆP·,γ are the distributions of z; ∅ represents the unconditional concept. θ and θ⋆ are the parameters to be optimized and the teacher’s parameters; λ, T, xt, zt are the penalty loss’ weight, maximum t, noised images in pixel space, and latent space respectively; zT ∼ N (0, I). ϵθ and sθ are parameter- ized networks that predict ϵ and the score. For readability, P (zt|∅) is expressed as P (zt) and ϵθ(zt, ∅, t) as ϵθ(zt). Revisiting the concept of classifier guidance, we utilize the following equation: ∇ log ˆP (zt|c) = ∇ log P (zt|∅) + γ∇ log P (c|zt), (7) where γ∇ log P (c|zt) is the adversarial gradient (Santurkar et al. 2019) that steers zt to class c. Now, if all we want is to update the meaning of our target condition, then the update of ∇ log P (c|zt) would suffice. In this respect, our loss revolves around this second term as follows: θ⋆ = arg min θ [∥γ1∇ log P (c′|zt) − γ2∇ log P (c|zt)∥ 2 2]. (8) Moreover, CFG showed that the expression ∇ log P (c|zt) can be decomposed as follows: ∇ log P (c|zt) = ∇ log P (zt|c) − ∇ log P (zt|∅). Intu- itively, this suggests composability (Du, Li, and Mordatch 2020) that takes the unconditional score ∇ log P (zt|∅) to the direction of the class guidance term ∇ log P (zt|c). However, updating ∇ log P (zt|c) alone without consider- ing the changes in ∇ log P (zt) may harm the overall util- ity of the model. This might be the case because, while ∇ log Pθ(zt|c) and ∇ log Pθ(zt) are modeled to have funda- mentally different properties, they are jointly parameterized by θ, and the change of one can affect the other and vice- versa. If we consider ∇ log ˆP (zt|c) in Eq. (7), the change in ∇ log P (zt) can build up on top of ∇ log P (c|zt) and act as an unprovisioned concept. Therefore, the distribution of U-Net U-Net U-Net (a) Our approach (b) Sampling U-Net reverse diffusion Figure 3: Our method revolves around decomposing the conditional score and updating only one of its term ∇ log P (c|zt). Additionally, we incorporate δ into our algorithm, which will both steer our sampling zt (Kwon, Jeong, and Uh 2022) and be matched by our training model. ∇ log P (zt) must remain unchanged to preserve the utility of the model. In this respect, the minimization objective of our work is: min θ Ez,t[∥γ1∇ log Pθ⋆ (c′|zt) − γ2∇ log Pθ(c|zt)∥ 2 2] s. t.∇ log Pθ⋆ (zt) − ∇ log Pθ(zt) = 0, ∀zt, t = 1, . . . , T, (9) where c ∈ C, c ′ ∈ C′ . This type of constraint optimization problem is commonly solvable using the Lagrangian Multi- plier. Here, we relax the constraints and optimize Eq. (10) in the following way: min θ Ec,c′,z,t[∥γ1∇ log Pθ⋆ (c ′|zt) − γ2∇ log Pθ(c|zt)∥ ︸ ︷︷ ︸ concept loss term ] +λ (∥∇ log Pθ⋆ (zt|∅) − ∇ log Pθ(zt|∅)∥ − ϵ) ︸ ︷︷ ︸ penalty term , (10) where λ ≥ 0, ϵ = 0, and when λ = 1, our loss is equivalent to minimizing the upper bound of ||LU + LC||2 : Ezt∼Pθ⋆ (zt|c′)[DKL(Pθ⋆ (zt−1|zt, c ′)∥Pθ(zt−1|zt, c))]. (11) In order to avoid the loss being attributed to ∇ log Pθ(zt|∅), we do not propagate any gradients through it. To do so, a stop gradient is applied to the ϵθ (zt) . sg() term. This will prevent the feedback on c from flowing directly to the un- conditional term. Ultimately, any feedback on the uncondi- tional is expected to be controlled through the penalty term. In the end, our loss formula is: Lmodel = Ezt∼Pθ⋆ (zt|c′),c,c′,t[Lconcept + λLpenalty] Lconcept(c, c ′, zt, γ1, γ2) = ∥γ2 (ϵθ (zt, c) − ϵθ (zt) . sg()) −γ1 (ϵθ⋆ (zt, c ′) − ϵθ⋆ (zt, )) ∥ 2 2 Lpenalty(t, zt) = ∥ϵθ(zt, ) − ϵθ⋆ (zt, )∥ 2 2 (12) Search for δ. Let δ be the residual concept for which it transports the original concept c to the alternate concept Algorithm 1: Our training algorithm Input: Target concept set C, instruction concept list CI, model weight θ, text encoder E, number of iteration N , num- ber of sampling step T , sampler P, penalty coefficient λ. Output: 1: θ⋆ ← θ, Cs ← E(C) 2: while N ̸= 0 do 3: t ∼ U({1, . . . , T }), cs ∼ Cs, τ ← T , xT ∼ N (0, I) 4: repeat 5: ˆϵ ← ϵθ⋆ (xτ , ∅, τ ) 6: ˆϵ ← ˆϵ + γ1(ϵθ⋆ (xτ , cs, τ ) − ϵθ⋆ (xτ , ∅, τ )) 7: ˆϵ ← ˆϵ + δ(CI, xτ , θ⋆) 8: xτ −1 ← P(xτ , ˆϵ, τ ) 9: τ ← τ − 1 10: until τ = t 11: Lconcept = ∥γ2(ϵθ (xt, c) − ϵθ (xt, ∅) .sg) − (γ1(ϵθ⋆ (xt, c) − ϵθ⋆ (xt, ∅)) + δ(CI, xt, θ⋆))∥ 2 2 12: Lpenalty = ∥ϵθ(xt, ∅) − ϵ⋆ θ(xt, ∅)∥ 2 2 13: θ ← θ − η∇θ(Lconcept + λLpenalty) 14: N ← N − 1 15: end while 16: return θ c ′. Put simply, δ is the embodiment of the erasing signal needed to transform c to c′. The challenge is to obtain this erasing signal δ so that Pθ,ϕ(xt−1|xt, c ′) = N (µθ(xt) + γΣ∇ log Pϕ(c|xt), Σ) + δ. Here, we present two sampling methods for δ: implicit and explicit. Implicit Erasing Signal. These large-scale diffusion models have learned a rich prior with generalizing power. Hertz et al. (2022) shows that when the attention maps in the cross-attention layers are amplified or suppressed, the token’s concept manifestation varies proportionally. When these attention maps are suppressed, the model not only suppresses the erasing concept but also replaces it with other concepts, thanks to its learned prior. We utilize this internal representation of the model to suppress the attention maps of our erasing concept and map to its closest concept. Formally, we sample from xT to xt with Prompt-to-Prompt and suppress the respective attention maps of our erasing tokens. Then, we can obtain ∇ log Pϕ(c ′|xt), which incor- porates the “overwriting” concept. We append visual results of this implicit δ in the Supplementary to show its viability. Explicit Erasing Signal. In practical scenarios, the user may wish to map the erasing concept to an explicitly stated concept. If the sole goal is to overwrite one concept with an- other concept, we can match the score ∇ log Pϕ(c|xt) with ∇ log Pϕ(c′|xt). However, even within each concept, there exists a distribution of features/semantics. When we con- sider modifying a concept, matching the entire source dis- tribution of features to the target distribution is not what we seek. More specifically, we are only interested in the feature mode with the highest density. For example, when we want to replace “bubble guns” with “guns”, we do not want to inherit all of the contexts that the word “gun” carries (e.g. “war”, “violence”). Instead, we want to solely inherit the “gun” feature itself. Moreover, disruption of the orig- inal model will be proportional to the amount of supervi- sion signal we consider using. Now, to ensure that we are utilizing only the most representative feature from the pre- dicted epsilon, we take inspiration from Semantic Guidance (SEGA) (Brack et al. 2023). Formally, SEGA states that the representative semantic information is mainly contained in the highest and lowest pixel values in the predicted epsilon. In this respect, we bottleneck this signal by ablating the val- ues below a percentile as follows: δ(CI, zt, θ) = ∑ c′′∈CI gc′′ β(c ′′, zt)∆c(c′′, zt, θ), β(c, zt, θ) = {1 if 1Bc ⋂ Bw (c, t), |∆c| ≥ ηκ(|∆c|) 0 otherwise , ∆c(c, zt, θ) = − √1 − ¯α(∇ log Pθ(zt|c) − ∇ log Pθ(zt)), Bc = {t|t ∈ Z, 0 ≤ tchigh ≤ t ≤ tclow ≤ T }, Bw = {t|t ∈ Z, t ≥ twarmup}, where function ηκ(·) returns κ-th percentile of inputs, and gc is the guidance scale of concept c that is an elements of instruction concept CI. The function δc should take three arguments, but the notation is omitted at function β. Then, our Lconcept loss is updated as follows: Lconcept(c, zt, γ1, γ2, CI) =∥γ2(ϵθ(zt, c) − ϵθ (zt) . sg()) −γ1(ϵθ⋆ (zt, c)−ϵθ⋆ (zt)) + δ(CI, zt, θ⋆))∥ 2 2, (13) where CI is instruction concept set to make δ. While we attained desirable results with both implicit and explicit su- pervision, the Prompt-to-Prompt (Hertz et al. 2022) showed considerable sensitivity from the attention map reweighting hyperparameters, which detriments the quality of our sam- pling ϵptp t . Therefore, most of our experiments are based on the explicit method. The results of using implicit guidance are provided in Suppl. Material. Finally, we present our over- all diagram in Fig. 3. Experimental Results Experiment Settings Baselines. We compare the performance of our method with four different latest concept-erasing fine-tuning meth- ods: ESD, SDD, “Ablating” (Kumari et al. 2023), and Forget-Me-Not. Because of the applicability and the utility of a sexual-content censored model, our experiments are centered around erasing “nudity”. Nevertheless, we do show that our model can generalize beyond this concept by showing the erasure of concepts, styles, and objects in Fig. 1.a. All of our experiments are performed using the Stable Diffusion ver. 1.4. Training Setup. For all of our experiments on erasing “nudity”, our erasing concept is “nudity”, 200 steps of update, the optimizer is AdamW (a learning rate of 1e − 5, γ1 = γ2 = 7.5, adam ϵ = 1.0e − 8), we use the DDIM (η = 0.0) sampler with T = 35, where we run with GPU A5000, twarmup = 5, λ = 5. Evaluation Metrics. We emphasize that our focus is on im- proving the areas where previous models fall short in terms of the utility of these erased models. In this aspect, our per- formance evaluation takes into consideration the following aspects: 1) how much the model preserves the remaining concept without degradation, 2) the spatial consistency of the erased and the remaining concepts, 3) how well it erases the target concept, and 4) the training efficiency of a dif- ferent method. To quantify model preservation, we gener- ate images with MS-COCO captions and calculate the FID (Heusel et al. 2017), and KID (Bi´nkowski et al. 2018) be- tween the generated images and the actual COCO images. We also use these images to calculate the CLIP score (Hessel et al. 2021) between the image and the caption to evaluate Table 1: Evaluation metric for best “nudity” erased models. The highest and second-highest scores are printed in bold and underlined, respectively. We treat statistics from both COCO and SD v1.4 datasets as the oracle and attribute rank- ing among different methods. Method NudeNet(%)↓ FID↓ KID↓ CLIP Score↑ SSIM↑ SD v1.4 0.69 13.59 0.00479 0.2765 - ESD 0.04 14.27 0.00421 0.2619 0.231 SDD 0.05 14.11 0.00499 0.2677 0.309 Ablating 0.45 13.68 0.00478 0.2756 0.657 Forget-Me-Not 0.66 13.78 0.00496 0.2732 0.476 Ours 0.33 13.19 0.00447 0.2762 0.762 COCO 0.2693 0.2617 0.4063 0.15870.1723 SDv1.4 Reference Image “Nudity” Erasing 0.46105 Figure 4: Each model’s run end at their recommended iteration stop, and their nudity confidence and SSIM(25x25 window) is reported alongside. The images above share the same seed and prompt at the respective last iteration. While SSD and ESD show low nudity confidence, the seed and the prompt lose their original meaning. Also, due to the high false positives, the decision threshold was set to 0.7. Our model’s update decays when the erasing concept is estimated to be erased. Forget-Me-Not returns a static nudity score of 0.66% Figure 5: Iteration timeline for the same prompt and seed. The first image is the generation with the base checkpoint and each image is 10 iterations apart. While recommended iteration stop is 200, we append the results of iteration 450 at the last image to show spatial consistency even beyond our recommended iteration stop. if the semantic meaning of the images is still intact. How- ever, we find that these are not enough to show that these models do not shift away from their original position. The Structural Similarity Index metric (SSIM) is known to cap- ture these structural elements. For erasure success rate, we show how well the target concept “nudity” is erased through NudeNet (Praneeth, brett koonce, and Ayinmehr 2019)’s confidence score. Lastly, we provide an over-viewing assess- ment of each model’s training efficiency. Results Model Preservation and Spatial Consistency. Despite competitive erasing, ESD and SDD have shown degrada- tion in image generation, as shown in Fig. 2. In particular, for short prompts, this degradation is even amplified. We hypothesize that this occurs due to the direct matching of arbitrary concepts to the unconditional concept, causing dis- ruption in the semantic space. While this “textualizing” is- sue is exclusive to ESD and SDD, all models suffer from a shift in the spatial and semantic representation. The seman- tic representation can be captured by metrics with FID, KID, and CLIP score. However, the spatial consistency is not well captured with these metrics alone. To this end, we generate 1,000 random objects with the same seed for all fine-tuning methods and calculate the SSIM between the images generated by these methods and by the original checkpoint. Considering the image size, we use a window size of 25x25. As shown in Table 1, while the scores in FID, KID, and CLIP do not show strong variation across models, the SSIM scores show more sensitivity to the spatial structure changes. In addition to the SSIM score, we show the rate of erasure of different models over the iterations in Fig. 4. Here, while “Ablating” and Forget-me-not have shown better spatial consistency, their “nudity” erasing capabilities are quite limited. Finally, we present qualitative results on how our model erases for a given image in Fig. 5. Training Efficiency. A single assessment of the training efficiency of these models is non-trivial due to their het- erogeneous optimization schemes. Firstly, ESD and SDD take 1,000 or more iterations, which can be regarded as inefficient considering the absolute number of iterations. Ablation recommends 200 steps similar to our method, but their erasure is considerably weaker. Lastly, Forget-Me-Not has the fastest training, only requiring 35 steps. Yet, they deliver insufficient erasure of “nudity”. Concept Purification. A natural corollary of the derivation of our objective is that we can tune how much we want to allow the model to “shift” away from its original parameter placing by adjusting λ. An interesting consequence of setting λ = 0 is that the model gains the ability to erase concepts through image inversion. Formally, we noise a real image with “nudity” and denoise it with our trained model through DDIM inversion (Dhariwal and Nichol 2021). Both inverting using the null token or the concept-related token can erase the concept of the image. We report that our model is the only one that reasonably inherited this property with consistency, as shown in Fig. 1.a. Hyperparameter λ. We introduce hyperparameter λ, which controls how strongly we want to anchor its unconditional score behavior to its original checkpoint’s unconditional score. We train with different λs, where λ = 0 is the ablated version, as shown in Fig. 6. It is noticeable that there is an inverse proportionality between the model’s ability to erase and its spatial constraint. the stronger the constraint is, as in λ = 1.5, the loss of the lambda saturates over the erasing signal.Seeing from the lens of the Lagrangian Multiplier Method, we can view the objective as a function of λ but unlike the conventional Lagrangian Multiplier Method, we demonstrate that it is not the optimization of λ that is of interest, but rather the ability to control the learning policy through λ. In our work, this control is illustrated through the introduction of hyperparameter λ, which dictates how strongly we want to anchor its unconditional score behavior to its original checkpoint’s unconditional score. We train with different values of λ, where λ=0 is the ablated version, as shown in Fig. 6. Interestingly, there is an inverse propor- tionality between the model’s ability to erase and its spatial SD v1.4 λ = 0 λ = 0.01 λ = 0.5 λ = 1 λ = 1.5 Figure 6: Hyperparameter λ’s effect Van GoghSalvador DaliMonet Pretrained Fine-tuned Figure 7: Erasing different styles of painting constraint. When the constraint is too strong, as in λ = 1.5, the effect of the lambda overshadows over the erasing signal. Limitation and Future Work. While our model shows su- periority in many aspects, it also has its weaknesses. First, when erasing painting styles, the model either erases most painting styles uniformly or the constraint is too strong and the erasing is too conservative, as shown in Fig. 7. Also, ex- plicit guidance is mostly necessary although there is some minimal effect by subtracting the erasing term itself. In re- gards to its future work, we argue that this same erasure from the model is a promising type of model personalization that can pave an extension to the notion of controllability in gen- erative models. Conclusion In this work, we observe the weaknesses and issues in the current erasing algorithms and revisit the true objective and practical implication behind the task of erasing. The focus on the utility of these “erased” models motivated us to shape our algorithm so that only our concept of interest changes meaning and the rest remains constant. The derivation of our method grants us a hyperparameter to control the strength of the erasing. Owing to this implementation, we address many of the issues presented in current erasing algorithms. We hope our approach can be readily available and practi- cally usable to prevent such unsafe content generation. Ethical Statements and Social Impact Our model involves nudity and sexually explicit content, but as all models are publicly available, our institution’s IRB advised that approval was not required. All researchers in- volved are over 21 and have carefully reviewed relevant ethics guidelines (NeurIPS 2023; CSET 2021; Goldstein et al. 2023)and undergone training to handle and analyze research results properly. Although no practical defense against creating nudity in generative models exists, we em- phasize the urgency of developing preventive technologies given our work’s focus on explicit and unsafe content. Acknowledgments The authors would thank anonymous reviewers. Seunghoo Hong and Juhun Lee contributed equally. Simon S. Woo is the corresponding author. This work was partly supported by Institute for Information & communication Technology Planning & evaluation (IITP) grants funded by the Korean government MSIT: (No. 2022-0-01199, Graduate School of Convergence Security at Sungkyunkwan University), (No. 2022-0-01045, Self-directed Multi-Modal Intelligence for solving unknown, open domain problems), (No. 2022-0- 00688, AI Platform to Fully Adapt and Reflect Privacy- Policy Changes), (No. 2021-0-02068, Artificial Intelligence Innovation Hub), (No. 2019-0-00421, AI Graduate School Support Program at Sungkyunkwan University), and (No. RS-2023-00230337, Advanced and Proactive AI Platform Research and Development Against Malicious deepfakes). References Bi´nkowski, M.; Sutherland, D. J.; Arbel, M.; and Gret- ton, A. 2018. Demystifying mmd gans. arXiv preprint arXiv:1801.01401. Brack, M.; Friedrich, F.; Hintersdorf, D.; Struppek, L.; Schramowski, P.; and Kersting, K. 2023. Sega: Instruct- ing diffusion using semantic dimensions. arXiv preprint arXiv:2301.12247. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020. Language models are few-shot learners. Ad- vances in neural information processing systems, 33: 1877– 1901. CSET. 2021. Key Concepts in AI Safety: An Overview. https://cset.georgetown.edu/publication/key- concepts-in-ai-safety-an-overview/. Accessed: 2023-07-07. Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34: 8780–8794. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Du, Y.; Li, S.; and Mordatch, I. 2020. Compositional visual generation and inference with energy based models. arXiv preprint arXiv:2004.06030. Gandikota, R.; Materzy´nska, J.; Fiotto-Kaufman, J.; and Bau, D. 2023. Erasing Concepts from Diffusion Models. In Proceedings of the 2023 IEEE International Conference on Computer Vision. Goldstein, J. A.; Sastry, G.; Musser, M.; DiResta, R.; Gentzel, M.; and Sedova, K. 2023. Generative lan- guage models and automated influence operations: Emerg- ing threats and potential mitigations. arXiv preprint arXiv:2301.04246. Hertz, A.; Mokady, R.; Tenenbaum, J.; Aberman, K.; Pritch, Y.; and Cohen-Or, D. 2022. Prompt-to-prompt im- age editing with cross attention control. arXiv preprint arXiv:2208.01626. Hessel, J.; Holtzman, A.; Forbes, M.; Bras, R. L.; and Choi, Y. 2021. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718. Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. Gans trained by a two time-scale up- date rule converge to a local nash equilibrium. Advances in neural information processing systems, 30. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in neural information pro- cessing systems, 33: 6840–6851. Ho, J.; and Salimans, T. 2022. Classifier-free diffusion guid- ance. arXiv preprint arXiv:2207.12598. Kim, S.; Jung, S.; Kim, B.; Choi, M.; Shin, J.; and Lee, J. 2023. Towards Safe Self-Distillation of Internet- Scale Text-to-Image Diffusion Models. arXiv preprint arXiv:2307.05977. Kumari, N.; Zhang, B.; Wang, S.-Y.; Shechtman, E.; Zhang, R.; and Zhu, J.-Y. 2023. Ablating concepts in text-to-image diffusion models. arXiv preprint arXiv:2303.13516. Kwon, M.; Jeong, J.; and Uh, Y. 2022. Diffusion mod- els already have a semantic latent space. arXiv preprint arXiv:2210.10960. Lee, J.; Cho, K.; and Kiela, D. 2019. Countering language drift via visual grounding. arXiv preprint arXiv:1909.04499. Lu, Y.; Singhal, S.; Strub, F.; Courville, A.; and Pietquin, O. 2020. Countering language drift with seeded iterated learning. In International Conference on Machine Learn- ing, 6437–6447. PMLR. Mehrabi, N.; Morstatter, F.; Saxena, N.; Lerman, K.; and Galstyan, A. 2021. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6): 1–35. NeurIPS. 2023. NeurIPS Code of Ethics. https://nips.cc/ public/EthicsGuidelines. Accessed: 2023-07-07. Oord, A. v. d.; Vinyals, O.; and Kavukcuoglu, K. 2017. Neural discrete representation learning. arXiv preprint arXiv:1711.00937. Praneeth, B.; brett koonce; and Ayinmehr, A. 2019. beda- pudi6788/NudeNet: place for checkpoint files. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from nat- ural language supervision. In International conference on machine learning, 8748–8763. PMLR. Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Rad- ford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to- image generation. In International Conference on Machine Learning, 8821–8831. PMLR. Rando, J.; Paleka, D.; Lindner, D.; Heim, L.; and Tram`er, F. 2022. Red-teaming the stable diffusion safety filter. arXiv preprint arXiv:2210.04610. Razavi, A.; Van den Oord, A.; and Vinyals, O. 2019. Gener- ating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om- mer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer- ence on computer vision and pattern recognition, 10684– 10695. Ruiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein, M.; and Aberman, K. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 22500–22510. Saharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton, E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, B.; Salimans, T.; et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. Ad- vances in Neural Information Processing Systems, 35: 36479–36494. Santurkar, S.; Ilyas, A.; Tsipras, D.; Engstrom, L.; Tran, B.; and Madry, A. 2019. Image synthesis with a single (robust) classifier. Advances in Neural Information Processing Sys- tems, 32. Schuhmann, C.; Beaumont, R.; Vencu, R.; Gordon, C.; Wightman, R.; Cherti, M.; Coombes, T.; Katta, A.; Mullis, C.; Wortsman, M.; et al. 2022. Laion-5b: An open large- scale dataset for training next generation image-text mod- els. Advances in Neural Information Processing Systems, 35: 25278–25294. Sohl-Dickstein, J.; Weiss, E.; Maheswaranathan, N.; and Ganguli, S. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International confer- ence on machine learning, 2256–2265. PMLR. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Song, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Er- mon, S.; and Poole, B. 2020. Score-based generative model- ing through stochastic differential equations. arXiv preprint arXiv:2011.13456. von Platen, P.; Patil, S.; Lozhkov, A.; Cuenca, P.; Lambert, N.; Rasul, K.; Davaadorj, M.; and Wolf, T. 2022. Dif- fusers: State-of-the-art diffusion models. https://github.com/ huggingface/diffusers. Zhang, E.; Wang, K.; Xu, X.; Wang, Z.; and Shi, H. 2023. Forget-me-not: Learning to forget in text-to-image diffusion models. arXiv preprint arXiv:2303.17591. Zhang, L.; Song, J.; Gao, A.; Chen, J.; Bao, C.; and Ma, K. 2019. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, 3713–3722. Supplementary Materials Implicit guidance with Prompt-to-Prompt. Our method states that either explicit or implicit guidance δ works. We show its compatibility with implicit guidance with Prompt- to-Prompt (Hertz et al. 2022). Prompt-to-prompt allows re- weighting the attention maps while constraining their spatial distribution. If the attention maps corresponding to our tar- get concept are suppressed, the learned prior of the model will introduce another concept to fill its place. Then, we use this modified ϵ to synthesize our guidance δ. Although training with Prompt-to-Prompt presents its own instabili- ties (e.g. re-weighting hyperparameter, layer of application, number of timesteps to apply), we show in Fig 8. that it pro- duces good erased samples. Pre-trained Fine-tuned Figure 8: Images before and after fine-tuning using Prompt- to-Prompt as our source for δ sampling. Parameter Selection. The current consensus is that text embeddings are contextualized in the cross-attention lay- ers (Gandikota et al. 2023; Kumari et al. 2023; Kim et al. 2023; Zhang et al. 2023). Accordingly, previous approaches have chosen to update these layers as a standard practice. However, erasing fine-tuning relies on training with a very limited sample size, and the model is prone to overfitting. Furthermore, while the attention layer is of our interest, we wish to preserve the spatial priors learned by the key, query weights as much as possible. To this end, we completely bypass the weights responsible for synthesizing the atten- tion maps and consider updating the final linear layer of the attention layer (to out) (von Platen et al. 2022) of both cross and self-attention. This choice of parameter granted us more spatial consistency across all images. Additionally, the to out layer processes the feature embedding yielded from every attention head. One can argue that the update on these shared weights has an implicit regulatory effect. Lambda Sensitivity. In this section, we emphasize the role of λ in our optimization. We state that λ constrains the model from distribution shifts due to the erasing fine-tuning. Then, we can expect a higher spatial consistency after the opti- mization. In Fig. 9, we calculate the SSIM for 1,000 random objects compared to the object images generated by the base model SD v1.4. We show the sensitivity for models with cross-attention, and to out layers updated. As soon as λ > 1, its spatial regularization effect is evident, fading away in a logarithmic fashion. Concept Purification. A corollary from optimizing with our method is that our DDIM inversion (Dhariwal and Nichol Figure 9: SSIM sensitivity for different λ values. Optimiz- ing the to out layer regularizes the spatial priors to a greater degree. 2021) conditioned on the erasing concept “purifies” and erases the concept. To our surprise, when tested with the null or the erasing concept condition, none of the other baselines generated comparable results, shown in Fig. 10 . This fur- ther shows how the underlying mechanism of our method is different from others. It is noticeable how the spatial con- sistency is near flawless. This “purification” capability may have applications to censorship preprocessing, such as eras- ing “car plates” to censor private information in the dataset. Additional Results and Details of Objective Derivation Qualitative Results. In this section, we provide qualitative results from every method. In Figs. 11,12,13, we show eras- ing of “nudity” from different baseline methods along with ours. Because the training ends at different rates, we nor- malized and expressed the progress in percentage. For the sake of demonstrating optimization convergence, we show iteration 400 in the last column of our model instead of our recommended iteration 200. Lastly, we set iteration 1,400 as the last step for SDD, as the model completely breaks at iteration 2,000. Each shows a different paradigm of update. ESD shows fast but unstable erasing early on. Just as their objective function suggests, it is visible that all images with the con- cept “nudity” are mapped to near-unconditional images. SDD shows a more stable update. However, starting from iteration 1,000, the images collapse to the “army” concept. While they introduced self-distillation to minimize unde- sirable oscillation in the supervision, this comes with a rather adverse outcome at the end of the training. While Forget-Me-Not shows great performance in erasing specific or memorized concepts, it fails to work with more general concepts such as “nudity”. “Ablating” shows good spatial consistency with consistency. However, it often fails to erase nudity. Lastly, our model shows that it erases and does not overfit or optimize more than necessary. When the training model learns the distribution of δ, it converges and no more visual changes are attributed. Lastly, we present how con- cepts close to the erasing concept change over the iterations in Fig. 14 Reference images ESD Forget-Me-Not SDD Ablation Ours “Nude women” “Nude woman” “Nude Statue” “Nude man” Figure 10: DDIM inversion with respective text prompts. For other models, it does not purify even with the null text condition. Objective Formulation. Consider unconditional reverse noising process Pθ(zt|zt+1) = N (µ, Σ), conditional re- verse noising process is Pθ(zt|zt+1, c). (Dhariwal and Nichol 2021) show that Pθ(zt|zt+1, c) ∼ N (µ + γΣg, Σ) where g = ∇zt log(Pϕ(c|zt))|zt=µ, γ is guidance scale. Then, given some timestep t and conditions c, c ′, the distribution minimization between Pθ⋆ (zt−1|zt, c ′) and Pθ(zt−1|zt, c)) can be expressed as follows: Ezt∼Pθ⋆ (zt|c′)[DKL(Pθ⋆ (zt−1|zt, c ′)∥Pθ(zt−1|zt, c))] (14) Here, the KL divergence can be formulated as: DKL(Pθ⋆ (zt−1|zt, c ′)∥Pθ(zt−1|zt, c)) (15) = DKL(N (µθ⋆ + γ1Σg′ ︸ ︷︷ ︸ ˜µθ⋆ , Σ)||N (µθ + γ2Σg ︸ ︷︷ ︸ ˜µθ , Σ)) (16) = w′(t)||( 1 √αt zt + 1 − αt √αt sθ⋆ (zt, t, c′)) − ( 1 √αt zt + 1 − αt √αt sθ(zt, t, c))|| 2 2 (17) = w(t)||sθ⋆(zt, t, c′)) − sθ(zt, t, c))|| 2 2 (18) = w(t)||(∇zt log(Pθ⋆ (zt)) + γ1∇zt log(Pθ⋆ (c′|zt))) − (∇zt log(Pθ(zt)) + γ2∇zt log(Pθ(c|zt)))|| (19) = w(t)|| (∇zt log(Pθ⋆ (zt)) − ∇zt log(Pθ(zt))) ︸ ︷︷ ︸ LU : unconditional loss term (20) − (γ1∇zt log(Pθ⋆ (c′|zt)) − γ2∇zt log(Pθ(c|zt))) ︸ ︷︷ ︸ LC : conditional loss term || 2 2 (21) = w(t)||LU + LC|| 2 2, (22) where γ1, γ2 are guidance scales, and µ· is the estimated de- noising transition mean, ˜µθ is conditional guided denoising transition mean: ˜µθ(zt, t, c) = 1 √αt zt + 1 − αt √αt sθ(zt, t, c), Here, αt is scheduled noise variance, ¯αt = ∏t i=1 αi, and w(t), w′(t) are timestep-dependent loss weights as below: w(t) = 2(1 − ¯αt)(1 − αt) 2 (1 − αt)(1 − ¯αt−1)(αt) w′(t) = 2(1 − ¯αt) (1 − αt)(1 − ¯αt−1) Through our derivation, our initial KL divergence summa- rizes into: Ezt∼Pθ⋆ (zt|c′)[DKL(Pθ⋆ (zt−1|zt, c ′)∥Pθ(zt−1|zt, c))] = Ezt∼Pθ⋆ (zt|c′)[w(t)||LU + LC|| 2 2] Then, by triangle inequality, ||LU + LC||2 gets ||LU ||2 + ||LC||2 as upper bound. Therefore, minimizing our loss ||LU ||2 + ||LC||2 is equivalent to minimizing the upper bound of ||LU + LC||2 0% 100%50% Reference Image SDD Ablating ESD Ours Forget-Me-Not Figure 11: Iteration timeline for images of the same seed prompted with ”naked boy”. Because the training ends at different rates, we normalized and expressed the progress in percentage. For the sake of demonstrating optimization convergence, we show iteration 400 in the last column of our model instead of our recommended iteration 200. Lastly, we set iteration 1,400 as the last step for SDD, as the model completely breaks at iteration 2,000. 0% 100%50% SDD Ablating ESD Ours Forget-Me-NotReference Image Figure 12: Iteration timeline for images of the same seed prompted with “Hentai”. 0% 100%50% SDD Ablating ESD Ours Forget-Me-NotReference Image Figure 13: Iteration timeline for images of the same seed prompted with “sexual girl”. 0% 100%50% SDD Ablating ESD Ours Forget-Me-Not Reference Image Figure 14: Iteration timeline for images of the same seed prompted with “photo of a woman”. Here, we expect related concepts such as this to not change. Again, for our last column, we present the image for iteration 400 Experiment Setting. To sample δ, the following are the hy- perparameters for erasing ”nudity”: guidance scale: γ = 7.5, lambda : λ = 1, max sampling step: T = 35, and training parameter set is either ”cross attention” or ”to out” layer. For c′, we choose the following concepts: {ctext = sexual, gc = −7.5, tchigh = ⌊0.35T ⌋, tclow = ⌊T ⌋, κ = 0.95} {ctext = bikini, gc = 6.5, tchigh = ⌊0.35T ⌋, tclow = ⌊T ⌋, κ = 0.95} {ctext = pants, gc = 6.5, tchigh = ⌊0.35T ⌋, tclow = ⌊T ⌋, κ = 0.95} (23) For Concept Purification (”nudity”), guidance scale: γ = 7.5, lambda : λ = 0, max sampling step: T = 35, train- ing parameter set: “cross attention”. For c′, we choose the following concepts: {ctext = sexual, gc = −7.5, tchigh = ⌊0.35T ⌋, tclow = ⌊T ⌋, κ = 0.95} {ctext = bikini, gc = 6.5, tchigh = ⌊0.35T ⌋, tclow = ⌊T ⌋, κ = 0.95} {ctext = pants, gc = 6.5, tchigh = ⌊0.35T ⌋, tclow = ⌊T ⌋, κ = 0.95} (24)","libVersion":"0.3.2","langs":""}